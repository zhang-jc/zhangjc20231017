{"meta":{"version":1,"warehouse":"4.0.0"},"models":{"Asset":[{"_id":"node_modules/hexo-theme-next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/css/noscript.styl","path":"css/noscript.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/logo-algolia-nebula-blue-full.svg","path":"images/logo-algolia-nebula-blue-full.svg","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/images/logo.svg","path":"images/logo.svg","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/bookmark.js","path":"js/bookmark.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/comments-buttons.js","path":"js/comments-buttons.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/comments.js","path":"js/comments.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/config.js","path":"js/config.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/motion.js","path":"js/motion.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/next-boot.js","path":"js/next-boot.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/pjax.js","path":"js/pjax.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/schedule.js","path":"js/schedule.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/utils.js","path":"js/utils.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/schemes/muse.js","path":"js/schemes/muse.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/nprogress.js","path":"js/third-party/nprogress.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/quicklink.js","path":"js/third-party/quicklink.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/rating.js","path":"js/third-party/rating.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/baidu-analytics.js","path":"js/third-party/analytics/baidu-analytics.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/google-analytics.js","path":"js/third-party/analytics/google-analytics.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/growingio.js","path":"js/third-party/analytics/growingio.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/chatra.js","path":"js/third-party/chat/chatra.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/gitter.js","path":"js/third-party/chat/gitter.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/tidio.js","path":"js/third-party/chat/tidio.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/changyan.js","path":"js/third-party/comments/changyan.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqusjs.js","path":"js/third-party/comments/disqusjs.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqus.js","path":"js/third-party/comments/disqus.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/gitalk.js","path":"js/third-party/comments/gitalk.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/isso.js","path":"js/third-party/comments/isso.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/livere.js","path":"js/third-party/comments/livere.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/utterances.js","path":"js/third-party/comments/utterances.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/katex.js","path":"js/third-party/math/katex.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/algolia-search.js","path":"js/third-party/search/algolia-search.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/local-search.js","path":"js/third-party/search/local-search.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/mathjax.js","path":"js/third-party/math/mathjax.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/firestore.js","path":"js/third-party/statistics/firestore.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/lean-analytics.js","path":"js/third-party/statistics/lean-analytics.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/mermaid.js","path":"js/third-party/tags/mermaid.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/pdf.js","path":"js/third-party/tags/pdf.js","modified":0,"renderable":1},{"_id":"source/images/android-chrome-192x192.png","path":"images/android-chrome-192x192.png","modified":0,"renderable":0},{"_id":"source/images/android-chrome-512x512.png","path":"images/android-chrome-512x512.png","modified":0,"renderable":0},{"_id":"source/images/apple-touch-icon.png","path":"images/apple-touch-icon.png","modified":0,"renderable":0},{"_id":"source/images/avatar.jpg","path":"images/avatar.jpg","modified":0,"renderable":0},{"_id":"source/images/browserconfig.xml","path":"images/browserconfig.xml","modified":0,"renderable":0},{"_id":"source/images/favicon-16x16.png","path":"images/favicon-16x16.png","modified":0,"renderable":0},{"_id":"source/images/favicon-32x32.png","path":"images/favicon-32x32.png","modified":0,"renderable":0},{"_id":"source/images/favicon.ico","path":"images/favicon.ico","modified":0,"renderable":0},{"_id":"source/images/mstile-150x150.png","path":"images/mstile-150x150.png","modified":0,"renderable":0},{"_id":"source/images/safari-pinned-tab.svg","path":"images/safari-pinned-tab.svg","modified":0,"renderable":0},{"_id":"source/images/site.webmanifest","path":"images/site.webmanifest","modified":0,"renderable":0},{"_id":"source/images/wechat.jpg","path":"images/wechat.jpg","modified":0,"renderable":0},{"_id":"source/uploads/aggregation-pipeline.png","path":"uploads/aggregation-pipeline.png","modified":0,"renderable":0},{"_id":"source/uploads/java-se-site-map.png","path":"uploads/java-se-site-map.png","modified":0,"renderable":0},{"_id":"source/uploads/map-reduce.png","path":"uploads/map-reduce.png","modified":0,"renderable":0},{"_id":"source/uploads/20151122/vbox1.png","path":"uploads/20151122/vbox1.png","modified":0,"renderable":0},{"_id":"source/uploads/20151122/vbox4.png","path":"uploads/20151122/vbox4.png","modified":0,"renderable":0},{"_id":"source/uploads/20151122/vbox2.png","path":"uploads/20151122/vbox2.png","modified":0,"renderable":0},{"_id":"source/uploads/20151122/vbox3.png","path":"uploads/20151122/vbox3.png","modified":0,"renderable":0},{"_id":"source/uploads/20151122/vbox5.png","path":"uploads/20151122/vbox5.png","modified":0,"renderable":0},{"_id":"source/uploads/20151125/nodejs1.png","path":"uploads/20151125/nodejs1.png","modified":0,"renderable":0},{"_id":"source/uploads/20160416/mongodb_start_log.png","path":"uploads/20160416/mongodb_start_log.png","modified":0,"renderable":0},{"_id":"source/uploads/20160417/mongodb-script.png","path":"uploads/20160417/mongodb-script.png","modified":0,"renderable":0},{"_id":"source/uploads/20160417/mongodb-test-result.png","path":"uploads/20160417/mongodb-test-result.png","modified":0,"renderable":0},{"_id":"source/uploads/20160417/mongodb-source-config.png","path":"uploads/20160417/mongodb-source-config.png","modified":0,"renderable":0},{"_id":"source/uploads/20160417/mongodb-thread-group.png","path":"uploads/20160417/mongodb-thread-group.png","modified":0,"renderable":0},{"_id":"source/uploads/20160518/mysql-alter-partition.png","path":"uploads/20160518/mysql-alter-partition.png","modified":0,"renderable":0},{"_id":"source/uploads/20160518/mysql-information-schema-partitions.png","path":"uploads/20160518/mysql-information-schema-partitions.png","modified":0,"renderable":0},{"_id":"source/uploads/20160522/virtualbox-install-dmg.png","path":"uploads/20160522/virtualbox-install-dmg.png","modified":0,"renderable":0},{"_id":"source/uploads/20160521/personal-blog-site.png","path":"uploads/20160521/personal-blog-site.png","modified":0,"renderable":0},{"_id":"source/uploads/20160522/virtualbox-run-apps.png","path":"uploads/20160522/virtualbox-run-apps.png","modified":0,"renderable":0},{"_id":"source/uploads/20160522/virtualbox-run-spotlight.png","path":"uploads/20160522/virtualbox-run-spotlight.png","modified":0,"renderable":0},{"_id":"source/uploads/20160525/ssh-PermitRootLogin.png","path":"uploads/20160525/ssh-PermitRootLogin.png","modified":0,"renderable":0},{"_id":"source/uploads/20160511/mongoDB-web-console.png","path":"uploads/20160511/mongoDB-web-console.png","modified":0,"renderable":0},{"_id":"source/uploads/20160602/Socket.png","path":"uploads/20160602/Socket.png","modified":0,"renderable":0},{"_id":"source/uploads/20160603/2q.png","path":"uploads/20160603/2q.png","modified":0,"renderable":0},{"_id":"source/uploads/20160603/lru-k.png","path":"uploads/20160603/lru-k.png","modified":0,"renderable":0},{"_id":"source/uploads/20160603/lru.png","path":"uploads/20160603/lru.png","modified":0,"renderable":0},{"_id":"source/uploads/20160603/mq.png","path":"uploads/20160603/mq.png","modified":0,"renderable":0},{"_id":"source/uploads/20160626/ssh.png","path":"uploads/20160626/ssh.png","modified":0,"renderable":0},{"_id":"source/uploads/20160604/console2.png","path":"uploads/20160604/console2.png","modified":0,"renderable":0},{"_id":"source/uploads/20160604/console.png","path":"uploads/20160604/console.png","modified":0,"renderable":0},{"_id":"source/uploads/20160604/index.png","path":"uploads/20160604/index.png","modified":0,"renderable":0},{"_id":"source/uploads/20160604/index_page.png","path":"uploads/20160604/index_page.png","modified":0,"renderable":0},{"_id":"source/uploads/20160604/index_page_2.png","path":"uploads/20160604/index_page_2.png","modified":0,"renderable":0},{"_id":"source/uploads/20160604/video.mp4","path":"uploads/20160604/video.mp4","modified":0,"renderable":0},{"_id":"source/uploads/20160604/video2.mp4","path":"uploads/20160604/video2.mp4","modified":0,"renderable":0},{"_id":"source/uploads/20160910/argument.png","path":"uploads/20160910/argument.png","modified":0,"renderable":0},{"_id":"source/uploads/20160910/debug-configuration.png","path":"uploads/20160910/debug-configuration.png","modified":0,"renderable":0},{"_id":"source/uploads/20160910/dfs-locations.png","path":"uploads/20160910/dfs-locations.png","modified":0,"renderable":0},{"_id":"source/uploads/20160910/debug-view.png","path":"uploads/20160910/debug-view.png","modified":0,"renderable":0},{"_id":"source/uploads/20160910/hadoop-installation-directory.png","path":"uploads/20160910/hadoop-installation-directory.png","modified":0,"renderable":0},{"_id":"source/uploads/20160910/map-reduce-locations-conf.png","path":"uploads/20160910/map-reduce-locations-conf.png","modified":0,"renderable":0},{"_id":"source/uploads/20160910/input-path-not-exists.png","path":"uploads/20160910/input-path-not-exists.png","modified":0,"renderable":0},{"_id":"source/uploads/20160910/map-reduce-locations-error.png","path":"uploads/20160910/map-reduce-locations-error.png","modified":0,"renderable":0},{"_id":"source/uploads/20160910/map-reduce-locations-view.png","path":"uploads/20160910/map-reduce-locations-view.png","modified":0,"renderable":0},{"_id":"source/uploads/20160910/word-count.png","path":"uploads/20160910/word-count.png","modified":0,"renderable":0},{"_id":"source/uploads/20160910/word-count-result.png","path":"uploads/20160910/word-count-result.png","modified":0,"renderable":0},{"_id":"source/uploads/20160911/namenode-breakpoints.png","path":"uploads/20160911/namenode-breakpoints.png","modified":0,"renderable":0},{"_id":"source/uploads/20160911/remote-java-application-debug-conf.png","path":"uploads/20160911/remote-java-application-debug-conf.png","modified":0,"renderable":0},{"_id":"source/uploads/20161016/socketio.png","path":"uploads/20161016/socketio.png","modified":0,"renderable":0},{"_id":"source/uploads/20161127/generics-payloadListHierarchy.gif","path":"uploads/20161127/generics-payloadListHierarchy.gif","modified":0,"renderable":0},{"_id":"source/uploads/20161127/generics-sampleHierarchy.gif","path":"uploads/20161127/generics-sampleHierarchy.gif","modified":0,"renderable":0},{"_id":"source/uploads/20161127/generics-subtypeRelationship.gif","path":"uploads/20161127/generics-subtypeRelationship.gif","modified":0,"renderable":0},{"_id":"source/uploads/20161203/generics-listParent.gif","path":"uploads/20161203/generics-listParent.gif","modified":0,"renderable":0},{"_id":"source/uploads/20161203/generics-wildcardSubtyping.gif","path":"uploads/20161203/generics-wildcardSubtyping.gif","modified":0,"renderable":0},{"_id":"source/uploads/20170108/secondarynamenode.png","path":"uploads/20170108/secondarynamenode.png","modified":0,"renderable":0},{"_id":"source/uploads/20170224/no-resourcemanager-to-stop.png","path":"uploads/20170224/no-resourcemanager-to-stop.png","modified":0,"renderable":0},{"_id":"source/uploads/20170303/mapreduce-jobhistory-server.png","path":"uploads/20170303/mapreduce-jobhistory-server.png","modified":0,"renderable":0},{"_id":"source/uploads/20170303/namenode.png","path":"uploads/20170303/namenode.png","modified":0,"renderable":0},{"_id":"source/uploads/20170303/resourcemanager.png","path":"uploads/20170303/resourcemanager.png","modified":0,"renderable":0},{"_id":"source/uploads/20170327/hdfs-ha-qjm.png","path":"uploads/20170327/hdfs-ha-qjm.png","modified":0,"renderable":0},{"_id":"source/uploads/20170327/namenode1.png","path":"uploads/20170327/namenode1.png","modified":0,"renderable":0},{"_id":"source/uploads/20170327/namenode2.png","path":"uploads/20170327/namenode2.png","modified":0,"renderable":0},{"_id":"source/uploads/20170704/hdfs-data.png","path":"uploads/20170704/hdfs-data.png","modified":0,"renderable":0},{"_id":"source/uploads/20170704/mysql-data.png","path":"uploads/20170704/mysql-data.png","modified":0,"renderable":0},{"_id":"source/uploads/20171019/cd_dvd.png","path":"uploads/20171019/cd_dvd.png","modified":0,"renderable":0},{"_id":"source/uploads/20171019/java_update_tip.png","path":"uploads/20171019/java_update_tip.png","modified":0,"renderable":0},{"_id":"source/uploads/20171019/launch_ask.png","path":"uploads/20171019/launch_ask.png","modified":0,"renderable":0},{"_id":"source/uploads/20171019/security_warning.png","path":"uploads/20171019/security_warning.png","modified":0,"renderable":0},{"_id":"source/uploads/20171019/security_warning2.png","path":"uploads/20171019/security_warning2.png","modified":0,"renderable":0},{"_id":"source/uploads/20171019/virtual_console.png","path":"uploads/20171019/virtual_console.png","modified":0,"renderable":0},{"_id":"source/uploads/20171019/virtual_console_pic.png","path":"uploads/20171019/virtual_console_pic.png","modified":0,"renderable":0},{"_id":"source/uploads/20171019/virtual_medium.png","path":"uploads/20171019/virtual_medium.png","modified":0,"renderable":0},{"_id":"source/uploads/20171125/eclipse-sqoop-import.png","path":"uploads/20171125/eclipse-sqoop-import.png","modified":0,"renderable":0},{"_id":"source/uploads/20180928/azkaban-executor-activate.png","path":"uploads/20180928/azkaban-executor-activate.png","modified":0,"renderable":0},{"_id":"source/uploads/20181214/namenodeInfo.png","path":"uploads/20181214/namenodeInfo.png","modified":0,"renderable":0},{"_id":"source/uploads/20181214/namenodeLogs.png","path":"uploads/20181214/namenodeLogs.png","modified":0,"renderable":0},{"_id":"source/uploads/20181214/sqoop.txt","path":"uploads/20181214/sqoop.txt","modified":0,"renderable":0},{"_id":"source/uploads/20190301/jenkinsUpdateSite.png","path":"uploads/20190301/jenkinsUpdateSite.png","modified":0,"renderable":0},{"_id":"source/uploads/20190314/GitLabWebHook.png","path":"uploads/20190314/GitLabWebHook.png","modified":0,"renderable":0},{"_id":"source/uploads/20190314/JenkinsProject1.png","path":"uploads/20190314/JenkinsProject1.png","modified":0,"renderable":0},{"_id":"source/uploads/20190314/数据仓库开发测试代码分发流程.png","path":"uploads/20190314/数据仓库开发测试代码分发流程.png","modified":0,"renderable":0},{"_id":"source/uploads/20190314/JenkinsProject2.png","path":"uploads/20190314/JenkinsProject2.png","modified":0,"renderable":0},{"_id":"source/uploads/20190315/Rsyslog依赖关系.png","path":"uploads/20190315/Rsyslog依赖关系.png","modified":0,"renderable":0},{"_id":"source/uploads/20190320/jenkins-LD_LIBRARY_PATH.png","path":"uploads/20190320/jenkins-LD_LIBRARY_PATH.png","modified":0,"renderable":0},{"_id":"source/uploads/20190320/jenkins-path.png","path":"uploads/20190320/jenkins-path.png","modified":0,"renderable":0},{"_id":"source/uploads/20190406/omhttp.so","path":"uploads/20190406/omhttp.so","modified":0,"renderable":0},{"_id":"source/uploads/20190507/continue-connecting.png","path":"uploads/20190507/continue-connecting.png","modified":0,"renderable":0},{"_id":"source/uploads/20190507/lftp-sftp.png","path":"uploads/20190507/lftp-sftp.png","modified":0,"renderable":0},{"_id":"source/uploads/20191017/vs-code-terminal-font.png","path":"uploads/20191017/vs-code-terminal-font.png","modified":0,"renderable":0},{"_id":"source/uploads/20191017/vs-code-terminal.png","path":"uploads/20191017/vs-code-terminal.png","modified":0,"renderable":0},{"_id":"source/uploads/20210419/ctaException.png","path":"uploads/20210419/ctaException.png","modified":0,"renderable":0},{"_id":"source/uploads/20210520/KafkaSource.java","path":"uploads/20210520/KafkaSource.java","modified":0,"renderable":0},{"_id":"source/uploads/20191023/rtl8821ce.zip","path":"uploads/20191023/rtl8821ce.zip","modified":0,"renderable":0},{"_id":"source/uploads/20181214/application_1533196506314_4460157/192-168-72-12_27463.txt","path":"uploads/20181214/application_1533196506314_4460157/192-168-72-12_27463.txt","modified":0,"renderable":0},{"_id":"source/uploads/20181214/application_1533196506314_4460157/192-168-72-24_16310.txt","path":"uploads/20181214/application_1533196506314_4460157/192-168-72-24_16310.txt","modified":0,"renderable":0},{"_id":"source/uploads/20181214/application_1533196506314_4460157/192-168-72-73_2363.txt","path":"uploads/20181214/application_1533196506314_4460157/192-168-72-73_2363.txt","modified":0,"renderable":0},{"_id":"source/uploads/20181214/application_1533196506314_4460157/192-168-72-23_18284.txt","path":"uploads/20181214/application_1533196506314_4460157/192-168-72-23_18284.txt","modified":0,"renderable":0},{"_id":"source/uploads/20181214/application_1533196506314_4460157/192-168-72-84_13498.txt","path":"uploads/20181214/application_1533196506314_4460157/192-168-72-84_13498.txt","modified":0,"renderable":0},{"_id":"source/uploads/20181214/application_1533196506314_4460157/192-168-72-88_24481.txt","path":"uploads/20181214/application_1533196506314_4460157/192-168-72-88_24481.txt","modified":0,"renderable":0},{"_id":"source/uploads/20181214/application_1533196506314_4460157/192-168-72-93_53778.txt","path":"uploads/20181214/application_1533196506314_4460157/192-168-72-93_53778.txt","modified":0,"renderable":0},{"_id":"source/uploads/20181214/application_1533196506314_4460157/192-168-72-94_54353.txt","path":"uploads/20181214/application_1533196506314_4460157/192-168-72-94_54353.txt","modified":0,"renderable":0}],"Cache":[{"_id":"source/_posts/AttributeError-module-requests-has-no-attribute-post.md","hash":"5c3c1393fe2003c852fb02eeeaf9aab66703a74f","modified":1626712080224},{"_id":"source/_posts/CentOS-6-5-编译-Rsyslog-8-1903-0.md","hash":"a5b965fc46e53dd2e546d2cc5be16cb267e4a3b4","modified":1626712080228},{"_id":"source/_posts/Azkaban-3-52-0-搭建遇到的问题.md","hash":"a5e5d3bff263ea3f0c82535c8987251ad723d7cf","modified":1626712080228},{"_id":"source/_posts/CentOS-6-8-Python3-遇到的坑.md","hash":"50e1793036be072c735188274f3c2f70b37a4fdc","modified":1626712080228},{"_id":"source/_posts/CentOS-6-8-安装-Nginx.md","hash":"0a6482658c5ceba7c5983e2463f51b2be56ca721","modified":1626712080228},{"_id":"source/_posts/CentOS-6-8-安装-Python3-Could-not-build-the-ssl-module.md","hash":"5a4b547a3f348b7ac9078319b1382c7f9e816aec","modified":1626712080228},{"_id":"source/_posts/CentOS-7-3-给一批服务器创建同一用户的简便方法.md","hash":"a2c23286f712f327f46e3ea77dba317c305eac17","modified":1626712080228},{"_id":"source/_posts/CentOS-6-5-编译-omhttp-so-下载.md","hash":"00fed30afefde99e98c457b1856577c9b1caa719","modified":1626712080228},{"_id":"source/_posts/CentOS-7-3-编译-Rsyslog-8-1903-0.md","hash":"6f63e7dbd19346e228c69cda4a4ef31ced663845","modified":1626712080232},{"_id":"source/_posts/CentOS-7-3-Python-3-7-2-编译-Hue-问题解决.md","hash":"13b7ec33e6b6949c154ee57bea513efbf149ac29","modified":1626712080360},{"_id":"source/_posts/CentOS-7-查看磁盘文件系统格式.md","hash":"7055ec1a49454c71dd0153b39d000f07d6998acc","modified":1626712080232},{"_id":"source/_posts/CentOS-删除-libc-so-6-后命令不可用故障恢复方法.md","hash":"243d810f4b85924ba80442992b1f04ef0e1aeaa3","modified":1626712080232},{"_id":"source/_posts/CentOS-curl-没有到主机的路由.md","hash":"1123d61951ba740219466dfb53aa339c7b3b8bab","modified":1626712080232},{"_id":"source/_posts/CentOS7-7-安装-MySQL-5-7.md","hash":"76101cbb857b07fec29f4386e5a90dc59c03d5f8","modified":1626712079992},{"_id":"source/_posts/CentOS-挂载掉了的硬盘.md","hash":"b73274bd551cb4725e28ab3428a17a8da7fd5479","modified":1626712080232},{"_id":"source/_posts/CentOS7-3-安装-Python3.md","hash":"f9868928f6d5b4d28a3ddbca90d5aae66ede156d","modified":1626712080244},{"_id":"source/_posts/DELL-R730xd-通过管理卡远程安装-Centos7-3.md","hash":"969eeaf4b8cd2113fafc5c6a6c0b2f19c436d6b7","modified":1626712080244},{"_id":"source/_posts/CentOS-重装-MongoDB.md","hash":"c753fdc0500cda0533d6a87503a1475c8dd8ef7f","modified":1626712080232},{"_id":"source/_posts/Eclipse-导入-Sqoop.md","hash":"34d19dbd1978cd22e8776c806c9c091fd4b064e2","modified":1626712080244},{"_id":"source/_posts/Emacs-Shell-上下键切换历史命令.md","hash":"a8e23866d8232254770777f15f45f3d788d4d8e8","modified":1626712080244},{"_id":"source/_posts/Emacs-org-文档-Table-行和列操作.md","hash":"fa381e2edd7d2b6c3a1325163a348751891fcdff","modified":1626712080244},{"_id":"source/_posts/Emacs-org-设置自动换行.md","hash":"f1a13a3be563e4003108211cc2cca5c4de521d39","modified":1626712080328},{"_id":"source/_posts/Centos-6-8-No-package-libffi-devel-available-问题解决.md","hash":"34c54d790b8db38b119217f93f00bdaecc58fdbf","modified":1626712080232},{"_id":"source/_posts/Emacs-恢复自动保存文件.md","hash":"c58abf585261a6da2ef320988c87080e14919537","modified":1626712080244},{"_id":"source/_posts/GitLab-Jenkins-Nginx-Lua-实现代码自动分发.md","hash":"101aec703099e6e430787ac1f213608fddfa2803","modified":1626712080244},{"_id":"source/_posts/Emacs-禁用自动备份及修改备份目录.md","hash":"723a0e4350aea82accd087c66ccd91e1365061e5","modified":1626712080244},{"_id":"source/_posts/HDFS-NameNode-HA-sshfence-端口.md","hash":"42a508aebd13981104705251557f61d781a997dc","modified":1626712080248},{"_id":"source/_posts/HDFS-Rebalance.md","hash":"f7cc5b09ae588b1843f9fc4e0c7d895f3ee530ac","modified":1626712080248},{"_id":"source/_posts/HDFS-Trash-特性.md","hash":"1674e813ff48d988bf7daee362892212d7150179","modified":1626712080248},{"_id":"source/_posts/HDFS-getmerge-Operation-not-permitted.md","hash":"38386b7b192b61ad05b1d3712374fc8269ffadc6","modified":1626712080248},{"_id":"source/_posts/HDFS-快照.md","hash":"c9c02e74d4fe2b2e94cef15e85b0861777821ca4","modified":1626712080248},{"_id":"source/_posts/HDFS-ls-操作-OOM问题解决.md","hash":"6799387b5fca4354a82cfa02c592cc13306c0a74","modified":1626712080248},{"_id":"source/_posts/HDFS-权限.md","hash":"2f10d9ab23fe22c3a3f8968dfbd70462bfb40f69","modified":1626712080248},{"_id":"source/_posts/Flume1-9支持Kafka-Record的Header信息读取.md","hash":"6eed7e13100e5f4795dc565ce52d1de6aaddec25","modified":1626712080168},{"_id":"source/_posts/Hadoop-DataNode-磁盘扩容.md","hash":"c8ebf51fb3b5a48a08bb431432823ecc6d078092","modified":1626712080248},{"_id":"source/_posts/Hadoop-Eclipse-插件.md","hash":"2c720fea3fb02112e502000f016e3162eaaf8768","modified":1626712080248},{"_id":"source/_posts/Hadoop-2-7-3-文档-expunge-命令排版错误.md","hash":"1d79ce1a71399b1d698009eb6980c8346440c3d0","modified":1626712080248},{"_id":"source/_posts/Hadoop-distcp-数据同步.md","hash":"4f6e06583fb24d423346b5a2b38485d7e386d783","modified":1626712080252},{"_id":"source/_posts/Hadoop-进程关闭时报-no-进程名-to-stop.md","hash":"0367ba30460cd4ed97d66ec1562df37c8c79e123","modified":1626712080252},{"_id":"source/_posts/Hadoop-集群修改服务绑定地址.md","hash":"b249a9331435eb1e0354d98b7e67a8f1e6a41e9a","modified":1626712080252},{"_id":"source/_posts/Hadoop-集群各服务进程-PID-文件位置.md","hash":"8f575bc8757350ee2b51065404694b8910c24e62","modified":1626712080252},{"_id":"source/_posts/Hadoop-集群安装.md","hash":"0935bcbf1b92a841b6bac2930ac263da6d0a3e6d","modified":1626712080252},{"_id":"source/_posts/Hadoop2-7-3-编译错误一例.md","hash":"adcd4370fd97839b87c950fd68d2fd3441df2384","modified":1626712080116},{"_id":"source/_posts/Hadoop-集群安装及配置实战.md","hash":"ae45084435a368827e8eaa8cb622eda3029639a6","modified":1626712080252},{"_id":"source/_posts/Hexo-Markdown-特殊字符转义.md","hash":"269fac38a9dba44a5a40dfc33fc1ad00a90a6a2c","modified":1626712080252},{"_id":"source/_posts/Hadoop：安装单个节点的集群.md","hash":"797f0c85cf7c932b48fe7e7a961556fc0bc51f1a","modified":1626712080252},{"_id":"source/_posts/Hive-2-1-1-编译安装.md","hash":"5b9ecc95361e4a1bd3b5561a7165f11a997d8a2a","modified":1626712080252},{"_id":"source/_posts/Hive-MapJoin-的限制.md","hash":"18416c6041d760ef0b530780391d363cddace377","modified":1626712080252},{"_id":"source/_posts/Hive-MetaException-Invalid-partition-key-values.md","hash":"7d5016ea6e076bca5c4ea4347a296085752a37d6","modified":1626712080252},{"_id":"source/_posts/Hive-join-要点.md","hash":"ade9fc9a4beabe53d1bfb54ead5ceaecb6005971","modified":1626712080256},{"_id":"source/_posts/Hive-NoSuchMethodError-Preconditions-checkArgument.md","hash":"d1570bdccc4694ebceb477c167b9bc655bc20bba","modified":1626712080196},{"_id":"source/_posts/Hive-加载文件数据到表中.md","hash":"c8f80806dc48cdadb2b7ffe237eb3ad74be1e80e","modified":1626712080256},{"_id":"source/_posts/Hive-limit-操作假死问题.md","hash":"0291b228c4ba73d8775db6fbb90a9de626077128","modified":1626712080256},{"_id":"source/_posts/HiverServer2-启用-Ranger-行过滤规则后表-Insert-Values操作权限问题.md","hash":"a12bdb04a515718b1bf23aa0c96bb1d0ddbd87f4","modified":1626712080132},{"_id":"source/_posts/Hiveserver2-User-is-not-allowed-to-impersonate.md","hash":"f5c9a3061b8ac308b5ebc8ae42d4bd7b030f2882","modified":1626712080360},{"_id":"source/_posts/Hive启用RowLevelFilter后Create-Table-AS异常.md","hash":"e7747838b9d8b03c282f6f57fbfafabefb1c68a9","modified":1626712080144},{"_id":"source/_posts/Hive-Metastore-User-XX-is-not-allowed-to-perform-this-API-call.md","hash":"3d6ad00809b86f1de15770560ab13749b74a7c28","modified":1626712080044},{"_id":"source/_posts/HiveServer2-Load-Data-To-Table-AccessControlException.md","hash":"c49a80893606a1db6bbab3974444817cac964308","modified":1626712079916},{"_id":"source/_posts/Hue-Load-Balance配置.md","hash":"395ae87df45ccdd5a5932dd8a44811f5a9603aed","modified":1626712080004},{"_id":"source/_posts/Hue-UI-展示中文.md","hash":"5ee0164b9bc3741d059821cdf0e67d00575d4edd","modified":1626712079940},{"_id":"source/_posts/Hue-编译异常：ImportError-cannot-import-name-six-from-urllib3-packages.md","hash":"0b000611bfbbbc81a5e07d88c123e0ed7292e600","modified":1626712080208},{"_id":"source/_posts/Hue安装ImportError错误一例.md","hash":"e58f044ca5fa24d576e51bc0a81926524c898b77","modified":1626712080100},{"_id":"source/_posts/ImportError-libffi-so-5-cannot-open-shared-object-file-No-such-file-or-directory.md","hash":"e2d89bb02e472e32bd6d98a7d2affc2830ea041e","modified":1626712080260},{"_id":"source/_posts/JMeter-测试-MongoDB.md","hash":"c684fa6425cbcdc87706069cfefe65577aa6e9fb","modified":1626712080264},{"_id":"source/_posts/Java-Socket-多线程实例.md","hash":"7dad7b5e0f638c6f2aa6502ca2f00ca1a5b3ab44","modified":1626712080264},{"_id":"source/_posts/Java-单例模式.md","hash":"dfda704b1eb502b1b78ef58585cd6340a9bed98b","modified":1626712080264},{"_id":"source/_posts/Java-读取-Properties-文件的方法.md","hash":"460e7ced2dae7bee66ddad9703fbf48fb2475c45","modified":1626712080264},{"_id":"source/_posts/Jenkins-REST-API.md","hash":"a4748554599f2d703a4239e6d810f41775286f54","modified":1626712080264},{"_id":"source/_posts/Jenkins-protoc-command-not-found.md","hash":"47c50155cd43f72049edd9addc0c7e5f8a21a93d","modified":1626712080264},{"_id":"source/_posts/Jenkins-安装插件网络错误问题解决.md","hash":"1ccf4ca5152763a17c911189bdd074527ae0bd2b","modified":1626712080264},{"_id":"source/_posts/Jenkins-重启后-Maven-的-Project-加载失败.md","hash":"8d998df3c4608956d5d029a3ecc9458853d70d3d","modified":1626712080264},{"_id":"source/_posts/Jetty-嵌入式-HelloWorld.md","hash":"a847c320f315ca9cb62e639d88ad3085abf89b93","modified":1626712080264},{"_id":"source/_posts/Kafka-RecordTooLargeException-问题解决.md","hash":"e5f6e06795bbf86dc7b5cae525a67170dcddb347","modified":1626712080264},{"_id":"source/_posts/LRU-算法.md","hash":"2dfdc012b88d9c45e167eb66d5ea302b5c13310f","modified":1626712080264},{"_id":"source/_posts/Linux-Iptables-示例一则.md","hash":"f5feac2e82ec31f45e97e3d6f00afa358ed329d7","modified":1626712080012},{"_id":"source/_posts/Linux-命令行导出-Emacs-ORG-文档为-HTML.md","hash":"8e4d8c3517c4accdf7fa83162a5da58dd8286ef3","modified":1626712080280},{"_id":"source/_posts/Linux-命令后-的含义.md","hash":"3b205e1976aea1d9be36bfc459f6d46cbfc326ac","modified":1626712080276},{"_id":"source/_posts/Linux-远程-kill-进程及-处理.md","hash":"47fa06dab51555efe295b09dcdaf09f2abb953fc","modified":1626712080280},{"_id":"source/_posts/Linux下Nodejs应用service配置.md","hash":"908076f2dbc2d5d54e09fe29bc037472f5c8fb29","modified":1626712080280},{"_id":"source/_posts/Mac-OS-JAVA-HOME-设置.md","hash":"7e8fb0c43827a600c7bae5bde7656b1711845288","modified":1626712080280},{"_id":"source/_posts/Mac-OS-Shell-远程执行-Shell-命令.md","hash":"b527c6843fbeb699e85e04ef94c9ec5b516cb434","modified":1626712080280},{"_id":"source/_posts/Mac-OS-安装-VirtualBox.md","hash":"12dcba3dc9eed1cb113af673ba089256ba2c7e3c","modified":1626712080280},{"_id":"source/_posts/Mac-OS-安装-Emacs.md","hash":"3726b167ee5bbac35b37964d9d96a53481fc37e4","modified":1626712080280},{"_id":"source/_posts/Mac-OS-常用快捷键.md","hash":"b61c1c5d1dbea83978d41274e16bf54e625a0544","modified":1626712080280},{"_id":"source/_posts/MapReduce-读取-Hive-ORC-ArrayIndexOutOfBoundsException-1024-异常解决.md","hash":"e557abf6794806f574f04e1cb41fd69aad4ff337","modified":1626712080280},{"_id":"source/_posts/Markdown基本语法.md","hash":"fcf4b28ecd710b6505773a522516a4a4d327ec20","modified":1626712080284},{"_id":"source/_posts/Markdown-转义.md","hash":"723d68db12bcdb55110c700751f6fe3747e303b2","modified":1626712080280},{"_id":"source/_posts/Maven-Too-many-files-with-unapproved-license.md","hash":"23314efe09324d19f37d925ca9d0b5af1df5cbe4","modified":1626712079980},{"_id":"source/_posts/Missing-artifact-jdk-tools-jdk-tools-jar-1-6.md","hash":"a988b28276d9e27c18f462036b48cfa03875458d","modified":1626712080284},{"_id":"source/_posts/MongoDB-between-and-操作.md","hash":"f1c19102b1222216a15e7eb87cf42e50aa6cbbb3","modified":1626712080284},{"_id":"source/_posts/MongoDB-常用查询.md","hash":"335b007d2fa32f2372759f3fe23c37a34404e6a6","modified":1626712080284},{"_id":"source/_posts/MongoDB-数据导出工具-mongoexport.md","hash":"06aa9131deb7a68561e83c190d0804f1dfb3ad38","modified":1626712080284},{"_id":"source/_posts/MongoDB-sum.md","hash":"0f90137d494379e95d3e4018cb50b1301130730c","modified":1626712080284},{"_id":"source/_posts/MongoDB-查询和投影操作符.md","hash":"6d033c4a8887eeae8ed17583edc7b00a6308e608","modified":1626712080284},{"_id":"source/_posts/MongoDB：listDatabases-failed-not-master-and-slaveOk-false.md","hash":"1554343905516c506547732450471c68e45a5eec","modified":1626712080284},{"_id":"source/_posts/MySQL-JDBC-连接异常：javax-net-ssl-SSLException-closing-inbound-before-receiving-peer-s-close-notify.md","hash":"9075c6192a1f04df9c146f176771b2016a0c717a","modified":1626712080032},{"_id":"source/_posts/MySQL-修改-max-allowed-packet.md","hash":"9cf81f1da64a6077b8808182a7b63f464f80d597","modified":1626712080284},{"_id":"source/_posts/MySQL-表分区修改操作不支持-IF-NOT-EXISTS.md","hash":"ab4072c4d9f53fa0cbbb8a165202ce20b2879cc1","modified":1626712080284},{"_id":"source/_posts/Nginx-配置支持-HTTPS-代理.md","hash":"f3dc4fb41336c0d2c70ac759eda276671a86eb99","modified":1626712080284},{"_id":"source/_posts/Node-JS-环境搭建.md","hash":"10fb268188ab7d988d232fac7052926eb670177d","modified":1626712080284},{"_id":"source/_posts/Python3-ImportError-cannot-import-name-XXX-from-XXX.md","hash":"e5109f23180529f648432136101c255b933de5f6","modified":1626712080352},{"_id":"source/_posts/Python3-创建虚拟环境.md","hash":"d74267a17fa643ab42a2793329ff01b3a47ac102","modified":1626712080284},{"_id":"source/_posts/Python3-命令行交互不能使用方向键.md","hash":"e4736fed42b605f0621caf8138e42f9084777cca","modified":1626712080328},{"_id":"source/_posts/Python3-操作-MongoDB-批量-upsert.md","hash":"ebb2492fa35f28b2ed308106680abe69c7c0e8f9","modified":1626712080284},{"_id":"source/_posts/Python3-连接-MySQL-并且读取-Blob-字段信息.md","hash":"80cf289fb53615eea1aced4405ee1bda3f5875e5","modified":1626712080288},{"_id":"source/_posts/Python3连接MongoDB并写入数据.md","hash":"d4580c1dac34f8abcb46f03d53788c4fa08932dd","modified":1626712080288},{"_id":"source/_posts/REDUCE-capability-required-is-more-than-the-supported-max-container-capability-in-the-cluster.md","hash":"fafd1846ef66bfe276e5c436f0e5e1b37043b10f","modified":1626712080288},{"_id":"source/_posts/Ranger-Admin安装MySQL初始化问题解决.md","hash":"de3b49bacbf178421f65037fecb4180c5c92a9ed","modified":1626712080076},{"_id":"source/_posts/Ranger-2-1-0-Admin安装.md","hash":"62004cd7c5f254006ac595541f60ecb8127f7c02","modified":1626712080188},{"_id":"source/_posts/Ranger-Hive-Service连接测试失败问题解决.md","hash":"22b58a8eea82d8600ff0c373e94b134b8b62407e","modified":1626712080064},{"_id":"source/_posts/Redis-初探.md","hash":"16425bb3cf1748f8e52980a2cec48094cbd9632c","modified":1626712080288},{"_id":"source/_posts/Rsyslog-omhttp（HTTP-输出模块）.md","hash":"336e2d86c45443f7766908a492d81aab7c464274","modified":1626712080288},{"_id":"source/_posts/Shell-字符串截取技巧.md","hash":"ed12f53570fe210dbf37e9f5d20db481229713b8","modified":1626712079964},{"_id":"source/_posts/Shell-dollar0.md","hash":"64c2ca13867c13d1d7f585e98b9fc1ad937b6c01","modified":1626712080288},{"_id":"source/_posts/Shell-文件判断条件.md","hash":"c9be7f391215c6b8bb17e2d1d820100cc3989ca4","modified":1626712080288},{"_id":"source/_posts/Socket-IO-Java-客户端.md","hash":"1bb6ac1e63c584548028f089aa93699729c023ed","modified":1626712080288},{"_id":"source/_posts/Shell-条件变量替换.md","hash":"9d31b11d9169493a3ce7e316483a49183af47959","modified":1626712080288},{"_id":"source/_posts/Socket-IO-聊天应用实例.md","hash":"bdd3bd1aa571ac6a54d904f20acd5620e5d33228","modified":1626712080288},{"_id":"source/_posts/Socket-IO-负载均衡.md","hash":"1be7f0160e86a1e8e471911e9278186446f55846","modified":1626712080292},{"_id":"source/_posts/Sqoop-导入-MySQL-中含有回车换行符的数据.md","hash":"b23a4db7caea74e48a5dbcfd868daf1f59600b7d","modified":1626712080292},{"_id":"source/_posts/Sqoop-支持-ORC-文件格式.md","hash":"fca5a3956336a1f1055293a61cb8553b2e474a1a","modified":1626712080292},{"_id":"source/_posts/Sqoop-源码修改：增加落地-HDFS-文件数与-MapTask-数量一致性检查.md","hash":"0057ca38c1cc5dbe86778b8a51b00482e194bc3e","modified":1626712080292},{"_id":"source/_posts/Ubuntu-16-04-安装-Lua.md","hash":"98b322fa03c4368a7eadf4da0491b6f26bf381b5","modified":1626712080292},{"_id":"source/_posts/ThinkPad-E480-安装-Ubuntu-18-04-无线网卡驱动.md","hash":"c179a6ec33ddceef6b2564cb32c4c996950c35b7","modified":1626712080328},{"_id":"source/_posts/Ubuntu-16-04-用-APT-安装-MySQL.md","hash":"f2ab15b1d017dddc404747f54a8843f397671351","modified":1626712080292},{"_id":"source/_posts/Ubuntu-18-04-安装-Emacs-26-2-问题解决.md","hash":"e9ae5666d00986ef9815c525b68157958ccb5c8b","modified":1626712080292},{"_id":"source/_posts/Ubuntu-20-04安装Protocol-Buffers-2-5-0.md","hash":"5b78aac94b62d94e9dbc2cdfa8cf6cb3e9adb69b","modified":1626712080188},{"_id":"source/_posts/Ubuntu-SSH-免密码登陆.md","hash":"bdbb88564119c95117ac0d8f1daea9ff109aadb8","modified":1626712080292},{"_id":"source/_posts/Ubuntu-安装-SSH，并开启-root-远程登录.md","hash":"d74ddc0146c7b070fe3e56533c86e9c8265d8bbf","modified":1626712080292},{"_id":"source/_posts/Ubuntu-安装-MongoDB.md","hash":"86ec858e39909293069600fd0db73d87e9173049","modified":1626712080292},{"_id":"source/_posts/Ubuntu-常见安装源更新问题.md","hash":"39414fb1c044a2f90d4cbe7233c74a5b7fee9922","modified":1626712080296},{"_id":"source/_posts/Ubuntu-终端-CTRL-S-被锁定后解锁快捷键.md","hash":"ca7ccb3c959d95d8629a17b637ee20d03ed1cc94","modified":1626712079892},{"_id":"source/_posts/VirtualBox快捷键.md","hash":"4d0d93ca516697910d4a720d398869537f8e6e54","modified":1626712080296},{"_id":"source/_posts/VirtualBox启用无缝模式.md","hash":"bc1b19aa0ae6e5e385dfab96341c6ec419c030ac","modified":1626712080296},{"_id":"source/_posts/Visual-Studio-Code-修改-terminal-字体.md","hash":"eb6d81f7d60086a3fc877995bba127b5d3650ba5","modified":1626712080296},{"_id":"source/_posts/Window7-搭建-Hadoop-2-7-3-源码阅读环境问题解决列表.md","hash":"7a6f1a0aabe6b59b4007897ff884567dee2f4cce","modified":1626712080296},{"_id":"source/_posts/Windows7-Emacs-设置及中文乱码解决.md","hash":"2fc5a89c41bc892b528400ed123e6e0957c94f4b","modified":1626712080296},{"_id":"source/_posts/Windows7-上构建并安装-Hadoop-2-7-3.md","hash":"16a9d3b90ebd0742b077dfe026845739f572e7ed","modified":1626712080296},{"_id":"source/_posts/Zookeeper-单机操作.md","hash":"13dd205afd5a41246302fb3c6095b7f8246240ad","modified":1626712080296},{"_id":"source/_posts/Zookeeper-集群搭建.md","hash":"ac2de23a5c59594d0170454811d0bed88e1d3f69","modified":1626712080296},{"_id":"source/_posts/addShutdownHook.md","hash":"906dbdcb326d270573d60a0cf14ba287cb85f1e2","modified":1626712080296},{"_id":"source/_posts/hadoop-2-7-3-集群安装.md","hash":"73f4cf452d4cd55654e14f827a4da3c05311a0d2","modified":1626712080300},{"_id":"source/_posts/dos命令.md","hash":"a76aeae132a562da046a292b84a1fd1d2967dc1f","modified":1626712080300},{"_id":"source/_posts/hdfs-audit-log-Permission-denied.md","hash":"e9e998e40d02bc97b549d0dacb25b89b1bbf921c","modified":1626712080300},{"_id":"source/_posts/hexo-generate-处理-hashes.md","hash":"c191412723419019fa071aa88880e6b05ea912bf","modified":1626712080300},{"_id":"source/_posts/hive-内置-String-函数.md","hash":"863764f32aaede4151c210e15599fadae7d7b3a8","modified":1626712080300},{"_id":"source/_posts/lftp-的-sftp-使用时遇到的坑.md","hash":"b95f372273d6a69aee0cbc7d87b40fd39fbba402","modified":1626712080300},{"_id":"source/_posts/javap.md","hash":"d81f029160a2d1485002f16f70d3355e79b750fa","modified":1626712080300},{"_id":"source/_posts/nginx-ngx-http-core-module-location指令.md","hash":"88a12d85719d37d15bad6560de6e229776910ef4","modified":1626712080300},{"_id":"source/_posts/mongoose安装.md","hash":"86f441ed189777d33cbf439f22e146f8c93f5a80","modified":1626712080300},{"_id":"source/_posts/nginx-ngx-http-core-module内置变量.md","hash":"5691fd484110eb7a45fccd913099c5570be47caf","modified":1626712080304},{"_id":"source/_posts/no-matching-cipher-found-问题一次解决经历.md","hash":"6b5d47deb7e73402b222ad22a0e4a0ded5bc5306","modified":1626712080332},{"_id":"source/_posts/npm-代理.md","hash":"7b770d5763f68d321d6e5ddff80f68ffd7c8fd3f","modified":1626712079900},{"_id":"source/_posts/org-apache-kafka-common-errors-TimeoutException.md","hash":"63bba872121d46f536b2d4720e4b1b4947af9b74","modified":1626712080304},{"_id":"source/_posts/pip-安装指定版本的包.md","hash":"1d93ed2bad4c6e770aac511a4a5bbfa74bcf47e7","modified":1626712080304},{"_id":"source/_posts/sed变量中特殊字符-处理方式.md","hash":"a689835fdaf127c0311768174c54066d743e7a8f","modified":1626712080156},{"_id":"source/_posts/spacemacs-gnuplot.md","hash":"c081fda8a5ada72b120abeb05cfcab380f278394","modified":1626712080304},{"_id":"source/_posts/shell-c.md","hash":"9b5c0aff6682ae29567f20195d49276dc5c908f8","modified":1626712080084},{"_id":"source/_posts/timed-out-waiting-for-input-auto-logout.md","hash":"b41dc8cb5b3fd8bad270a106b6609017464e6709","modified":1626712080304},{"_id":"source/_posts/react-native-windows-系统本地图片不能显示问题.md","hash":"39d469d22bcfbfc7db49797e9b46052badddc960","modified":1626712080304},{"_id":"source/_posts/unary-operator-expected.md","hash":"f9e80fc6350e404cf2eccdba8af46d3b5dadd488","modified":1626712080304},{"_id":"source/_posts/修改-Linux-符号链接的属主和组.md","hash":"fae8620819ac240f7b13dbf6b44ddbcc7e3c8fee","modified":1626712080304},{"_id":"source/_posts/使用-Rsyslog-将-Nginx-Access-Log-写入-Kafka.md","hash":"67dfa6555a8da84353d3df6260616c37092ef548","modified":1626712080304},{"_id":"source/_posts/修改MySQL参数max-connections.md","hash":"e213773dd22bef4a2bd198251efa4c6c6bd9c9f4","modified":1626712080304},{"_id":"source/_posts/博傻理论.md","hash":"33ae2be8e3739a1f97fab51e2945e5f4f850f1b6","modified":1626712080348},{"_id":"source/_posts/在-Linux-平台上安装-64-位-JDK.md","hash":"6775d2013c95d9c4d75737803d8fecf83bd07928","modified":1626712080304},{"_id":"source/_posts/基于-Zookeeper-的-HDFS-QJM（Quorum-Journal-Manager）-HA-自动-Failover.md","hash":"5e89a60ff7a89fd8268cccd2d4616606157bd5ea","modified":1626712080308},{"_id":"source/_posts/基于-hexo-github-的个人博客系统搭建.md","hash":"b366e66140415989237b45e952491b080edaed6d","modified":1626712080308},{"_id":"source/_posts/基于QJM（Quorum-Journal-Manager）实现-HDFS-HA.md","hash":"6201ba46f9381e199f4abbdd0e656750a7ec2d92","modified":1626712080308},{"_id":"source/_posts/安装-Nginx.md","hash":"fd2a793a929a5321c1921e1215f5c1327ac1a235","modified":1626712080308},{"_id":"source/_posts/开启-Hadoop-HDFS-审计日志.md","hash":"c63d87d2a640f436e084d6784d33fabd53a720fd","modified":1626712080308},{"_id":"source/_posts/搭建-Hadoop-源代码阅读环境.md","hash":"ea1b6572b1d815746221934ce22c7d766a90b5b2","modified":1626712080308},{"_id":"source/_posts/嵌入式-Jetty.md","hash":"949d8a501b4150f19defa62d4d4697b42fc45e97","modified":1626712080308},{"_id":"source/_posts/机会成本.md","hash":"2af01ae68df24d826c767399da21004449cf7afc","modified":1626712080328},{"_id":"source/_posts/沉没成本.md","hash":"ccda2f3b1f881f27af1883d8b5a50d7d262bb975","modified":1626712080328},{"_id":"source/_posts/注解（三）：声明一个注解类型.md","hash":"9070dab685733d544e4d1117019e32858220bb5e","modified":1626712080308},{"_id":"source/_posts/注解（一）：综述.md","hash":"542960f144a3b2565709e3909c42cb24aeb8100b","modified":1626712080308},{"_id":"source/_posts/注解（二）：注解基础.md","hash":"9ee150801dde49ef3ec5bd3eec9649faaebd641f","modified":1626712080308},{"_id":"source/_posts/注解（五）：类型注解和可插拔类型系统.md","hash":"af02bee70401a9ea499687eb6aac881e08ecfdb0","modified":1626712080312},{"_id":"source/_posts/注解（六）：重复注解.md","hash":"69030575b160c0b3254c114547c4c8f48caee912","modified":1626712080312},{"_id":"source/_posts/注解（四）：预定义注解类型.md","hash":"f7006b819477a59b1e74138cb3f25a1dd91b9c8d","modified":1626712080312},{"_id":"source/_posts/浅谈-Hadoop-NameNode、SecondaryNameNode、CheckPoint-Node、BackupNode.md","hash":"6a7708a4d4e4ee354ec32211a403efcb047fcbdb","modified":1626712080312},{"_id":"source/_posts/范型（一）：综述.md","hash":"f86b6beb1b663ce302e877b3317c09c6f13f3381","modified":1626712080312},{"_id":"source/_posts/范型（七）：限定类型参数之范型方法和限定类型参数.md","hash":"803cbd4113b10a0daab9ff30e4ce2b7b79f24381","modified":1626712080312},{"_id":"source/_posts/编译-Sqoop.md","hash":"f4ee75e68df03cd9bec74037dafe1ac8e2ddf7fa","modified":1626712080312},{"_id":"source/_posts/范型（三）：范型类型.md","hash":"cba9b7c15ae04c64665213f15f7ec4caabbf27d2","modified":1626712080312},{"_id":"source/_posts/范型（二）：为什么使用范型？.md","hash":"7e04b864b6db607594846922cbbcf6385d3ed6e1","modified":1626712080312},{"_id":"source/_posts/范型（六）：限定类型参数.md","hash":"b50065a7588352f854a1c326c50ffe3e8b6c63db","modified":1626712080316},{"_id":"source/_posts/范型（十一）：类型擦除.md","hash":"b1a225bcc16991033b4dabb7568304cd5449e7f1","modified":1626712080316},{"_id":"source/_posts/范型（十二）：范型限制.md","hash":"1fddcc01c54f9370f4cdb1427052692295c2158c","modified":1626712080316},{"_id":"source/_posts/范型（五）：范型方法.md","hash":"54d4bbd81324ed307da3e92ec73cb62eef911ffc","modified":1626712080312},{"_id":"source/_posts/范型（九）：类型推断.md","hash":"c293ef677c2cb3ff9be19a1800b9a7f6bad862a8","modified":1626712080312},{"_id":"source/_posts/范型（八）：范型、继承和子类型.md","hash":"63d8602abb5896e6ecf097cea4f8deec870df385","modified":1626712080316},{"_id":"source/_posts/范型（十）：通配符.md","hash":"cca66015c9e23bd3ce740ea41ad38e42fdead9be","modified":1626712080316},{"_id":"source/_posts/范型（四）：范型类型之原始类型.md","hash":"774844828f9146233c2d467b1773db033fec17d0","modified":1626712080316},{"_id":"source/_posts/解决-HTTP-POST-请求-Nginx-静态内容-405-错误.md","hash":"33340c7790131c6880c8564e5ba6a6ec09c284f6","modified":1626712080316},{"_id":"source/_posts/调试-Hadoop-源代码.md","hash":"fae08118a8e9d0913a1864382ff1f480c48e1239","modified":1626712080316},{"_id":"source/_posts/记录一次-Sqoop-从-MySQL-导入数据到-Hive-问题的排查经过.md","hash":"6f0a5374306af205003e2960086ecc4bc4b1c133","modified":1626712080316},{"_id":"source/_posts/通过-MongoDB-日志信息定位操作来源.md","hash":"ae791027cd54d1a130caf8f967b5b36374b373b0","modified":1626712080316},{"_id":"node_modules/hexo-theme-next/LICENSE.md","hash":"68fc9a03d50fd4b5ea97092b05967d1819dea2c4","modified":499162500000},{"_id":"node_modules/hexo-theme-next/_vendors.yml","hash":"c352385539c77d3599017304a2e0186983ee254e","modified":499162500000},{"_id":"node_modules/hexo-theme-next/README.md","hash":"fab15a85d9d8d90ecd8879525b9b74fb1c197978","modified":499162500000},{"_id":"node_modules/hexo-theme-next/package.json","hash":"7286403d20b76978552272ebf08ebabb4f54c951","modified":1626880382927},{"_id":"node_modules/hexo-theme-next/_config.yml","hash":"2924be391e4841e2166e83d4d18515c4d247653f","modified":499162500000},{"_id":"node_modules/hexo-theme-next/docs/LICENSE.txt","hash":"f5b14f791b7cfa1d16da981d929152e088a5d1b8","modified":499162500000},{"_id":"node_modules/hexo-theme-next/languages/README.md","hash":"b2567e32805dda79601157351a07e5ca9fe01315","modified":499162500000},{"_id":"node_modules/hexo-theme-next/docs/AUTHORS.md","hash":"a648823121563c34a177ae91f5a774b5e29f01a0","modified":499162500000},{"_id":"node_modules/hexo-theme-next/languages/ar.yml","hash":"bca66db21c015dbd32970d8708b898518a773e1e","modified":499162500000},{"_id":"node_modules/hexo-theme-next/languages/de.yml","hash":"4be7b8b76c81bf1853eb36d2e874b17546a0e792","modified":499162500000},{"_id":"node_modules/hexo-theme-next/docs/AGPL3.md","hash":"0d2b8c5fa8a614723be0767cc3bca39c49578036","modified":499162500000},{"_id":"node_modules/hexo-theme-next/languages/en.yml","hash":"814d81c27fed736055ee300e0a6505b26ff4313c","modified":499162500000},{"_id":"node_modules/hexo-theme-next/languages/fa.yml","hash":"6456d40dd42f44101d9d6e7054e9884e9163f948","modified":499162500000},{"_id":"node_modules/hexo-theme-next/languages/es.yml","hash":"651e3b33d86a7cdb9fd7895ca28279f8b1a24faa","modified":499162500000},{"_id":"node_modules/hexo-theme-next/languages/id.yml","hash":"14e794db4eca36b257994d81eb513e61d1edcbd6","modified":499162500000},{"_id":"node_modules/hexo-theme-next/languages/fr.yml","hash":"b15dc05afdc94de02e5d3fee4f8d3dc5594dd37e","modified":499162500000},{"_id":"node_modules/hexo-theme-next/languages/ko.yml","hash":"6387357ac2dd498e8b8d630d27050a59180d7e8f","modified":499162500000},{"_id":"node_modules/hexo-theme-next/languages/it.yml","hash":"c1eeab4992c76bfd436bb205ce58b1cfeef55ee6","modified":499162500000},{"_id":"node_modules/hexo-theme-next/languages/ja.yml","hash":"d48c4157e0e02e847aac7b513580d3364c81948c","modified":499162500000},{"_id":"node_modules/hexo-theme-next/languages/nl.yml","hash":"ecb8e39c6225f3c068a5fdd569ee7dafd5c41a1f","modified":499162500000},{"_id":"node_modules/hexo-theme-next/languages/pt-BR.yml","hash":"a1f27b3a592fc58f17d247f5563ff4a90a3da5f2","modified":499162500000},{"_id":"node_modules/hexo-theme-next/languages/pt.yml","hash":"63a3e1e728ba5e6e22150de7331bb8a654f34960","modified":499162500000},{"_id":"node_modules/hexo-theme-next/languages/tr.yml","hash":"55b38c7617c24bdc27c9de6cf39f4b191d154fb8","modified":499162500000},{"_id":"node_modules/hexo-theme-next/languages/ru.yml","hash":"e9af1afe529ca747a04b801401d394b2ad696fde","modified":499162500000},{"_id":"node_modules/hexo-theme-next/languages/si.yml","hash":"2a9861db4547a524b2609c1e7e1061d2e9d48ee4","modified":499162500000},{"_id":"node_modules/hexo-theme-next/languages/uk.yml","hash":"7dd24580c0865c5a7bc4d391855045366a598936","modified":499162500000},{"_id":"node_modules/hexo-theme-next/languages/vi.yml","hash":"c669c34da544a563ceae3e196addc9df6a78e024","modified":499162500000},{"_id":"node_modules/hexo-theme-next/languages/zh-HK.yml","hash":"f195bb0502ffe66e850077a1af1033455ea65f93","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_layout.njk","hash":"20e4160cd0deb4fa272cc3aed0f43520b3cf4a9c","modified":499162500000},{"_id":"node_modules/hexo-theme-next/languages/zh-CN.yml","hash":"5a3ab21210304efef736e96bad254f789f42c567","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/archive.njk","hash":"d759f4d2cf5ddc6875ea250113a00662c1caf6d1","modified":499162500000},{"_id":"node_modules/hexo-theme-next/languages/zh-TW.yml","hash":"92256b90028de9a1e79c6bc0e5885b93e7fb4b17","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/category.njk","hash":"0a590e87af50e57b15fc37695c9a3bf4a97c3d92","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/page.njk","hash":"9cd3eca2c468bb46c7c5bf391bea4b025af178f6","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/index.njk","hash":"37ec3d1bcd20b8ac1d18e0d68f990450890b46cd","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/tag.njk","hash":"6cd707f846bfd6becbcfb060c26958bb4015c31f","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/post.njk","hash":"6abeb85fb3e4c382ed4bb6049b12a807e6226e67","modified":499162500000},{"_id":"node_modules/hexo-theme-next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"7a06d443f374bd1e84294067a0ac796afd9fbe60","modified":499162500000},{"_id":"node_modules/hexo-theme-next/docs/zh-CN/README.md","hash":"02bafc6ee86263790603861e356596f0c916e392","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_macro/post-collapse.njk","hash":"1a30d751871dabfa80940042ddb1f77d07d830b9","modified":499162500000},{"_id":"node_modules/hexo-theme-next/docs/zh-CN/CONTRIBUTING.md","hash":"8ee5ca39ac4a372a5c0f16e344bbe578af4aeae4","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_macro/sidebar.njk","hash":"ba06ad089d01b45b217249af338145881eab3e25","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_partials/comments.njk","hash":"c12f8a7497596441503f2541d2f746f2ee7dd594","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_macro/post.njk","hash":"b106599106285ecd4c0c1e0ed11c46a36b5a69d5","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_partials/languages.njk","hash":"e43f22198cccb5f6e306b1ce0d28d12a4fb891f8","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_partials/pagination.njk","hash":"9876dbfc15713c7a47d4bcaa301f4757bd978269","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_partials/footer.njk","hash":"19713f472972caac33ae5fbcfe9105da61257de4","modified":499162500000},{"_id":"node_modules/hexo-theme-next/docs/ru/README.md","hash":"87edab5a3eb7577a409c01df3f1631de40f8956f","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_partials/widgets.njk","hash":"852a750524decf1efa587cd52b09e387ed8315de","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_scripts/vendors.njk","hash":"be80b9fe415a9a09d74c28e230995fd292dfc123","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_scripts/index.njk","hash":"6668878a0f9a1166c6a879755f54a08d942da870","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_third-party/nprogress.njk","hash":"80fd1a45e91207fc16df1136e12b8b71b4f65dea","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_third-party/quicklink.njk","hash":"0efed71ed530447718c4ea5bbd5fc8695b0b0d5f","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_third-party/index.njk","hash":"13e7badc036e989307069f65b179deb107012435","modified":499162500000},{"_id":"node_modules/hexo-theme-next/scripts/events/index.js","hash":"89091bc943cd8b8c63b8af3d26fb0a027048e9ba","modified":499162500000},{"_id":"node_modules/hexo-theme-next/scripts/filters/default-injects.js","hash":"872f01cb10e422a648ea505436532e776e92926b","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_third-party/rating.njk","hash":"1bcdbc7fde26d6d9ef4e7fa43ffcff5a9506b20e","modified":499162500000},{"_id":"node_modules/hexo-theme-next/scripts/filters/locals.js","hash":"0cd7da6755459d60779f0a7ccf311e26e184d55d","modified":499162500000},{"_id":"node_modules/hexo-theme-next/scripts/filters/post.js","hash":"42a9b81c5449afa9d67770604478168333c93804","modified":499162500000},{"_id":"node_modules/hexo-theme-next/scripts/helpers/font.js","hash":"3394185a7f0393c16ce52c8028f90da3e9239c55","modified":499162500000},{"_id":"node_modules/hexo-theme-next/scripts/filters/minify.js","hash":"99ab10d9aef1ee5ae5e8f3572ac0de25c3cc4416","modified":499162500000},{"_id":"node_modules/hexo-theme-next/scripts/helpers/engine.js","hash":"b9785bc737470e9b8e910e7da9e8c45c2ead58fa","modified":499162500000},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-config.js","hash":"454ed1fac07be972c7ec911092f0995cd925aab3","modified":499162500000},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-vendors.js","hash":"afdd6a188a74c188f0dd154fac70efd4080ca262","modified":499162500000},{"_id":"node_modules/hexo-theme-next/scripts/helpers/next-url.js","hash":"a11b71ba0c5012e2cdcab31c15439156b215563e","modified":499162500000},{"_id":"node_modules/hexo-theme-next/scripts/tags/button.js","hash":"c6ad2ed544fbb25ecb5d820c36e76302504271b7","modified":499162500000},{"_id":"node_modules/hexo-theme-next/scripts/tags/caniuse.js","hash":"935a311142a409c1896b3ae3f01fe7a9e2db1134","modified":499162500000},{"_id":"node_modules/hexo-theme-next/scripts/tags/index.js","hash":"17f9451ce1f10f78437f52218757d38d4e1591b0","modified":499162500000},{"_id":"node_modules/hexo-theme-next/scripts/tags/group-pictures.js","hash":"79102d9e9bccff6224e77a77c4d2d363094ae3df","modified":499162500000},{"_id":"node_modules/hexo-theme-next/scripts/tags/label.js","hash":"8a73348186113bae0a51ea2f891c1bb882fab05a","modified":499162500000},{"_id":"node_modules/hexo-theme-next/scripts/tags/center-quote.js","hash":"92c19d796bdb3320df9caea59bf52df7a95d9da9","modified":499162500000},{"_id":"node_modules/hexo-theme-next/scripts/tags/link-grid.js","hash":"18a483c2d5afd701f6080ffdddf2d1321370336c","modified":499162500000},{"_id":"node_modules/hexo-theme-next/scripts/tags/mermaid.js","hash":"4fb01ca650fa8b256b8d48f50dc1b18350bd3d6d","modified":499162500000},{"_id":"node_modules/hexo-theme-next/scripts/tags/note.js","hash":"7b94ddb46b7d4b0fe815f2fbe4bd375f07f55363","modified":499162500000},{"_id":"node_modules/hexo-theme-next/scripts/tags/pdf.js","hash":"344636b6fd7e27e8831c1e194039afc0d61931cd","modified":499162500000},{"_id":"node_modules/hexo-theme-next/scripts/tags/video.js","hash":"2ee926448583be8f95af1f2884ae2c9c4830151d","modified":499162500000},{"_id":"node_modules/hexo-theme-next/scripts/tags/tabs.js","hash":"0eabe51da40b4b13e16419c8fe02452d9a4fef73","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/main.styl","hash":"78ce791cc4ac95386cf6839ca72f5f7b51f86ee9","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/noscript.styl","hash":"76bba5d7916e9930e68215a0fce3a7d81c44510f","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_colors.styl","hash":"b37f9847d2f95632e911df670b51921a7d748068","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/images/avatar.gif","hash":"2dbc3e2f2d624b2ca1afe6edc2ca17307f1950c8","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_mixins.styl","hash":"acef5acc728f24cb657be8d7010d836b4d556b0e","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/images/logo.svg","hash":"2cb74fd3ea2635e015eabc58a8d488aed6cf6417","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/js/comments-buttons.js","hash":"1a7344440321713426a0b2ab17e276b5bdf85ade","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/js/bookmark.js","hash":"0f563ffbf05fad30e854e413ab17ff7164ab5a53","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/images/logo-algolia-nebula-blue-full.svg","hash":"b85e274207b1392782476a0430feac98db1e7da0","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/js/config.js","hash":"4c4ebbe3b3f3841a26f9d5af6d0ba8bc6da01c54","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/js/comments.js","hash":"66ae2e26ea36a41b72c638ea8b220296638ae952","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/js/pjax.js","hash":"5bfc1cea214bd31847adc356f37df8e0b6a449df","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/js/motion.js","hash":"6d4bd07a6f8e1b4083119dca0acb5b289533b619","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/js/next-boot.js","hash":"2ecaa30ec42d28ef769d875cf2d26959b8de7d24","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/js/schedule.js","hash":"71d62fc3584c47ff2d4cc945226e412264399be9","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/brand.njk","hash":"ffb6c69a9c90793cbe9bf0544b55f7a41c016d8f","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/menu.njk","hash":"c9390824b57f23b7c8a5c23a9834514123673766","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/index.njk","hash":"53895b3af95667edc5bf5d7356f8a2b4fe091447","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/menu-item.njk","hash":"b46f412c0b4f775fd329d50357f722f5d7c1a3ba","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_partials/head/head-unique.njk","hash":"49aa8de07918a11399131dfd703418af3f0a1a19","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_partials/head/head.njk","hash":"1084e38df2f05a2f58df7f987e660775e9be1bf2","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/js/utils.js","hash":"2ffeac130c5309bc83b1e9b764dd23688c0cd7db","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_partials/header/sub-menu.njk","hash":"75a158a5b54a3a76ee6590f5e0e2dd4a9f0be869","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/breadcrumb.njk","hash":"edb3bb6d644b7407673c5ef3a426a244e98fcf89","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/page-header.njk","hash":"7ed4f102a1825195cff8d7995bf9219f323a9034","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/categories.njk","hash":"17156d99941f28a225951ffdcfa9a115e20dc2d2","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/schedule.njk","hash":"ca2ccf3cf1874c45712f192ad45dea96fbd9920d","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_partials/page/tags.njk","hash":"a18d1598e36cc72f2b0b24c3cc3c5990dfaa3254","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-copyright.njk","hash":"133942922e34abae9e4de7ea5591d77c0caa4b37","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-followme.njk","hash":"154df0bb323c332d8c25343f258ee865e5553423","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-footer.njk","hash":"8f14f3f8a1b2998d5114cc56b680fb5c419a6b07","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-related.njk","hash":"7384e6390067ef2a84e7310d6adb3f6104ed62e2","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_partials/search/index.njk","hash":"8f6f256ab3b351ffc80f1f3f1d9834e9a7cfac31","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-reward.njk","hash":"002b51d0cae3f2e2e008bdc58be90c728282de5b","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_partials/search/algolia-search.njk","hash":"efb2b6f19df02ba5ae623a1f274fff52aed21e6f","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_partials/post/post-meta.njk","hash":"cc1a11190b1a55ae8d4252d296803bc19db90bc5","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_partials/search/localsearch.njk","hash":"661f7acae43f0be694266323320f977d84119abe","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/baidu-analytics.njk","hash":"6215309aee028dcb734452beec448c5afb6c63fc","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/cloudflare.njk","hash":"c978e9efd472c4825f93b83524b11f1c4f7efaab","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/google-analytics.njk","hash":"d89066ff53879693f023e540d59c86137172c529","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/growingio.njk","hash":"8afaa772c390bd9d53a5cff9645ac3168334eb98","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_third-party/chat/chatra.njk","hash":"d7263fca16d0278ccf1f6aa1c6df6902a6344a09","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_third-party/analytics/index.njk","hash":"2d36a481a70d5f450f1f166dc556ac1218b18537","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_partials/sidebar/site-overview.njk","hash":"5994278b3900828fd02d418f99f6c3b4e88e27a9","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_third-party/chat/gitter.njk","hash":"f8cc14b7aa949999a1faaeb7855e2f20b59a386d","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_third-party/chat/tidio.njk","hash":"02aab857c27fc103216029be991688b12a73a525","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/changyan.njk","hash":"d1c950f8fbdf85e7a3eae5463767a89e858e8220","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/disqus.njk","hash":"9375b19a89b7fa9474e558d085af5448d4c5c50c","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/gitalk.njk","hash":"b63b7e2ede0d3e66e732fa1a06bda9b19e1e85d4","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/disqusjs.njk","hash":"0749cb6902baecdfd01f779a2a2513f6d2f6a823","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/isso.njk","hash":"64cc3bdaf644fd32c0d0a247f29f5b6904da9af3","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/livere.njk","hash":"3b13b09fba84ec6000886890a6710736a2b8fafe","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_third-party/comments/utterances.njk","hash":"5a94032bc3512a10ad4328fc19ec07b819a1d687","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_third-party/math/katex.njk","hash":"d82c24136bbd3443b85f07f5579845833b594684","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_third-party/math/index.njk","hash":"abf37fc55aa86702118e8fdf5bf2d389dd589aa0","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_third-party/search/algolia-search.njk","hash":"24ed76e0c72a25ac152820c750a05826a706b6f4","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_third-party/math/mathjax.njk","hash":"3677017fd4572b158311f5f5d870590ab25184e0","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_third-party/search/localsearch.njk","hash":"3fc91aba5b17a5ae60b8e04707a7da87db4d0340","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/busuanzi-counter.njk","hash":"a4bc501da0f22f7e420f0ca47e83988ce90b1368","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/firestore.njk","hash":"d32ebe94560fa95824478ebbff531bffc47b194d","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/index.njk","hash":"568ddf7955d11d93fb5e842b403a7ac8b1b7fdb1","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_third-party/statistics/lean-analytics.njk","hash":"2446e748cdc102c78492216319ac02148db7daf6","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_third-party/tags/mermaid.njk","hash":"099e031f52fb8e47b3af5b2684737efc9e643ee7","modified":499162500000},{"_id":"node_modules/hexo-theme-next/layout/_third-party/tags/pdf.njk","hash":"2c81984cc4f5123103460442f6e046f5b6c97127","modified":499162500000},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/highlight.js","hash":"6aec7b2c38c50989a23bfaa0d560e75c7f553e12","modified":499162500000},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/config.js","hash":"92ab6bb3c3122980699f0613dac27a7cabfedecc","modified":499162500000},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/injects.js","hash":"d987709267a1bc6e5014411e9983d7c49c102c16","modified":499162500000},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/utils.js","hash":"b281be775b693f9bf32766c8f6ef703c72ac9b00","modified":499162500000},{"_id":"node_modules/hexo-theme-next/scripts/events/lib/vendors.js","hash":"13b3301aa9b613975630502d93e6ae82c5293aa8","modified":499162500000},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/common.js","hash":"2486f3e0150c753e5f3af1a3665d074704b8ee2c","modified":499162500000},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/changyan.js","hash":"aa05e6b3d613a756178b8ba06832ad27499d4c14","modified":499162500000},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/default-config.js","hash":"93ee5f9109dad885dc38c49bcee630c10f9dce6e","modified":499162500000},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/disqusjs.js","hash":"135b87d151055eefdbc711d9e704b112b3214a84","modified":499162500000},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/gitalk.js","hash":"7bb7dafdd7f6bca8464b54e17e552ce7f1714195","modified":499162500000},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/disqus.js","hash":"7f71d6b271ba65ff333d5682e7575711d368c0d2","modified":499162500000},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/livere.js","hash":"5a07d8bb52bc1d51a624ca8db54be144566c306b","modified":499162500000},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/isso.js","hash":"ff8b5b5145220a17d0ecd9508ba9bd2d3b2da47d","modified":499162500000},{"_id":"node_modules/hexo-theme-next/scripts/filters/comment/utterances.js","hash":"d3bded697bc32dace689d2a6dfb6eb7514169d15","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Gemini.styl","hash":"96e0a7c2a65ce68215e17e369085b2ea2f1334f2","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Mist.styl","hash":"e1fbf169b9b6a194b518240cbd06ec3c48b83d61","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Muse.styl","hash":"e3be898f5ebcf435a26542653a9297ff2c71aeb0","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/js/third-party/nprogress.js","hash":"7d56b18a2bdece5468470c70c5f3eb79d4120964","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_variables/Pisces.styl","hash":"c65536a128b9bc9dbe2fbb1b235a3cded2891002","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/js/schemes/muse.js","hash":"62f4638674c92dfb72454fee38751d0227d3d225","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/js/third-party/rating.js","hash":"4e92c2d107ba47b47826829f9668030d5ea9bfb8","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/back-to-top-sidebar.styl","hash":"d4809783ded05625675b1b4bbd9e99d7f5f7d7f9","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/js/third-party/quicklink.js","hash":"6f58cd7aa8f6f1ab92d5a96551add293f4e55312","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/back-to-top.styl","hash":"ece860218125bdb2578f373ed4f5040c9670e4b1","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/index.styl","hash":"3c7ae405dd30b9b46494a6b9a6cb1b7ec6138ba9","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/reading-progress.styl","hash":"2a29bf3692f42f84e8f29314e0fb349f86fc727a","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_variables/base.styl","hash":"32a1b73944561655087d80f025208a84e012b3cf","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/index.styl","hash":"0c9f72ad98807521cbdcee7b5bbe2e884311db39","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/base.styl","hash":"8d7ad58c9086161b05843aa7e44973148be33611","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/buttons.styl","hash":"a042571d85ff7265f799004239a45f36b716b8a6","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/comments.styl","hash":"e4fecc889ba3317a64e9abba5842c79dff9b7827","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/mobile.styl","hash":"9f88d350df8115d26c6adbc2025a27ef9a42d7ff","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/index.styl","hash":"523fb7b653b87ae37fc91fc8813e4ffad87b0d7e","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tables.styl","hash":"e840b23d33023e6d45e018f6e84b683dd56efd8d","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/toggles.styl","hash":"572a41499391677d84b16d8dbd6a996a3d5ce041","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/pagination.styl","hash":"41cba8c4c5637a6b8f1b62e67673b33676f5d734","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_layout.styl","hash":"5604ac1e161099a4d3e5657d53507268866dc717","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/normalize.styl","hash":"b56367ea676ea8e8783ea89cd4ab150c7da7a060","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_header.styl","hash":"4817e77577896ab5c0da434549917ee703a3f4cf","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_menu.styl","hash":"357b899ac0f0dfbbbebf1ea972030c7cefa463ce","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Gemini/index.styl","hash":"fd49b521d67eaccc629f77b4e095cb7310327565","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/index.styl","hash":"ab16a3dcdc0393b9b582ef59dcc13db9320e917c","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_header.styl","hash":"06080fd963c904d96c00eff098a284e337953013","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_layout.styl","hash":"82a29572dd90451f75358a2ee2522b87304a0bb8","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Mist/_posts-expand.styl","hash":"b332868d76d9f1651efd65abfc0d3c9d699b1a45","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_sub-menu.styl","hash":"c48ccd8d6651fe1a01faff8f01179456d39ba9b1","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/index.styl","hash":"6ad168288b213cec357e9b5a97674ff2ef3a910c","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_sidebar.styl","hash":"944364893bd7160d954c10ba931af641c91515a4","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_header.styl","hash":"be5c46b983df08b9dbac1b4749b1a101b54b6b50","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Muse/_menu.styl","hash":"8a70d51d8f7cd113e5fbc9f0e70c46a072f282c8","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_layout.styl","hash":"6eee86c8f0175d6c09e434053516cd8556f78d44","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/baidu-analytics.js","hash":"f629acc46ff40c071ffd31b77d5c7616f0fdd778","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_menu.styl","hash":"72dc825c50357402c342d62ab60fc0c478ab6bc1","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/growingio.js","hash":"78dd3cf04082b7dbe6246e404b2aa8e726922402","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_sidebar.styl","hash":"d9141e6e14a56b5952488101e9a8388c2170e270","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/index.styl","hash":"8000075b227749a7495eaf417cac6ccfbe441580","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/gitter.js","hash":"cc38c94125f90dadde11b5ebac7d8bf99a1a08a2","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/js/third-party/analytics/google-analytics.js","hash":"59684383385059dc4f8a1ff85dbbeb703bcdbcb5","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"778ed2ad5643b93970c95626b325defeb586733f","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/chatra.js","hash":"c32180522788c10e51df1803aa6842ef0432ddc9","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/js/third-party/chat/tidio.js","hash":"b0079f6a4601e06ca6fe46e83a2f5af553e9bc3c","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/changyan.js","hash":"8c8ebec444c727b704ea41ad88b0b96ed2e4b8d4","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqusjs.js","hash":"b6c58f098473b526d6a3cd35655caf34b77f7cff","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/disqus.js","hash":"e1cc671b0d524864fd445e3ab4ade9ee6d07e565","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/gitalk.js","hash":"0ec038cf83e8ec067534f16a54041e47a3c1e59a","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/isso.js","hash":"753a873b6f566aff5ba77ca23f91b78eb880ca64","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/katex.js","hash":"83c54ee536e487a1031783443fe0cb63b1b4767e","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/livere.js","hash":"2247d88c934c765c43013337860774aaa99f0b31","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/js/third-party/comments/utterances.js","hash":"f67f90eb03e284c82da2b8cf2f1e31801813c16d","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/algolia-search.js","hash":"ac401e3736d56a3c9cb85ab885744cce0b813c55","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/js/third-party/math/mathjax.js","hash":"9771db8b6b2541181a0e463c4bf305276d47657d","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/firestore.js","hash":"0960f16107ed61452fb0dffc6ed22dc143de34ef","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/js/third-party/statistics/lean-analytics.js","hash":"5a928990856b8e456f0663cf3b6b406733672e39","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/mermaid.js","hash":"aafb764c64f6cd5a48ad194adf65ac77078242b1","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/js/third-party/tags/pdf.js","hash":"af78c22f0e61c8c8aa8794e585e0d632c6d4fcb8","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/breadcrumb.styl","hash":"8afdc311c6b8db121758371f95cf1c5e77354f42","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/js/third-party/search/local-search.js","hash":"b37f1b14aef2402d0c2d88f999ff0ce5ef6823c1","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/categories.styl","hash":"b6e2eb1550a7845cb2adf86081a4ab6c7bde1e68","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/tag-cloud.styl","hash":"1a81d1a71fcf0699629ce6e72dfd0a15f3a2dd0a","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/schedule.styl","hash":"6b816c2511242ee503fb5f34cd3e4dcdafc06b85","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/pages/index.styl","hash":"7504dbc5c70262b048143b2c37d2b5aa2809afa2","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/index.styl","hash":"b3fa752f72ca1413289b76c56fbd33a00e3d25d7","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-collapse.styl","hash":"ec37a36e94ba791663607a5022f763915778578f","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-body.styl","hash":"5d61dedb3bec1021d52894f9b379e4d0953f6a35","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-followme.styl","hash":"76d0dfb3a8b873a6180604ac6daecf38b6a963a2","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-gallery.styl","hash":"aa366d37389760c8595529b850f461569577a1c5","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-footer.styl","hash":"1d284f3ea03ba9b4feb76b375e539a8e0bccf1c3","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-nav.styl","hash":"9ac6f477177264c26a46e8333b8456720a0444dc","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-header.styl","hash":"2ca4dea5e7785c9b77566d95cabf896d20a42741","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-widgets.styl","hash":"5b5649b9749e3fd8b63aef22ceeece0a6e1df605","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/post/post-reward.styl","hash":"07cff69f2d57e6321595f64c16d8b763dc88df6a","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/gitalk.styl","hash":"7102f8e819b62cf7d121fd063dc663fd068feaa6","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/search.styl","hash":"2896840ab8ac8ab2a7f76d18df893f290ac31625","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/related-posts.styl","hash":"2c534d2b2dbc932ad65d335a720a7ba9612bac04","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/utterances.styl","hash":"bf88d9c585d7b00463c46352402cfea415c29493","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/index.styl","hash":"b457756758f0632767e8a560e3033059cbe4a67b","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/footer/index.styl","hash":"4f482514230c941c9475e1272188e53a54975463","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/bookmark.styl","hash":"d6d60f02b5e9f89dbfce180b3884030898022664","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/github-banner.styl","hash":"cf194bea1c9e67fde871a04de3bc81df72c54277","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/menu.styl","hash":"392fd53a8dd4e3f33a853ebb24290a622300e0ff","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/index.styl","hash":"f1778d2c56974b96dae429456d5c55be325c4946","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/components/third-party/math.styl","hash":"83c6588c51cd418336f4945813410a100ddfe2a4","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/site-meta.styl","hash":"9a47c9045e443b8d20932f9c564a3a05fa4c6b51","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/header/site-nav.styl","hash":"bf3ad8b4268f763a1e26377681644887694bc009","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/index.styl","hash":"eae7b83f1c711ad2ab3a41d89f517445856ffec8","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-author.styl","hash":"5b38ac4a0f1ade0e681aff0e3366c481d9cf3dcd","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-author-links.styl","hash":"52fc98b1435129eb3edb9293ced9e507741f1350","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-blogroll.styl","hash":"9950c3188a28e1c63b5498b7bdcd14b12ace3e28","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-dimmer.styl","hash":"fbdb63c6a8887d19b7137325ba7d6806f728139c","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-button.styl","hash":"b926e368f702f8686aaa2eb98d3d2e533418958c","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-nav.styl","hash":"f7ff85fe6c4efb8ff036fab2c3277b7d8bed69a8","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-toc.styl","hash":"6394340c28a21f6aa90e786f3bfe24fb26595653","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/highlight/index.styl","hash":"08e79881d58d01afab6dbed37ab4f52356564d7e","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/sidebar-toggle.styl","hash":"432e73bc4f99322af6af1852e0ea6e674919c31a","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/outline/sidebar/site-state.styl","hash":"69eb1c282a8fd5dbab606cc09c34c5dc8e44e753","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/blockquote-center.styl","hash":"15a5e273a8137550c93c8d2a60f9fcf86e04a89e","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/group-pictures.styl","hash":"393ff96234e4196b569d4b11496774eb78e147de","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/index.styl","hash":"cef4e779473daa3761709958243c6b8a57bbd814","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/highlight/copy-code.styl","hash":"83ee4993710fc8daa1c8dbfccd5d5091fd244c30","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/label.styl","hash":"debee14539272fbe3835a7d3853af2230baa3501","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/mermaid.styl","hash":"3c029a003e9bf747e1b9cc7c0c127f6028374876","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/link-grid.styl","hash":"2421500e447822ef1b6826403a3e1df3345641d7","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/note.styl","hash":"f53e6c12bd4805888f696386d00668f23cd335e7","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/tabs.styl","hash":"50b00218e854200c4ec0573a841e226d49c45cba","modified":499162500000},{"_id":"node_modules/hexo-theme-next/source/css/_common/scaffolding/tags/pdf.styl","hash":"b49c64f8e9a6ca1c45c0ba98febf1974fdd03616","modified":499162500000},{"_id":"source/images/android-chrome-192x192.png","hash":"ec85cab384b6de5cb09e9fa1b7a9fe8ebefaff58","modified":1626925623000},{"_id":"source/images/apple-touch-icon.png","hash":"6aa30c473d2c59553573d6fac337e56391ec3027","modified":1626925623000},{"_id":"source/images/browserconfig.xml","hash":"974aea18bda5a95802c06b80126ab1d96d91d708","modified":1626925623000},{"_id":"source/images/favicon-16x16.png","hash":"99b49ec55b8772da1fb13f3561d8f6e1069b841a","modified":1626925624000},{"_id":"source/images/favicon-32x32.png","hash":"fed1f61e63edb7c6aa1d5cecd12005419bd335d9","modified":1626925624000},{"_id":"source/images/favicon.ico","hash":"5ce209455e167b392a7db27da320e9e91d70042f","modified":1626925625000},{"_id":"source/images/mstile-150x150.png","hash":"dafeda228d4d4924772447e8f3811bc3d65cd433","modified":1626925625000},{"_id":"source/images/site.webmanifest","hash":"13e5aa58eb2182d8ace63266856c8cc29c47c083","modified":1626925625000},{"_id":"source/images/wechat.jpg","hash":"d3f36f0898723a61fc130c40f63da9c5a11fe53c","modified":1626934255591},{"_id":"source/uploads/java-se-site-map.png","hash":"9728ac67818883fe04ba0e7144c2f134163bfe07","modified":1626882206502},{"_id":"source/uploads/20151122/vbox5.png","hash":"4ddae1a2a55e302a067caff7d088106523bea12f","modified":1626882206378},{"_id":"source/uploads/20151125/nodejs1.png","hash":"88c612317d9a34f714a09556e8cac9e1bb0b65be","modified":1626882206378},{"_id":"source/uploads/20160417/mongodb-script.png","hash":"8e90e06a04259fba91d24c0900848df2cd7c2dee","modified":1626882206382},{"_id":"source/uploads/20160417/mongodb-test-result.png","hash":"107ccdb862e795de536189f51fa8eeb202e11027","modified":1626882206382},{"_id":"source/uploads/20160417/mongodb-source-config.png","hash":"51b75614aa935341e44014dc4f0a555758463d43","modified":1626882206382},{"_id":"source/uploads/20160417/mongodb-thread-group.png","hash":"49049c3eb66c9d2c680a1070148219a7754e53a4","modified":1626882206382},{"_id":"source/uploads/20160521/personal-blog-site.png","hash":"d747ade0d9329af13229aea366a609a0d81246bf","modified":1626882206386},{"_id":"source/uploads/20160525/ssh-PermitRootLogin.png","hash":"37f614f102eb6e0ddae1f93829f5c922a85a0252","modified":1626882206390},{"_id":"source/uploads/20160511/mongoDB-web-console.png","hash":"4e1965ab4f8f9ce04340c1e9921214bab16b2fb3","modified":1626882206382},{"_id":"source/uploads/20160602/Socket.png","hash":"0165193ebcf1f0b15e6d758cc2173e35c2fc7de2","modified":1626882206390},{"_id":"source/uploads/20160603/2q.png","hash":"2af690b0b539dc20ba33c2546aeb71cad459c1ba","modified":1626882206390},{"_id":"source/uploads/20160603/lru-k.png","hash":"f745685ddbc3772e3d9a5502949b8300957ba666","modified":1626882206390},{"_id":"source/uploads/20160603/lru.png","hash":"6b50059d4e160350644a0531b49ed56d70f4eaf6","modified":1626882206390},{"_id":"source/uploads/20160603/mq.png","hash":"91e8c0510cac28d49321d78eb648f9db19d5e841","modified":1626882206390},{"_id":"source/uploads/20160604/console2.png","hash":"f74ba775afd0981b2ee8c48e1e75d9fd447dd6cd","modified":1626882206394},{"_id":"source/uploads/20160604/console.png","hash":"a4b16fa05c96654177f6e0bf09f0bb28ffa36fc0","modified":1626882206390},{"_id":"source/uploads/20160604/index.png","hash":"a07473dac4f8293855010c2d34c23733f32e86a4","modified":1626882206394},{"_id":"source/uploads/20160604/index_page.png","hash":"c6fe7b40a6937b837deedd080a9656fbb428c2f5","modified":1626882206394},{"_id":"source/uploads/20160604/index_page_2.png","hash":"737d36910f8d1d10284fc795c5f0194730489e1a","modified":1626882206394},{"_id":"source/uploads/20160626/ssh.png","hash":"14ab5135c964cf9cfd62eb96f26090f550734379","modified":1626882206398},{"_id":"source/uploads/20161016/socketio.png","hash":"44aa5296182df687f19699731b2cc9b60b4da1b0","modified":1626882206446},{"_id":"source/uploads/20161127/generics-payloadListHierarchy.gif","hash":"fa73c769e332e3b6adf76958092811b7e288ba3f","modified":1626882206446},{"_id":"source/uploads/20161127/generics-sampleHierarchy.gif","hash":"1d8a6cc5d726d674e97c0338111b6d9e543c4204","modified":1626882206450},{"_id":"source/uploads/20161127/generics-subtypeRelationship.gif","hash":"fdd89a29c7b104828a1c9d355fc5c2e39a12db1d","modified":1626882206454},{"_id":"source/uploads/20161203/generics-listParent.gif","hash":"ba211809c085657f6ea7dd2af5b7c7c573a7868e","modified":1626882206454},{"_id":"source/uploads/20170108/secondarynamenode.png","hash":"f417a675b6122a3576507458bb17471c70bde48a","modified":1626882206458},{"_id":"source/uploads/20170303/namenode.png","hash":"002b85cf0964195ea072db75c9fd2383da9d52d1","modified":1626882206470},{"_id":"source/uploads/20161203/generics-wildcardSubtyping.gif","hash":"e4e6a448efff06409737de76a24c959b44f83676","modified":1626882206458},{"_id":"source/uploads/20170303/mapreduce-jobhistory-server.png","hash":"abdb99dede069f580d42dd993e5bc54a83bddb3f","modified":1626882206466},{"_id":"source/uploads/20170224/no-resourcemanager-to-stop.png","hash":"4961bfe88e0d94ea5a3a35f4e203549109485ca4","modified":1626882206458},{"_id":"source/uploads/20170327/hdfs-ha-qjm.png","hash":"7adf8cafa782b0a8747a627ae13be4d0b9402761","modified":1626882206478},{"_id":"source/uploads/20170327/namenode1.png","hash":"44f475089c74948dd4a4c19f4ab9ce5b12e58469","modified":1626882206478},{"_id":"source/uploads/20170327/namenode2.png","hash":"5b04e3714cfe329cb08452f5d8c7db44648f9168","modified":1626882206478},{"_id":"source/uploads/20171019/cd_dvd.png","hash":"86aa86d5f4914b35c733f6566bdb8655783a6164","modified":1626882206482},{"_id":"source/uploads/20170704/mysql-data.png","hash":"eb346bf48e529a1daaadb2676179f1d7bda1d327","modified":1626882206482},{"_id":"source/uploads/20171019/java_update_tip.png","hash":"20de255adc1f71619532f317706b8ce800692cca","modified":1626882206482},{"_id":"source/uploads/20171019/launch_ask.png","hash":"4d074bfd4427a03fa86c44e3e1607273908f4f32","modified":1626882206482},{"_id":"source/uploads/20171019/security_warning.png","hash":"29a13b5049a4234aaac226b89b6d0f5cab982af1","modified":1626882206482},{"_id":"source/uploads/20171019/security_warning2.png","hash":"cfe4c625fd22c0b15957ff08d9e2afbc0cf189f1","modified":1626882206482},{"_id":"source/uploads/20171019/virtual_medium.png","hash":"f51765b2aa9108d41d53df1620e6d1524d16e529","modified":1626882206486},{"_id":"source/uploads/20180928/azkaban-executor-activate.png","hash":"eb6a7c3af0824a23147af108505c348810d113ca","modified":1626882206486},{"_id":"source/uploads/20181214/namenodeInfo.png","hash":"638e39f86c2ffaa5d00f94aba63ea63906d91bfd","modified":1626882206490},{"_id":"source/uploads/20190301/jenkinsUpdateSite.png","hash":"1075107a53e6a4a5eb369642932d546c72c14c9e","modified":1626882206494},{"_id":"source/uploads/20190314/GitLabWebHook.png","hash":"410c0058810b463b420abc273acc6ccc7a911e88","modified":1626882206494},{"_id":"source/uploads/20190314/JenkinsProject1.png","hash":"a9af7fd03a17435527909976de54539a1d369df4","modified":1626882206494},{"_id":"source/uploads/20190314/数据仓库开发测试代码分发流程.png","hash":"f5f65b5c6002f73d409aea82608fd0a775bd0eda","modified":1626882206494},{"_id":"source/uploads/20190314/JenkinsProject2.png","hash":"8bf15276ccc5ce742bab0d74f020c6ff8a938c4f","modified":1626882206494},{"_id":"source/uploads/20190320/jenkins-LD_LIBRARY_PATH.png","hash":"5c9d7127aa096434940260b194a079427068f97c","modified":1626882206498},{"_id":"source/uploads/20190315/Rsyslog依赖关系.png","hash":"76a9ff73966ce73afb848339c590f0352890c9bf","modified":1626882206494},{"_id":"source/uploads/20190320/jenkins-path.png","hash":"2030c7175f566ac37b5ec7f29d03856cb245f5a9","modified":1626882206498},{"_id":"source/uploads/20190507/continue-connecting.png","hash":"28319510be38cfca8f02a04de501485698865b1f","modified":1626882206498},{"_id":"source/uploads/20190507/lftp-sftp.png","hash":"8d720baf19f501cf39e45db56ac1d808ec6a5070","modified":1626882206498},{"_id":"source/uploads/20191017/vs-code-terminal-font.png","hash":"562fc731e9f66f3d5187f323798082901e8084e4","modified":1626882206498},{"_id":"source/uploads/20181214/application_1533196506314_4460157/192-168-72-12_27463.txt","hash":"c2828424e2894ff58af79129540748ff27c62a82","modified":1626882206486},{"_id":"source/uploads/20210419/ctaException.png","hash":"26e1e2ca05814660c92ab1d120ccebfbf5ab6ade","modified":1626882206606},{"_id":"source/uploads/20181214/application_1533196506314_4460157/192-168-72-24_16310.txt","hash":"49b55ff0b089190380d855f87caa26a845b50a19","modified":1626882206486},{"_id":"source/uploads/20210520/KafkaSource.java","hash":"5fb075b29c0cc07460939ade3aecc599c77438be","modified":1626882206634},{"_id":"source/uploads/20181214/application_1533196506314_4460157/192-168-72-73_2363.txt","hash":"d340b6f800ab42e99e81d1642f9db759f7c71ded","modified":1626882206490},{"_id":"source/uploads/20181214/application_1533196506314_4460157/192-168-72-23_18284.txt","hash":"de2fa06511c83b50f45e7746b1611b8169eed68d","modified":1626882206486},{"_id":"source/uploads/20181214/application_1533196506314_4460157/192-168-72-84_13498.txt","hash":"8c1f787def2cbbf51dad550a93b67db70b8ed456","modified":1626882206490},{"_id":"source/uploads/20181214/application_1533196506314_4460157/192-168-72-88_24481.txt","hash":"808e561d1fc03255488a03d65bf91bc626924f77","modified":1626882206490},{"_id":"source/uploads/20181214/application_1533196506314_4460157/192-168-72-94_54353.txt","hash":"26f90b3f1a021a8affc7bb0d03e1785e489a5f85","modified":1626882206490},{"_id":"source/images/avatar.jpg","hash":"5042d0fcde0c5061c1f736771bd1ab2febcbd6d3","modified":1626882206502},{"_id":"source/images/safari-pinned-tab.svg","hash":"0943388790702fc4c987af2b9f7a2681cb57d0de","modified":1626925625000},{"_id":"source/uploads/aggregation-pipeline.png","hash":"2eacb6e047b5e152a51d6c1406fc572ee9532574","modified":1626882206502},{"_id":"source/uploads/map-reduce.png","hash":"1db458eade7381d19ff1f0bd298b5121ac25e94a","modified":1626882206502},{"_id":"source/uploads/20160518/mysql-information-schema-partitions.png","hash":"feb17cc7c6e9dbc7476fa7095adb74b29c831fde","modified":1626882206386},{"_id":"source/uploads/20160522/virtualbox-run-apps.png","hash":"d1a8d08f8e2785dcabbb25c016e81d37ff22eb43","modified":1626882206386},{"_id":"source/uploads/20160522/virtualbox-run-spotlight.png","hash":"94ab381f36c37923e0cac8fd9134ab9daafe937e","modified":1626882206390},{"_id":"source/uploads/20160910/map-reduce-locations-conf.png","hash":"4f1149e5768391f333b24d645e998f18b49af522","modified":1626882206418},{"_id":"source/uploads/20170303/resourcemanager.png","hash":"083741194cbbb2435d5f181efded0e00010428bf","modified":1626882206478},{"_id":"source/uploads/20170704/hdfs-data.png","hash":"3a58ebb18c07af046e204f22bf6e23e867572612","modified":1626882206482},{"_id":"source/uploads/20171019/virtual_console_pic.png","hash":"92641687395cb628309ed990fb2768bae4cb1d5a","modified":1626882206486},{"_id":"source/uploads/20181214/sqoop.txt","hash":"06e698ce5850edccdc3339e3bce20d5424e5cd1e","modified":1626882206494},{"_id":"source/uploads/20191017/vs-code-terminal.png","hash":"8fb3cf0a2e4361ca7e99254956512de5c0eeffae","modified":1626882206498},{"_id":"source/uploads/20160518/mysql-alter-partition.png","hash":"4037ca01d1de1fad177215ad09ab613bd6191dbd","modified":1626882206382},{"_id":"source/uploads/20160910/hadoop-installation-directory.png","hash":"34021d8f032a57591b92efc6881210ce6500de7a","modified":1626882206414},{"_id":"source/uploads/20160910/map-reduce-locations-error.png","hash":"84f8415b6b62d7eb9d4b08ceb5df96efe62f726d","modified":1626882206422},{"_id":"source/uploads/20160910/word-count.png","hash":"592f9594e5d9675f409d430fba18371ca7356d61","modified":1626882206426},{"_id":"source/uploads/20160910/word-count-result.png","hash":"594fe515486a8abef6c7265c188db7539cf89442","modified":1626882206426},{"_id":"source/uploads/20171019/virtual_console.png","hash":"14c6fcf43ccce1fa64d05cdd23bc45a84569fd1d","modified":1626882206482},{"_id":"source/uploads/20181214/namenodeLogs.png","hash":"641deba318fea0031815d1820820ac38fb44bee4","modified":1626882206490},{"_id":"source/uploads/20190406/omhttp.so","hash":"8c8bd8d82b18d86db6d037c05295f5b8fdfee05e","modified":1626882206498},{"_id":"source/uploads/20181214/application_1533196506314_4460157/192-168-72-93_53778.txt","hash":"bbf43930dd7f107b5a7b6cdd8adfb5054ba7f600","modified":1626882206490},{"_id":"source/uploads/20151122/vbox4.png","hash":"7fd5b3612bd04a51ff6efb34946c62771ce15639","modified":1626882206378},{"_id":"source/uploads/20151122/vbox3.png","hash":"13cb18906975556ff7d7e766376df69dfb32ae17","modified":1626882206362},{"_id":"source/uploads/20160416/mongodb_start_log.png","hash":"b475fdec27a4a70801dbd830d4e5554b23e9055a","modified":1626882206378},{"_id":"source/uploads/20160604/video.mp4","hash":"dcdf726ea725ed12c2b0c12cc73198802d9e8be7","modified":1626882206394},{"_id":"source/uploads/20160910/debug-configuration.png","hash":"35b47e6c7151efed90f258021058d5b0c547783c","modified":1626882206398},{"_id":"source/uploads/20160911/remote-java-application-debug-conf.png","hash":"6efa3354a0c486578e956e104ab05e4c223e6c02","modified":1626882206430},{"_id":"source/uploads/20171125/eclipse-sqoop-import.png","hash":"24a004788c50a667301c8b5ec0676cd4cbffe506","modified":1626882206486},{"_id":"source/images/android-chrome-512x512.png","hash":"c438e2fdede7f2e6dd3a96feb2330648a69b30bb","modified":1626925623000},{"_id":"source/uploads/20151122/vbox2.png","hash":"4f8733643e9d8c818f4dd666f8e418ba6a7461a0","modified":1626882206362},{"_id":"source/uploads/20160910/argument.png","hash":"d1b258d02b26d72035e450bd9b811a1220e78a06","modified":1626882206398},{"_id":"source/uploads/20160910/input-path-not-exists.png","hash":"5d53691571bdaf9650c279d009add084647c9598","modified":1626882206418},{"_id":"source/uploads/20160910/dfs-locations.png","hash":"c8866787bcdb75d278e8c221e0da5cb646de93a6","modified":1626882206406},{"_id":"source/uploads/20160522/virtualbox-install-dmg.png","hash":"907651b9df2c2542349ce1874ae7ca7071af94a4","modified":1626882206386},{"_id":"source/uploads/20160604/video2.mp4","hash":"5e2b175efb47f3f7cd45adf9512e67ae69ef9ed5","modified":1626882206398},{"_id":"source/uploads/20151122/vbox1.png","hash":"d4af2019cf183a98dfcfafd5ca446ebc7cdf5622","modified":1626882206358},{"_id":"source/uploads/20160910/map-reduce-locations-view.png","hash":"55e5cd18f5ee65bb73244951eb7983eec823b365","modified":1626882206426},{"_id":"source/uploads/20160910/debug-view.png","hash":"7d7843da9782b5b63d1f8a2693ead1cf4d6f570b","modified":1626882206402},{"_id":"source/uploads/20160911/namenode-breakpoints.png","hash":"bea8501c74c1c074fa340cbf34222e86dac999df","modified":1626882206430},{"_id":"source/uploads/20191023/rtl8821ce.zip","hash":"479701f07bac633964211df05e0e35b519029006","modified":1626882206598},{"_id":"source/categories/index.md","hash":"30b9d6c8831c4142d0f60c2afabcdabcf1531d13","modified":1626935815005},{"_id":"source/tags/index.md","hash":"9a6f7f2227b6047618a90434e6cad629905a8c67","modified":1626935826374}],"Category":[{"name":"工具箱","_id":"ckrdmphl10002itd3g51r6c2s"},{"name":"大数据","_id":"ckrdmphl60007itd3d7k2ekpn"},{"name":"Requests","parent":"ckrdmphl10002itd3g51r6c2s","_id":"ckrdmphld000hitd38twhb5sw"},{"name":"开发工具","_id":"ckrdmphlh000litd30oyhfkeb"},{"name":"Azkaban","parent":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphlk000sitd38ca2bjm8"},{"name":"语言","_id":"ckrdmphlo000zitd36325ex8g"},{"name":"Rsyslog","parent":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphlq0016itd392hc3jb3"},{"name":"操作系统","_id":"ckrdmphlv001citd32wx0c5wu"},{"name":"Nginx","parent":"ckrdmphlh000litd30oyhfkeb","_id":"ckrdmphm90024itd38jug783d"},{"name":"数据库","_id":"ckrdmphme002gitd3c07whctp"},{"name":"开发","_id":"ckrdmphmh002pitd32f3dbd90"},{"name":"Python3","parent":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphmn002zitd3dmqvawqg"},{"name":"Sqoop","parent":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphmv003fitd37eq59bod"},{"name":"Emacs","parent":"ckrdmphlh000litd30oyhfkeb","_id":"ckrdmphmz003nitd382nfb1c0"},{"name":"Linux","parent":"ckrdmphlv001citd32wx0c5wu","_id":"ckrdmphn2003uitd30zcn5mx2"},{"name":"Flume","parent":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphnj0053itd327habqx0"},{"name":"Hadoop","parent":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphnt005sitd3f93f81tu"},{"name":"MongoDB","parent":"ckrdmphme002gitd3c07whctp","_id":"ckrdmphos007zitd3av7c4e8d"},{"name":"环境搭建","parent":"ckrdmphmh002pitd32f3dbd90","_id":"ckrdmphp0008oitd30ofe3icl"},{"name":"MySQL","parent":"ckrdmphme002gitd3c07whctp","_id":"ckrdmphp8009ditd34kim7a8b"},{"name":"Markdown","parent":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphpw00apitd3abusfzfz"},{"name":"Hive","parent":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphq000ayitd3fj842bd3"},{"name":"Ranger","parent":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphqv00d8itd39jpo12sh"},{"name":"Hue","parent":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphr300dxitd3amim1w6t"},{"name":"JMeter","parent":"ckrdmphl10002itd3g51r6c2s","_id":"ckrdmphrf00esitd3fcy056pv"},{"name":"Java","parent":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphrj00f2itd3984lahfj"},{"name":"Jenkins","parent":"ckrdmphlh000litd30oyhfkeb","_id":"ckrdmphrs00fritd39njve9ao"},{"name":"CI","parent":"ckrdmphmh002pitd32f3dbd90","_id":"ckrdmphru00g0itd3ao9oaowd"},{"name":"Jetty","parent":"ckrdmphlh000litd30oyhfkeb","_id":"ckrdmphsd00gwitd3dopo5tsf"},{"name":"Kafka","parent":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphsi00h5itd35d4fas04"},{"name":"算法","_id":"ckrdmphsl00heitd3augqh62c"},{"name":"Shell","parent":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphsn00hnitd30w3x5ko5"},{"name":"Node.js","parent":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphsr00hvitd3bhxl5evi"},{"name":"Mac OS","parent":"ckrdmphlv001citd32wx0c5wu","_id":"ckrdmphsy00icitd371gmdqc5"},{"name":"虚拟机","parent":"ckrdmphlv001citd32wx0c5wu","_id":"ckrdmpht200ilitd3h55a0pyn"},{"name":"Maven","parent":"ckrdmphlh000litd30oyhfkeb","_id":"ckrdmphtg00jiitd37x246h6z"},{"name":"Redis","parent":"ckrdmphl10002itd3g51r6c2s","_id":"ckrdmphu300luitd3fom44yt1"},{"name":"Socket.IO","parent":"ckrdmphl10002itd3g51r6c2s","_id":"ckrdmphu900moitd3erzqcate"},{"name":"Ubuntu","parent":"ckrdmphlv001citd32wx0c5wu","_id":"ckrdmphue00nditd37ljn7n2f"},{"name":"Lua","parent":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphuf00nkitd33qds82za"},{"name":"Visual Studio Code","parent":"ckrdmphlh000litd30oyhfkeb","_id":"ckrdmphuk00oditd3cp4s49a9"},{"name":"Zookeeper","parent":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphul00ojitd3as045qeq"},{"name":"DOS","parent":"ckrdmphlv001citd32wx0c5wu","_id":"ckrdmphur00p1itd36woaers4"},{"name":"Node.js","parent":"ckrdmphmh002pitd32f3dbd90","_id":"ckrdmphut00p7itd34c7bas3v"},{"name":"react native","parent":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphux00ptitd3g0743koe"},{"name":"经济学","_id":"ckrdmphv100qaitd344ex2xwx"},{"name":"LRU","parent":"ckrdmphsl00heitd3augqh62c","_id":"ckrdmphv600qtitd3bxcc4w0n"},{"name":"成本","parent":"ckrdmphv100qaitd344ex2xwx","_id":"ckrdmphv700qyitd3gtsg4jd6"}],"Data":[],"Page":[{"title":"分类","date":"2015-11-11T14:23:02.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"title: 分类\ndate: 2015-11-11 22:23:02\ntype: \"categories\"\n---\n","updated":"2021-07-22T06:36:55.005Z","path":"categories/index.html","comments":1,"layout":"page","_id":"ckrejlxh80000wbd332og76zu","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"Tagcloud","date":"2015-11-11T14:12:49.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"title: Tagcloud\ndate: 2015-11-11 22:12:49\ntype: \"tags\"\n---\n","updated":"2021-07-22T06:37:06.374Z","path":"tags/index.html","comments":1,"layout":"page","_id":"ckrejlxhb0001wbd374xrgr02","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"AttributeError: module 'requests' has no attribute 'post'","date":"2019-10-10T06:21:45.000Z","_content":"发现是因为安装的 Requests 模块权限的问题。安装完成后模型权限如下：\n\n    # ls -l ./lib/python3.7/site-packages/requests\n    total 212\n    -rw-r----- 1 root root 19537 Sep 30 17:52 adapters.py\n    -rw-r----- 1 root root  6496 Sep 30 17:52 api.py\n    -rw-r----- 1 root root 10207 Sep 30 17:52 auth.py\n    -rw-r----- 1 root root   453 Sep 30 17:52 certs.py\n    -rw-r----- 1 root root  1782 Sep 30 17:52 compat.py\n    -rw-r----- 1 root root 18430 Sep 30 17:52 cookies.py\n    -rw-r----- 1 root root  3185 Sep 30 17:52 exceptions.py\n    -rw-r----- 1 root root  3515 Sep 30 17:52 help.py\n    -rw-r----- 1 root root   757 Sep 30 17:52 hooks.py\n    -rw-r----- 1 root root  3922 Sep 30 17:52 __init__.py\n    -rw-r----- 1 root root  1096 Sep 30 17:52 _internal_utils.py\n    -rw-r----- 1 root root 34830 Sep 30 17:52 models.py\n    -rw-r----- 1 root root   542 Sep 30 17:52 packages.py\n    drwxr-x--- 2 root root  4096 Oct 10 13:22 __pycache__\n    -rw-r----- 1 root root 29277 Sep 30 17:52 sessions.py\n    -rw-r----- 1 root root  4188 Sep 30 17:52 status_codes.py\n    -rw-r----- 1 root root  3005 Sep 30 17:52 structures.py\n    -rw-r----- 1 root root 30176 Sep 30 17:52 utils.py\n    -rw-r----- 1 root root   436 Sep 30 17:52 __version__.py\n\n修改权限为 755 后问题解决。如下：\n\n    # chmod -R 755 ./lib/python3.7/site-packages/requests\n    # ls -l ./lib/python3.7/site-packages/requests\n    total 212\n    -rwxr-xr-x 1 root root 19537 Sep 30 17:52 adapters.py\n    -rwxr-xr-x 1 root root  6496 Sep 30 17:52 api.py\n    -rwxr-xr-x 1 root root 10207 Sep 30 17:52 auth.py\n    -rwxr-xr-x 1 root root   453 Sep 30 17:52 certs.py\n    -rwxr-xr-x 1 root root  1782 Sep 30 17:52 compat.py\n    -rwxr-xr-x 1 root root 18430 Sep 30 17:52 cookies.py\n    -rwxr-xr-x 1 root root  3185 Sep 30 17:52 exceptions.py\n    -rwxr-xr-x 1 root root  3515 Sep 30 17:52 help.py\n    -rwxr-xr-x 1 root root   757 Sep 30 17:52 hooks.py\n    -rwxr-xr-x 1 root root  3922 Sep 30 17:52 __init__.py\n    -rwxr-xr-x 1 root root  1096 Sep 30 17:52 _internal_utils.py\n    -rwxr-xr-x 1 root root 34830 Sep 30 17:52 models.py\n    -rwxr-xr-x 1 root root   542 Sep 30 17:52 packages.py\n    drwxr-xr-x 2 root root  4096 Oct 10 13:22 __pycache__\n    -rwxr-xr-x 1 root root 29277 Sep 30 17:52 sessions.py\n    -rwxr-xr-x 1 root root  4188 Sep 30 17:52 status_codes.py\n    -rwxr-xr-x 1 root root  3005 Sep 30 17:52 structures.py\n    -rwxr-xr-x 1 root root 30176 Sep 30 17:52 utils.py\n    -rwxr-xr-x 1 root root   436 Sep 30 17:52 __version__.py","source":"_posts/AttributeError-module-requests-has-no-attribute-post.md","raw":"title: 'AttributeError: module ''requests'' has no attribute ''post'''\ndate: 2019-10-10 14:21:45\ntags:\n- Requests\n- Python3\ncategories:\n- 工具箱\n- Requests\n---\n发现是因为安装的 Requests 模块权限的问题。安装完成后模型权限如下：\n\n    # ls -l ./lib/python3.7/site-packages/requests\n    total 212\n    -rw-r----- 1 root root 19537 Sep 30 17:52 adapters.py\n    -rw-r----- 1 root root  6496 Sep 30 17:52 api.py\n    -rw-r----- 1 root root 10207 Sep 30 17:52 auth.py\n    -rw-r----- 1 root root   453 Sep 30 17:52 certs.py\n    -rw-r----- 1 root root  1782 Sep 30 17:52 compat.py\n    -rw-r----- 1 root root 18430 Sep 30 17:52 cookies.py\n    -rw-r----- 1 root root  3185 Sep 30 17:52 exceptions.py\n    -rw-r----- 1 root root  3515 Sep 30 17:52 help.py\n    -rw-r----- 1 root root   757 Sep 30 17:52 hooks.py\n    -rw-r----- 1 root root  3922 Sep 30 17:52 __init__.py\n    -rw-r----- 1 root root  1096 Sep 30 17:52 _internal_utils.py\n    -rw-r----- 1 root root 34830 Sep 30 17:52 models.py\n    -rw-r----- 1 root root   542 Sep 30 17:52 packages.py\n    drwxr-x--- 2 root root  4096 Oct 10 13:22 __pycache__\n    -rw-r----- 1 root root 29277 Sep 30 17:52 sessions.py\n    -rw-r----- 1 root root  4188 Sep 30 17:52 status_codes.py\n    -rw-r----- 1 root root  3005 Sep 30 17:52 structures.py\n    -rw-r----- 1 root root 30176 Sep 30 17:52 utils.py\n    -rw-r----- 1 root root   436 Sep 30 17:52 __version__.py\n\n修改权限为 755 后问题解决。如下：\n\n    # chmod -R 755 ./lib/python3.7/site-packages/requests\n    # ls -l ./lib/python3.7/site-packages/requests\n    total 212\n    -rwxr-xr-x 1 root root 19537 Sep 30 17:52 adapters.py\n    -rwxr-xr-x 1 root root  6496 Sep 30 17:52 api.py\n    -rwxr-xr-x 1 root root 10207 Sep 30 17:52 auth.py\n    -rwxr-xr-x 1 root root   453 Sep 30 17:52 certs.py\n    -rwxr-xr-x 1 root root  1782 Sep 30 17:52 compat.py\n    -rwxr-xr-x 1 root root 18430 Sep 30 17:52 cookies.py\n    -rwxr-xr-x 1 root root  3185 Sep 30 17:52 exceptions.py\n    -rwxr-xr-x 1 root root  3515 Sep 30 17:52 help.py\n    -rwxr-xr-x 1 root root   757 Sep 30 17:52 hooks.py\n    -rwxr-xr-x 1 root root  3922 Sep 30 17:52 __init__.py\n    -rwxr-xr-x 1 root root  1096 Sep 30 17:52 _internal_utils.py\n    -rwxr-xr-x 1 root root 34830 Sep 30 17:52 models.py\n    -rwxr-xr-x 1 root root   542 Sep 30 17:52 packages.py\n    drwxr-xr-x 2 root root  4096 Oct 10 13:22 __pycache__\n    -rwxr-xr-x 1 root root 29277 Sep 30 17:52 sessions.py\n    -rwxr-xr-x 1 root root  4188 Sep 30 17:52 status_codes.py\n    -rwxr-xr-x 1 root root  3005 Sep 30 17:52 structures.py\n    -rwxr-xr-x 1 root root 30176 Sep 30 17:52 utils.py\n    -rwxr-xr-x 1 root root   436 Sep 30 17:52 __version__.py","slug":"AttributeError-module-requests-has-no-attribute-post","published":1,"updated":"2021-07-19T16:28:00.224Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphku0000itd3cgmh1jp7","content":"<p>发现是因为安装的 Requests 模块权限的问题。安装完成后模型权限如下：</p>\n<pre><code># ls -l ./lib/python3.7/site-packages/requests\ntotal 212\n-rw-r----- 1 root root 19537 Sep 30 17:52 adapters.py\n-rw-r----- 1 root root  6496 Sep 30 17:52 api.py\n-rw-r----- 1 root root 10207 Sep 30 17:52 auth.py\n-rw-r----- 1 root root   453 Sep 30 17:52 certs.py\n-rw-r----- 1 root root  1782 Sep 30 17:52 compat.py\n-rw-r----- 1 root root 18430 Sep 30 17:52 cookies.py\n-rw-r----- 1 root root  3185 Sep 30 17:52 exceptions.py\n-rw-r----- 1 root root  3515 Sep 30 17:52 help.py\n-rw-r----- 1 root root   757 Sep 30 17:52 hooks.py\n-rw-r----- 1 root root  3922 Sep 30 17:52 __init__.py\n-rw-r----- 1 root root  1096 Sep 30 17:52 _internal_utils.py\n-rw-r----- 1 root root 34830 Sep 30 17:52 models.py\n-rw-r----- 1 root root   542 Sep 30 17:52 packages.py\ndrwxr-x--- 2 root root  4096 Oct 10 13:22 __pycache__\n-rw-r----- 1 root root 29277 Sep 30 17:52 sessions.py\n-rw-r----- 1 root root  4188 Sep 30 17:52 status_codes.py\n-rw-r----- 1 root root  3005 Sep 30 17:52 structures.py\n-rw-r----- 1 root root 30176 Sep 30 17:52 utils.py\n-rw-r----- 1 root root   436 Sep 30 17:52 __version__.py\n</code></pre>\n<p>修改权限为 755 后问题解决。如下：</p>\n<pre><code># chmod -R 755 ./lib/python3.7/site-packages/requests\n# ls -l ./lib/python3.7/site-packages/requests\ntotal 212\n-rwxr-xr-x 1 root root 19537 Sep 30 17:52 adapters.py\n-rwxr-xr-x 1 root root  6496 Sep 30 17:52 api.py\n-rwxr-xr-x 1 root root 10207 Sep 30 17:52 auth.py\n-rwxr-xr-x 1 root root   453 Sep 30 17:52 certs.py\n-rwxr-xr-x 1 root root  1782 Sep 30 17:52 compat.py\n-rwxr-xr-x 1 root root 18430 Sep 30 17:52 cookies.py\n-rwxr-xr-x 1 root root  3185 Sep 30 17:52 exceptions.py\n-rwxr-xr-x 1 root root  3515 Sep 30 17:52 help.py\n-rwxr-xr-x 1 root root   757 Sep 30 17:52 hooks.py\n-rwxr-xr-x 1 root root  3922 Sep 30 17:52 __init__.py\n-rwxr-xr-x 1 root root  1096 Sep 30 17:52 _internal_utils.py\n-rwxr-xr-x 1 root root 34830 Sep 30 17:52 models.py\n-rwxr-xr-x 1 root root   542 Sep 30 17:52 packages.py\ndrwxr-xr-x 2 root root  4096 Oct 10 13:22 __pycache__\n-rwxr-xr-x 1 root root 29277 Sep 30 17:52 sessions.py\n-rwxr-xr-x 1 root root  4188 Sep 30 17:52 status_codes.py\n-rwxr-xr-x 1 root root  3005 Sep 30 17:52 structures.py\n-rwxr-xr-x 1 root root 30176 Sep 30 17:52 utils.py\n-rwxr-xr-x 1 root root   436 Sep 30 17:52 __version__.py\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>发现是因为安装的 Requests 模块权限的问题。安装完成后模型权限如下：</p>\n<pre><code># ls -l ./lib/python3.7/site-packages/requests\ntotal 212\n-rw-r----- 1 root root 19537 Sep 30 17:52 adapters.py\n-rw-r----- 1 root root  6496 Sep 30 17:52 api.py\n-rw-r----- 1 root root 10207 Sep 30 17:52 auth.py\n-rw-r----- 1 root root   453 Sep 30 17:52 certs.py\n-rw-r----- 1 root root  1782 Sep 30 17:52 compat.py\n-rw-r----- 1 root root 18430 Sep 30 17:52 cookies.py\n-rw-r----- 1 root root  3185 Sep 30 17:52 exceptions.py\n-rw-r----- 1 root root  3515 Sep 30 17:52 help.py\n-rw-r----- 1 root root   757 Sep 30 17:52 hooks.py\n-rw-r----- 1 root root  3922 Sep 30 17:52 __init__.py\n-rw-r----- 1 root root  1096 Sep 30 17:52 _internal_utils.py\n-rw-r----- 1 root root 34830 Sep 30 17:52 models.py\n-rw-r----- 1 root root   542 Sep 30 17:52 packages.py\ndrwxr-x--- 2 root root  4096 Oct 10 13:22 __pycache__\n-rw-r----- 1 root root 29277 Sep 30 17:52 sessions.py\n-rw-r----- 1 root root  4188 Sep 30 17:52 status_codes.py\n-rw-r----- 1 root root  3005 Sep 30 17:52 structures.py\n-rw-r----- 1 root root 30176 Sep 30 17:52 utils.py\n-rw-r----- 1 root root   436 Sep 30 17:52 __version__.py\n</code></pre>\n<p>修改权限为 755 后问题解决。如下：</p>\n<pre><code># chmod -R 755 ./lib/python3.7/site-packages/requests\n# ls -l ./lib/python3.7/site-packages/requests\ntotal 212\n-rwxr-xr-x 1 root root 19537 Sep 30 17:52 adapters.py\n-rwxr-xr-x 1 root root  6496 Sep 30 17:52 api.py\n-rwxr-xr-x 1 root root 10207 Sep 30 17:52 auth.py\n-rwxr-xr-x 1 root root   453 Sep 30 17:52 certs.py\n-rwxr-xr-x 1 root root  1782 Sep 30 17:52 compat.py\n-rwxr-xr-x 1 root root 18430 Sep 30 17:52 cookies.py\n-rwxr-xr-x 1 root root  3185 Sep 30 17:52 exceptions.py\n-rwxr-xr-x 1 root root  3515 Sep 30 17:52 help.py\n-rwxr-xr-x 1 root root   757 Sep 30 17:52 hooks.py\n-rwxr-xr-x 1 root root  3922 Sep 30 17:52 __init__.py\n-rwxr-xr-x 1 root root  1096 Sep 30 17:52 _internal_utils.py\n-rwxr-xr-x 1 root root 34830 Sep 30 17:52 models.py\n-rwxr-xr-x 1 root root   542 Sep 30 17:52 packages.py\ndrwxr-xr-x 2 root root  4096 Oct 10 13:22 __pycache__\n-rwxr-xr-x 1 root root 29277 Sep 30 17:52 sessions.py\n-rwxr-xr-x 1 root root  4188 Sep 30 17:52 status_codes.py\n-rwxr-xr-x 1 root root  3005 Sep 30 17:52 structures.py\n-rwxr-xr-x 1 root root 30176 Sep 30 17:52 utils.py\n-rwxr-xr-x 1 root root   436 Sep 30 17:52 __version__.py\n</code></pre>\n"},{"title":"Azkaban 3.52.0 搭建遇到的问题","date":"2018-09-28T06:21:00.000Z","_content":"\n### Mysql数据初始化\n\n如果 MySQL 使用库表字符集使用 UTF-8，在初始化 Azkaban 的 MySQL 库时报错如下：\n\n    ERROR 1071 (42000): Specified key was too long; max key length is 767 bytes\n    ERROR 1146 (42S02): Table 'azkaban3.execution_jobs' doesn't exist\n    ERROR 1071 (42000): Specified key was too long; max key length is 767 bytes\n    ERROR 1146 (42S02): Table 'azkaban3.execution_logs' doesn't exist\n    ERROR 1146 (42S02): Table 'azkaban3.execution_logs' doesn't exist\n    ERROR 1146 (42S02): Table 'azkaban3.execution_logs' doesn't exist\n    \n这个是因为表 execution_logs 的主键总的长度字符超过 767 个字节导致的。解决方法是将该表的编码改为 Latin1。\n\n### 通过 curl 激活 executor\n\n在执行官方文档命令时异常如下：\n\n    $ curl -G \"localhost:$(<./executor.port)/executor?action=activate\" && echo\n    curl: (7) Failed connect to 172.16.72.69:443; No route to host\n\n我的解决方法是从安装目录 azkaban-exec-server-3.52.0 下的文件 executor.port 中找到端口号，然后从浏览器中进行激活。如下图：\n\n![azkaban executor activate](/uploads/20180928/azkaban-executor-activate.png)\n\n### 支持 SSL\n\n如要通过 Azkaban 的 REST API 进行某些操作的话，则需要 Azkaban web server 支持 SSL。\n\n使用下面的命令创建 keystore：\n\n    keytool -keystore keystore -alias azkaban -genkey -keyalg RSA\n    \n根据提示数据相应的信息即可。命令执行完成后会在当前目录生成 keystore 证书文件，将 keystore 考贝到 Azkaban web server 根目录中。修改 conf/azkaban.properties 中的配置如下：\n\n    # Azkaban Jetty server properties.\n    jetty.use.ssl=true\n    jetty.maxThreads=25\n    jetty.ssl.port=8443\n    jetty.keystore=keystore\n    jetty.password=azkaban\n    jetty.keypassword=azkaban\n    jetty.truststore=keystore\n    jetty.trustpassword=azkaban\n","source":"_posts/Azkaban-3-52-0-搭建遇到的问题.md","raw":"title: Azkaban 3.52.0 搭建遇到的问题\ntags:\n  - 大数据\n  - Azkaban\ncategories:\n  - 大数据\n  - Azkaban\ndate: 2018-09-28 14:21:00\n---\n\n### Mysql数据初始化\n\n如果 MySQL 使用库表字符集使用 UTF-8，在初始化 Azkaban 的 MySQL 库时报错如下：\n\n    ERROR 1071 (42000): Specified key was too long; max key length is 767 bytes\n    ERROR 1146 (42S02): Table 'azkaban3.execution_jobs' doesn't exist\n    ERROR 1071 (42000): Specified key was too long; max key length is 767 bytes\n    ERROR 1146 (42S02): Table 'azkaban3.execution_logs' doesn't exist\n    ERROR 1146 (42S02): Table 'azkaban3.execution_logs' doesn't exist\n    ERROR 1146 (42S02): Table 'azkaban3.execution_logs' doesn't exist\n    \n这个是因为表 execution_logs 的主键总的长度字符超过 767 个字节导致的。解决方法是将该表的编码改为 Latin1。\n\n### 通过 curl 激活 executor\n\n在执行官方文档命令时异常如下：\n\n    $ curl -G \"localhost:$(<./executor.port)/executor?action=activate\" && echo\n    curl: (7) Failed connect to 172.16.72.69:443; No route to host\n\n我的解决方法是从安装目录 azkaban-exec-server-3.52.0 下的文件 executor.port 中找到端口号，然后从浏览器中进行激活。如下图：\n\n![azkaban executor activate](/uploads/20180928/azkaban-executor-activate.png)\n\n### 支持 SSL\n\n如要通过 Azkaban 的 REST API 进行某些操作的话，则需要 Azkaban web server 支持 SSL。\n\n使用下面的命令创建 keystore：\n\n    keytool -keystore keystore -alias azkaban -genkey -keyalg RSA\n    \n根据提示数据相应的信息即可。命令执行完成后会在当前目录生成 keystore 证书文件，将 keystore 考贝到 Azkaban web server 根目录中。修改 conf/azkaban.properties 中的配置如下：\n\n    # Azkaban Jetty server properties.\n    jetty.use.ssl=true\n    jetty.maxThreads=25\n    jetty.ssl.port=8443\n    jetty.keystore=keystore\n    jetty.password=azkaban\n    jetty.keypassword=azkaban\n    jetty.truststore=keystore\n    jetty.trustpassword=azkaban\n","slug":"Azkaban-3-52-0-搭建遇到的问题","published":1,"updated":"2021-07-19T16:28:00.228Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphkz0001itd37n6yah8m","content":"<h3 id=\"Mysql数据初始化\"><a href=\"#Mysql数据初始化\" class=\"headerlink\" title=\"Mysql数据初始化\"></a>Mysql数据初始化</h3><p>如果 MySQL 使用库表字符集使用 UTF-8，在初始化 Azkaban 的 MySQL 库时报错如下：</p>\n<pre><code>ERROR 1071 (42000): Specified key was too long; max key length is 767 bytes\nERROR 1146 (42S02): Table &#39;azkaban3.execution_jobs&#39; doesn&#39;t exist\nERROR 1071 (42000): Specified key was too long; max key length is 767 bytes\nERROR 1146 (42S02): Table &#39;azkaban3.execution_logs&#39; doesn&#39;t exist\nERROR 1146 (42S02): Table &#39;azkaban3.execution_logs&#39; doesn&#39;t exist\nERROR 1146 (42S02): Table &#39;azkaban3.execution_logs&#39; doesn&#39;t exist\n</code></pre>\n<p>这个是因为表 execution_logs 的主键总的长度字符超过 767 个字节导致的。解决方法是将该表的编码改为 Latin1。</p>\n<h3 id=\"通过-curl-激活-executor\"><a href=\"#通过-curl-激活-executor\" class=\"headerlink\" title=\"通过 curl 激活 executor\"></a>通过 curl 激活 executor</h3><p>在执行官方文档命令时异常如下：</p>\n<pre><code>$ curl -G &quot;localhost:$(&lt;./executor.port)/executor?action=activate&quot; &amp;&amp; echo\ncurl: (7) Failed connect to 172.16.72.69:443; No route to host\n</code></pre>\n<p>我的解决方法是从安装目录 azkaban-exec-server-3.52.0 下的文件 executor.port 中找到端口号，然后从浏览器中进行激活。如下图：</p>\n<p><img src=\"/uploads/20180928/azkaban-executor-activate.png\" alt=\"azkaban executor activate\"></p>\n<h3 id=\"支持-SSL\"><a href=\"#支持-SSL\" class=\"headerlink\" title=\"支持 SSL\"></a>支持 SSL</h3><p>如要通过 Azkaban 的 REST API 进行某些操作的话，则需要 Azkaban web server 支持 SSL。</p>\n<p>使用下面的命令创建 keystore：</p>\n<pre><code>keytool -keystore keystore -alias azkaban -genkey -keyalg RSA\n</code></pre>\n<p>根据提示数据相应的信息即可。命令执行完成后会在当前目录生成 keystore 证书文件，将 keystore 考贝到 Azkaban web server 根目录中。修改 conf/azkaban.properties 中的配置如下：</p>\n<pre><code># Azkaban Jetty server properties.\njetty.use.ssl=true\njetty.maxThreads=25\njetty.ssl.port=8443\njetty.keystore=keystore\njetty.password=azkaban\njetty.keypassword=azkaban\njetty.truststore=keystore\njetty.trustpassword=azkaban\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"Mysql数据初始化\"><a href=\"#Mysql数据初始化\" class=\"headerlink\" title=\"Mysql数据初始化\"></a>Mysql数据初始化</h3><p>如果 MySQL 使用库表字符集使用 UTF-8，在初始化 Azkaban 的 MySQL 库时报错如下：</p>\n<pre><code>ERROR 1071 (42000): Specified key was too long; max key length is 767 bytes\nERROR 1146 (42S02): Table &#39;azkaban3.execution_jobs&#39; doesn&#39;t exist\nERROR 1071 (42000): Specified key was too long; max key length is 767 bytes\nERROR 1146 (42S02): Table &#39;azkaban3.execution_logs&#39; doesn&#39;t exist\nERROR 1146 (42S02): Table &#39;azkaban3.execution_logs&#39; doesn&#39;t exist\nERROR 1146 (42S02): Table &#39;azkaban3.execution_logs&#39; doesn&#39;t exist\n</code></pre>\n<p>这个是因为表 execution_logs 的主键总的长度字符超过 767 个字节导致的。解决方法是将该表的编码改为 Latin1。</p>\n<h3 id=\"通过-curl-激活-executor\"><a href=\"#通过-curl-激活-executor\" class=\"headerlink\" title=\"通过 curl 激活 executor\"></a>通过 curl 激活 executor</h3><p>在执行官方文档命令时异常如下：</p>\n<pre><code>$ curl -G &quot;localhost:$(&lt;./executor.port)/executor?action=activate&quot; &amp;&amp; echo\ncurl: (7) Failed connect to 172.16.72.69:443; No route to host\n</code></pre>\n<p>我的解决方法是从安装目录 azkaban-exec-server-3.52.0 下的文件 executor.port 中找到端口号，然后从浏览器中进行激活。如下图：</p>\n<p><img src=\"/uploads/20180928/azkaban-executor-activate.png\" alt=\"azkaban executor activate\"></p>\n<h3 id=\"支持-SSL\"><a href=\"#支持-SSL\" class=\"headerlink\" title=\"支持 SSL\"></a>支持 SSL</h3><p>如要通过 Azkaban 的 REST API 进行某些操作的话，则需要 Azkaban web server 支持 SSL。</p>\n<p>使用下面的命令创建 keystore：</p>\n<pre><code>keytool -keystore keystore -alias azkaban -genkey -keyalg RSA\n</code></pre>\n<p>根据提示数据相应的信息即可。命令执行完成后会在当前目录生成 keystore 证书文件，将 keystore 考贝到 Azkaban web server 根目录中。修改 conf/azkaban.properties 中的配置如下：</p>\n<pre><code># Azkaban Jetty server properties.\njetty.use.ssl=true\njetty.maxThreads=25\njetty.ssl.port=8443\njetty.keystore=keystore\njetty.password=azkaban\njetty.keypassword=azkaban\njetty.truststore=keystore\njetty.trustpassword=azkaban\n</code></pre>\n"},{"title":"CentOS 6.5 编译 Rsyslog 8.1903.0","date":"2019-04-03T08:43:16.000Z","_content":"\n\n源码下载参考我的另外一片博文：[CentOS 7.3 编译 Rsyslog 8.1903.0](http://zhang-jc.github.io/2019/04/03/CentOS-7-3-%E7%BC%96%E8%AF%91-Rsyslog-8-1903-0/)。\n\n本篇博文从创建构建环境开始填坑/(ㄒoㄒ)/~~。通过上一篇博文可以解决的问题此处不再赘述。\n\n执行配置时出现以下错误：\n\n    # ./configure --enable-omhttp\n    configure: error: cannot find install-sh or install.sh in \".\" \"./..\" \"./../..\"\n\n<!--more-->\n\n先执行 autogen.sh 出现以下错误：\n\n    # sh autogen.sh\n    checking for LIBESTR... configure: error: Package requirements (libestr >= 0.1.9) were not met:\n    No package 'libestr' found\n\n检查没有安装 libestr 库，所以重新安装：\n\n    # yum install libestr libestr-devel\n\n再次执行 autogen.sh 问题依旧o(╯□╰)o。需要编译安装 libestr。\n\n    # wget https://libestr.adiscon.com/files/download/libestr-0.1.9.tar.gz\n    # tar xzvf libestr-0.1.9.tar.gz\n    # ./configure CC=\"gcc -m64\" --prefix=/usr --libdir=/usr/lib64\n    # make\n    # make install\n\n安装完成后重新执行 autogen.sh 问题解决。\n\n问题逐一解决后执行自动配置成功：\n\n    # autoreconf -fvi\n\n执行配置：\n\n    # ./configure --enable-omhttp\n\n配置过程中出现以下错误信息：\n\n    make[3]: Entering directory `/data/kafka/rsyslog-8.1903.0/grammar'\n      LEX    lexer.c\n    make[3]: *** [lexer.c] 错误 1\n    make[3]: Leaving directory `/data/kafka/rsyslog-8.1903.0/grammar'\n    make[2]: *** [all] 错误 2\n    make[2]: Leaving directory `/data/kafka/rsyslog-8.1903.0/grammar'\n    make[1]: *** [all-recursive] 错误 1\n    make[1]: Leaving directory `/data/kafka/rsyslog-8.1903.0'\n    make: *** [all] 错误 2\n\n安装 flex：\n\n    yum install flex\n\n安装后执行 autogen.sh，再次执行 configure，然后执行 make 成功。进入 omhttp 目录执行 make 出现以下错误：\n\n    # cd contrib/omhttp\n    # make\n      CC     omhttp_la-omhttp.lo\n    omhttp.c: 在函数‘curlPostSetup’中:\n    omhttp.c:1554: 错误：‘CURLOPT_TCP_KEEPALIVE’未声明(在此函数内第一次使用)\n    omhttp.c:1554: 错误：(即使在一个函数内多次出现，每个未声明的标识符在其\n    omhttp.c:1554: 错误：所在的函数内也只报告一次。)\n    omhttp.c:1554: 警告：在‘_curl_opt’的声明中，类型默认为‘int’\n    omhttp.c:1558: 错误：‘CURLOPT_TCP_KEEPIDLE’未声明(在此函数内第一次使用)\n    omhttp.c:1558: 警告：在‘_curl_opt’的声明中，类型默认为‘int’\n    omhttp.c:1562: 错误：‘CURLOPT_TCP_KEEPINTVL’未声明(在此函数内第一次使用)\n    omhttp.c:1562: 警告：在‘_curl_opt’的声明中，类型默认为‘int’\n    make: *** [omhttp_la-omhttp.lo] 错误 1\n\n经过 Google 后发现 CURLOPT_TCP_KEEPALIVE 是在 libcurl 7.25.0 之后才定义的。使用以下命令检查系统当前 libcurl 版本：\n\n    # rpm -q libcurl\n    \n  libcurl-7.19.7-52.el6.x86_64\n\n所以需要升级 curl 版本，参考：https://www.jianshu.com/p/14f5f145453e。升级完成后创建以下软连接：\n\n    # ln -s /usr/local/curl/include/curl/ /usr/include/curl\n\n再次执行编译 omhttp，出现以下错误：\n\n    make[2]: Entering directory `/data/kafka/rsyslog-8.1903.0/contrib/omhttp'\n      CC     omhttp_la-omhttp.lo\n    omhttp.c: 在函数‘checkConn’中:\n    omhttp.c:470: 警告：此函数中的‘healthUrl’在使用前可能未初始化\n      CCLD   omhttp.la\n    /usr/bin/ld: cannot find -lcurl\n    collect2: ld returned 1 exit status\n    make[2]: *** [omhttp.la] 错误 1\n    make[2]: Leaving directory `/data/kafka/rsyslog-8.1903.0/contrib/omhttp'\n    make[1]: *** [all-recursive] 错误 1\n    make[1]: Leaving directory `/data/kafka/rsyslog-8.1903.0'\n    make: *** [all] 错误 2\n\n创建以下软连接：\n\n    cd /lib64\n    # ln -s /usr/local/curl/lib/libcurl.so.4.5.0 libcurl.so\n    # ln -s /usr/local/curl/lib/libcurl.so.4.5.0 libcurl.so.4\n\n再次编译 omhttp 成功！！终于大功告成O(∩_∩)O哈哈~","source":"_posts/CentOS-6-5-编译-Rsyslog-8-1903-0.md","raw":"title: CentOS 6.5 编译 Rsyslog 8.1903.0\ndate: 2019-04-03 16:43:16\ntags:\n---\n\n\n源码下载参考我的另外一片博文：[CentOS 7.3 编译 Rsyslog 8.1903.0](http://zhang-jc.github.io/2019/04/03/CentOS-7-3-%E7%BC%96%E8%AF%91-Rsyslog-8-1903-0/)。\n\n本篇博文从创建构建环境开始填坑/(ㄒoㄒ)/~~。通过上一篇博文可以解决的问题此处不再赘述。\n\n执行配置时出现以下错误：\n\n    # ./configure --enable-omhttp\n    configure: error: cannot find install-sh or install.sh in \".\" \"./..\" \"./../..\"\n\n<!--more-->\n\n先执行 autogen.sh 出现以下错误：\n\n    # sh autogen.sh\n    checking for LIBESTR... configure: error: Package requirements (libestr >= 0.1.9) were not met:\n    No package 'libestr' found\n\n检查没有安装 libestr 库，所以重新安装：\n\n    # yum install libestr libestr-devel\n\n再次执行 autogen.sh 问题依旧o(╯□╰)o。需要编译安装 libestr。\n\n    # wget https://libestr.adiscon.com/files/download/libestr-0.1.9.tar.gz\n    # tar xzvf libestr-0.1.9.tar.gz\n    # ./configure CC=\"gcc -m64\" --prefix=/usr --libdir=/usr/lib64\n    # make\n    # make install\n\n安装完成后重新执行 autogen.sh 问题解决。\n\n问题逐一解决后执行自动配置成功：\n\n    # autoreconf -fvi\n\n执行配置：\n\n    # ./configure --enable-omhttp\n\n配置过程中出现以下错误信息：\n\n    make[3]: Entering directory `/data/kafka/rsyslog-8.1903.0/grammar'\n      LEX    lexer.c\n    make[3]: *** [lexer.c] 错误 1\n    make[3]: Leaving directory `/data/kafka/rsyslog-8.1903.0/grammar'\n    make[2]: *** [all] 错误 2\n    make[2]: Leaving directory `/data/kafka/rsyslog-8.1903.0/grammar'\n    make[1]: *** [all-recursive] 错误 1\n    make[1]: Leaving directory `/data/kafka/rsyslog-8.1903.0'\n    make: *** [all] 错误 2\n\n安装 flex：\n\n    yum install flex\n\n安装后执行 autogen.sh，再次执行 configure，然后执行 make 成功。进入 omhttp 目录执行 make 出现以下错误：\n\n    # cd contrib/omhttp\n    # make\n      CC     omhttp_la-omhttp.lo\n    omhttp.c: 在函数‘curlPostSetup’中:\n    omhttp.c:1554: 错误：‘CURLOPT_TCP_KEEPALIVE’未声明(在此函数内第一次使用)\n    omhttp.c:1554: 错误：(即使在一个函数内多次出现，每个未声明的标识符在其\n    omhttp.c:1554: 错误：所在的函数内也只报告一次。)\n    omhttp.c:1554: 警告：在‘_curl_opt’的声明中，类型默认为‘int’\n    omhttp.c:1558: 错误：‘CURLOPT_TCP_KEEPIDLE’未声明(在此函数内第一次使用)\n    omhttp.c:1558: 警告：在‘_curl_opt’的声明中，类型默认为‘int’\n    omhttp.c:1562: 错误：‘CURLOPT_TCP_KEEPINTVL’未声明(在此函数内第一次使用)\n    omhttp.c:1562: 警告：在‘_curl_opt’的声明中，类型默认为‘int’\n    make: *** [omhttp_la-omhttp.lo] 错误 1\n\n经过 Google 后发现 CURLOPT_TCP_KEEPALIVE 是在 libcurl 7.25.0 之后才定义的。使用以下命令检查系统当前 libcurl 版本：\n\n    # rpm -q libcurl\n    \n  libcurl-7.19.7-52.el6.x86_64\n\n所以需要升级 curl 版本，参考：https://www.jianshu.com/p/14f5f145453e。升级完成后创建以下软连接：\n\n    # ln -s /usr/local/curl/include/curl/ /usr/include/curl\n\n再次执行编译 omhttp，出现以下错误：\n\n    make[2]: Entering directory `/data/kafka/rsyslog-8.1903.0/contrib/omhttp'\n      CC     omhttp_la-omhttp.lo\n    omhttp.c: 在函数‘checkConn’中:\n    omhttp.c:470: 警告：此函数中的‘healthUrl’在使用前可能未初始化\n      CCLD   omhttp.la\n    /usr/bin/ld: cannot find -lcurl\n    collect2: ld returned 1 exit status\n    make[2]: *** [omhttp.la] 错误 1\n    make[2]: Leaving directory `/data/kafka/rsyslog-8.1903.0/contrib/omhttp'\n    make[1]: *** [all-recursive] 错误 1\n    make[1]: Leaving directory `/data/kafka/rsyslog-8.1903.0'\n    make: *** [all] 错误 2\n\n创建以下软连接：\n\n    cd /lib64\n    # ln -s /usr/local/curl/lib/libcurl.so.4.5.0 libcurl.so\n    # ln -s /usr/local/curl/lib/libcurl.so.4.5.0 libcurl.so.4\n\n再次编译 omhttp 成功！！终于大功告成O(∩_∩)O哈哈~","slug":"CentOS-6-5-编译-Rsyslog-8-1903-0","published":1,"updated":"2021-07-19T16:28:00.228Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphl30004itd36zvegb54","content":"<p>源码下载参考我的另外一片博文：<a href=\"http://zhang-jc.github.io/2019/04/03/CentOS-7-3-%E7%BC%96%E8%AF%91-Rsyslog-8-1903-0/\">CentOS 7.3 编译 Rsyslog 8.1903.0</a>。</p>\n<p>本篇博文从创建构建环境开始填坑/(ㄒoㄒ)/~~。通过上一篇博文可以解决的问题此处不再赘述。</p>\n<p>执行配置时出现以下错误：</p>\n<pre><code># ./configure --enable-omhttp\nconfigure: error: cannot find install-sh or install.sh in &quot;.&quot; &quot;./..&quot; &quot;./../..&quot;\n</code></pre>\n<span id=\"more\"></span>\n\n<p>先执行 autogen.sh 出现以下错误：</p>\n<pre><code># sh autogen.sh\nchecking for LIBESTR... configure: error: Package requirements (libestr &gt;= 0.1.9) were not met:\nNo package &#39;libestr&#39; found\n</code></pre>\n<p>检查没有安装 libestr 库，所以重新安装：</p>\n<pre><code># yum install libestr libestr-devel\n</code></pre>\n<p>再次执行 autogen.sh 问题依旧o(╯□╰)o。需要编译安装 libestr。</p>\n<pre><code># wget https://libestr.adiscon.com/files/download/libestr-0.1.9.tar.gz\n# tar xzvf libestr-0.1.9.tar.gz\n# ./configure CC=&quot;gcc -m64&quot; --prefix=/usr --libdir=/usr/lib64\n# make\n# make install\n</code></pre>\n<p>安装完成后重新执行 autogen.sh 问题解决。</p>\n<p>问题逐一解决后执行自动配置成功：</p>\n<pre><code># autoreconf -fvi\n</code></pre>\n<p>执行配置：</p>\n<pre><code># ./configure --enable-omhttp\n</code></pre>\n<p>配置过程中出现以下错误信息：</p>\n<pre><code>make[3]: Entering directory `/data/kafka/rsyslog-8.1903.0/grammar&#39;\n  LEX    lexer.c\nmake[3]: *** [lexer.c] 错误 1\nmake[3]: Leaving directory `/data/kafka/rsyslog-8.1903.0/grammar&#39;\nmake[2]: *** [all] 错误 2\nmake[2]: Leaving directory `/data/kafka/rsyslog-8.1903.0/grammar&#39;\nmake[1]: *** [all-recursive] 错误 1\nmake[1]: Leaving directory `/data/kafka/rsyslog-8.1903.0&#39;\nmake: *** [all] 错误 2\n</code></pre>\n<p>安装 flex：</p>\n<pre><code>yum install flex\n</code></pre>\n<p>安装后执行 autogen.sh，再次执行 configure，然后执行 make 成功。进入 omhttp 目录执行 make 出现以下错误：</p>\n<pre><code># cd contrib/omhttp\n# make\n  CC     omhttp_la-omhttp.lo\nomhttp.c: 在函数‘curlPostSetup’中:\nomhttp.c:1554: 错误：‘CURLOPT_TCP_KEEPALIVE’未声明(在此函数内第一次使用)\nomhttp.c:1554: 错误：(即使在一个函数内多次出现，每个未声明的标识符在其\nomhttp.c:1554: 错误：所在的函数内也只报告一次。)\nomhttp.c:1554: 警告：在‘_curl_opt’的声明中，类型默认为‘int’\nomhttp.c:1558: 错误：‘CURLOPT_TCP_KEEPIDLE’未声明(在此函数内第一次使用)\nomhttp.c:1558: 警告：在‘_curl_opt’的声明中，类型默认为‘int’\nomhttp.c:1562: 错误：‘CURLOPT_TCP_KEEPINTVL’未声明(在此函数内第一次使用)\nomhttp.c:1562: 警告：在‘_curl_opt’的声明中，类型默认为‘int’\nmake: *** [omhttp_la-omhttp.lo] 错误 1\n</code></pre>\n<p>经过 Google 后发现 CURLOPT_TCP_KEEPALIVE 是在 libcurl 7.25.0 之后才定义的。使用以下命令检查系统当前 libcurl 版本：</p>\n<pre><code># rpm -q libcurl\n</code></pre>\n<p>  libcurl-7.19.7-52.el6.x86_64</p>\n<p>所以需要升级 curl 版本，参考：<a href=\"https://www.jianshu.com/p/14f5f145453e%E3%80%82%E5%8D%87%E7%BA%A7%E5%AE%8C%E6%88%90%E5%90%8E%E5%88%9B%E5%BB%BA%E4%BB%A5%E4%B8%8B%E8%BD%AF%E8%BF%9E%E6%8E%A5%EF%BC%9A\">https://www.jianshu.com/p/14f5f145453e。升级完成后创建以下软连接：</a></p>\n<pre><code># ln -s /usr/local/curl/include/curl/ /usr/include/curl\n</code></pre>\n<p>再次执行编译 omhttp，出现以下错误：</p>\n<pre><code>make[2]: Entering directory `/data/kafka/rsyslog-8.1903.0/contrib/omhttp&#39;\n  CC     omhttp_la-omhttp.lo\nomhttp.c: 在函数‘checkConn’中:\nomhttp.c:470: 警告：此函数中的‘healthUrl’在使用前可能未初始化\n  CCLD   omhttp.la\n/usr/bin/ld: cannot find -lcurl\ncollect2: ld returned 1 exit status\nmake[2]: *** [omhttp.la] 错误 1\nmake[2]: Leaving directory `/data/kafka/rsyslog-8.1903.0/contrib/omhttp&#39;\nmake[1]: *** [all-recursive] 错误 1\nmake[1]: Leaving directory `/data/kafka/rsyslog-8.1903.0&#39;\nmake: *** [all] 错误 2\n</code></pre>\n<p>创建以下软连接：</p>\n<pre><code>cd /lib64\n# ln -s /usr/local/curl/lib/libcurl.so.4.5.0 libcurl.so\n# ln -s /usr/local/curl/lib/libcurl.so.4.5.0 libcurl.so.4\n</code></pre>\n<p>再次编译 omhttp 成功！！终于大功告成O(∩_∩)O哈哈~</p>\n","site":{"data":{}},"excerpt":"<p>源码下载参考我的另外一片博文：<a href=\"http://zhang-jc.github.io/2019/04/03/CentOS-7-3-%E7%BC%96%E8%AF%91-Rsyslog-8-1903-0/\">CentOS 7.3 编译 Rsyslog 8.1903.0</a>。</p>\n<p>本篇博文从创建构建环境开始填坑/(ㄒoㄒ)/~~。通过上一篇博文可以解决的问题此处不再赘述。</p>\n<p>执行配置时出现以下错误：</p>\n<pre><code># ./configure --enable-omhttp\nconfigure: error: cannot find install-sh or install.sh in &quot;.&quot; &quot;./..&quot; &quot;./../..&quot;\n</code></pre>","more":"<p>先执行 autogen.sh 出现以下错误：</p>\n<pre><code># sh autogen.sh\nchecking for LIBESTR... configure: error: Package requirements (libestr &gt;= 0.1.9) were not met:\nNo package &#39;libestr&#39; found\n</code></pre>\n<p>检查没有安装 libestr 库，所以重新安装：</p>\n<pre><code># yum install libestr libestr-devel\n</code></pre>\n<p>再次执行 autogen.sh 问题依旧o(╯□╰)o。需要编译安装 libestr。</p>\n<pre><code># wget https://libestr.adiscon.com/files/download/libestr-0.1.9.tar.gz\n# tar xzvf libestr-0.1.9.tar.gz\n# ./configure CC=&quot;gcc -m64&quot; --prefix=/usr --libdir=/usr/lib64\n# make\n# make install\n</code></pre>\n<p>安装完成后重新执行 autogen.sh 问题解决。</p>\n<p>问题逐一解决后执行自动配置成功：</p>\n<pre><code># autoreconf -fvi\n</code></pre>\n<p>执行配置：</p>\n<pre><code># ./configure --enable-omhttp\n</code></pre>\n<p>配置过程中出现以下错误信息：</p>\n<pre><code>make[3]: Entering directory `/data/kafka/rsyslog-8.1903.0/grammar&#39;\n  LEX    lexer.c\nmake[3]: *** [lexer.c] 错误 1\nmake[3]: Leaving directory `/data/kafka/rsyslog-8.1903.0/grammar&#39;\nmake[2]: *** [all] 错误 2\nmake[2]: Leaving directory `/data/kafka/rsyslog-8.1903.0/grammar&#39;\nmake[1]: *** [all-recursive] 错误 1\nmake[1]: Leaving directory `/data/kafka/rsyslog-8.1903.0&#39;\nmake: *** [all] 错误 2\n</code></pre>\n<p>安装 flex：</p>\n<pre><code>yum install flex\n</code></pre>\n<p>安装后执行 autogen.sh，再次执行 configure，然后执行 make 成功。进入 omhttp 目录执行 make 出现以下错误：</p>\n<pre><code># cd contrib/omhttp\n# make\n  CC     omhttp_la-omhttp.lo\nomhttp.c: 在函数‘curlPostSetup’中:\nomhttp.c:1554: 错误：‘CURLOPT_TCP_KEEPALIVE’未声明(在此函数内第一次使用)\nomhttp.c:1554: 错误：(即使在一个函数内多次出现，每个未声明的标识符在其\nomhttp.c:1554: 错误：所在的函数内也只报告一次。)\nomhttp.c:1554: 警告：在‘_curl_opt’的声明中，类型默认为‘int’\nomhttp.c:1558: 错误：‘CURLOPT_TCP_KEEPIDLE’未声明(在此函数内第一次使用)\nomhttp.c:1558: 警告：在‘_curl_opt’的声明中，类型默认为‘int’\nomhttp.c:1562: 错误：‘CURLOPT_TCP_KEEPINTVL’未声明(在此函数内第一次使用)\nomhttp.c:1562: 警告：在‘_curl_opt’的声明中，类型默认为‘int’\nmake: *** [omhttp_la-omhttp.lo] 错误 1\n</code></pre>\n<p>经过 Google 后发现 CURLOPT_TCP_KEEPALIVE 是在 libcurl 7.25.0 之后才定义的。使用以下命令检查系统当前 libcurl 版本：</p>\n<pre><code># rpm -q libcurl\n</code></pre>\n<p>  libcurl-7.19.7-52.el6.x86_64</p>\n<p>所以需要升级 curl 版本，参考：<a href=\"https://www.jianshu.com/p/14f5f145453e%E3%80%82%E5%8D%87%E7%BA%A7%E5%AE%8C%E6%88%90%E5%90%8E%E5%88%9B%E5%BB%BA%E4%BB%A5%E4%B8%8B%E8%BD%AF%E8%BF%9E%E6%8E%A5%EF%BC%9A\">https://www.jianshu.com/p/14f5f145453e。升级完成后创建以下软连接：</a></p>\n<pre><code># ln -s /usr/local/curl/include/curl/ /usr/include/curl\n</code></pre>\n<p>再次执行编译 omhttp，出现以下错误：</p>\n<pre><code>make[2]: Entering directory `/data/kafka/rsyslog-8.1903.0/contrib/omhttp&#39;\n  CC     omhttp_la-omhttp.lo\nomhttp.c: 在函数‘checkConn’中:\nomhttp.c:470: 警告：此函数中的‘healthUrl’在使用前可能未初始化\n  CCLD   omhttp.la\n/usr/bin/ld: cannot find -lcurl\ncollect2: ld returned 1 exit status\nmake[2]: *** [omhttp.la] 错误 1\nmake[2]: Leaving directory `/data/kafka/rsyslog-8.1903.0/contrib/omhttp&#39;\nmake[1]: *** [all-recursive] 错误 1\nmake[1]: Leaving directory `/data/kafka/rsyslog-8.1903.0&#39;\nmake: *** [all] 错误 2\n</code></pre>\n<p>创建以下软连接：</p>\n<pre><code>cd /lib64\n# ln -s /usr/local/curl/lib/libcurl.so.4.5.0 libcurl.so\n# ln -s /usr/local/curl/lib/libcurl.so.4.5.0 libcurl.so.4\n</code></pre>\n<p>再次编译 omhttp 成功！！终于大功告成O(∩_∩)O哈哈~</p>"},{"title":"CentOS 6.5 编译 omhttp.so 下载","date":"2019-04-06T15:08:24.000Z","_content":"\n[CentOS 6.5 编译 omhttp.so](/uploads/20190406/omhttp.so)\n本篇博文只提供下载，如何编译请参见我的另外两篇博文：\n[CentOS 7.3 编译 Rsyslog 8.1903.0](http://zhang-jc.github.io/2019/04/03/CentOS-7-3-%E7%BC%96%E8%AF%91-Rsyslog-8-1903-0/)\n[CentOS 6.5 编译 Rsyslog 8.1903.0](http://zhang-jc.github.io/2019/04/03/CentOS-6-5-%E7%BC%96%E8%AF%91-Rsyslog-8-1903-0/)","source":"_posts/CentOS-6-5-编译-omhttp-so-下载.md","raw":"title: CentOS 6.5 编译 omhttp.so 下载\ndate: 2019-04-06 23:08:24\ntags:\n- CentOS\n- Rsyslog\ncategories:\n- 大数据\n- Rsyslog\n---\n\n[CentOS 6.5 编译 omhttp.so](/uploads/20190406/omhttp.so)\n本篇博文只提供下载，如何编译请参见我的另外两篇博文：\n[CentOS 7.3 编译 Rsyslog 8.1903.0](http://zhang-jc.github.io/2019/04/03/CentOS-7-3-%E7%BC%96%E8%AF%91-Rsyslog-8-1903-0/)\n[CentOS 6.5 编译 Rsyslog 8.1903.0](http://zhang-jc.github.io/2019/04/03/CentOS-6-5-%E7%BC%96%E8%AF%91-Rsyslog-8-1903-0/)","slug":"CentOS-6-5-编译-omhttp-so-下载","published":1,"updated":"2021-07-19T16:28:00.228Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphl40005itd3cqqwe1j7","content":"<p><a href=\"/uploads/20190406/omhttp.so\">CentOS 6.5 编译 omhttp.so</a><br>本篇博文只提供下载，如何编译请参见我的另外两篇博文：<br><a href=\"http://zhang-jc.github.io/2019/04/03/CentOS-7-3-%E7%BC%96%E8%AF%91-Rsyslog-8-1903-0/\">CentOS 7.3 编译 Rsyslog 8.1903.0</a><br><a href=\"http://zhang-jc.github.io/2019/04/03/CentOS-6-5-%E7%BC%96%E8%AF%91-Rsyslog-8-1903-0/\">CentOS 6.5 编译 Rsyslog 8.1903.0</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p><a href=\"/uploads/20190406/omhttp.so\">CentOS 6.5 编译 omhttp.so</a><br>本篇博文只提供下载，如何编译请参见我的另外两篇博文：<br><a href=\"http://zhang-jc.github.io/2019/04/03/CentOS-7-3-%E7%BC%96%E8%AF%91-Rsyslog-8-1903-0/\">CentOS 7.3 编译 Rsyslog 8.1903.0</a><br><a href=\"http://zhang-jc.github.io/2019/04/03/CentOS-6-5-%E7%BC%96%E8%AF%91-Rsyslog-8-1903-0/\">CentOS 6.5 编译 Rsyslog 8.1903.0</a></p>\n"},{"title":"CentOS 6.8 Python3 遇到的坑","date":"2018-11-30T07:46:37.000Z","_content":"\n##### python3: error while loading shared libraries: libssl.so.1.1\n\n这个问题是因为安装 OpenSSL 时是用 root 用户安装，对于其他用户没有读和执行的权限。解决方法是将 OpenSSL 安装目录及下面的文件权限改为 755。\n\nCentOS 6.8 下安装 Python3 并启用 SSL 模块请参考另外一篇博文：[CentOS 6.8 安装 Python3 Could not build the ssl module](http://zhang-jc.github.io/2018/11/27/CentOS-6-8-%E5%AE%89%E8%A3%85-Python3-Could-not-build-the-ssl-module/)。\n\n##### ModuleNotFoundError: No module named 'mysql'\n\n使用 pip3 安装完 mysql-connector-python-8.0.13 后，在 root 用户下测试没问题，但是其他用户使用时出现找不到的错误。所以还是权限的问题。将 Python3 整个安装目录及下面的文件和子目录权限都改为 755 问题解决。\n\n##### _mysql_connector.MySQLInterfaceError: Bad handshake\n\n完整异常信息如下：\n\n    Traceback (most recent call last):\n      File \"/opt/python/lib/python3.7/site-packages/mysql/connector/connection_cext.py\", line 176, in _open_connection\n        self._cmysql.connect(**cnx_kwargs)\n    _mysql_connector.MySQLInterfaceError: Bad handshake\n\n    During handling of the above exception, another exception occurred:\n\n    Traceback (most recent call last):\n      File \"/data/jenkins/releases/ops/DataWarehouse/azkabanJx.py\", line 26, in <module>\n        cnx = mysql.connector.connect(**config)\n      File \"/opt/python/lib/python3.7/site-packages/mysql/connector/__init__.py\", line 172, in connect\n        return CMySQLConnection(*args, **kwargs)\n      File \"/opt/python/lib/python3.7/site-packages/mysql/connector/connection_cext.py\", line 78, in __init__\n        self.connect(**kwargs)\n      File \"/opt/python/lib/python3.7/site-packages/mysql/connector/abstracts.py\", line 731, in connect\n        self._open_connection()\n      File \"/opt/python/lib/python3.7/site-packages/mysql/connector/connection_cext.py\", line 179, in _open_connection\n        sqlstate=exc.sqlstate)\n    mysql.connector.errors.OperationalError: 1043 (08S01): Bad handshake\n\nGoogle 后了解到原因是：\n\n> The C extension was added in version 2.1.1 and is enabled by default as of 8.0.11. The use_pure option determines whether the Python or C version of this connector is enabled and used.1\n\n服务器上没有 C 扩展，所以需要设置参数 'use_pure': True。完整配置样例如下：\n\n    config = {\n      'user': 'scott',\n      'password': 'password',\n      'host': '127.0.0.1',\n      'database': 'employees',\n      'use_pure': True,\n    }\n\n*注意，添加参数 'use_pure': True 之后，从 MySQL 中读取的 BLOB 字段是字节数组类型。*\n","source":"_posts/CentOS-6-8-Python3-遇到的坑.md","raw":"title: CentOS 6.8 Python3 遇到的坑\ndate: 2018-11-30 15:46:37\ntags:\n- CentOS\n- Python3\n- MySQL\n- Linux\n---\n\n##### python3: error while loading shared libraries: libssl.so.1.1\n\n这个问题是因为安装 OpenSSL 时是用 root 用户安装，对于其他用户没有读和执行的权限。解决方法是将 OpenSSL 安装目录及下面的文件权限改为 755。\n\nCentOS 6.8 下安装 Python3 并启用 SSL 模块请参考另外一篇博文：[CentOS 6.8 安装 Python3 Could not build the ssl module](http://zhang-jc.github.io/2018/11/27/CentOS-6-8-%E5%AE%89%E8%A3%85-Python3-Could-not-build-the-ssl-module/)。\n\n##### ModuleNotFoundError: No module named 'mysql'\n\n使用 pip3 安装完 mysql-connector-python-8.0.13 后，在 root 用户下测试没问题，但是其他用户使用时出现找不到的错误。所以还是权限的问题。将 Python3 整个安装目录及下面的文件和子目录权限都改为 755 问题解决。\n\n##### _mysql_connector.MySQLInterfaceError: Bad handshake\n\n完整异常信息如下：\n\n    Traceback (most recent call last):\n      File \"/opt/python/lib/python3.7/site-packages/mysql/connector/connection_cext.py\", line 176, in _open_connection\n        self._cmysql.connect(**cnx_kwargs)\n    _mysql_connector.MySQLInterfaceError: Bad handshake\n\n    During handling of the above exception, another exception occurred:\n\n    Traceback (most recent call last):\n      File \"/data/jenkins/releases/ops/DataWarehouse/azkabanJx.py\", line 26, in <module>\n        cnx = mysql.connector.connect(**config)\n      File \"/opt/python/lib/python3.7/site-packages/mysql/connector/__init__.py\", line 172, in connect\n        return CMySQLConnection(*args, **kwargs)\n      File \"/opt/python/lib/python3.7/site-packages/mysql/connector/connection_cext.py\", line 78, in __init__\n        self.connect(**kwargs)\n      File \"/opt/python/lib/python3.7/site-packages/mysql/connector/abstracts.py\", line 731, in connect\n        self._open_connection()\n      File \"/opt/python/lib/python3.7/site-packages/mysql/connector/connection_cext.py\", line 179, in _open_connection\n        sqlstate=exc.sqlstate)\n    mysql.connector.errors.OperationalError: 1043 (08S01): Bad handshake\n\nGoogle 后了解到原因是：\n\n> The C extension was added in version 2.1.1 and is enabled by default as of 8.0.11. The use_pure option determines whether the Python or C version of this connector is enabled and used.1\n\n服务器上没有 C 扩展，所以需要设置参数 'use_pure': True。完整配置样例如下：\n\n    config = {\n      'user': 'scott',\n      'password': 'password',\n      'host': '127.0.0.1',\n      'database': 'employees',\n      'use_pure': True,\n    }\n\n*注意，添加参数 'use_pure': True 之后，从 MySQL 中读取的 BLOB 字段是字节数组类型。*\n","slug":"CentOS-6-8-Python3-遇到的坑","published":1,"updated":"2021-07-19T16:28:00.228Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphl50006itd3f5xke0kx","content":"<h5 id=\"python3-error-while-loading-shared-libraries-libssl-so-1-1\"><a href=\"#python3-error-while-loading-shared-libraries-libssl-so-1-1\" class=\"headerlink\" title=\"python3: error while loading shared libraries: libssl.so.1.1\"></a>python3: error while loading shared libraries: libssl.so.1.1</h5><p>这个问题是因为安装 OpenSSL 时是用 root 用户安装，对于其他用户没有读和执行的权限。解决方法是将 OpenSSL 安装目录及下面的文件权限改为 755。</p>\n<p>CentOS 6.8 下安装 Python3 并启用 SSL 模块请参考另外一篇博文：<a href=\"http://zhang-jc.github.io/2018/11/27/CentOS-6-8-%E5%AE%89%E8%A3%85-Python3-Could-not-build-the-ssl-module/\">CentOS 6.8 安装 Python3 Could not build the ssl module</a>。</p>\n<h5 id=\"ModuleNotFoundError-No-module-named-‘mysql’\"><a href=\"#ModuleNotFoundError-No-module-named-‘mysql’\" class=\"headerlink\" title=\"ModuleNotFoundError: No module named ‘mysql’\"></a>ModuleNotFoundError: No module named ‘mysql’</h5><p>使用 pip3 安装完 mysql-connector-python-8.0.13 后，在 root 用户下测试没问题，但是其他用户使用时出现找不到的错误。所以还是权限的问题。将 Python3 整个安装目录及下面的文件和子目录权限都改为 755 问题解决。</p>\n<h5 id=\"mysql-connector-MySQLInterfaceError-Bad-handshake\"><a href=\"#mysql-connector-MySQLInterfaceError-Bad-handshake\" class=\"headerlink\" title=\"_mysql_connector.MySQLInterfaceError: Bad handshake\"></a>_mysql_connector.MySQLInterfaceError: Bad handshake</h5><p>完整异常信息如下：</p>\n<pre><code>Traceback (most recent call last):\n  File &quot;/opt/python/lib/python3.7/site-packages/mysql/connector/connection_cext.py&quot;, line 176, in _open_connection\n    self._cmysql.connect(**cnx_kwargs)\n_mysql_connector.MySQLInterfaceError: Bad handshake\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;/data/jenkins/releases/ops/DataWarehouse/azkabanJx.py&quot;, line 26, in &lt;module&gt;\n    cnx = mysql.connector.connect(**config)\n  File &quot;/opt/python/lib/python3.7/site-packages/mysql/connector/__init__.py&quot;, line 172, in connect\n    return CMySQLConnection(*args, **kwargs)\n  File &quot;/opt/python/lib/python3.7/site-packages/mysql/connector/connection_cext.py&quot;, line 78, in __init__\n    self.connect(**kwargs)\n  File &quot;/opt/python/lib/python3.7/site-packages/mysql/connector/abstracts.py&quot;, line 731, in connect\n    self._open_connection()\n  File &quot;/opt/python/lib/python3.7/site-packages/mysql/connector/connection_cext.py&quot;, line 179, in _open_connection\n    sqlstate=exc.sqlstate)\nmysql.connector.errors.OperationalError: 1043 (08S01): Bad handshake\n</code></pre>\n<p>Google 后了解到原因是：</p>\n<blockquote>\n<p>The C extension was added in version 2.1.1 and is enabled by default as of 8.0.11. The use_pure option determines whether the Python or C version of this connector is enabled and used.1</p>\n</blockquote>\n<p>服务器上没有 C 扩展，所以需要设置参数 ‘use_pure’: True。完整配置样例如下：</p>\n<pre><code>config = &#123;\n  &#39;user&#39;: &#39;scott&#39;,\n  &#39;password&#39;: &#39;password&#39;,\n  &#39;host&#39;: &#39;127.0.0.1&#39;,\n  &#39;database&#39;: &#39;employees&#39;,\n  &#39;use_pure&#39;: True,\n&#125;\n</code></pre>\n<p><em>注意，添加参数 ‘use_pure’: True 之后，从 MySQL 中读取的 BLOB 字段是字节数组类型。</em></p>\n","site":{"data":{}},"excerpt":"","more":"<h5 id=\"python3-error-while-loading-shared-libraries-libssl-so-1-1\"><a href=\"#python3-error-while-loading-shared-libraries-libssl-so-1-1\" class=\"headerlink\" title=\"python3: error while loading shared libraries: libssl.so.1.1\"></a>python3: error while loading shared libraries: libssl.so.1.1</h5><p>这个问题是因为安装 OpenSSL 时是用 root 用户安装，对于其他用户没有读和执行的权限。解决方法是将 OpenSSL 安装目录及下面的文件权限改为 755。</p>\n<p>CentOS 6.8 下安装 Python3 并启用 SSL 模块请参考另外一篇博文：<a href=\"http://zhang-jc.github.io/2018/11/27/CentOS-6-8-%E5%AE%89%E8%A3%85-Python3-Could-not-build-the-ssl-module/\">CentOS 6.8 安装 Python3 Could not build the ssl module</a>。</p>\n<h5 id=\"ModuleNotFoundError-No-module-named-‘mysql’\"><a href=\"#ModuleNotFoundError-No-module-named-‘mysql’\" class=\"headerlink\" title=\"ModuleNotFoundError: No module named ‘mysql’\"></a>ModuleNotFoundError: No module named ‘mysql’</h5><p>使用 pip3 安装完 mysql-connector-python-8.0.13 后，在 root 用户下测试没问题，但是其他用户使用时出现找不到的错误。所以还是权限的问题。将 Python3 整个安装目录及下面的文件和子目录权限都改为 755 问题解决。</p>\n<h5 id=\"mysql-connector-MySQLInterfaceError-Bad-handshake\"><a href=\"#mysql-connector-MySQLInterfaceError-Bad-handshake\" class=\"headerlink\" title=\"_mysql_connector.MySQLInterfaceError: Bad handshake\"></a>_mysql_connector.MySQLInterfaceError: Bad handshake</h5><p>完整异常信息如下：</p>\n<pre><code>Traceback (most recent call last):\n  File &quot;/opt/python/lib/python3.7/site-packages/mysql/connector/connection_cext.py&quot;, line 176, in _open_connection\n    self._cmysql.connect(**cnx_kwargs)\n_mysql_connector.MySQLInterfaceError: Bad handshake\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;/data/jenkins/releases/ops/DataWarehouse/azkabanJx.py&quot;, line 26, in &lt;module&gt;\n    cnx = mysql.connector.connect(**config)\n  File &quot;/opt/python/lib/python3.7/site-packages/mysql/connector/__init__.py&quot;, line 172, in connect\n    return CMySQLConnection(*args, **kwargs)\n  File &quot;/opt/python/lib/python3.7/site-packages/mysql/connector/connection_cext.py&quot;, line 78, in __init__\n    self.connect(**kwargs)\n  File &quot;/opt/python/lib/python3.7/site-packages/mysql/connector/abstracts.py&quot;, line 731, in connect\n    self._open_connection()\n  File &quot;/opt/python/lib/python3.7/site-packages/mysql/connector/connection_cext.py&quot;, line 179, in _open_connection\n    sqlstate=exc.sqlstate)\nmysql.connector.errors.OperationalError: 1043 (08S01): Bad handshake\n</code></pre>\n<p>Google 后了解到原因是：</p>\n<blockquote>\n<p>The C extension was added in version 2.1.1 and is enabled by default as of 8.0.11. The use_pure option determines whether the Python or C version of this connector is enabled and used.1</p>\n</blockquote>\n<p>服务器上没有 C 扩展，所以需要设置参数 ‘use_pure’: True。完整配置样例如下：</p>\n<pre><code>config = &#123;\n  &#39;user&#39;: &#39;scott&#39;,\n  &#39;password&#39;: &#39;password&#39;,\n  &#39;host&#39;: &#39;127.0.0.1&#39;,\n  &#39;database&#39;: &#39;employees&#39;,\n  &#39;use_pure&#39;: True,\n&#125;\n</code></pre>\n<p><em>注意，添加参数 ‘use_pure’: True 之后，从 MySQL 中读取的 BLOB 字段是字节数组类型。</em></p>\n"},{"title":"CentOS 6.8 安装 Nginx","date":"2019-03-27T07:09:45.000Z","_content":"提前安装：\n\n    # sudo yum install yum-utils\n\n一般情况下这个工具系统已经安装。\n\n<!--more-->\n\n创建文件 /etc/yum.repos.d/nginx.repo，输入内容如下：\n\n    [nginx-stable]\n    name=nginx stable repo\n    baseurl=http://nginx.org/packages/centos/$releasever/$basearch/\n    gpgcheck=1\n    enabled=1\n    gpgkey=https://nginx.org/keys/nginx_signing.key\n    \n    [nginx-mainline]\n    name=nginx mainline repo\n    baseurl=http://nginx.org/packages/mainline/centos/$releasever/$basearch/\n    gpgcheck=1\n    enabled=0\n    gpgkey=https://nginx.org/keys/nginx_signing.key\n\n注意，根据自己服务器的情况替换参数 $releasever 和 $basearch。\n\n一般默认使用稳定安装包。如果想使用主线安装包则运行以下命令：\n\n    # sudo yum-config-manager --enable nginx-mainline\n\n运行以下命令安装 Nginx：\n\n    # sudo yum install nginx","source":"_posts/CentOS-6-8-安装-Nginx.md","raw":"title: CentOS 6.8 安装 Nginx\ndate: 2019-03-27 15:09:45\ntags:\n- Nginx\n- CentOS\ncategories:\n- 开发工具\n- Nginx\n---\n提前安装：\n\n    # sudo yum install yum-utils\n\n一般情况下这个工具系统已经安装。\n\n<!--more-->\n\n创建文件 /etc/yum.repos.d/nginx.repo，输入内容如下：\n\n    [nginx-stable]\n    name=nginx stable repo\n    baseurl=http://nginx.org/packages/centos/$releasever/$basearch/\n    gpgcheck=1\n    enabled=1\n    gpgkey=https://nginx.org/keys/nginx_signing.key\n    \n    [nginx-mainline]\n    name=nginx mainline repo\n    baseurl=http://nginx.org/packages/mainline/centos/$releasever/$basearch/\n    gpgcheck=1\n    enabled=0\n    gpgkey=https://nginx.org/keys/nginx_signing.key\n\n注意，根据自己服务器的情况替换参数 $releasever 和 $basearch。\n\n一般默认使用稳定安装包。如果想使用主线安装包则运行以下命令：\n\n    # sudo yum-config-manager --enable nginx-mainline\n\n运行以下命令安装 Nginx：\n\n    # sudo yum install nginx","slug":"CentOS-6-8-安装-Nginx","published":1,"updated":"2021-07-19T16:28:00.228Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphl60009itd34zezgytj","content":"<p>提前安装：</p>\n<pre><code># sudo yum install yum-utils\n</code></pre>\n<p>一般情况下这个工具系统已经安装。</p>\n<span id=\"more\"></span>\n\n<p>创建文件 /etc/yum.repos.d/nginx.repo，输入内容如下：</p>\n<pre><code>[nginx-stable]\nname=nginx stable repo\nbaseurl=http://nginx.org/packages/centos/$releasever/$basearch/\ngpgcheck=1\nenabled=1\ngpgkey=https://nginx.org/keys/nginx_signing.key\n\n[nginx-mainline]\nname=nginx mainline repo\nbaseurl=http://nginx.org/packages/mainline/centos/$releasever/$basearch/\ngpgcheck=1\nenabled=0\ngpgkey=https://nginx.org/keys/nginx_signing.key\n</code></pre>\n<p>注意，根据自己服务器的情况替换参数 $releasever 和 $basearch。</p>\n<p>一般默认使用稳定安装包。如果想使用主线安装包则运行以下命令：</p>\n<pre><code># sudo yum-config-manager --enable nginx-mainline\n</code></pre>\n<p>运行以下命令安装 Nginx：</p>\n<pre><code># sudo yum install nginx\n</code></pre>\n","site":{"data":{}},"excerpt":"<p>提前安装：</p>\n<pre><code># sudo yum install yum-utils\n</code></pre>\n<p>一般情况下这个工具系统已经安装。</p>","more":"<p>创建文件 /etc/yum.repos.d/nginx.repo，输入内容如下：</p>\n<pre><code>[nginx-stable]\nname=nginx stable repo\nbaseurl=http://nginx.org/packages/centos/$releasever/$basearch/\ngpgcheck=1\nenabled=1\ngpgkey=https://nginx.org/keys/nginx_signing.key\n\n[nginx-mainline]\nname=nginx mainline repo\nbaseurl=http://nginx.org/packages/mainline/centos/$releasever/$basearch/\ngpgcheck=1\nenabled=0\ngpgkey=https://nginx.org/keys/nginx_signing.key\n</code></pre>\n<p>注意，根据自己服务器的情况替换参数 $releasever 和 $basearch。</p>\n<p>一般默认使用稳定安装包。如果想使用主线安装包则运行以下命令：</p>\n<pre><code># sudo yum-config-manager --enable nginx-mainline\n</code></pre>\n<p>运行以下命令安装 Nginx：</p>\n<pre><code># sudo yum install nginx\n</code></pre>"},{"title":"CentOS 6.8 安装 Python3 Could not build the ssl module","date":"2018-11-27T11:27:13.000Z","_content":"\n在 CentOS 6.8 下编译安装 Python3 时出现以下提示信息：\n\n    Could not build the ssl module!\n    Python requires an OpenSSL 1.0.2 or 1.1 compatible libssl with X509_VERIFY_PARAM_set1_host().\n    LibreSSL 2.6.4 and earlier do not provide the necessary APIs,     https://github.com/libressl-portable/portable/issues/38\n\n*注意，其实这个只是提示信息，按照下面的步骤可以成功支持 SSL*\n\n###### 第一步：安装 OpenSSL\n\n从 OpenSSL 的官方网站 <https://www.openssl.org/> 下载源代码。此处使用“openssl-1.1.1a.tar.gz”。\n\n依次执行以下命令进行安装：\n\n    #tar xzvf openssl-1.1.1a.tar.gz\n    #cd openssl-1.1.1a\n    #./config --prefix=/usr/local/openssl shared\n    #make\n    #make install\n\n*注意，在执行 ./config 的时候一定要指定 --prefix=/usr/local/openssl 参数，方便后面使用。*\n\n验证 OpenSSL 安装时报以下异常信息：\n\n    #openssl version\n    openssl: error while loading shared libraries: libssl.so.1.1: cannot open shared object file: No such file or directory\n\n这个是因为 OpenSSL 库的位置没有在系统共享 Lib 目录中导致的。通过创建软连接方式解决。\n\n    #ln -s /usr/local/openssl/lib/libssl.so.1.1 /usr/lib64/libssl.so.1.1\n    #ln -s /usr/local/openssl/lib/libcrypto.so.1.1 /usr/lib64/libcrypto.so.1.1\n    #openssl version\n    OpenSSL 1.1.1a  20 Nov 2018\n\n最后一行表示安装成功。\n\n###### 第二步：安装 LibreSSL\n\n从 LibreSSL 官网 <http://www.libressl.org/> 下载源代码。此处使用“libressl-2.8.2.tar.gz”。\n\n以此执行以下命令安装：\n\n    #wget https://ftp.openbsd.org/pub/OpenBSD/LibreSSL/libressl-2.8.2.tar.gz\n    #tar xzvf libressl-2.8.2.tar.gz\n    #cd libressl-2.8.2\n    #./config\n    #make\n    #make install\n\n此时再验证 OpenSSL 版本，如下：\n\n    #openssl version\n    LibreSSL 2.8.2\n\n###### 第三步：修改 Python 安装配置文件\n\n在 Python2.7.1 源代码目录下修改配置文件  Modules/Setup，将以下前面的注释去掉。\n\n    #SSL=/usr/local/ssl\n    #_ssl _ssl.c \\\n    #        -DUSE_SSL -I$(SSL)/include -I$(SSL)/include/openssl \\\n    #        -L$(SSL)/lib -lssl -lcrypto\n\n###### 第四步：安装 Python\n\n开始安装前需要先设置以下环境变量：\n\nexport LDFLAGS=\"-L/usr/local/openssl/lib\"\nexport CPPFLAGS=\"-I/usr/local/openssl/include\"\nexport PKG_CONFIG_PATH=\"/usr/local/openssl/lib/pkgconfig\"\n\n参考我的另外一篇文章[Centos7.3 安装 Python3](http://zhang-jc.github.io/2018/11/19/Centos7-3-%E5%AE%89%E8%A3%85-Python3/)。\n\n###### 第五步：测试 Python3 https 请求\n\n此处需要取消证书验证。\n\n    #python3\n    Python 3.7.1 (default, Nov 28 2018, 17:42:41)\n    [GCC 4.4.7 20120313 (Red Hat 4.4.7-17)] on linux\n    Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n    >>> import ssl\n    >>> import urllib.request\n    >>> context = ssl._create_unverified_context()\n    >>> urllib.request.urlopen('https://www.sogou.com/',context=context).read()\n\n测试成功！！\n","source":"_posts/CentOS-6-8-安装-Python3-Could-not-build-the-ssl-module.md","raw":"title: CentOS 6.8 安装 Python3 Could not build the ssl module\ndate: 2018-11-27 19:27:13\ntags:\n- Linux\n- CentOS\n- Python3\ncategories:\n- 语言\n- Python3\n---\n\n在 CentOS 6.8 下编译安装 Python3 时出现以下提示信息：\n\n    Could not build the ssl module!\n    Python requires an OpenSSL 1.0.2 or 1.1 compatible libssl with X509_VERIFY_PARAM_set1_host().\n    LibreSSL 2.6.4 and earlier do not provide the necessary APIs,     https://github.com/libressl-portable/portable/issues/38\n\n*注意，其实这个只是提示信息，按照下面的步骤可以成功支持 SSL*\n\n###### 第一步：安装 OpenSSL\n\n从 OpenSSL 的官方网站 <https://www.openssl.org/> 下载源代码。此处使用“openssl-1.1.1a.tar.gz”。\n\n依次执行以下命令进行安装：\n\n    #tar xzvf openssl-1.1.1a.tar.gz\n    #cd openssl-1.1.1a\n    #./config --prefix=/usr/local/openssl shared\n    #make\n    #make install\n\n*注意，在执行 ./config 的时候一定要指定 --prefix=/usr/local/openssl 参数，方便后面使用。*\n\n验证 OpenSSL 安装时报以下异常信息：\n\n    #openssl version\n    openssl: error while loading shared libraries: libssl.so.1.1: cannot open shared object file: No such file or directory\n\n这个是因为 OpenSSL 库的位置没有在系统共享 Lib 目录中导致的。通过创建软连接方式解决。\n\n    #ln -s /usr/local/openssl/lib/libssl.so.1.1 /usr/lib64/libssl.so.1.1\n    #ln -s /usr/local/openssl/lib/libcrypto.so.1.1 /usr/lib64/libcrypto.so.1.1\n    #openssl version\n    OpenSSL 1.1.1a  20 Nov 2018\n\n最后一行表示安装成功。\n\n###### 第二步：安装 LibreSSL\n\n从 LibreSSL 官网 <http://www.libressl.org/> 下载源代码。此处使用“libressl-2.8.2.tar.gz”。\n\n以此执行以下命令安装：\n\n    #wget https://ftp.openbsd.org/pub/OpenBSD/LibreSSL/libressl-2.8.2.tar.gz\n    #tar xzvf libressl-2.8.2.tar.gz\n    #cd libressl-2.8.2\n    #./config\n    #make\n    #make install\n\n此时再验证 OpenSSL 版本，如下：\n\n    #openssl version\n    LibreSSL 2.8.2\n\n###### 第三步：修改 Python 安装配置文件\n\n在 Python2.7.1 源代码目录下修改配置文件  Modules/Setup，将以下前面的注释去掉。\n\n    #SSL=/usr/local/ssl\n    #_ssl _ssl.c \\\n    #        -DUSE_SSL -I$(SSL)/include -I$(SSL)/include/openssl \\\n    #        -L$(SSL)/lib -lssl -lcrypto\n\n###### 第四步：安装 Python\n\n开始安装前需要先设置以下环境变量：\n\nexport LDFLAGS=\"-L/usr/local/openssl/lib\"\nexport CPPFLAGS=\"-I/usr/local/openssl/include\"\nexport PKG_CONFIG_PATH=\"/usr/local/openssl/lib/pkgconfig\"\n\n参考我的另外一篇文章[Centos7.3 安装 Python3](http://zhang-jc.github.io/2018/11/19/Centos7-3-%E5%AE%89%E8%A3%85-Python3/)。\n\n###### 第五步：测试 Python3 https 请求\n\n此处需要取消证书验证。\n\n    #python3\n    Python 3.7.1 (default, Nov 28 2018, 17:42:41)\n    [GCC 4.4.7 20120313 (Red Hat 4.4.7-17)] on linux\n    Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n    >>> import ssl\n    >>> import urllib.request\n    >>> context = ssl._create_unverified_context()\n    >>> urllib.request.urlopen('https://www.sogou.com/',context=context).read()\n\n测试成功！！\n","slug":"CentOS-6-8-安装-Python3-Could-not-build-the-ssl-module","published":1,"updated":"2021-07-19T16:28:00.228Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphl7000aitd30qxeg0e7","content":"<p>在 CentOS 6.8 下编译安装 Python3 时出现以下提示信息：</p>\n<pre><code>Could not build the ssl module!\nPython requires an OpenSSL 1.0.2 or 1.1 compatible libssl with X509_VERIFY_PARAM_set1_host().\nLibreSSL 2.6.4 and earlier do not provide the necessary APIs,     https://github.com/libressl-portable/portable/issues/38\n</code></pre>\n<p><em>注意，其实这个只是提示信息，按照下面的步骤可以成功支持 SSL</em></p>\n<h6 id=\"第一步：安装-OpenSSL\"><a href=\"#第一步：安装-OpenSSL\" class=\"headerlink\" title=\"第一步：安装 OpenSSL\"></a>第一步：安装 OpenSSL</h6><p>从 OpenSSL 的官方网站 <a href=\"https://www.openssl.org/\">https://www.openssl.org/</a> 下载源代码。此处使用“openssl-1.1.1a.tar.gz”。</p>\n<p>依次执行以下命令进行安装：</p>\n<pre><code>#tar xzvf openssl-1.1.1a.tar.gz\n#cd openssl-1.1.1a\n#./config --prefix=/usr/local/openssl shared\n#make\n#make install\n</code></pre>\n<p><em>注意，在执行 ./config 的时候一定要指定 –prefix=/usr/local/openssl 参数，方便后面使用。</em></p>\n<p>验证 OpenSSL 安装时报以下异常信息：</p>\n<pre><code>#openssl version\nopenssl: error while loading shared libraries: libssl.so.1.1: cannot open shared object file: No such file or directory\n</code></pre>\n<p>这个是因为 OpenSSL 库的位置没有在系统共享 Lib 目录中导致的。通过创建软连接方式解决。</p>\n<pre><code>#ln -s /usr/local/openssl/lib/libssl.so.1.1 /usr/lib64/libssl.so.1.1\n#ln -s /usr/local/openssl/lib/libcrypto.so.1.1 /usr/lib64/libcrypto.so.1.1\n#openssl version\nOpenSSL 1.1.1a  20 Nov 2018\n</code></pre>\n<p>最后一行表示安装成功。</p>\n<h6 id=\"第二步：安装-LibreSSL\"><a href=\"#第二步：安装-LibreSSL\" class=\"headerlink\" title=\"第二步：安装 LibreSSL\"></a>第二步：安装 LibreSSL</h6><p>从 LibreSSL 官网 <a href=\"http://www.libressl.org/\">http://www.libressl.org/</a> 下载源代码。此处使用“libressl-2.8.2.tar.gz”。</p>\n<p>以此执行以下命令安装：</p>\n<pre><code>#wget https://ftp.openbsd.org/pub/OpenBSD/LibreSSL/libressl-2.8.2.tar.gz\n#tar xzvf libressl-2.8.2.tar.gz\n#cd libressl-2.8.2\n#./config\n#make\n#make install\n</code></pre>\n<p>此时再验证 OpenSSL 版本，如下：</p>\n<pre><code>#openssl version\nLibreSSL 2.8.2\n</code></pre>\n<h6 id=\"第三步：修改-Python-安装配置文件\"><a href=\"#第三步：修改-Python-安装配置文件\" class=\"headerlink\" title=\"第三步：修改 Python 安装配置文件\"></a>第三步：修改 Python 安装配置文件</h6><p>在 Python2.7.1 源代码目录下修改配置文件  Modules/Setup，将以下前面的注释去掉。</p>\n<pre><code>#SSL=/usr/local/ssl\n#_ssl _ssl.c \\\n#        -DUSE_SSL -I$(SSL)/include -I$(SSL)/include/openssl \\\n#        -L$(SSL)/lib -lssl -lcrypto\n</code></pre>\n<h6 id=\"第四步：安装-Python\"><a href=\"#第四步：安装-Python\" class=\"headerlink\" title=\"第四步：安装 Python\"></a>第四步：安装 Python</h6><p>开始安装前需要先设置以下环境变量：</p>\n<p>export LDFLAGS=”-L/usr/local/openssl/lib”<br>export CPPFLAGS=”-I/usr/local/openssl/include”<br>export PKG_CONFIG_PATH=”/usr/local/openssl/lib/pkgconfig”</p>\n<p>参考我的另外一篇文章<a href=\"http://zhang-jc.github.io/2018/11/19/Centos7-3-%E5%AE%89%E8%A3%85-Python3/\">Centos7.3 安装 Python3</a>。</p>\n<h6 id=\"第五步：测试-Python3-https-请求\"><a href=\"#第五步：测试-Python3-https-请求\" class=\"headerlink\" title=\"第五步：测试 Python3 https 请求\"></a>第五步：测试 Python3 https 请求</h6><p>此处需要取消证书验证。</p>\n<pre><code>#python3\nPython 3.7.1 (default, Nov 28 2018, 17:42:41)\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-17)] on linux\nType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.\n&gt;&gt;&gt; import ssl\n&gt;&gt;&gt; import urllib.request\n&gt;&gt;&gt; context = ssl._create_unverified_context()\n&gt;&gt;&gt; urllib.request.urlopen(&#39;https://www.sogou.com/&#39;,context=context).read()\n</code></pre>\n<p>测试成功！！</p>\n","site":{"data":{}},"excerpt":"","more":"<p>在 CentOS 6.8 下编译安装 Python3 时出现以下提示信息：</p>\n<pre><code>Could not build the ssl module!\nPython requires an OpenSSL 1.0.2 or 1.1 compatible libssl with X509_VERIFY_PARAM_set1_host().\nLibreSSL 2.6.4 and earlier do not provide the necessary APIs,     https://github.com/libressl-portable/portable/issues/38\n</code></pre>\n<p><em>注意，其实这个只是提示信息，按照下面的步骤可以成功支持 SSL</em></p>\n<h6 id=\"第一步：安装-OpenSSL\"><a href=\"#第一步：安装-OpenSSL\" class=\"headerlink\" title=\"第一步：安装 OpenSSL\"></a>第一步：安装 OpenSSL</h6><p>从 OpenSSL 的官方网站 <a href=\"https://www.openssl.org/\">https://www.openssl.org/</a> 下载源代码。此处使用“openssl-1.1.1a.tar.gz”。</p>\n<p>依次执行以下命令进行安装：</p>\n<pre><code>#tar xzvf openssl-1.1.1a.tar.gz\n#cd openssl-1.1.1a\n#./config --prefix=/usr/local/openssl shared\n#make\n#make install\n</code></pre>\n<p><em>注意，在执行 ./config 的时候一定要指定 –prefix=/usr/local/openssl 参数，方便后面使用。</em></p>\n<p>验证 OpenSSL 安装时报以下异常信息：</p>\n<pre><code>#openssl version\nopenssl: error while loading shared libraries: libssl.so.1.1: cannot open shared object file: No such file or directory\n</code></pre>\n<p>这个是因为 OpenSSL 库的位置没有在系统共享 Lib 目录中导致的。通过创建软连接方式解决。</p>\n<pre><code>#ln -s /usr/local/openssl/lib/libssl.so.1.1 /usr/lib64/libssl.so.1.1\n#ln -s /usr/local/openssl/lib/libcrypto.so.1.1 /usr/lib64/libcrypto.so.1.1\n#openssl version\nOpenSSL 1.1.1a  20 Nov 2018\n</code></pre>\n<p>最后一行表示安装成功。</p>\n<h6 id=\"第二步：安装-LibreSSL\"><a href=\"#第二步：安装-LibreSSL\" class=\"headerlink\" title=\"第二步：安装 LibreSSL\"></a>第二步：安装 LibreSSL</h6><p>从 LibreSSL 官网 <a href=\"http://www.libressl.org/\">http://www.libressl.org/</a> 下载源代码。此处使用“libressl-2.8.2.tar.gz”。</p>\n<p>以此执行以下命令安装：</p>\n<pre><code>#wget https://ftp.openbsd.org/pub/OpenBSD/LibreSSL/libressl-2.8.2.tar.gz\n#tar xzvf libressl-2.8.2.tar.gz\n#cd libressl-2.8.2\n#./config\n#make\n#make install\n</code></pre>\n<p>此时再验证 OpenSSL 版本，如下：</p>\n<pre><code>#openssl version\nLibreSSL 2.8.2\n</code></pre>\n<h6 id=\"第三步：修改-Python-安装配置文件\"><a href=\"#第三步：修改-Python-安装配置文件\" class=\"headerlink\" title=\"第三步：修改 Python 安装配置文件\"></a>第三步：修改 Python 安装配置文件</h6><p>在 Python2.7.1 源代码目录下修改配置文件  Modules/Setup，将以下前面的注释去掉。</p>\n<pre><code>#SSL=/usr/local/ssl\n#_ssl _ssl.c \\\n#        -DUSE_SSL -I$(SSL)/include -I$(SSL)/include/openssl \\\n#        -L$(SSL)/lib -lssl -lcrypto\n</code></pre>\n<h6 id=\"第四步：安装-Python\"><a href=\"#第四步：安装-Python\" class=\"headerlink\" title=\"第四步：安装 Python\"></a>第四步：安装 Python</h6><p>开始安装前需要先设置以下环境变量：</p>\n<p>export LDFLAGS=”-L/usr/local/openssl/lib”<br>export CPPFLAGS=”-I/usr/local/openssl/include”<br>export PKG_CONFIG_PATH=”/usr/local/openssl/lib/pkgconfig”</p>\n<p>参考我的另外一篇文章<a href=\"http://zhang-jc.github.io/2018/11/19/Centos7-3-%E5%AE%89%E8%A3%85-Python3/\">Centos7.3 安装 Python3</a>。</p>\n<h6 id=\"第五步：测试-Python3-https-请求\"><a href=\"#第五步：测试-Python3-https-请求\" class=\"headerlink\" title=\"第五步：测试 Python3 https 请求\"></a>第五步：测试 Python3 https 请求</h6><p>此处需要取消证书验证。</p>\n<pre><code>#python3\nPython 3.7.1 (default, Nov 28 2018, 17:42:41)\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-17)] on linux\nType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.\n&gt;&gt;&gt; import ssl\n&gt;&gt;&gt; import urllib.request\n&gt;&gt;&gt; context = ssl._create_unverified_context()\n&gt;&gt;&gt; urllib.request.urlopen(&#39;https://www.sogou.com/&#39;,context=context).read()\n</code></pre>\n<p>测试成功！！</p>\n"},{"title":"CentOS 7.3 Python 3.7.2 编译 Hue 问题解决","date":"2020-06-24T06:43:18.000Z","_content":"\n### 环境说明\n\n- CentOS 7.3\n- Python 3.7.2，我服务器上的安装是直接从其他服务器拷贝过来，然后配置环境变量的版本。正式因为这样我才遇到如下的问题。 \n\n### 问题解决\n\n#### python3.7-devel\n\n    $ make apps\n    \"PYTHON_VER is python3.7.\"\n    \"Python 3 module install via pip\"\n    /opt/hue-4.7.1/Makefile.vars:65: *** \"Error: must have python development packages for python3.7. Could not find Python.h. Please install python3.7-devel\".  Stop.\n\n尝试安装 python3.7-devel 包，发现该包不存在，所以通过以下方式解决：\n\n    # ln -s /opt/python/include/python3.7m /usr/include/python3.7\n\n我的 python 是安装在 /opt/ 目录下的。\n\n#### 找不到对应版本的 python\n\n    $ make apps\n    \"PYTHON_VER is python3.7.\"\n    \"Python 3 module install via pip\"\n    /opt/hue-4.7.1/Makefile.vars:73: *** \"Error: Need python version 2.7 or >= 3.5\".  Stop.\n\n创建以下软连接解决：\n\n    # ln -s /opt/python/bin/python3.7 /usr/bin/python3.7\n\n#### 找不到 npm 命令\n\n    npm --version\n    /bin/bash: npm: command not found\n    make[1]: *** [npm-install] Error 127\n\n不能使用yum安装npm，因为安装的版本太低。需要安装 node 10 以上的版本：\n\n    # cd /opt/\n    # wget -c https://npm.taobao.org/mirrors/node/latest-v10.x/node-v10.21.0-linux-x64.tar.gz\n    # tar xzf node-v10.21.0-linux-x64.tar.gz\n    # ln -s node-v10.21.0-linux-x64 node\n\n#### No module named 'pysqlite2'\n\n    Traceback (most recent call last):\n      File \"/opt/hue-4.7.1/build/env/lib/python3.7/site-packages/django/db/backends/sqlite3/base.py\", line 31, in <module>\n        from pysqlite2 import dbapi2 as Database\n    ModuleNotFoundError: No module named 'pysqlite2'\n\n\nPython3 只支持 sqlite3：\n\n    # pip install pysqlite\n    Collecting pysqlite\n      Using cached pysqlite-2.8.3.tar.gz (80 kB)\n        ERROR: Command errored out with exit status 1:\n         command: /opt/hue-4.7.1/build/env/bin/python3.7 -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-42esl1q6/pysqlite/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-42esl1q6/pysqlite/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-v9a4at6e\n             cwd: /tmp/pip-install-42esl1q6/pysqlite/\n        Complete output (1 lines):\n        pysqlite is not supported on Python 3. When using Python 3, use the sqlite3 module from the standard library.\n        ----------------------------------------\n    ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n    (env) [root@ycluster-resourcemanager1 bin]# pip install pysqlite3\n    Collecting pysqlite3\n      Downloading pysqlite3-0.4.3.tar.gz (40 kB)\n         |████████████████████████████████| 40 kB 682 kB/s \n    Building wheels for collected packages: pysqlite3\n      Building wheel for pysqlite3 (setup.py) ... done\n      Created wheel for pysqlite3: filename=pysqlite3-0.4.3-cp37-cp37m-linux_x86_64.whl size=128283 sha256=9a55112d80511438c46512efb0f4021301ba7b04af2897b6d6746158d5cd0fe5\n      Stored in directory: /root/.cache/pip/wheels/85/19/01/0a17a6cacac8cf8a4e8e8994d314abf10a9b11a1c4bc18218a\n    Successfully built pysqlite3\n    Installing collected packages: pysqlite3\n    Successfully installed pysqlite3-0.4.3\n\n重新编译，但是问题依旧：\n\n    Traceback (most recent call last):\n      File \"/opt/hue-4.7.1/build/env/lib/python3.7/site-packages/django/db/backends/sqlite3/base.py\", line 33, in <module>\n        from sqlite3 import dbapi2 as Database\n      File \"/opt/python/lib/python3.7/sqlite3/__init__.py\", line 23, in <module>\n        from sqlite3.dbapi2 import *\n      File \"/opt/python/lib/python3.7/sqlite3/dbapi2.py\", line 27, in <module>\n        from _sqlite3 import *\n    ModuleNotFoundError: No module named '_sqlite3'\n\n解决方案是在系统中安装 sqlit 相关的包，然后重新编译安装 Python3：\n\n    # yum install sqlite*\n    \n安装Pyhton3参考博文：http://zhang-jc.github.io/2018/11/19/Centos7-3-%E5%AE%89%E8%A3%85-Python3/\n\n#### 重试可以解决的问题\n\n编译的过程可能会出现以下错误，直接重试即可：\n\n    ERROR: Could not find a version that satisfies the requirement threadloop<2,>=1 (from jaeger-client==4.3.0->-r /opt/hue-4.7.1/desktop/core/requirements.txt (line 30)) (from versions: none)\n    ERROR: No matching distribution found for threadloop<2,>=1 (from jaeger-client==4.3.0->-r /opt/hue-4.7.1/desktop/core/requirements.txt (line 30))\n    \n    ERROR: Could not find a version that satisfies the requirement vine<5.0.0a1,>=1.1.3 (from amqp<2.7,>=2.6.0->kombu<5.0,>=4.2.0->celery==4.2.1->-r /opt/hue-4.7.1/desktop/core/requirements.txt (line 7)) (from versions: none)\n    ERROR: No matching distribution found for vine<5.0.0a1,>=1.1.3 (from amqp<2.7,>=2.6.0->kombu<5.0,>=4.2.0->celery==4.2.1->-r /opt/hue-4.7.1/desktop/core/requirements.txt (line 7))\n\n### 总结\n\n    通过查看源代码可以轻松定位并解决以上问题。所以，快乐的享受有源代码的日子吧^_^\n","source":"_posts/CentOS-7-3-Python-3-7-2-编译-Hue-问题解决.md","raw":"title: CentOS 7.3 Python 3.7.2 编译 Hue 问题解决\ndate: 2020-06-24 14:43:18\ntags:\n---\n\n### 环境说明\n\n- CentOS 7.3\n- Python 3.7.2，我服务器上的安装是直接从其他服务器拷贝过来，然后配置环境变量的版本。正式因为这样我才遇到如下的问题。 \n\n### 问题解决\n\n#### python3.7-devel\n\n    $ make apps\n    \"PYTHON_VER is python3.7.\"\n    \"Python 3 module install via pip\"\n    /opt/hue-4.7.1/Makefile.vars:65: *** \"Error: must have python development packages for python3.7. Could not find Python.h. Please install python3.7-devel\".  Stop.\n\n尝试安装 python3.7-devel 包，发现该包不存在，所以通过以下方式解决：\n\n    # ln -s /opt/python/include/python3.7m /usr/include/python3.7\n\n我的 python 是安装在 /opt/ 目录下的。\n\n#### 找不到对应版本的 python\n\n    $ make apps\n    \"PYTHON_VER is python3.7.\"\n    \"Python 3 module install via pip\"\n    /opt/hue-4.7.1/Makefile.vars:73: *** \"Error: Need python version 2.7 or >= 3.5\".  Stop.\n\n创建以下软连接解决：\n\n    # ln -s /opt/python/bin/python3.7 /usr/bin/python3.7\n\n#### 找不到 npm 命令\n\n    npm --version\n    /bin/bash: npm: command not found\n    make[1]: *** [npm-install] Error 127\n\n不能使用yum安装npm，因为安装的版本太低。需要安装 node 10 以上的版本：\n\n    # cd /opt/\n    # wget -c https://npm.taobao.org/mirrors/node/latest-v10.x/node-v10.21.0-linux-x64.tar.gz\n    # tar xzf node-v10.21.0-linux-x64.tar.gz\n    # ln -s node-v10.21.0-linux-x64 node\n\n#### No module named 'pysqlite2'\n\n    Traceback (most recent call last):\n      File \"/opt/hue-4.7.1/build/env/lib/python3.7/site-packages/django/db/backends/sqlite3/base.py\", line 31, in <module>\n        from pysqlite2 import dbapi2 as Database\n    ModuleNotFoundError: No module named 'pysqlite2'\n\n\nPython3 只支持 sqlite3：\n\n    # pip install pysqlite\n    Collecting pysqlite\n      Using cached pysqlite-2.8.3.tar.gz (80 kB)\n        ERROR: Command errored out with exit status 1:\n         command: /opt/hue-4.7.1/build/env/bin/python3.7 -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-42esl1q6/pysqlite/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-42esl1q6/pysqlite/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-v9a4at6e\n             cwd: /tmp/pip-install-42esl1q6/pysqlite/\n        Complete output (1 lines):\n        pysqlite is not supported on Python 3. When using Python 3, use the sqlite3 module from the standard library.\n        ----------------------------------------\n    ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n    (env) [root@ycluster-resourcemanager1 bin]# pip install pysqlite3\n    Collecting pysqlite3\n      Downloading pysqlite3-0.4.3.tar.gz (40 kB)\n         |████████████████████████████████| 40 kB 682 kB/s \n    Building wheels for collected packages: pysqlite3\n      Building wheel for pysqlite3 (setup.py) ... done\n      Created wheel for pysqlite3: filename=pysqlite3-0.4.3-cp37-cp37m-linux_x86_64.whl size=128283 sha256=9a55112d80511438c46512efb0f4021301ba7b04af2897b6d6746158d5cd0fe5\n      Stored in directory: /root/.cache/pip/wheels/85/19/01/0a17a6cacac8cf8a4e8e8994d314abf10a9b11a1c4bc18218a\n    Successfully built pysqlite3\n    Installing collected packages: pysqlite3\n    Successfully installed pysqlite3-0.4.3\n\n重新编译，但是问题依旧：\n\n    Traceback (most recent call last):\n      File \"/opt/hue-4.7.1/build/env/lib/python3.7/site-packages/django/db/backends/sqlite3/base.py\", line 33, in <module>\n        from sqlite3 import dbapi2 as Database\n      File \"/opt/python/lib/python3.7/sqlite3/__init__.py\", line 23, in <module>\n        from sqlite3.dbapi2 import *\n      File \"/opt/python/lib/python3.7/sqlite3/dbapi2.py\", line 27, in <module>\n        from _sqlite3 import *\n    ModuleNotFoundError: No module named '_sqlite3'\n\n解决方案是在系统中安装 sqlit 相关的包，然后重新编译安装 Python3：\n\n    # yum install sqlite*\n    \n安装Pyhton3参考博文：http://zhang-jc.github.io/2018/11/19/Centos7-3-%E5%AE%89%E8%A3%85-Python3/\n\n#### 重试可以解决的问题\n\n编译的过程可能会出现以下错误，直接重试即可：\n\n    ERROR: Could not find a version that satisfies the requirement threadloop<2,>=1 (from jaeger-client==4.3.0->-r /opt/hue-4.7.1/desktop/core/requirements.txt (line 30)) (from versions: none)\n    ERROR: No matching distribution found for threadloop<2,>=1 (from jaeger-client==4.3.0->-r /opt/hue-4.7.1/desktop/core/requirements.txt (line 30))\n    \n    ERROR: Could not find a version that satisfies the requirement vine<5.0.0a1,>=1.1.3 (from amqp<2.7,>=2.6.0->kombu<5.0,>=4.2.0->celery==4.2.1->-r /opt/hue-4.7.1/desktop/core/requirements.txt (line 7)) (from versions: none)\n    ERROR: No matching distribution found for vine<5.0.0a1,>=1.1.3 (from amqp<2.7,>=2.6.0->kombu<5.0,>=4.2.0->celery==4.2.1->-r /opt/hue-4.7.1/desktop/core/requirements.txt (line 7))\n\n### 总结\n\n    通过查看源代码可以轻松定位并解决以上问题。所以，快乐的享受有源代码的日子吧^_^\n","slug":"CentOS-7-3-Python-3-7-2-编译-Hue-问题解决","published":1,"updated":"2021-07-19T16:28:00.360Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphla000ditd32ns06eyo","content":"<h3 id=\"环境说明\"><a href=\"#环境说明\" class=\"headerlink\" title=\"环境说明\"></a>环境说明</h3><ul>\n<li>CentOS 7.3</li>\n<li>Python 3.7.2，我服务器上的安装是直接从其他服务器拷贝过来，然后配置环境变量的版本。正式因为这样我才遇到如下的问题。 </li>\n</ul>\n<h3 id=\"问题解决\"><a href=\"#问题解决\" class=\"headerlink\" title=\"问题解决\"></a>问题解决</h3><h4 id=\"python3-7-devel\"><a href=\"#python3-7-devel\" class=\"headerlink\" title=\"python3.7-devel\"></a>python3.7-devel</h4><pre><code>$ make apps\n&quot;PYTHON_VER is python3.7.&quot;\n&quot;Python 3 module install via pip&quot;\n/opt/hue-4.7.1/Makefile.vars:65: *** &quot;Error: must have python development packages for python3.7. Could not find Python.h. Please install python3.7-devel&quot;.  Stop.\n</code></pre>\n<p>尝试安装 python3.7-devel 包，发现该包不存在，所以通过以下方式解决：</p>\n<pre><code># ln -s /opt/python/include/python3.7m /usr/include/python3.7\n</code></pre>\n<p>我的 python 是安装在 /opt/ 目录下的。</p>\n<h4 id=\"找不到对应版本的-python\"><a href=\"#找不到对应版本的-python\" class=\"headerlink\" title=\"找不到对应版本的 python\"></a>找不到对应版本的 python</h4><pre><code>$ make apps\n&quot;PYTHON_VER is python3.7.&quot;\n&quot;Python 3 module install via pip&quot;\n/opt/hue-4.7.1/Makefile.vars:73: *** &quot;Error: Need python version 2.7 or &gt;= 3.5&quot;.  Stop.\n</code></pre>\n<p>创建以下软连接解决：</p>\n<pre><code># ln -s /opt/python/bin/python3.7 /usr/bin/python3.7\n</code></pre>\n<h4 id=\"找不到-npm-命令\"><a href=\"#找不到-npm-命令\" class=\"headerlink\" title=\"找不到 npm 命令\"></a>找不到 npm 命令</h4><pre><code>npm --version\n/bin/bash: npm: command not found\nmake[1]: *** [npm-install] Error 127\n</code></pre>\n<p>不能使用yum安装npm，因为安装的版本太低。需要安装 node 10 以上的版本：</p>\n<pre><code># cd /opt/\n# wget -c https://npm.taobao.org/mirrors/node/latest-v10.x/node-v10.21.0-linux-x64.tar.gz\n# tar xzf node-v10.21.0-linux-x64.tar.gz\n# ln -s node-v10.21.0-linux-x64 node\n</code></pre>\n<h4 id=\"No-module-named-‘pysqlite2’\"><a href=\"#No-module-named-‘pysqlite2’\" class=\"headerlink\" title=\"No module named ‘pysqlite2’\"></a>No module named ‘pysqlite2’</h4><pre><code>Traceback (most recent call last):\n  File &quot;/opt/hue-4.7.1/build/env/lib/python3.7/site-packages/django/db/backends/sqlite3/base.py&quot;, line 31, in &lt;module&gt;\n    from pysqlite2 import dbapi2 as Database\nModuleNotFoundError: No module named &#39;pysqlite2&#39;\n</code></pre>\n<p>Python3 只支持 sqlite3：</p>\n<pre><code># pip install pysqlite\nCollecting pysqlite\n  Using cached pysqlite-2.8.3.tar.gz (80 kB)\n    ERROR: Command errored out with exit status 1:\n     command: /opt/hue-4.7.1/build/env/bin/python3.7 -c &#39;import sys, setuptools, tokenize; sys.argv[0] = &#39;&quot;&#39;&quot;&#39;/tmp/pip-install-42esl1q6/pysqlite/setup.py&#39;&quot;&#39;&quot;&#39;; __file__=&#39;&quot;&#39;&quot;&#39;/tmp/pip-install-42esl1q6/pysqlite/setup.py&#39;&quot;&#39;&quot;&#39;;f=getattr(tokenize, &#39;&quot;&#39;&quot;&#39;open&#39;&quot;&#39;&quot;&#39;, open)(__file__);code=f.read().replace(&#39;&quot;&#39;&quot;&#39;\\r\\n&#39;&quot;&#39;&quot;&#39;, &#39;&quot;&#39;&quot;&#39;\\n&#39;&quot;&#39;&quot;&#39;);f.close();exec(compile(code, __file__, &#39;&quot;&#39;&quot;&#39;exec&#39;&quot;&#39;&quot;&#39;))&#39; egg_info --egg-base /tmp/pip-pip-egg-info-v9a4at6e\n         cwd: /tmp/pip-install-42esl1q6/pysqlite/\n    Complete output (1 lines):\n    pysqlite is not supported on Python 3. When using Python 3, use the sqlite3 module from the standard library.\n    ----------------------------------------\nERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n(env) [root@ycluster-resourcemanager1 bin]# pip install pysqlite3\nCollecting pysqlite3\n  Downloading pysqlite3-0.4.3.tar.gz (40 kB)\n     |████████████████████████████████| 40 kB 682 kB/s \nBuilding wheels for collected packages: pysqlite3\n  Building wheel for pysqlite3 (setup.py) ... done\n  Created wheel for pysqlite3: filename=pysqlite3-0.4.3-cp37-cp37m-linux_x86_64.whl size=128283 sha256=9a55112d80511438c46512efb0f4021301ba7b04af2897b6d6746158d5cd0fe5\n  Stored in directory: /root/.cache/pip/wheels/85/19/01/0a17a6cacac8cf8a4e8e8994d314abf10a9b11a1c4bc18218a\nSuccessfully built pysqlite3\nInstalling collected packages: pysqlite3\nSuccessfully installed pysqlite3-0.4.3\n</code></pre>\n<p>重新编译，但是问题依旧：</p>\n<pre><code>Traceback (most recent call last):\n  File &quot;/opt/hue-4.7.1/build/env/lib/python3.7/site-packages/django/db/backends/sqlite3/base.py&quot;, line 33, in &lt;module&gt;\n    from sqlite3 import dbapi2 as Database\n  File &quot;/opt/python/lib/python3.7/sqlite3/__init__.py&quot;, line 23, in &lt;module&gt;\n    from sqlite3.dbapi2 import *\n  File &quot;/opt/python/lib/python3.7/sqlite3/dbapi2.py&quot;, line 27, in &lt;module&gt;\n    from _sqlite3 import *\nModuleNotFoundError: No module named &#39;_sqlite3&#39;\n</code></pre>\n<p>解决方案是在系统中安装 sqlit 相关的包，然后重新编译安装 Python3：</p>\n<pre><code># yum install sqlite*\n</code></pre>\n<p>安装Pyhton3参考博文：<a href=\"http://zhang-jc.github.io/2018/11/19/Centos7-3-%E5%AE%89%E8%A3%85-Python3/\">http://zhang-jc.github.io/2018/11/19/Centos7-3-%E5%AE%89%E8%A3%85-Python3/</a></p>\n<h4 id=\"重试可以解决的问题\"><a href=\"#重试可以解决的问题\" class=\"headerlink\" title=\"重试可以解决的问题\"></a>重试可以解决的问题</h4><p>编译的过程可能会出现以下错误，直接重试即可：</p>\n<pre><code>ERROR: Could not find a version that satisfies the requirement threadloop&lt;2,&gt;=1 (from jaeger-client==4.3.0-&gt;-r /opt/hue-4.7.1/desktop/core/requirements.txt (line 30)) (from versions: none)\nERROR: No matching distribution found for threadloop&lt;2,&gt;=1 (from jaeger-client==4.3.0-&gt;-r /opt/hue-4.7.1/desktop/core/requirements.txt (line 30))\n\nERROR: Could not find a version that satisfies the requirement vine&lt;5.0.0a1,&gt;=1.1.3 (from amqp&lt;2.7,&gt;=2.6.0-&gt;kombu&lt;5.0,&gt;=4.2.0-&gt;celery==4.2.1-&gt;-r /opt/hue-4.7.1/desktop/core/requirements.txt (line 7)) (from versions: none)\nERROR: No matching distribution found for vine&lt;5.0.0a1,&gt;=1.1.3 (from amqp&lt;2.7,&gt;=2.6.0-&gt;kombu&lt;5.0,&gt;=4.2.0-&gt;celery==4.2.1-&gt;-r /opt/hue-4.7.1/desktop/core/requirements.txt (line 7))\n</code></pre>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><pre><code>通过查看源代码可以轻松定位并解决以上问题。所以，快乐的享受有源代码的日子吧^_^\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"环境说明\"><a href=\"#环境说明\" class=\"headerlink\" title=\"环境说明\"></a>环境说明</h3><ul>\n<li>CentOS 7.3</li>\n<li>Python 3.7.2，我服务器上的安装是直接从其他服务器拷贝过来，然后配置环境变量的版本。正式因为这样我才遇到如下的问题。 </li>\n</ul>\n<h3 id=\"问题解决\"><a href=\"#问题解决\" class=\"headerlink\" title=\"问题解决\"></a>问题解决</h3><h4 id=\"python3-7-devel\"><a href=\"#python3-7-devel\" class=\"headerlink\" title=\"python3.7-devel\"></a>python3.7-devel</h4><pre><code>$ make apps\n&quot;PYTHON_VER is python3.7.&quot;\n&quot;Python 3 module install via pip&quot;\n/opt/hue-4.7.1/Makefile.vars:65: *** &quot;Error: must have python development packages for python3.7. Could not find Python.h. Please install python3.7-devel&quot;.  Stop.\n</code></pre>\n<p>尝试安装 python3.7-devel 包，发现该包不存在，所以通过以下方式解决：</p>\n<pre><code># ln -s /opt/python/include/python3.7m /usr/include/python3.7\n</code></pre>\n<p>我的 python 是安装在 /opt/ 目录下的。</p>\n<h4 id=\"找不到对应版本的-python\"><a href=\"#找不到对应版本的-python\" class=\"headerlink\" title=\"找不到对应版本的 python\"></a>找不到对应版本的 python</h4><pre><code>$ make apps\n&quot;PYTHON_VER is python3.7.&quot;\n&quot;Python 3 module install via pip&quot;\n/opt/hue-4.7.1/Makefile.vars:73: *** &quot;Error: Need python version 2.7 or &gt;= 3.5&quot;.  Stop.\n</code></pre>\n<p>创建以下软连接解决：</p>\n<pre><code># ln -s /opt/python/bin/python3.7 /usr/bin/python3.7\n</code></pre>\n<h4 id=\"找不到-npm-命令\"><a href=\"#找不到-npm-命令\" class=\"headerlink\" title=\"找不到 npm 命令\"></a>找不到 npm 命令</h4><pre><code>npm --version\n/bin/bash: npm: command not found\nmake[1]: *** [npm-install] Error 127\n</code></pre>\n<p>不能使用yum安装npm，因为安装的版本太低。需要安装 node 10 以上的版本：</p>\n<pre><code># cd /opt/\n# wget -c https://npm.taobao.org/mirrors/node/latest-v10.x/node-v10.21.0-linux-x64.tar.gz\n# tar xzf node-v10.21.0-linux-x64.tar.gz\n# ln -s node-v10.21.0-linux-x64 node\n</code></pre>\n<h4 id=\"No-module-named-‘pysqlite2’\"><a href=\"#No-module-named-‘pysqlite2’\" class=\"headerlink\" title=\"No module named ‘pysqlite2’\"></a>No module named ‘pysqlite2’</h4><pre><code>Traceback (most recent call last):\n  File &quot;/opt/hue-4.7.1/build/env/lib/python3.7/site-packages/django/db/backends/sqlite3/base.py&quot;, line 31, in &lt;module&gt;\n    from pysqlite2 import dbapi2 as Database\nModuleNotFoundError: No module named &#39;pysqlite2&#39;\n</code></pre>\n<p>Python3 只支持 sqlite3：</p>\n<pre><code># pip install pysqlite\nCollecting pysqlite\n  Using cached pysqlite-2.8.3.tar.gz (80 kB)\n    ERROR: Command errored out with exit status 1:\n     command: /opt/hue-4.7.1/build/env/bin/python3.7 -c &#39;import sys, setuptools, tokenize; sys.argv[0] = &#39;&quot;&#39;&quot;&#39;/tmp/pip-install-42esl1q6/pysqlite/setup.py&#39;&quot;&#39;&quot;&#39;; __file__=&#39;&quot;&#39;&quot;&#39;/tmp/pip-install-42esl1q6/pysqlite/setup.py&#39;&quot;&#39;&quot;&#39;;f=getattr(tokenize, &#39;&quot;&#39;&quot;&#39;open&#39;&quot;&#39;&quot;&#39;, open)(__file__);code=f.read().replace(&#39;&quot;&#39;&quot;&#39;\\r\\n&#39;&quot;&#39;&quot;&#39;, &#39;&quot;&#39;&quot;&#39;\\n&#39;&quot;&#39;&quot;&#39;);f.close();exec(compile(code, __file__, &#39;&quot;&#39;&quot;&#39;exec&#39;&quot;&#39;&quot;&#39;))&#39; egg_info --egg-base /tmp/pip-pip-egg-info-v9a4at6e\n         cwd: /tmp/pip-install-42esl1q6/pysqlite/\n    Complete output (1 lines):\n    pysqlite is not supported on Python 3. When using Python 3, use the sqlite3 module from the standard library.\n    ----------------------------------------\nERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n(env) [root@ycluster-resourcemanager1 bin]# pip install pysqlite3\nCollecting pysqlite3\n  Downloading pysqlite3-0.4.3.tar.gz (40 kB)\n     |████████████████████████████████| 40 kB 682 kB/s \nBuilding wheels for collected packages: pysqlite3\n  Building wheel for pysqlite3 (setup.py) ... done\n  Created wheel for pysqlite3: filename=pysqlite3-0.4.3-cp37-cp37m-linux_x86_64.whl size=128283 sha256=9a55112d80511438c46512efb0f4021301ba7b04af2897b6d6746158d5cd0fe5\n  Stored in directory: /root/.cache/pip/wheels/85/19/01/0a17a6cacac8cf8a4e8e8994d314abf10a9b11a1c4bc18218a\nSuccessfully built pysqlite3\nInstalling collected packages: pysqlite3\nSuccessfully installed pysqlite3-0.4.3\n</code></pre>\n<p>重新编译，但是问题依旧：</p>\n<pre><code>Traceback (most recent call last):\n  File &quot;/opt/hue-4.7.1/build/env/lib/python3.7/site-packages/django/db/backends/sqlite3/base.py&quot;, line 33, in &lt;module&gt;\n    from sqlite3 import dbapi2 as Database\n  File &quot;/opt/python/lib/python3.7/sqlite3/__init__.py&quot;, line 23, in &lt;module&gt;\n    from sqlite3.dbapi2 import *\n  File &quot;/opt/python/lib/python3.7/sqlite3/dbapi2.py&quot;, line 27, in &lt;module&gt;\n    from _sqlite3 import *\nModuleNotFoundError: No module named &#39;_sqlite3&#39;\n</code></pre>\n<p>解决方案是在系统中安装 sqlit 相关的包，然后重新编译安装 Python3：</p>\n<pre><code># yum install sqlite*\n</code></pre>\n<p>安装Pyhton3参考博文：<a href=\"http://zhang-jc.github.io/2018/11/19/Centos7-3-%E5%AE%89%E8%A3%85-Python3/\">http://zhang-jc.github.io/2018/11/19/Centos7-3-%E5%AE%89%E8%A3%85-Python3/</a></p>\n<h4 id=\"重试可以解决的问题\"><a href=\"#重试可以解决的问题\" class=\"headerlink\" title=\"重试可以解决的问题\"></a>重试可以解决的问题</h4><p>编译的过程可能会出现以下错误，直接重试即可：</p>\n<pre><code>ERROR: Could not find a version that satisfies the requirement threadloop&lt;2,&gt;=1 (from jaeger-client==4.3.0-&gt;-r /opt/hue-4.7.1/desktop/core/requirements.txt (line 30)) (from versions: none)\nERROR: No matching distribution found for threadloop&lt;2,&gt;=1 (from jaeger-client==4.3.0-&gt;-r /opt/hue-4.7.1/desktop/core/requirements.txt (line 30))\n\nERROR: Could not find a version that satisfies the requirement vine&lt;5.0.0a1,&gt;=1.1.3 (from amqp&lt;2.7,&gt;=2.6.0-&gt;kombu&lt;5.0,&gt;=4.2.0-&gt;celery==4.2.1-&gt;-r /opt/hue-4.7.1/desktop/core/requirements.txt (line 7)) (from versions: none)\nERROR: No matching distribution found for vine&lt;5.0.0a1,&gt;=1.1.3 (from amqp&lt;2.7,&gt;=2.6.0-&gt;kombu&lt;5.0,&gt;=4.2.0-&gt;celery==4.2.1-&gt;-r /opt/hue-4.7.1/desktop/core/requirements.txt (line 7))\n</code></pre>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><pre><code>通过查看源代码可以轻松定位并解决以上问题。所以，快乐的享受有源代码的日子吧^_^\n</code></pre>\n"},{"title":"CentOS 7.3 给一批服务器创建同一用户的简便方法","date":"2017-11-13T15:59:42.000Z","_content":"\n  \n场景是：为一批服务器创建同一用户，用户名及密码相同。\n\n<!-- more -->\n\n第一步：打通某台服务器到其他服务器的 ssh 免密钥登陆。将需要的 IP 保存的文件 ips.txt 中。如下：\n\n    172.16.72.58\n    172.16.72.59\n    172.16.72.60\n    172.16.72.62\n    172.16.72.64\n    172.16.72.65\n\n使用以下命令进行操作，需要输入密码。\n\n    cat ips.txt | xargs -i ssh-copy-id root@{}\n    \n第二步：远程执行命令脚本 remote-shell.sh\n\n    #!/bin/bash\n    \n    slaves=$1\n    command=$2\n      cat ${slaves} | while read ip\n      do\n        echo \"--------${ip}--------\"\n    ssh -p65522 ${ip} << EOF\n      eval \"${command}\"\n      exit\n    EOF\n      done\n\n第三步：创建用户 test\n\n    sh remote-shell.sh ips.txt \"useradd test\"\n\n第四步：修改用户密码\n\n    sh remote-shell.sh ips.txt 'echo \"test:afeafefa\"|chpasswd'\n","source":"_posts/CentOS-7-3-给一批服务器创建同一用户的简便方法.md","raw":"title: CentOS 7.3 给一批服务器创建同一用户的简便方法\ntags:\n  - CentOS\ncategories:\n  - 操作系统\n  - Linux\ndate: 2017-11-13 23:59:42\n---\n\n  \n场景是：为一批服务器创建同一用户，用户名及密码相同。\n\n<!-- more -->\n\n第一步：打通某台服务器到其他服务器的 ssh 免密钥登陆。将需要的 IP 保存的文件 ips.txt 中。如下：\n\n    172.16.72.58\n    172.16.72.59\n    172.16.72.60\n    172.16.72.62\n    172.16.72.64\n    172.16.72.65\n\n使用以下命令进行操作，需要输入密码。\n\n    cat ips.txt | xargs -i ssh-copy-id root@{}\n    \n第二步：远程执行命令脚本 remote-shell.sh\n\n    #!/bin/bash\n    \n    slaves=$1\n    command=$2\n      cat ${slaves} | while read ip\n      do\n        echo \"--------${ip}--------\"\n    ssh -p65522 ${ip} << EOF\n      eval \"${command}\"\n      exit\n    EOF\n      done\n\n第三步：创建用户 test\n\n    sh remote-shell.sh ips.txt \"useradd test\"\n\n第四步：修改用户密码\n\n    sh remote-shell.sh ips.txt 'echo \"test:afeafefa\"|chpasswd'\n","slug":"CentOS-7-3-给一批服务器创建同一用户的简便方法","published":1,"updated":"2021-07-19T16:28:00.228Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphlc000fitd3dfus9zq1","content":"<p>场景是：为一批服务器创建同一用户，用户名及密码相同。</p>\n<span id=\"more\"></span>\n\n<p>第一步：打通某台服务器到其他服务器的 ssh 免密钥登陆。将需要的 IP 保存的文件 ips.txt 中。如下：</p>\n<pre><code>172.16.72.58\n172.16.72.59\n172.16.72.60\n172.16.72.62\n172.16.72.64\n172.16.72.65\n</code></pre>\n<p>使用以下命令进行操作，需要输入密码。</p>\n<pre><code>cat ips.txt | xargs -i ssh-copy-id root@&#123;&#125;\n</code></pre>\n<p>第二步：远程执行命令脚本 remote-shell.sh</p>\n<pre><code>#!/bin/bash\n\nslaves=$1\ncommand=$2\n  cat $&#123;slaves&#125; | while read ip\n  do\n    echo &quot;--------$&#123;ip&#125;--------&quot;\nssh -p65522 $&#123;ip&#125; &lt;&lt; EOF\n  eval &quot;$&#123;command&#125;&quot;\n  exit\nEOF\n  done\n</code></pre>\n<p>第三步：创建用户 test</p>\n<pre><code>sh remote-shell.sh ips.txt &quot;useradd test&quot;\n</code></pre>\n<p>第四步：修改用户密码</p>\n<pre><code>sh remote-shell.sh ips.txt &#39;echo &quot;test:afeafefa&quot;|chpasswd&#39;\n</code></pre>\n","site":{"data":{}},"excerpt":"<p>场景是：为一批服务器创建同一用户，用户名及密码相同。</p>","more":"<p>第一步：打通某台服务器到其他服务器的 ssh 免密钥登陆。将需要的 IP 保存的文件 ips.txt 中。如下：</p>\n<pre><code>172.16.72.58\n172.16.72.59\n172.16.72.60\n172.16.72.62\n172.16.72.64\n172.16.72.65\n</code></pre>\n<p>使用以下命令进行操作，需要输入密码。</p>\n<pre><code>cat ips.txt | xargs -i ssh-copy-id root@&#123;&#125;\n</code></pre>\n<p>第二步：远程执行命令脚本 remote-shell.sh</p>\n<pre><code>#!/bin/bash\n\nslaves=$1\ncommand=$2\n  cat $&#123;slaves&#125; | while read ip\n  do\n    echo &quot;--------$&#123;ip&#125;--------&quot;\nssh -p65522 $&#123;ip&#125; &lt;&lt; EOF\n  eval &quot;$&#123;command&#125;&quot;\n  exit\nEOF\n  done\n</code></pre>\n<p>第三步：创建用户 test</p>\n<pre><code>sh remote-shell.sh ips.txt &quot;useradd test&quot;\n</code></pre>\n<p>第四步：修改用户密码</p>\n<pre><code>sh remote-shell.sh ips.txt &#39;echo &quot;test:afeafefa&quot;|chpasswd&#39;\n</code></pre>"},{"title":"CentOS 7.3 编译 Rsyslog 8.1903.0","date":"2019-04-03T04:18:08.000Z","_content":"\n源码下载：\n\n    # wget https://github.com/rsyslog/rsyslog/archive/v8.1903.0.tar.gz\n    # tar xzvf v8.1903.0.tar.gz\n\n<!-- more -->\n\n创建构建环境：\n\n    # autoreconf -fvi\n\n创建过程中出现以下错误：\n\n    configure.ac:46: error: possibly undefined macro: AC_DISABLE_STATIC\n          If this token and others are legitimate, please use m4_pattern_allow.\n          See the Autoconf documentation.\n    configure.ac:49: error: possibly undefined macro: AC_LIBTOOL_DLOPEN\n    configure.ac:52: error: possibly undefined macro: AC_PROG_LIBTOOL\n    autoreconf: /usr/bin/autoconf failed with exit status: 1\n\n原因是缺少 libtool，执行以下命令安装：\n\n    # yum install libtool\n\n执行配置，可以自定义选项，我是因为要编译 omhttp 模块，所以使用以下命令：\n\n    # ./configure --enable-omhttp\n\n执行过程汇总出现以下错误：\n\n    checking for LIBESTR... no\n    configure: error: Package requirements (libestr >= 0.1.9) were not met:\n    \n    No package 'libestr' found\n\n使用以下命令检查：\n\n    # rpm -q libestr\n    libestr-0.1.9-2.el7.x86_64\n\n安装开发包后解决：\n\n    # yum install libestr-devel\n\n再次执行配置出现以下错误：\n\n    checking for LIBFASTJSON... no\n    configure: error: Package requirements (libfastjson >= 0.99.8) were not met:\n    \n    No package 'libfastjson' found\n\n检查系统安装的包：\n\n    # rpm -q libfastjson\n    libfastjson-0.99.4-3.el7.x86_64\n\n安装新版本。源码下载：http://download.rsyslog.com/libfastjson/。下载 libfastjson-0.99.8.tar.gz。\n\n    # wget http://download.rsyslog.com/libfastjson/libfastjson-0.99.8.tar.gz\n    # tar xzvf libfastjson-0.99.8.tar.gz\n    # cd libfastjson-0.99.8/\n    # ./configure --prefix=/usr CC=\"gcc -m64\" PKG_CONFIG_PATH=\"/usr/lib64/pkgconfig\" --libdir=/usr/lib64\n    # make\n    # make install\n\n再次执行配置出现以下错误：\n\n    checking for LIBUUID... no\n    configure: error: Package requirements (uuid) were not met:\n    \n    No package 'uuid' found\n\n安装 libuuid：\n\n    # yum install libuuid libuuid-devel\n\n再次执行配置出现以下错误：\n\n    configure: error: in `/data/rsyslog/rsyslog-8.1903.0':\n    configure: error: libgcrypt-config not found in PATH\n\n安装 libgcrypt 模块：\n\n    # yum install libgcrypt libgcrypt-devel\n\n再次执行配置成功。^_^\n\n执行编译：\n\n    # make\n\n编译过程出现以下错误：\n\n    make[2]: 进入目录“/data/rsyslog/rsyslog-8.1903.0/grammar”\n      YACC     grammar.c\n    ../ylwrap:行178: yacc: 未找到命令\n    make[2]: *** [grammar.c] 错误 127\n    make[2]: 离开目录“/data/rsyslog/rsyslog-8.1903.0/grammar”\n    make[1]: *** [all-recursive] 错误 1\n    make[1]: 离开目录“/data/rsyslog/rsyslog-8.1903.0”\n    make: *** [all] 错误 2\n\n安装 byacc：\n\n    # yum install byacc\n\n再次编译出现以下错误：\n\n    make  all-am\n    make[3]: 进入目录“/data/rsyslog/rsyslog-8.1903.0/grammar”\n      CC       libgrammar_la-grammar.lo\n      LEX      lexer.c\n      CC       libgrammar_la-lexer.lo\n    gcc: error: ./lexer.c: No such file or directory\n    gcc: fatal error: no input files\n\n这是因为在 make 前需要执行：\n\n    # sh autogen.sh\n\n编译 omhttp 模块：\n\n    # contrib/omhttp\n    # make\n        CC       omhttp_la-omhttp.lo\n        CCLD     omhttp.la\n\n执行完成后再次 make，生成的 omhttp.so 文件在 contrib/omhttp/.libs 目录下。\n\n至此，大功告成！！O(∩_∩)O哈哈~","source":"_posts/CentOS-7-3-编译-Rsyslog-8-1903-0.md","raw":"title: CentOS 7.3 编译 Rsyslog 8.1903.0\ndate: 2019-04-03 12:18:08\ntags:\n- CentOS\n- Rsyslog\ncategories:\n- 大数据\n- Rsyslog\n---\n\n源码下载：\n\n    # wget https://github.com/rsyslog/rsyslog/archive/v8.1903.0.tar.gz\n    # tar xzvf v8.1903.0.tar.gz\n\n<!-- more -->\n\n创建构建环境：\n\n    # autoreconf -fvi\n\n创建过程中出现以下错误：\n\n    configure.ac:46: error: possibly undefined macro: AC_DISABLE_STATIC\n          If this token and others are legitimate, please use m4_pattern_allow.\n          See the Autoconf documentation.\n    configure.ac:49: error: possibly undefined macro: AC_LIBTOOL_DLOPEN\n    configure.ac:52: error: possibly undefined macro: AC_PROG_LIBTOOL\n    autoreconf: /usr/bin/autoconf failed with exit status: 1\n\n原因是缺少 libtool，执行以下命令安装：\n\n    # yum install libtool\n\n执行配置，可以自定义选项，我是因为要编译 omhttp 模块，所以使用以下命令：\n\n    # ./configure --enable-omhttp\n\n执行过程汇总出现以下错误：\n\n    checking for LIBESTR... no\n    configure: error: Package requirements (libestr >= 0.1.9) were not met:\n    \n    No package 'libestr' found\n\n使用以下命令检查：\n\n    # rpm -q libestr\n    libestr-0.1.9-2.el7.x86_64\n\n安装开发包后解决：\n\n    # yum install libestr-devel\n\n再次执行配置出现以下错误：\n\n    checking for LIBFASTJSON... no\n    configure: error: Package requirements (libfastjson >= 0.99.8) were not met:\n    \n    No package 'libfastjson' found\n\n检查系统安装的包：\n\n    # rpm -q libfastjson\n    libfastjson-0.99.4-3.el7.x86_64\n\n安装新版本。源码下载：http://download.rsyslog.com/libfastjson/。下载 libfastjson-0.99.8.tar.gz。\n\n    # wget http://download.rsyslog.com/libfastjson/libfastjson-0.99.8.tar.gz\n    # tar xzvf libfastjson-0.99.8.tar.gz\n    # cd libfastjson-0.99.8/\n    # ./configure --prefix=/usr CC=\"gcc -m64\" PKG_CONFIG_PATH=\"/usr/lib64/pkgconfig\" --libdir=/usr/lib64\n    # make\n    # make install\n\n再次执行配置出现以下错误：\n\n    checking for LIBUUID... no\n    configure: error: Package requirements (uuid) were not met:\n    \n    No package 'uuid' found\n\n安装 libuuid：\n\n    # yum install libuuid libuuid-devel\n\n再次执行配置出现以下错误：\n\n    configure: error: in `/data/rsyslog/rsyslog-8.1903.0':\n    configure: error: libgcrypt-config not found in PATH\n\n安装 libgcrypt 模块：\n\n    # yum install libgcrypt libgcrypt-devel\n\n再次执行配置成功。^_^\n\n执行编译：\n\n    # make\n\n编译过程出现以下错误：\n\n    make[2]: 进入目录“/data/rsyslog/rsyslog-8.1903.0/grammar”\n      YACC     grammar.c\n    ../ylwrap:行178: yacc: 未找到命令\n    make[2]: *** [grammar.c] 错误 127\n    make[2]: 离开目录“/data/rsyslog/rsyslog-8.1903.0/grammar”\n    make[1]: *** [all-recursive] 错误 1\n    make[1]: 离开目录“/data/rsyslog/rsyslog-8.1903.0”\n    make: *** [all] 错误 2\n\n安装 byacc：\n\n    # yum install byacc\n\n再次编译出现以下错误：\n\n    make  all-am\n    make[3]: 进入目录“/data/rsyslog/rsyslog-8.1903.0/grammar”\n      CC       libgrammar_la-grammar.lo\n      LEX      lexer.c\n      CC       libgrammar_la-lexer.lo\n    gcc: error: ./lexer.c: No such file or directory\n    gcc: fatal error: no input files\n\n这是因为在 make 前需要执行：\n\n    # sh autogen.sh\n\n编译 omhttp 模块：\n\n    # contrib/omhttp\n    # make\n        CC       omhttp_la-omhttp.lo\n        CCLD     omhttp.la\n\n执行完成后再次 make，生成的 omhttp.so 文件在 contrib/omhttp/.libs 目录下。\n\n至此，大功告成！！O(∩_∩)O哈哈~","slug":"CentOS-7-3-编译-Rsyslog-8-1903-0","published":1,"updated":"2021-07-19T16:28:00.232Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphle000jitd3e70o6b63","content":"<p>源码下载：</p>\n<pre><code># wget https://github.com/rsyslog/rsyslog/archive/v8.1903.0.tar.gz\n# tar xzvf v8.1903.0.tar.gz\n</code></pre>\n<span id=\"more\"></span>\n\n<p>创建构建环境：</p>\n<pre><code># autoreconf -fvi\n</code></pre>\n<p>创建过程中出现以下错误：</p>\n<pre><code>configure.ac:46: error: possibly undefined macro: AC_DISABLE_STATIC\n      If this token and others are legitimate, please use m4_pattern_allow.\n      See the Autoconf documentation.\nconfigure.ac:49: error: possibly undefined macro: AC_LIBTOOL_DLOPEN\nconfigure.ac:52: error: possibly undefined macro: AC_PROG_LIBTOOL\nautoreconf: /usr/bin/autoconf failed with exit status: 1\n</code></pre>\n<p>原因是缺少 libtool，执行以下命令安装：</p>\n<pre><code># yum install libtool\n</code></pre>\n<p>执行配置，可以自定义选项，我是因为要编译 omhttp 模块，所以使用以下命令：</p>\n<pre><code># ./configure --enable-omhttp\n</code></pre>\n<p>执行过程汇总出现以下错误：</p>\n<pre><code>checking for LIBESTR... no\nconfigure: error: Package requirements (libestr &gt;= 0.1.9) were not met:\n\nNo package &#39;libestr&#39; found\n</code></pre>\n<p>使用以下命令检查：</p>\n<pre><code># rpm -q libestr\nlibestr-0.1.9-2.el7.x86_64\n</code></pre>\n<p>安装开发包后解决：</p>\n<pre><code># yum install libestr-devel\n</code></pre>\n<p>再次执行配置出现以下错误：</p>\n<pre><code>checking for LIBFASTJSON... no\nconfigure: error: Package requirements (libfastjson &gt;= 0.99.8) were not met:\n\nNo package &#39;libfastjson&#39; found\n</code></pre>\n<p>检查系统安装的包：</p>\n<pre><code># rpm -q libfastjson\nlibfastjson-0.99.4-3.el7.x86_64\n</code></pre>\n<p>安装新版本。源码下载：<a href=\"http://download.rsyslog.com/libfastjson/%E3%80%82%E4%B8%8B%E8%BD%BD\">http://download.rsyslog.com/libfastjson/。下载</a> libfastjson-0.99.8.tar.gz。</p>\n<pre><code># wget http://download.rsyslog.com/libfastjson/libfastjson-0.99.8.tar.gz\n# tar xzvf libfastjson-0.99.8.tar.gz\n# cd libfastjson-0.99.8/\n# ./configure --prefix=/usr CC=&quot;gcc -m64&quot; PKG_CONFIG_PATH=&quot;/usr/lib64/pkgconfig&quot; --libdir=/usr/lib64\n# make\n# make install\n</code></pre>\n<p>再次执行配置出现以下错误：</p>\n<pre><code>checking for LIBUUID... no\nconfigure: error: Package requirements (uuid) were not met:\n\nNo package &#39;uuid&#39; found\n</code></pre>\n<p>安装 libuuid：</p>\n<pre><code># yum install libuuid libuuid-devel\n</code></pre>\n<p>再次执行配置出现以下错误：</p>\n<pre><code>configure: error: in `/data/rsyslog/rsyslog-8.1903.0&#39;:\nconfigure: error: libgcrypt-config not found in PATH\n</code></pre>\n<p>安装 libgcrypt 模块：</p>\n<pre><code># yum install libgcrypt libgcrypt-devel\n</code></pre>\n<p>再次执行配置成功。^_^</p>\n<p>执行编译：</p>\n<pre><code># make\n</code></pre>\n<p>编译过程出现以下错误：</p>\n<pre><code>make[2]: 进入目录“/data/rsyslog/rsyslog-8.1903.0/grammar”\n  YACC     grammar.c\n../ylwrap:行178: yacc: 未找到命令\nmake[2]: *** [grammar.c] 错误 127\nmake[2]: 离开目录“/data/rsyslog/rsyslog-8.1903.0/grammar”\nmake[1]: *** [all-recursive] 错误 1\nmake[1]: 离开目录“/data/rsyslog/rsyslog-8.1903.0”\nmake: *** [all] 错误 2\n</code></pre>\n<p>安装 byacc：</p>\n<pre><code># yum install byacc\n</code></pre>\n<p>再次编译出现以下错误：</p>\n<pre><code>make  all-am\nmake[3]: 进入目录“/data/rsyslog/rsyslog-8.1903.0/grammar”\n  CC       libgrammar_la-grammar.lo\n  LEX      lexer.c\n  CC       libgrammar_la-lexer.lo\ngcc: error: ./lexer.c: No such file or directory\ngcc: fatal error: no input files\n</code></pre>\n<p>这是因为在 make 前需要执行：</p>\n<pre><code># sh autogen.sh\n</code></pre>\n<p>编译 omhttp 模块：</p>\n<pre><code># contrib/omhttp\n# make\n    CC       omhttp_la-omhttp.lo\n    CCLD     omhttp.la\n</code></pre>\n<p>执行完成后再次 make，生成的 omhttp.so 文件在 contrib/omhttp/.libs 目录下。</p>\n<p>至此，大功告成！！O(∩_∩)O哈哈~</p>\n","site":{"data":{}},"excerpt":"<p>源码下载：</p>\n<pre><code># wget https://github.com/rsyslog/rsyslog/archive/v8.1903.0.tar.gz\n# tar xzvf v8.1903.0.tar.gz\n</code></pre>","more":"<p>创建构建环境：</p>\n<pre><code># autoreconf -fvi\n</code></pre>\n<p>创建过程中出现以下错误：</p>\n<pre><code>configure.ac:46: error: possibly undefined macro: AC_DISABLE_STATIC\n      If this token and others are legitimate, please use m4_pattern_allow.\n      See the Autoconf documentation.\nconfigure.ac:49: error: possibly undefined macro: AC_LIBTOOL_DLOPEN\nconfigure.ac:52: error: possibly undefined macro: AC_PROG_LIBTOOL\nautoreconf: /usr/bin/autoconf failed with exit status: 1\n</code></pre>\n<p>原因是缺少 libtool，执行以下命令安装：</p>\n<pre><code># yum install libtool\n</code></pre>\n<p>执行配置，可以自定义选项，我是因为要编译 omhttp 模块，所以使用以下命令：</p>\n<pre><code># ./configure --enable-omhttp\n</code></pre>\n<p>执行过程汇总出现以下错误：</p>\n<pre><code>checking for LIBESTR... no\nconfigure: error: Package requirements (libestr &gt;= 0.1.9) were not met:\n\nNo package &#39;libestr&#39; found\n</code></pre>\n<p>使用以下命令检查：</p>\n<pre><code># rpm -q libestr\nlibestr-0.1.9-2.el7.x86_64\n</code></pre>\n<p>安装开发包后解决：</p>\n<pre><code># yum install libestr-devel\n</code></pre>\n<p>再次执行配置出现以下错误：</p>\n<pre><code>checking for LIBFASTJSON... no\nconfigure: error: Package requirements (libfastjson &gt;= 0.99.8) were not met:\n\nNo package &#39;libfastjson&#39; found\n</code></pre>\n<p>检查系统安装的包：</p>\n<pre><code># rpm -q libfastjson\nlibfastjson-0.99.4-3.el7.x86_64\n</code></pre>\n<p>安装新版本。源码下载：<a href=\"http://download.rsyslog.com/libfastjson/%E3%80%82%E4%B8%8B%E8%BD%BD\">http://download.rsyslog.com/libfastjson/。下载</a> libfastjson-0.99.8.tar.gz。</p>\n<pre><code># wget http://download.rsyslog.com/libfastjson/libfastjson-0.99.8.tar.gz\n# tar xzvf libfastjson-0.99.8.tar.gz\n# cd libfastjson-0.99.8/\n# ./configure --prefix=/usr CC=&quot;gcc -m64&quot; PKG_CONFIG_PATH=&quot;/usr/lib64/pkgconfig&quot; --libdir=/usr/lib64\n# make\n# make install\n</code></pre>\n<p>再次执行配置出现以下错误：</p>\n<pre><code>checking for LIBUUID... no\nconfigure: error: Package requirements (uuid) were not met:\n\nNo package &#39;uuid&#39; found\n</code></pre>\n<p>安装 libuuid：</p>\n<pre><code># yum install libuuid libuuid-devel\n</code></pre>\n<p>再次执行配置出现以下错误：</p>\n<pre><code>configure: error: in `/data/rsyslog/rsyslog-8.1903.0&#39;:\nconfigure: error: libgcrypt-config not found in PATH\n</code></pre>\n<p>安装 libgcrypt 模块：</p>\n<pre><code># yum install libgcrypt libgcrypt-devel\n</code></pre>\n<p>再次执行配置成功。^_^</p>\n<p>执行编译：</p>\n<pre><code># make\n</code></pre>\n<p>编译过程出现以下错误：</p>\n<pre><code>make[2]: 进入目录“/data/rsyslog/rsyslog-8.1903.0/grammar”\n  YACC     grammar.c\n../ylwrap:行178: yacc: 未找到命令\nmake[2]: *** [grammar.c] 错误 127\nmake[2]: 离开目录“/data/rsyslog/rsyslog-8.1903.0/grammar”\nmake[1]: *** [all-recursive] 错误 1\nmake[1]: 离开目录“/data/rsyslog/rsyslog-8.1903.0”\nmake: *** [all] 错误 2\n</code></pre>\n<p>安装 byacc：</p>\n<pre><code># yum install byacc\n</code></pre>\n<p>再次编译出现以下错误：</p>\n<pre><code>make  all-am\nmake[3]: 进入目录“/data/rsyslog/rsyslog-8.1903.0/grammar”\n  CC       libgrammar_la-grammar.lo\n  LEX      lexer.c\n  CC       libgrammar_la-lexer.lo\ngcc: error: ./lexer.c: No such file or directory\ngcc: fatal error: no input files\n</code></pre>\n<p>这是因为在 make 前需要执行：</p>\n<pre><code># sh autogen.sh\n</code></pre>\n<p>编译 omhttp 模块：</p>\n<pre><code># contrib/omhttp\n# make\n    CC       omhttp_la-omhttp.lo\n    CCLD     omhttp.la\n</code></pre>\n<p>执行完成后再次 make，生成的 omhttp.so 文件在 contrib/omhttp/.libs 目录下。</p>\n<p>至此，大功告成！！O(∩_∩)O哈哈~</p>"},{"title":"CentOS 7 查看磁盘文件系统格式","date":"2018-03-08T07:19:35.000Z","_content":"\n\n前提是磁盘已经格式化并挂载，可以直接查看 fstab 文件：\n\n    # cat /etc/fstab \n\n    #\n    # /etc/fstab\n    # Created by anaconda on Fri May  5 20:02:53 2017\n    #\n    # Accessible filesystems, by reference, are maintained under '/dev/disk'\n    # See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info\n    #\n    /dev/mapper/centos-root /                       xfs     defaults        0 0\n    UUID=12921ad4-88da-469d-94df-1aa2eaec6db4 /boot                   xfs     defaults        0 0\n    /dev/mapper/centos-swap swap                    swap    defaults        0 0\n    /dev/sda1 /data0                       xfs     defaults        0 0\n    /dev/sdb1 /data1                       xfs     defaults        0 0\n    /dev/sdc1 /data2                       xfs     defaults        0 0\n    /dev/sdd1 /data3                       xfs     defaults        0 0\n    /dev/sde1 /data4                       xfs     defaults        0 0\n    /dev/sdf1 /data5                       xfs     defaults        0 0\n    /dev/sdg1 /data6                       xfs     defaults        0 0\n    /dev/sdh1 /data7                       xfs     defaults        0 0\n    /dev/sdi1 /data8                       xfs     defaults        0 0\n    /dev/sdj1 /data9                       xfs     defaults        0 0\n    /dev/sdl1 /dataa                       xfs     defaults        0 0\n    /dev/sdm1 /datab                       xfs     defaults        0 0\n\n","source":"_posts/CentOS-7-查看磁盘文件系统格式.md","raw":"title: CentOS 7 查看磁盘文件系统格式\ntags:\n  - Linux\n  - CentOS\ncategories:\n  - 操作系统\n  - Linux\ndate: 2018-03-08 15:19:35\n---\n\n\n前提是磁盘已经格式化并挂载，可以直接查看 fstab 文件：\n\n    # cat /etc/fstab \n\n    #\n    # /etc/fstab\n    # Created by anaconda on Fri May  5 20:02:53 2017\n    #\n    # Accessible filesystems, by reference, are maintained under '/dev/disk'\n    # See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info\n    #\n    /dev/mapper/centos-root /                       xfs     defaults        0 0\n    UUID=12921ad4-88da-469d-94df-1aa2eaec6db4 /boot                   xfs     defaults        0 0\n    /dev/mapper/centos-swap swap                    swap    defaults        0 0\n    /dev/sda1 /data0                       xfs     defaults        0 0\n    /dev/sdb1 /data1                       xfs     defaults        0 0\n    /dev/sdc1 /data2                       xfs     defaults        0 0\n    /dev/sdd1 /data3                       xfs     defaults        0 0\n    /dev/sde1 /data4                       xfs     defaults        0 0\n    /dev/sdf1 /data5                       xfs     defaults        0 0\n    /dev/sdg1 /data6                       xfs     defaults        0 0\n    /dev/sdh1 /data7                       xfs     defaults        0 0\n    /dev/sdi1 /data8                       xfs     defaults        0 0\n    /dev/sdj1 /data9                       xfs     defaults        0 0\n    /dev/sdl1 /dataa                       xfs     defaults        0 0\n    /dev/sdm1 /datab                       xfs     defaults        0 0\n\n","slug":"CentOS-7-查看磁盘文件系统格式","published":1,"updated":"2021-07-19T16:28:00.232Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphlf000kitd35pld4lta","content":"<p>前提是磁盘已经格式化并挂载，可以直接查看 fstab 文件：</p>\n<pre><code># cat /etc/fstab \n\n#\n# /etc/fstab\n# Created by anaconda on Fri May  5 20:02:53 2017\n#\n# Accessible filesystems, by reference, are maintained under &#39;/dev/disk&#39;\n# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info\n#\n/dev/mapper/centos-root /                       xfs     defaults        0 0\nUUID=12921ad4-88da-469d-94df-1aa2eaec6db4 /boot                   xfs     defaults        0 0\n/dev/mapper/centos-swap swap                    swap    defaults        0 0\n/dev/sda1 /data0                       xfs     defaults        0 0\n/dev/sdb1 /data1                       xfs     defaults        0 0\n/dev/sdc1 /data2                       xfs     defaults        0 0\n/dev/sdd1 /data3                       xfs     defaults        0 0\n/dev/sde1 /data4                       xfs     defaults        0 0\n/dev/sdf1 /data5                       xfs     defaults        0 0\n/dev/sdg1 /data6                       xfs     defaults        0 0\n/dev/sdh1 /data7                       xfs     defaults        0 0\n/dev/sdi1 /data8                       xfs     defaults        0 0\n/dev/sdj1 /data9                       xfs     defaults        0 0\n/dev/sdl1 /dataa                       xfs     defaults        0 0\n/dev/sdm1 /datab                       xfs     defaults        0 0\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>前提是磁盘已经格式化并挂载，可以直接查看 fstab 文件：</p>\n<pre><code># cat /etc/fstab \n\n#\n# /etc/fstab\n# Created by anaconda on Fri May  5 20:02:53 2017\n#\n# Accessible filesystems, by reference, are maintained under &#39;/dev/disk&#39;\n# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info\n#\n/dev/mapper/centos-root /                       xfs     defaults        0 0\nUUID=12921ad4-88da-469d-94df-1aa2eaec6db4 /boot                   xfs     defaults        0 0\n/dev/mapper/centos-swap swap                    swap    defaults        0 0\n/dev/sda1 /data0                       xfs     defaults        0 0\n/dev/sdb1 /data1                       xfs     defaults        0 0\n/dev/sdc1 /data2                       xfs     defaults        0 0\n/dev/sdd1 /data3                       xfs     defaults        0 0\n/dev/sde1 /data4                       xfs     defaults        0 0\n/dev/sdf1 /data5                       xfs     defaults        0 0\n/dev/sdg1 /data6                       xfs     defaults        0 0\n/dev/sdh1 /data7                       xfs     defaults        0 0\n/dev/sdi1 /data8                       xfs     defaults        0 0\n/dev/sdj1 /data9                       xfs     defaults        0 0\n/dev/sdl1 /dataa                       xfs     defaults        0 0\n/dev/sdm1 /datab                       xfs     defaults        0 0\n</code></pre>\n"},{"title":"CentOS curl 没有到主机的路由","date":"2018-11-23T08:25:09.000Z","_content":"\n今天在 yum 安装时遇到网路不通的问题，使用 curl 命令测试如下：\n\n    # curl www.baidu.com\n    curl: (7) Failed connect to 172.16.72.69:443; 没有到主机的路由\n\n通过查找发现是服务器设置了代理导致的，代理配置在 /etc/profile 中，如下：\n\n    http_proxy=http://172.16.72.69:80\n    http_proxy=http://172.16.72.69:443\n    ftp_proxy=http://172.16.72.69:80\n    export http_proxy\n    export ftp_prox    \n\n该配置也可能会在其他环境变量配置文件中，可自行检查。\n","source":"_posts/CentOS-curl-没有到主机的路由.md","raw":"title: CentOS curl 没有到主机的路由\ndate: 2018-11-23 16:25:09\ntags:\n- CentOS\n- Linux\ncategories:\n- 操作系统\n- Linux\n---\n\n今天在 yum 安装时遇到网路不通的问题，使用 curl 命令测试如下：\n\n    # curl www.baidu.com\n    curl: (7) Failed connect to 172.16.72.69:443; 没有到主机的路由\n\n通过查找发现是服务器设置了代理导致的，代理配置在 /etc/profile 中，如下：\n\n    http_proxy=http://172.16.72.69:80\n    http_proxy=http://172.16.72.69:443\n    ftp_proxy=http://172.16.72.69:80\n    export http_proxy\n    export ftp_prox    \n\n该配置也可能会在其他环境变量配置文件中，可自行检查。\n","slug":"CentOS-curl-没有到主机的路由","published":1,"updated":"2021-07-19T16:28:00.232Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphlh000nitd3a7eof7cj","content":"<p>今天在 yum 安装时遇到网路不通的问题，使用 curl 命令测试如下：</p>\n<pre><code># curl www.baidu.com\ncurl: (7) Failed connect to 172.16.72.69:443; 没有到主机的路由\n</code></pre>\n<p>通过查找发现是服务器设置了代理导致的，代理配置在 /etc/profile 中，如下：</p>\n<pre><code>http_proxy=http://172.16.72.69:80\nhttp_proxy=http://172.16.72.69:443\nftp_proxy=http://172.16.72.69:80\nexport http_proxy\nexport ftp_prox    \n</code></pre>\n<p>该配置也可能会在其他环境变量配置文件中，可自行检查。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>今天在 yum 安装时遇到网路不通的问题，使用 curl 命令测试如下：</p>\n<pre><code># curl www.baidu.com\ncurl: (7) Failed connect to 172.16.72.69:443; 没有到主机的路由\n</code></pre>\n<p>通过查找发现是服务器设置了代理导致的，代理配置在 /etc/profile 中，如下：</p>\n<pre><code>http_proxy=http://172.16.72.69:80\nhttp_proxy=http://172.16.72.69:443\nftp_proxy=http://172.16.72.69:80\nexport http_proxy\nexport ftp_prox    \n</code></pre>\n<p>该配置也可能会在其他环境变量配置文件中，可自行检查。</p>\n"},{"title":"CentOS 挂载掉了的硬盘","date":"2017-10-17T06:03:19.000Z","_content":"\n\n如果服务器硬盘因为某种原因掉了（例如服务器搬迁后），此时使用 df 命令看不到需要的磁盘：\n\n    $ df -h\n    Filesystem      Size  Used Avail Use% Mounted on\n    /dev/sda3       267G   14G  240G   6% /\n    tmpfs            63G     0   63G   0% /dev/shm\n    /dev/sda1       194M   28M  157M  15% /boot\n\n<!-- more -->\n\n使用 fdisk 命令查看磁盘情况：\n\n    $ fdisk -l\n    \n    Disk /dev/sda: 299.4 GB, 299439751168 bytes\n    255 heads, 63 sectors/track, 36404 cylinders\n    Units = cylinders of 16065 * 512 = 8225280 bytes\n    Sector size (logical/physical): 512 bytes / 512 bytes\n    I/O size (minimum/optimal): 512 bytes / 512 bytes\n    Disk identifier: 0x000b6c80\n    \n       Device Boot      Start         End      Blocks   Id  System\n    /dev/sda1   *           1          26      204800   83  Linux\n    Partition 1 does not end on cylinder boundary.\n    /dev/sda2              26        1070     8388608   82  Linux swap / Solaris\n    /dev/sda3            1070       36405   283827200   83  Linux\n    \n    WARNING: GPT (GUID Partition Table) detected on '/dev/sdb'! The util fdisk doesn't support GPT. Use GNU Parted.\n    \n    \n    Disk /dev/sdb: 66006.7 GB, 66006668017664 bytes\n    255 heads, 63 sectors/track, 8024853 cylinders\n    Units = cylinders of 16065 * 512 = 8225280 bytes\n    Sector size (logical/physical): 512 bytes / 4096 bytes\n    I/O size (minimum/optimal): 4096 bytes / 4096 bytes\n    Disk identifier: 0x00000000\n    \n       Device Boot      Start         End      Blocks   Id  System\n    /dev/sdb1               1      267350  2147483647+  ee  GPT\n    Partition 1 does not start on physical sector boundary.\n\n从上面的信息可以看出 /dev/sdb 没有挂载。因为是原来的磁盘掉了，而且文件系统目录也是已知的，所以只需要重新挂载即可。注意，千万不要重新格式化，否则原来磁盘上的数据都会丢失。挂载命令如下：\n\n    $ mount /dev/sdb1 /data\n    \n再次查看：\n\n    $ df -h\n    Filesystem      Size  Used Avail Use% Mounted on\n    /dev/sda3       267G   14G  240G   6% /\n    tmpfs            63G     0   63G   0% /dev/shm\n    /dev/sda1       194M   28M  157M  15% /boot\n    /dev/sdb1        61T   23T   38T  38% /data\n\n磁盘已经挂载成功。\n","source":"_posts/CentOS-挂载掉了的硬盘.md","raw":"title: CentOS 挂载掉了的硬盘\ntags:\n  - Linux\n  - CentOS\ncategories:\n  - 操作系统\n  - Linux\ndate: 2017-10-17 14:03:19\n---\n\n\n如果服务器硬盘因为某种原因掉了（例如服务器搬迁后），此时使用 df 命令看不到需要的磁盘：\n\n    $ df -h\n    Filesystem      Size  Used Avail Use% Mounted on\n    /dev/sda3       267G   14G  240G   6% /\n    tmpfs            63G     0   63G   0% /dev/shm\n    /dev/sda1       194M   28M  157M  15% /boot\n\n<!-- more -->\n\n使用 fdisk 命令查看磁盘情况：\n\n    $ fdisk -l\n    \n    Disk /dev/sda: 299.4 GB, 299439751168 bytes\n    255 heads, 63 sectors/track, 36404 cylinders\n    Units = cylinders of 16065 * 512 = 8225280 bytes\n    Sector size (logical/physical): 512 bytes / 512 bytes\n    I/O size (minimum/optimal): 512 bytes / 512 bytes\n    Disk identifier: 0x000b6c80\n    \n       Device Boot      Start         End      Blocks   Id  System\n    /dev/sda1   *           1          26      204800   83  Linux\n    Partition 1 does not end on cylinder boundary.\n    /dev/sda2              26        1070     8388608   82  Linux swap / Solaris\n    /dev/sda3            1070       36405   283827200   83  Linux\n    \n    WARNING: GPT (GUID Partition Table) detected on '/dev/sdb'! The util fdisk doesn't support GPT. Use GNU Parted.\n    \n    \n    Disk /dev/sdb: 66006.7 GB, 66006668017664 bytes\n    255 heads, 63 sectors/track, 8024853 cylinders\n    Units = cylinders of 16065 * 512 = 8225280 bytes\n    Sector size (logical/physical): 512 bytes / 4096 bytes\n    I/O size (minimum/optimal): 4096 bytes / 4096 bytes\n    Disk identifier: 0x00000000\n    \n       Device Boot      Start         End      Blocks   Id  System\n    /dev/sdb1               1      267350  2147483647+  ee  GPT\n    Partition 1 does not start on physical sector boundary.\n\n从上面的信息可以看出 /dev/sdb 没有挂载。因为是原来的磁盘掉了，而且文件系统目录也是已知的，所以只需要重新挂载即可。注意，千万不要重新格式化，否则原来磁盘上的数据都会丢失。挂载命令如下：\n\n    $ mount /dev/sdb1 /data\n    \n再次查看：\n\n    $ df -h\n    Filesystem      Size  Used Avail Use% Mounted on\n    /dev/sda3       267G   14G  240G   6% /\n    tmpfs            63G     0   63G   0% /dev/shm\n    /dev/sda1       194M   28M  157M  15% /boot\n    /dev/sdb1        61T   23T   38T  38% /data\n\n磁盘已经挂载成功。\n","slug":"CentOS-挂载掉了的硬盘","published":1,"updated":"2021-07-19T16:28:00.232Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphlj000pitd3d9jy00lz","content":"<p>如果服务器硬盘因为某种原因掉了（例如服务器搬迁后），此时使用 df 命令看不到需要的磁盘：</p>\n<pre><code>$ df -h\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/sda3       267G   14G  240G   6% /\ntmpfs            63G     0   63G   0% /dev/shm\n/dev/sda1       194M   28M  157M  15% /boot\n</code></pre>\n<span id=\"more\"></span>\n\n<p>使用 fdisk 命令查看磁盘情况：</p>\n<pre><code>$ fdisk -l\n\nDisk /dev/sda: 299.4 GB, 299439751168 bytes\n255 heads, 63 sectors/track, 36404 cylinders\nUnits = cylinders of 16065 * 512 = 8225280 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk identifier: 0x000b6c80\n\n   Device Boot      Start         End      Blocks   Id  System\n/dev/sda1   *           1          26      204800   83  Linux\nPartition 1 does not end on cylinder boundary.\n/dev/sda2              26        1070     8388608   82  Linux swap / Solaris\n/dev/sda3            1070       36405   283827200   83  Linux\n\nWARNING: GPT (GUID Partition Table) detected on &#39;/dev/sdb&#39;! The util fdisk doesn&#39;t support GPT. Use GNU Parted.\n\n\nDisk /dev/sdb: 66006.7 GB, 66006668017664 bytes\n255 heads, 63 sectors/track, 8024853 cylinders\nUnits = cylinders of 16065 * 512 = 8225280 bytes\nSector size (logical/physical): 512 bytes / 4096 bytes\nI/O size (minimum/optimal): 4096 bytes / 4096 bytes\nDisk identifier: 0x00000000\n\n   Device Boot      Start         End      Blocks   Id  System\n/dev/sdb1               1      267350  2147483647+  ee  GPT\nPartition 1 does not start on physical sector boundary.\n</code></pre>\n<p>从上面的信息可以看出 /dev/sdb 没有挂载。因为是原来的磁盘掉了，而且文件系统目录也是已知的，所以只需要重新挂载即可。注意，千万不要重新格式化，否则原来磁盘上的数据都会丢失。挂载命令如下：</p>\n<pre><code>$ mount /dev/sdb1 /data\n</code></pre>\n<p>再次查看：</p>\n<pre><code>$ df -h\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/sda3       267G   14G  240G   6% /\ntmpfs            63G     0   63G   0% /dev/shm\n/dev/sda1       194M   28M  157M  15% /boot\n/dev/sdb1        61T   23T   38T  38% /data\n</code></pre>\n<p>磁盘已经挂载成功。</p>\n","site":{"data":{}},"excerpt":"<p>如果服务器硬盘因为某种原因掉了（例如服务器搬迁后），此时使用 df 命令看不到需要的磁盘：</p>\n<pre><code>$ df -h\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/sda3       267G   14G  240G   6% /\ntmpfs            63G     0   63G   0% /dev/shm\n/dev/sda1       194M   28M  157M  15% /boot\n</code></pre>","more":"<p>使用 fdisk 命令查看磁盘情况：</p>\n<pre><code>$ fdisk -l\n\nDisk /dev/sda: 299.4 GB, 299439751168 bytes\n255 heads, 63 sectors/track, 36404 cylinders\nUnits = cylinders of 16065 * 512 = 8225280 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk identifier: 0x000b6c80\n\n   Device Boot      Start         End      Blocks   Id  System\n/dev/sda1   *           1          26      204800   83  Linux\nPartition 1 does not end on cylinder boundary.\n/dev/sda2              26        1070     8388608   82  Linux swap / Solaris\n/dev/sda3            1070       36405   283827200   83  Linux\n\nWARNING: GPT (GUID Partition Table) detected on &#39;/dev/sdb&#39;! The util fdisk doesn&#39;t support GPT. Use GNU Parted.\n\n\nDisk /dev/sdb: 66006.7 GB, 66006668017664 bytes\n255 heads, 63 sectors/track, 8024853 cylinders\nUnits = cylinders of 16065 * 512 = 8225280 bytes\nSector size (logical/physical): 512 bytes / 4096 bytes\nI/O size (minimum/optimal): 4096 bytes / 4096 bytes\nDisk identifier: 0x00000000\n\n   Device Boot      Start         End      Blocks   Id  System\n/dev/sdb1               1      267350  2147483647+  ee  GPT\nPartition 1 does not start on physical sector boundary.\n</code></pre>\n<p>从上面的信息可以看出 /dev/sdb 没有挂载。因为是原来的磁盘掉了，而且文件系统目录也是已知的，所以只需要重新挂载即可。注意，千万不要重新格式化，否则原来磁盘上的数据都会丢失。挂载命令如下：</p>\n<pre><code>$ mount /dev/sdb1 /data\n</code></pre>\n<p>再次查看：</p>\n<pre><code>$ df -h\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/sda3       267G   14G  240G   6% /\ntmpfs            63G     0   63G   0% /dev/shm\n/dev/sda1       194M   28M  157M  15% /boot\n/dev/sdb1        61T   23T   38T  38% /data\n</code></pre>\n<p>磁盘已经挂载成功。</p>"},{"title":"CentOS 删除 libc.so.6 后命令不可用故障恢复方法","date":"2019-04-04T08:40:32.000Z","_content":"在升级 glibc 的过程中需要先删除软连接 /lib64/libc.so.6。删除后执行系统命令会一直报错：\n\n<!--more-->\n\n    # ls -l libc.so.6\n    lrwxrwxrwx 1 root root 12 4月   2 13:49 libc.so.6 -> libc-2.12.so\n    # mv libc.so.6 libc.so.6.bak\n    # mv libc.so.6.bak libc.so.6\n    mv: error while loading shared libraries: libc.so.6: cannot open shared object file: No such file or directory\n\n从上面的错误信息可以发现想通过 mv 命令恢复是行不通的。可以通过执行以下命令恢复：\n\n    # LD_PRELOAD=/lib64/libc-2.12.so ln -s /lib64/libc-2.12.so /lib64/libc.so.6\n\n如果软链到自己安装的 glibc 版本，通过以下命令：\n\n    # LD_PRELOAD=/opt/glibc-2.14/lib/libc-2.14.so ln -s /opt/glibc-2.14/lib/libc-2.14.so /lib64/libc.so.6\n\n如果上面的方法还是不行，可以使用下面的命令：\n\n    # sln /lib64/libc-2.12.so /lib64/libc.so.6","source":"_posts/CentOS-删除-libc-so-6-后命令不可用故障恢复方法.md","raw":"title: CentOS 删除 libc.so.6 后命令不可用故障恢复方法\ndate: 2019-04-04 16:40:32\ntags:\n- CentOS\ncategories:\n- 操作系统\n- Linux\n---\n在升级 glibc 的过程中需要先删除软连接 /lib64/libc.so.6。删除后执行系统命令会一直报错：\n\n<!--more-->\n\n    # ls -l libc.so.6\n    lrwxrwxrwx 1 root root 12 4月   2 13:49 libc.so.6 -> libc-2.12.so\n    # mv libc.so.6 libc.so.6.bak\n    # mv libc.so.6.bak libc.so.6\n    mv: error while loading shared libraries: libc.so.6: cannot open shared object file: No such file or directory\n\n从上面的错误信息可以发现想通过 mv 命令恢复是行不通的。可以通过执行以下命令恢复：\n\n    # LD_PRELOAD=/lib64/libc-2.12.so ln -s /lib64/libc-2.12.so /lib64/libc.so.6\n\n如果软链到自己安装的 glibc 版本，通过以下命令：\n\n    # LD_PRELOAD=/opt/glibc-2.14/lib/libc-2.14.so ln -s /opt/glibc-2.14/lib/libc-2.14.so /lib64/libc.so.6\n\n如果上面的方法还是不行，可以使用下面的命令：\n\n    # sln /lib64/libc-2.12.so /lib64/libc.so.6","slug":"CentOS-删除-libc-so-6-后命令不可用故障恢复方法","published":1,"updated":"2021-07-19T16:28:00.232Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphll000titd38wzg2ind","content":"<p>在升级 glibc 的过程中需要先删除软连接 /lib64/libc.so.6。删除后执行系统命令会一直报错：</p>\n<span id=\"more\"></span>\n\n<pre><code># ls -l libc.so.6\nlrwxrwxrwx 1 root root 12 4月   2 13:49 libc.so.6 -&gt; libc-2.12.so\n# mv libc.so.6 libc.so.6.bak\n# mv libc.so.6.bak libc.so.6\nmv: error while loading shared libraries: libc.so.6: cannot open shared object file: No such file or directory\n</code></pre>\n<p>从上面的错误信息可以发现想通过 mv 命令恢复是行不通的。可以通过执行以下命令恢复：</p>\n<pre><code># LD_PRELOAD=/lib64/libc-2.12.so ln -s /lib64/libc-2.12.so /lib64/libc.so.6\n</code></pre>\n<p>如果软链到自己安装的 glibc 版本，通过以下命令：</p>\n<pre><code># LD_PRELOAD=/opt/glibc-2.14/lib/libc-2.14.so ln -s /opt/glibc-2.14/lib/libc-2.14.so /lib64/libc.so.6\n</code></pre>\n<p>如果上面的方法还是不行，可以使用下面的命令：</p>\n<pre><code># sln /lib64/libc-2.12.so /lib64/libc.so.6\n</code></pre>\n","site":{"data":{}},"excerpt":"<p>在升级 glibc 的过程中需要先删除软连接 /lib64/libc.so.6。删除后执行系统命令会一直报错：</p>","more":"<pre><code># ls -l libc.so.6\nlrwxrwxrwx 1 root root 12 4月   2 13:49 libc.so.6 -&gt; libc-2.12.so\n# mv libc.so.6 libc.so.6.bak\n# mv libc.so.6.bak libc.so.6\nmv: error while loading shared libraries: libc.so.6: cannot open shared object file: No such file or directory\n</code></pre>\n<p>从上面的错误信息可以发现想通过 mv 命令恢复是行不通的。可以通过执行以下命令恢复：</p>\n<pre><code># LD_PRELOAD=/lib64/libc-2.12.so ln -s /lib64/libc-2.12.so /lib64/libc.so.6\n</code></pre>\n<p>如果软链到自己安装的 glibc 版本，通过以下命令：</p>\n<pre><code># LD_PRELOAD=/opt/glibc-2.14/lib/libc-2.14.so ln -s /opt/glibc-2.14/lib/libc-2.14.so /lib64/libc.so.6\n</code></pre>\n<p>如果上面的方法还是不行，可以使用下面的命令：</p>\n<pre><code># sln /lib64/libc-2.12.so /lib64/libc.so.6\n</code></pre>"},{"title":"CentOS 重装 MongoDB","date":"2017-09-15T03:46:13.000Z","_content":"\n\n服务器原来安装的 MongoDB 版本过久，在连接 MongoDB 3.4 后执行 show collections 命令无结果显示，所以需要重新安装 MongoDB 3.4。\n\n原安装版本查看及卸载：\n\n    # rpm -qa | grep mongodb\n    mongodb-2.4.14-4.el6.x86_64\n    # sudo yum erase $(rpm -qa | grep mongodb)\n    \n卸载完毕之后，按照 [MongoDB 官网手册](https://docs.mongodb.com/manual/tutorial/install-mongodb-on-red-hat/)重新安装。\n","source":"_posts/CentOS-重装-MongoDB.md","raw":"title: CentOS 重装 MongoDB\ntags:\n  - MongoDB\ncategories:\n  - 数据库\n  - MongoDB\ndate: 2017-09-15 11:46:13\n---\n\n\n服务器原来安装的 MongoDB 版本过久，在连接 MongoDB 3.4 后执行 show collections 命令无结果显示，所以需要重新安装 MongoDB 3.4。\n\n原安装版本查看及卸载：\n\n    # rpm -qa | grep mongodb\n    mongodb-2.4.14-4.el6.x86_64\n    # sudo yum erase $(rpm -qa | grep mongodb)\n    \n卸载完毕之后，按照 [MongoDB 官网手册](https://docs.mongodb.com/manual/tutorial/install-mongodb-on-red-hat/)重新安装。\n","slug":"CentOS-重装-MongoDB","published":1,"updated":"2021-07-19T16:28:00.232Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphlm000witd33buvddqm","content":"<p>服务器原来安装的 MongoDB 版本过久，在连接 MongoDB 3.4 后执行 show collections 命令无结果显示，所以需要重新安装 MongoDB 3.4。</p>\n<p>原安装版本查看及卸载：</p>\n<pre><code># rpm -qa | grep mongodb\nmongodb-2.4.14-4.el6.x86_64\n# sudo yum erase $(rpm -qa | grep mongodb)\n</code></pre>\n<p>卸载完毕之后，按照 <a href=\"https://docs.mongodb.com/manual/tutorial/install-mongodb-on-red-hat/\">MongoDB 官网手册</a>重新安装。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>服务器原来安装的 MongoDB 版本过久，在连接 MongoDB 3.4 后执行 show collections 命令无结果显示，所以需要重新安装 MongoDB 3.4。</p>\n<p>原安装版本查看及卸载：</p>\n<pre><code># rpm -qa | grep mongodb\nmongodb-2.4.14-4.el6.x86_64\n# sudo yum erase $(rpm -qa | grep mongodb)\n</code></pre>\n<p>卸载完毕之后，按照 <a href=\"https://docs.mongodb.com/manual/tutorial/install-mongodb-on-red-hat/\">MongoDB 官网手册</a>重新安装。</p>\n"},{"title":"CentOS7.3 安装 Python3","date":"2018-11-19T10:26:32.000Z","_content":"\n\n采用源代码构建的方式安装。此处安装的是 Python3.7.1 版本。源代码下载地址：<https://www.python.org/downloads/release/python-371/>。\n\n    # wget https://www.python.org/ftp/python/3.7.1/Python-3.7.1.tar.xz\n    --2018-11-19 17:13:28--  https://www.python.org/ftp/python/3.7.1/Python-3.7.1.tar.xz\n    正在解析主机 www.python.org (www.python.org)... 151.101.40.223, 2a04:4e42:2d::223\n    正在连接 www.python.org (www.python.org)|151.101.40.223|:443... 已连接。\n    已发出 HTTP 请求，正在等待回应... 200 OK\n    长度：16960060 (16M) [application/octet-stream]\n    正在保存至: “Python-3.7.1.tar.xz”\n\n    100%[======================================================================================================>] 16,960,060  4.67MB/s 用时 4.6s   \n\n    2018-11-19 17:13:34 (3.51 MB/s) - 已保存 “Python-3.7.1.tar.xz” [16960060/16960060])\n\n    # xz -d Python-3.7.1.tar.xz\n    # tar xvf Python-3.7.1.tar\n    # ./configure --prefix=/opt/Python-3.7.1\n    # make\n    Failed to build these modules:\n    _ctypes\n\n以上异常信息是因为 Python3.7.1 需要安装包 libffi-devel：\n\n    # yum install libffi-devel -y\n\n安装完成后重新执行 make 成功。\n\n    # make altinstall\n    # ln -s /opt/Python-3.7.1/ /opt/python\n    # ln -s /opt/python/bin/python3.7 /usr/bin/python3\n    # ln -s /opt/python/bin/pip3.7 /usr/bin/pip3\n\n最后升级新安装的 pip3：\n\n    # pip3 install --upgrade pip\n\n安装完成。\n","source":"_posts/CentOS7-3-安装-Python3.md","raw":"title: CentOS7.3 安装 Python3\ntags:\n  - Linux\n  - CentOS\n  - Python3\ncategories:\n  - 开发\n  - 环境搭建\ndate: 2018-11-19 18:26:32\n---\n\n\n采用源代码构建的方式安装。此处安装的是 Python3.7.1 版本。源代码下载地址：<https://www.python.org/downloads/release/python-371/>。\n\n    # wget https://www.python.org/ftp/python/3.7.1/Python-3.7.1.tar.xz\n    --2018-11-19 17:13:28--  https://www.python.org/ftp/python/3.7.1/Python-3.7.1.tar.xz\n    正在解析主机 www.python.org (www.python.org)... 151.101.40.223, 2a04:4e42:2d::223\n    正在连接 www.python.org (www.python.org)|151.101.40.223|:443... 已连接。\n    已发出 HTTP 请求，正在等待回应... 200 OK\n    长度：16960060 (16M) [application/octet-stream]\n    正在保存至: “Python-3.7.1.tar.xz”\n\n    100%[======================================================================================================>] 16,960,060  4.67MB/s 用时 4.6s   \n\n    2018-11-19 17:13:34 (3.51 MB/s) - 已保存 “Python-3.7.1.tar.xz” [16960060/16960060])\n\n    # xz -d Python-3.7.1.tar.xz\n    # tar xvf Python-3.7.1.tar\n    # ./configure --prefix=/opt/Python-3.7.1\n    # make\n    Failed to build these modules:\n    _ctypes\n\n以上异常信息是因为 Python3.7.1 需要安装包 libffi-devel：\n\n    # yum install libffi-devel -y\n\n安装完成后重新执行 make 成功。\n\n    # make altinstall\n    # ln -s /opt/Python-3.7.1/ /opt/python\n    # ln -s /opt/python/bin/python3.7 /usr/bin/python3\n    # ln -s /opt/python/bin/pip3.7 /usr/bin/pip3\n\n最后升级新安装的 pip3：\n\n    # pip3 install --upgrade pip\n\n安装完成。\n","slug":"CentOS7-3-安装-Python3","published":1,"updated":"2021-07-19T16:28:00.244Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphln000yitd3gi9ydez7","content":"<p>采用源代码构建的方式安装。此处安装的是 Python3.7.1 版本。源代码下载地址：<a href=\"https://www.python.org/downloads/release/python-371/\">https://www.python.org/downloads/release/python-371/</a>。</p>\n<pre><code># wget https://www.python.org/ftp/python/3.7.1/Python-3.7.1.tar.xz\n--2018-11-19 17:13:28--  https://www.python.org/ftp/python/3.7.1/Python-3.7.1.tar.xz\n正在解析主机 www.python.org (www.python.org)... 151.101.40.223, 2a04:4e42:2d::223\n正在连接 www.python.org (www.python.org)|151.101.40.223|:443... 已连接。\n已发出 HTTP 请求，正在等待回应... 200 OK\n长度：16960060 (16M) [application/octet-stream]\n正在保存至: “Python-3.7.1.tar.xz”\n\n100%[======================================================================================================&gt;] 16,960,060  4.67MB/s 用时 4.6s   \n\n2018-11-19 17:13:34 (3.51 MB/s) - 已保存 “Python-3.7.1.tar.xz” [16960060/16960060])\n\n# xz -d Python-3.7.1.tar.xz\n# tar xvf Python-3.7.1.tar\n# ./configure --prefix=/opt/Python-3.7.1\n# make\nFailed to build these modules:\n_ctypes\n</code></pre>\n<p>以上异常信息是因为 Python3.7.1 需要安装包 libffi-devel：</p>\n<pre><code># yum install libffi-devel -y\n</code></pre>\n<p>安装完成后重新执行 make 成功。</p>\n<pre><code># make altinstall\n# ln -s /opt/Python-3.7.1/ /opt/python\n# ln -s /opt/python/bin/python3.7 /usr/bin/python3\n# ln -s /opt/python/bin/pip3.7 /usr/bin/pip3\n</code></pre>\n<p>最后升级新安装的 pip3：</p>\n<pre><code># pip3 install --upgrade pip\n</code></pre>\n<p>安装完成。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>采用源代码构建的方式安装。此处安装的是 Python3.7.1 版本。源代码下载地址：<a href=\"https://www.python.org/downloads/release/python-371/\">https://www.python.org/downloads/release/python-371/</a>。</p>\n<pre><code># wget https://www.python.org/ftp/python/3.7.1/Python-3.7.1.tar.xz\n--2018-11-19 17:13:28--  https://www.python.org/ftp/python/3.7.1/Python-3.7.1.tar.xz\n正在解析主机 www.python.org (www.python.org)... 151.101.40.223, 2a04:4e42:2d::223\n正在连接 www.python.org (www.python.org)|151.101.40.223|:443... 已连接。\n已发出 HTTP 请求，正在等待回应... 200 OK\n长度：16960060 (16M) [application/octet-stream]\n正在保存至: “Python-3.7.1.tar.xz”\n\n100%[======================================================================================================&gt;] 16,960,060  4.67MB/s 用时 4.6s   \n\n2018-11-19 17:13:34 (3.51 MB/s) - 已保存 “Python-3.7.1.tar.xz” [16960060/16960060])\n\n# xz -d Python-3.7.1.tar.xz\n# tar xvf Python-3.7.1.tar\n# ./configure --prefix=/opt/Python-3.7.1\n# make\nFailed to build these modules:\n_ctypes\n</code></pre>\n<p>以上异常信息是因为 Python3.7.1 需要安装包 libffi-devel：</p>\n<pre><code># yum install libffi-devel -y\n</code></pre>\n<p>安装完成后重新执行 make 成功。</p>\n<pre><code># make altinstall\n# ln -s /opt/Python-3.7.1/ /opt/python\n# ln -s /opt/python/bin/python3.7 /usr/bin/python3\n# ln -s /opt/python/bin/pip3.7 /usr/bin/pip3\n</code></pre>\n<p>最后升级新安装的 pip3：</p>\n<pre><code># pip3 install --upgrade pip\n</code></pre>\n<p>安装完成。</p>\n"},{"title":"CentOS7.7 安装 MySQL 5.7","date":"2020-11-26T09:19:38.000Z","_content":"\n### 添加 MySQL Yum Repository\n\na. 从 MySQL Yum Repository 下载页面（https://dev.mysql.com/downloads/repo/yum/）选择对应的版本下载。\n\nb. 使用以下命令安装 RPM 包：\n\n\t# sudo yum localinstall mysql80-community-release-el7-3.noarch.rpm\n\t\n### 版本选择\n\na. 查看当前启用的版本：\n\n\t# yum repolist all | grep mysql\n\tmysql-cluster-7.5-community/x86_64 MySQL Cluster 7.5 Community      禁用\n\tmysql-cluster-7.5-community-source MySQL Cluster 7.5 Community - So 禁用\n\tmysql-cluster-7.6-community/x86_64 MySQL Cluster 7.6 Community      禁用\n\tmysql-cluster-7.6-community-source MySQL Cluster 7.6 Community - So 禁用\n\tmysql-cluster-8.0-community/x86_64 MySQL Cluster 8.0 Community      禁用\n\tmysql-cluster-8.0-community-source MySQL Cluster 8.0 Community - So 禁用\n\tmysql-connectors-community/x86_64  MySQL Connectors Community       启用:    175\n\tmysql-connectors-community-source  MySQL Connectors Community - Sou 禁用\n\tmysql-tools-community/x86_64       MySQL Tools Community            启用:    120\n\tmysql-tools-community-source       MySQL Tools Community - Source   禁用\n\tmysql-tools-preview/x86_64         MySQL Tools Preview              禁用\n\tmysql-tools-preview-source         MySQL Tools Preview - Source     禁用\n\tmysql55-community/x86_64           MySQL 5.5 Community Server       禁用\n\tmysql55-community-source           MySQL 5.5 Community Server - Sou 禁用\n\tmysql56-community/x86_64           MySQL 5.6 Community Server       禁用\n\tmysql56-community-source           MySQL 5.6 Community Server - Sou 禁用\n\tmysql57-community/x86_64           MySQL 5.7 Community Server       禁用\n\tmysql57-community-source           MySQL 5.7 Community Server - Sou 禁用\n\tmysql80-community/x86_64           MySQL 8.0 Community Server       启用:    211\n\tmysql80-community-source           MySQL 8.0 Community Server - Sou 禁用\n\t\nb. 启用 MySQL5.7 版本\n\n\t# sudo yum-config-manager --disable mysql80-community\n\t# sudo yum-config-manager --enable mysql57-community\n\t\n### 安装 MySQL5.7\n\n\t# sudo yum install mysql-community-server\n\t\n### 启动 MySQL Server\n\n\t# sudo service mysqld start\n\t\n检查 MySQL 启动状态\n\n\t# sudo service mysqld status\n\t\n安装时生成的超级用户初始密码查看：\n\n\t# sudo grep 'temporary password' /var/log/mysqld.log\n\t\n修改初始密码：\n\n\t# mysql -uroot -p\n\tmysql> ALTER USER 'root'@'localhost' IDENTIFIED BY 'MyNewPass4!';\n\tmysql> GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'Data-platform@212' WITH GRANT OPTION;\n\tQuery OK, 0 rows affected, 1 warning (0.00 sec)\n\t\n\tmysql> FLUSH   PRIVILEGES;\n\tQuery OK, 0 rows affected (0.00 sec)","source":"_posts/CentOS7-7-安装-MySQL-5-7.md","raw":"title: CentOS7.7 安装 MySQL 5.7\ndate: 2020-11-26 17:19:38\ntags:\n- Linux\n- CentOS\n- MySQL\ncategories:\n- 数据库\n- MySQL\n---\n\n### 添加 MySQL Yum Repository\n\na. 从 MySQL Yum Repository 下载页面（https://dev.mysql.com/downloads/repo/yum/）选择对应的版本下载。\n\nb. 使用以下命令安装 RPM 包：\n\n\t# sudo yum localinstall mysql80-community-release-el7-3.noarch.rpm\n\t\n### 版本选择\n\na. 查看当前启用的版本：\n\n\t# yum repolist all | grep mysql\n\tmysql-cluster-7.5-community/x86_64 MySQL Cluster 7.5 Community      禁用\n\tmysql-cluster-7.5-community-source MySQL Cluster 7.5 Community - So 禁用\n\tmysql-cluster-7.6-community/x86_64 MySQL Cluster 7.6 Community      禁用\n\tmysql-cluster-7.6-community-source MySQL Cluster 7.6 Community - So 禁用\n\tmysql-cluster-8.0-community/x86_64 MySQL Cluster 8.0 Community      禁用\n\tmysql-cluster-8.0-community-source MySQL Cluster 8.0 Community - So 禁用\n\tmysql-connectors-community/x86_64  MySQL Connectors Community       启用:    175\n\tmysql-connectors-community-source  MySQL Connectors Community - Sou 禁用\n\tmysql-tools-community/x86_64       MySQL Tools Community            启用:    120\n\tmysql-tools-community-source       MySQL Tools Community - Source   禁用\n\tmysql-tools-preview/x86_64         MySQL Tools Preview              禁用\n\tmysql-tools-preview-source         MySQL Tools Preview - Source     禁用\n\tmysql55-community/x86_64           MySQL 5.5 Community Server       禁用\n\tmysql55-community-source           MySQL 5.5 Community Server - Sou 禁用\n\tmysql56-community/x86_64           MySQL 5.6 Community Server       禁用\n\tmysql56-community-source           MySQL 5.6 Community Server - Sou 禁用\n\tmysql57-community/x86_64           MySQL 5.7 Community Server       禁用\n\tmysql57-community-source           MySQL 5.7 Community Server - Sou 禁用\n\tmysql80-community/x86_64           MySQL 8.0 Community Server       启用:    211\n\tmysql80-community-source           MySQL 8.0 Community Server - Sou 禁用\n\t\nb. 启用 MySQL5.7 版本\n\n\t# sudo yum-config-manager --disable mysql80-community\n\t# sudo yum-config-manager --enable mysql57-community\n\t\n### 安装 MySQL5.7\n\n\t# sudo yum install mysql-community-server\n\t\n### 启动 MySQL Server\n\n\t# sudo service mysqld start\n\t\n检查 MySQL 启动状态\n\n\t# sudo service mysqld status\n\t\n安装时生成的超级用户初始密码查看：\n\n\t# sudo grep 'temporary password' /var/log/mysqld.log\n\t\n修改初始密码：\n\n\t# mysql -uroot -p\n\tmysql> ALTER USER 'root'@'localhost' IDENTIFIED BY 'MyNewPass4!';\n\tmysql> GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'Data-platform@212' WITH GRANT OPTION;\n\tQuery OK, 0 rows affected, 1 warning (0.00 sec)\n\t\n\tmysql> FLUSH   PRIVILEGES;\n\tQuery OK, 0 rows affected (0.00 sec)","slug":"CentOS7-7-安装-MySQL-5-7","published":1,"updated":"2021-07-19T16:27:59.992Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphlo0011itd3ead94i46","content":"<h3 id=\"添加-MySQL-Yum-Repository\"><a href=\"#添加-MySQL-Yum-Repository\" class=\"headerlink\" title=\"添加 MySQL Yum Repository\"></a>添加 MySQL Yum Repository</h3><p>a. 从 MySQL Yum Repository 下载页面（<a href=\"https://dev.mysql.com/downloads/repo/yum/%EF%BC%89%E9%80%89%E6%8B%A9%E5%AF%B9%E5%BA%94%E7%9A%84%E7%89%88%E6%9C%AC%E4%B8%8B%E8%BD%BD%E3%80%82\">https://dev.mysql.com/downloads/repo/yum/）选择对应的版本下载。</a></p>\n<p>b. 使用以下命令安装 RPM 包：</p>\n<pre><code># sudo yum localinstall mysql80-community-release-el7-3.noarch.rpm\n</code></pre>\n<h3 id=\"版本选择\"><a href=\"#版本选择\" class=\"headerlink\" title=\"版本选择\"></a>版本选择</h3><p>a. 查看当前启用的版本：</p>\n<pre><code># yum repolist all | grep mysql\nmysql-cluster-7.5-community/x86_64 MySQL Cluster 7.5 Community      禁用\nmysql-cluster-7.5-community-source MySQL Cluster 7.5 Community - So 禁用\nmysql-cluster-7.6-community/x86_64 MySQL Cluster 7.6 Community      禁用\nmysql-cluster-7.6-community-source MySQL Cluster 7.6 Community - So 禁用\nmysql-cluster-8.0-community/x86_64 MySQL Cluster 8.0 Community      禁用\nmysql-cluster-8.0-community-source MySQL Cluster 8.0 Community - So 禁用\nmysql-connectors-community/x86_64  MySQL Connectors Community       启用:    175\nmysql-connectors-community-source  MySQL Connectors Community - Sou 禁用\nmysql-tools-community/x86_64       MySQL Tools Community            启用:    120\nmysql-tools-community-source       MySQL Tools Community - Source   禁用\nmysql-tools-preview/x86_64         MySQL Tools Preview              禁用\nmysql-tools-preview-source         MySQL Tools Preview - Source     禁用\nmysql55-community/x86_64           MySQL 5.5 Community Server       禁用\nmysql55-community-source           MySQL 5.5 Community Server - Sou 禁用\nmysql56-community/x86_64           MySQL 5.6 Community Server       禁用\nmysql56-community-source           MySQL 5.6 Community Server - Sou 禁用\nmysql57-community/x86_64           MySQL 5.7 Community Server       禁用\nmysql57-community-source           MySQL 5.7 Community Server - Sou 禁用\nmysql80-community/x86_64           MySQL 8.0 Community Server       启用:    211\nmysql80-community-source           MySQL 8.0 Community Server - Sou 禁用\n</code></pre>\n<p>b. 启用 MySQL5.7 版本</p>\n<pre><code># sudo yum-config-manager --disable mysql80-community\n# sudo yum-config-manager --enable mysql57-community\n</code></pre>\n<h3 id=\"安装-MySQL5-7\"><a href=\"#安装-MySQL5-7\" class=\"headerlink\" title=\"安装 MySQL5.7\"></a>安装 MySQL5.7</h3><pre><code># sudo yum install mysql-community-server\n</code></pre>\n<h3 id=\"启动-MySQL-Server\"><a href=\"#启动-MySQL-Server\" class=\"headerlink\" title=\"启动 MySQL Server\"></a>启动 MySQL Server</h3><pre><code># sudo service mysqld start\n</code></pre>\n<p>检查 MySQL 启动状态</p>\n<pre><code># sudo service mysqld status\n</code></pre>\n<p>安装时生成的超级用户初始密码查看：</p>\n<pre><code># sudo grep &#39;temporary password&#39; /var/log/mysqld.log\n</code></pre>\n<p>修改初始密码：</p>\n<pre><code># mysql -uroot -p\nmysql&gt; ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;MyNewPass4!&#39;;\nmysql&gt; GRANT ALL PRIVILEGES ON *.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;Data-platform@212&#39; WITH GRANT OPTION;\nQuery OK, 0 rows affected, 1 warning (0.00 sec)\n\nmysql&gt; FLUSH   PRIVILEGES;\nQuery OK, 0 rows affected (0.00 sec)\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"添加-MySQL-Yum-Repository\"><a href=\"#添加-MySQL-Yum-Repository\" class=\"headerlink\" title=\"添加 MySQL Yum Repository\"></a>添加 MySQL Yum Repository</h3><p>a. 从 MySQL Yum Repository 下载页面（<a href=\"https://dev.mysql.com/downloads/repo/yum/%EF%BC%89%E9%80%89%E6%8B%A9%E5%AF%B9%E5%BA%94%E7%9A%84%E7%89%88%E6%9C%AC%E4%B8%8B%E8%BD%BD%E3%80%82\">https://dev.mysql.com/downloads/repo/yum/）选择对应的版本下载。</a></p>\n<p>b. 使用以下命令安装 RPM 包：</p>\n<pre><code># sudo yum localinstall mysql80-community-release-el7-3.noarch.rpm\n</code></pre>\n<h3 id=\"版本选择\"><a href=\"#版本选择\" class=\"headerlink\" title=\"版本选择\"></a>版本选择</h3><p>a. 查看当前启用的版本：</p>\n<pre><code># yum repolist all | grep mysql\nmysql-cluster-7.5-community/x86_64 MySQL Cluster 7.5 Community      禁用\nmysql-cluster-7.5-community-source MySQL Cluster 7.5 Community - So 禁用\nmysql-cluster-7.6-community/x86_64 MySQL Cluster 7.6 Community      禁用\nmysql-cluster-7.6-community-source MySQL Cluster 7.6 Community - So 禁用\nmysql-cluster-8.0-community/x86_64 MySQL Cluster 8.0 Community      禁用\nmysql-cluster-8.0-community-source MySQL Cluster 8.0 Community - So 禁用\nmysql-connectors-community/x86_64  MySQL Connectors Community       启用:    175\nmysql-connectors-community-source  MySQL Connectors Community - Sou 禁用\nmysql-tools-community/x86_64       MySQL Tools Community            启用:    120\nmysql-tools-community-source       MySQL Tools Community - Source   禁用\nmysql-tools-preview/x86_64         MySQL Tools Preview              禁用\nmysql-tools-preview-source         MySQL Tools Preview - Source     禁用\nmysql55-community/x86_64           MySQL 5.5 Community Server       禁用\nmysql55-community-source           MySQL 5.5 Community Server - Sou 禁用\nmysql56-community/x86_64           MySQL 5.6 Community Server       禁用\nmysql56-community-source           MySQL 5.6 Community Server - Sou 禁用\nmysql57-community/x86_64           MySQL 5.7 Community Server       禁用\nmysql57-community-source           MySQL 5.7 Community Server - Sou 禁用\nmysql80-community/x86_64           MySQL 8.0 Community Server       启用:    211\nmysql80-community-source           MySQL 8.0 Community Server - Sou 禁用\n</code></pre>\n<p>b. 启用 MySQL5.7 版本</p>\n<pre><code># sudo yum-config-manager --disable mysql80-community\n# sudo yum-config-manager --enable mysql57-community\n</code></pre>\n<h3 id=\"安装-MySQL5-7\"><a href=\"#安装-MySQL5-7\" class=\"headerlink\" title=\"安装 MySQL5.7\"></a>安装 MySQL5.7</h3><pre><code># sudo yum install mysql-community-server\n</code></pre>\n<h3 id=\"启动-MySQL-Server\"><a href=\"#启动-MySQL-Server\" class=\"headerlink\" title=\"启动 MySQL Server\"></a>启动 MySQL Server</h3><pre><code># sudo service mysqld start\n</code></pre>\n<p>检查 MySQL 启动状态</p>\n<pre><code># sudo service mysqld status\n</code></pre>\n<p>安装时生成的超级用户初始密码查看：</p>\n<pre><code># sudo grep &#39;temporary password&#39; /var/log/mysqld.log\n</code></pre>\n<p>修改初始密码：</p>\n<pre><code># mysql -uroot -p\nmysql&gt; ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;MyNewPass4!&#39;;\nmysql&gt; GRANT ALL PRIVILEGES ON *.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;Data-platform@212&#39; WITH GRANT OPTION;\nQuery OK, 0 rows affected, 1 warning (0.00 sec)\n\nmysql&gt; FLUSH   PRIVILEGES;\nQuery OK, 0 rows affected (0.00 sec)\n</code></pre>\n"},{"title":"Centos 6.8 No package libffi-devel available 问题解决","date":"2018-11-27T11:08:16.000Z","_content":"\n在 CentOS 6.8 安装 libffi-devel 的时候出现以下问题：\n\n    #yum install -y libffi-devel\n    Loaded plugins: product-id, refresh-packagekit, search-disabled-repos, security, subscription-manager\n    This system is not registered to Red Hat Subscription Management. You can use subscription-manager to register.\n    Setting up Install Process\n    cdrom                                                                                                                                         | 4.1 kB     00:00 ...\n    No package libffi-devel available.\n    Error: Nothing to do\n\n可以通过以下方式解决。\n\n第一步，从以下地址找到对应的 libffi-bedel 版本：https://rpmfind.net/linux/rpm2html/search.php?query=libffi-devel\n\n第二步，通过以下命令安装：\n\n    #yum install https://rpmfind.net/linux/centos/6.10/os/x86_64/Packages/libffi-devel-3.0.5-3.2.el6.x86_64.rpm\n","source":"_posts/Centos-6-8-No-package-libffi-devel-available-问题解决.md","raw":"title: Centos 6.8 No package libffi-devel available 问题解决\ndate: 2018-11-27 19:08:16\ntags:\n- Linux\n- CentOS\ncategories:\n- 操作系统\n- Linux\n---\n\n在 CentOS 6.8 安装 libffi-devel 的时候出现以下问题：\n\n    #yum install -y libffi-devel\n    Loaded plugins: product-id, refresh-packagekit, search-disabled-repos, security, subscription-manager\n    This system is not registered to Red Hat Subscription Management. You can use subscription-manager to register.\n    Setting up Install Process\n    cdrom                                                                                                                                         | 4.1 kB     00:00 ...\n    No package libffi-devel available.\n    Error: Nothing to do\n\n可以通过以下方式解决。\n\n第一步，从以下地址找到对应的 libffi-bedel 版本：https://rpmfind.net/linux/rpm2html/search.php?query=libffi-devel\n\n第二步，通过以下命令安装：\n\n    #yum install https://rpmfind.net/linux/centos/6.10/os/x86_64/Packages/libffi-devel-3.0.5-3.2.el6.x86_64.rpm\n","slug":"Centos-6-8-No-package-libffi-devel-available-问题解决","published":1,"updated":"2021-07-19T16:28:00.232Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphlp0013itd3cg1u86ky","content":"<p>在 CentOS 6.8 安装 libffi-devel 的时候出现以下问题：</p>\n<pre><code>#yum install -y libffi-devel\nLoaded plugins: product-id, refresh-packagekit, search-disabled-repos, security, subscription-manager\nThis system is not registered to Red Hat Subscription Management. You can use subscription-manager to register.\nSetting up Install Process\ncdrom                                                                                                                                         | 4.1 kB     00:00 ...\nNo package libffi-devel available.\nError: Nothing to do\n</code></pre>\n<p>可以通过以下方式解决。</p>\n<p>第一步，从以下地址找到对应的 libffi-bedel 版本：<a href=\"https://rpmfind.net/linux/rpm2html/search.php?query=libffi-devel\">https://rpmfind.net/linux/rpm2html/search.php?query=libffi-devel</a></p>\n<p>第二步，通过以下命令安装：</p>\n<pre><code>#yum install https://rpmfind.net/linux/centos/6.10/os/x86_64/Packages/libffi-devel-3.0.5-3.2.el6.x86_64.rpm\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>在 CentOS 6.8 安装 libffi-devel 的时候出现以下问题：</p>\n<pre><code>#yum install -y libffi-devel\nLoaded plugins: product-id, refresh-packagekit, search-disabled-repos, security, subscription-manager\nThis system is not registered to Red Hat Subscription Management. You can use subscription-manager to register.\nSetting up Install Process\ncdrom                                                                                                                                         | 4.1 kB     00:00 ...\nNo package libffi-devel available.\nError: Nothing to do\n</code></pre>\n<p>可以通过以下方式解决。</p>\n<p>第一步，从以下地址找到对应的 libffi-bedel 版本：<a href=\"https://rpmfind.net/linux/rpm2html/search.php?query=libffi-devel\">https://rpmfind.net/linux/rpm2html/search.php?query=libffi-devel</a></p>\n<p>第二步，通过以下命令安装：</p>\n<pre><code>#yum install https://rpmfind.net/linux/centos/6.10/os/x86_64/Packages/libffi-devel-3.0.5-3.2.el6.x86_64.rpm\n</code></pre>\n"},{"title":"DELL R730xd 通过管理卡远程安装 CentOS 7.3","date":"2017-11-13T15:16:30.000Z","_content":"\n### 环境说明\n\n- 服务器：DELL R730xd\n- 远程管理卡：iDRAC8\n- 安装系统：CentOS 7.3\n- 客户端操作系统：Ubuntu 16.04\n- JDK 8\n\n<!-- more -->\n\n### 安装步骤\n\n#### 下载CentOS\n\n下载地址：<https://www.centos.org/download/>\n\n默认只会链接到最新版本。如：<http://mirrors.sohu.com/centos/7/isos/x86_64/CentOS-7-x86_64-DVD-1708.iso>。查看所有版本可以到上层目录：<http://mirrors.sohu.com/centos/>。\n\n#### 登陆虚拟控制台\n\n登录远程管理 https://+ip。选择“虚拟控制台”，然后点击“启动虚拟控制台”浏览器会提示下载一个“viewer.jnlp”文件。下载该文件并保存。![虚拟控制台](/uploads/20171019/virtual_console.png)\n\n通过命令行执行以下命令启动虚拟控制台：\n\n    $ javaws viewer.jnlp\n\n![JDK 升级提示](/uploads/20171019/java_update_tip.png)\nJDK 升级提示，选择 “Later”。\n\n![安全提示](/uploads/20171019/security_warning.png)\n安全警告提示选择“继续”。\n\n![是否运行](/uploads/20171019/launch_ask.png)\n选择“运行”。\n\n![安全提示2](/uploads/20171019/security_warning2.png)\n选择“运行”。\n![虚拟控制台](/uploads/20171019/virtual_console_pic.png)\n\n### 系统安装\n\n连接“虚拟介质”，然后“映射CD/DVD”，选中已下载的 iso 文件。然后点击“映射设备”。\n![连接虚拟介质](/uploads/20171019/cd_dvd.png)\n\n![映射设备](/uploads/20171019/virtual_medium.png)\n\n重新启动服务器，按 F11 进入 BIOS 设置，改为从虚拟介质启动。\n","source":"_posts/DELL-R730xd-通过管理卡远程安装-Centos7-3.md","raw":"title: DELL R730xd 通过管理卡远程安装 CentOS 7.3\ntags:\n  - Linux\n  - CentOS\ncategories:\n  - 操作系统\n  - Linux\ndate: 2017-11-13 23:16:30\n---\n\n### 环境说明\n\n- 服务器：DELL R730xd\n- 远程管理卡：iDRAC8\n- 安装系统：CentOS 7.3\n- 客户端操作系统：Ubuntu 16.04\n- JDK 8\n\n<!-- more -->\n\n### 安装步骤\n\n#### 下载CentOS\n\n下载地址：<https://www.centos.org/download/>\n\n默认只会链接到最新版本。如：<http://mirrors.sohu.com/centos/7/isos/x86_64/CentOS-7-x86_64-DVD-1708.iso>。查看所有版本可以到上层目录：<http://mirrors.sohu.com/centos/>。\n\n#### 登陆虚拟控制台\n\n登录远程管理 https://+ip。选择“虚拟控制台”，然后点击“启动虚拟控制台”浏览器会提示下载一个“viewer.jnlp”文件。下载该文件并保存。![虚拟控制台](/uploads/20171019/virtual_console.png)\n\n通过命令行执行以下命令启动虚拟控制台：\n\n    $ javaws viewer.jnlp\n\n![JDK 升级提示](/uploads/20171019/java_update_tip.png)\nJDK 升级提示，选择 “Later”。\n\n![安全提示](/uploads/20171019/security_warning.png)\n安全警告提示选择“继续”。\n\n![是否运行](/uploads/20171019/launch_ask.png)\n选择“运行”。\n\n![安全提示2](/uploads/20171019/security_warning2.png)\n选择“运行”。\n![虚拟控制台](/uploads/20171019/virtual_console_pic.png)\n\n### 系统安装\n\n连接“虚拟介质”，然后“映射CD/DVD”，选中已下载的 iso 文件。然后点击“映射设备”。\n![连接虚拟介质](/uploads/20171019/cd_dvd.png)\n\n![映射设备](/uploads/20171019/virtual_medium.png)\n\n重新启动服务器，按 F11 进入 BIOS 设置，改为从虚拟介质启动。\n","slug":"DELL-R730xd-通过管理卡远程安装-Centos7-3","published":1,"updated":"2021-07-19T16:28:00.244Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphlr0018itd37rp35qhw","content":"<h3 id=\"环境说明\"><a href=\"#环境说明\" class=\"headerlink\" title=\"环境说明\"></a>环境说明</h3><ul>\n<li>服务器：DELL R730xd</li>\n<li>远程管理卡：iDRAC8</li>\n<li>安装系统：CentOS 7.3</li>\n<li>客户端操作系统：Ubuntu 16.04</li>\n<li>JDK 8</li>\n</ul>\n<span id=\"more\"></span>\n\n<h3 id=\"安装步骤\"><a href=\"#安装步骤\" class=\"headerlink\" title=\"安装步骤\"></a>安装步骤</h3><h4 id=\"下载CentOS\"><a href=\"#下载CentOS\" class=\"headerlink\" title=\"下载CentOS\"></a>下载CentOS</h4><p>下载地址：<a href=\"https://www.centos.org/download/\">https://www.centos.org/download/</a></p>\n<p>默认只会链接到最新版本。如：<a href=\"http://mirrors.sohu.com/centos/7/isos/x86_64/CentOS-7-x86_64-DVD-1708.iso\">http://mirrors.sohu.com/centos/7/isos/x86_64/CentOS-7-x86_64-DVD-1708.iso</a>。查看所有版本可以到上层目录：<a href=\"http://mirrors.sohu.com/centos/\">http://mirrors.sohu.com/centos/</a>。</p>\n<h4 id=\"登陆虚拟控制台\"><a href=\"#登陆虚拟控制台\" class=\"headerlink\" title=\"登陆虚拟控制台\"></a>登陆虚拟控制台</h4><p>登录远程管理 https://+ip。选择“虚拟控制台”，然后点击“启动虚拟控制台”浏览器会提示下载一个“viewer.jnlp”文件。下载该文件并保存。<img src=\"/uploads/20171019/virtual_console.png\" alt=\"虚拟控制台\"></p>\n<p>通过命令行执行以下命令启动虚拟控制台：</p>\n<pre><code>$ javaws viewer.jnlp\n</code></pre>\n<p><img src=\"/uploads/20171019/java_update_tip.png\" alt=\"JDK 升级提示\"><br>JDK 升级提示，选择 “Later”。</p>\n<p><img src=\"/uploads/20171019/security_warning.png\" alt=\"安全提示\"><br>安全警告提示选择“继续”。</p>\n<p><img src=\"/uploads/20171019/launch_ask.png\" alt=\"是否运行\"><br>选择“运行”。</p>\n<p><img src=\"/uploads/20171019/security_warning2.png\" alt=\"安全提示2\"><br>选择“运行”。<br><img src=\"/uploads/20171019/virtual_console_pic.png\" alt=\"虚拟控制台\"></p>\n<h3 id=\"系统安装\"><a href=\"#系统安装\" class=\"headerlink\" title=\"系统安装\"></a>系统安装</h3><p>连接“虚拟介质”，然后“映射CD/DVD”，选中已下载的 iso 文件。然后点击“映射设备”。<br><img src=\"/uploads/20171019/cd_dvd.png\" alt=\"连接虚拟介质\"></p>\n<p><img src=\"/uploads/20171019/virtual_medium.png\" alt=\"映射设备\"></p>\n<p>重新启动服务器，按 F11 进入 BIOS 设置，改为从虚拟介质启动。</p>\n","site":{"data":{}},"excerpt":"<h3 id=\"环境说明\"><a href=\"#环境说明\" class=\"headerlink\" title=\"环境说明\"></a>环境说明</h3><ul>\n<li>服务器：DELL R730xd</li>\n<li>远程管理卡：iDRAC8</li>\n<li>安装系统：CentOS 7.3</li>\n<li>客户端操作系统：Ubuntu 16.04</li>\n<li>JDK 8</li>\n</ul>","more":"<h3 id=\"安装步骤\"><a href=\"#安装步骤\" class=\"headerlink\" title=\"安装步骤\"></a>安装步骤</h3><h4 id=\"下载CentOS\"><a href=\"#下载CentOS\" class=\"headerlink\" title=\"下载CentOS\"></a>下载CentOS</h4><p>下载地址：<a href=\"https://www.centos.org/download/\">https://www.centos.org/download/</a></p>\n<p>默认只会链接到最新版本。如：<a href=\"http://mirrors.sohu.com/centos/7/isos/x86_64/CentOS-7-x86_64-DVD-1708.iso\">http://mirrors.sohu.com/centos/7/isos/x86_64/CentOS-7-x86_64-DVD-1708.iso</a>。查看所有版本可以到上层目录：<a href=\"http://mirrors.sohu.com/centos/\">http://mirrors.sohu.com/centos/</a>。</p>\n<h4 id=\"登陆虚拟控制台\"><a href=\"#登陆虚拟控制台\" class=\"headerlink\" title=\"登陆虚拟控制台\"></a>登陆虚拟控制台</h4><p>登录远程管理 https://+ip。选择“虚拟控制台”，然后点击“启动虚拟控制台”浏览器会提示下载一个“viewer.jnlp”文件。下载该文件并保存。<img src=\"/uploads/20171019/virtual_console.png\" alt=\"虚拟控制台\"></p>\n<p>通过命令行执行以下命令启动虚拟控制台：</p>\n<pre><code>$ javaws viewer.jnlp\n</code></pre>\n<p><img src=\"/uploads/20171019/java_update_tip.png\" alt=\"JDK 升级提示\"><br>JDK 升级提示，选择 “Later”。</p>\n<p><img src=\"/uploads/20171019/security_warning.png\" alt=\"安全提示\"><br>安全警告提示选择“继续”。</p>\n<p><img src=\"/uploads/20171019/launch_ask.png\" alt=\"是否运行\"><br>选择“运行”。</p>\n<p><img src=\"/uploads/20171019/security_warning2.png\" alt=\"安全提示2\"><br>选择“运行”。<br><img src=\"/uploads/20171019/virtual_console_pic.png\" alt=\"虚拟控制台\"></p>\n<h3 id=\"系统安装\"><a href=\"#系统安装\" class=\"headerlink\" title=\"系统安装\"></a>系统安装</h3><p>连接“虚拟介质”，然后“映射CD/DVD”，选中已下载的 iso 文件。然后点击“映射设备”。<br><img src=\"/uploads/20171019/cd_dvd.png\" alt=\"连接虚拟介质\"></p>\n<p><img src=\"/uploads/20171019/virtual_medium.png\" alt=\"映射设备\"></p>\n<p>重新启动服务器，按 F11 进入 BIOS 设置，改为从虚拟介质启动。</p>"},{"title":"Eclipse 导入 Sqoop","date":"2017-11-25T15:37:16.000Z","_content":"\nSqoop 是使用 Ant 进行编译的，如果需要导入 Eclipse 首先执行以下命令生成 .project 和 .classpath 文件。\n\n    ant eclipse\n\n操作完成后就可以导入 Eclipse 了。\n\n<!-- more -->\n\n导入 Eclipse 后编译会提示以下错误：\n\n    Project 'sqoop' is missing required library: 'build/classes'\n    Project 'sqoop' is missing required library: 'build/test/classes'\n    Project 'sqoop' is missing required library: 'build/test/extraconf'\n    The project cannot be built until build path errors are resolved\n    \n执行 ant 命令编译 Sqoop，然后在 Eclipse 中刷新项目，出现如下错误：\n\n    The declared package \"org.apache.sqoop.fi\" does not match the expected package \"aop.org.apache.sqoop.fi\"\n    \n如下图所示配置，问题解决。\n![Eclipse Sqoop](/uploads/20171125/eclipse-sqoop-import.png)\n","source":"_posts/Eclipse-导入-Sqoop.md","raw":"title: Eclipse 导入 Sqoop\ntags:\n  - Eclipse\n  - Sqoop\ncategories:\n  - 大数据\n  - Sqoop\ndate: 2017-11-25 23:37:16\n---\n\nSqoop 是使用 Ant 进行编译的，如果需要导入 Eclipse 首先执行以下命令生成 .project 和 .classpath 文件。\n\n    ant eclipse\n\n操作完成后就可以导入 Eclipse 了。\n\n<!-- more -->\n\n导入 Eclipse 后编译会提示以下错误：\n\n    Project 'sqoop' is missing required library: 'build/classes'\n    Project 'sqoop' is missing required library: 'build/test/classes'\n    Project 'sqoop' is missing required library: 'build/test/extraconf'\n    The project cannot be built until build path errors are resolved\n    \n执行 ant 命令编译 Sqoop，然后在 Eclipse 中刷新项目，出现如下错误：\n\n    The declared package \"org.apache.sqoop.fi\" does not match the expected package \"aop.org.apache.sqoop.fi\"\n    \n如下图所示配置，问题解决。\n![Eclipse Sqoop](/uploads/20171125/eclipse-sqoop-import.png)\n","slug":"Eclipse-导入-Sqoop","published":1,"updated":"2021-07-19T16:28:00.244Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphls001aitd3gisfgpkx","content":"<p>Sqoop 是使用 Ant 进行编译的，如果需要导入 Eclipse 首先执行以下命令生成 .project 和 .classpath 文件。</p>\n<pre><code>ant eclipse\n</code></pre>\n<p>操作完成后就可以导入 Eclipse 了。</p>\n<span id=\"more\"></span>\n\n<p>导入 Eclipse 后编译会提示以下错误：</p>\n<pre><code>Project &#39;sqoop&#39; is missing required library: &#39;build/classes&#39;\nProject &#39;sqoop&#39; is missing required library: &#39;build/test/classes&#39;\nProject &#39;sqoop&#39; is missing required library: &#39;build/test/extraconf&#39;\nThe project cannot be built until build path errors are resolved\n</code></pre>\n<p>执行 ant 命令编译 Sqoop，然后在 Eclipse 中刷新项目，出现如下错误：</p>\n<pre><code>The declared package &quot;org.apache.sqoop.fi&quot; does not match the expected package &quot;aop.org.apache.sqoop.fi&quot;\n</code></pre>\n<p>如下图所示配置，问题解决。<br><img src=\"/uploads/20171125/eclipse-sqoop-import.png\" alt=\"Eclipse Sqoop\"></p>\n","site":{"data":{}},"excerpt":"<p>Sqoop 是使用 Ant 进行编译的，如果需要导入 Eclipse 首先执行以下命令生成 .project 和 .classpath 文件。</p>\n<pre><code>ant eclipse\n</code></pre>\n<p>操作完成后就可以导入 Eclipse 了。</p>","more":"<p>导入 Eclipse 后编译会提示以下错误：</p>\n<pre><code>Project &#39;sqoop&#39; is missing required library: &#39;build/classes&#39;\nProject &#39;sqoop&#39; is missing required library: &#39;build/test/classes&#39;\nProject &#39;sqoop&#39; is missing required library: &#39;build/test/extraconf&#39;\nThe project cannot be built until build path errors are resolved\n</code></pre>\n<p>执行 ant 命令编译 Sqoop，然后在 Eclipse 中刷新项目，出现如下错误：</p>\n<pre><code>The declared package &quot;org.apache.sqoop.fi&quot; does not match the expected package &quot;aop.org.apache.sqoop.fi&quot;\n</code></pre>\n<p>如下图所示配置，问题解决。<br><img src=\"/uploads/20171125/eclipse-sqoop-import.png\" alt=\"Eclipse Sqoop\"></p>"},{"title":"Emacs Shell 上下键切换历史命令","date":"2017-03-09T15:51:27.000Z","_content":"\nEmacs 开启 Shell 模式，UP 键默认是上一行，而不是上一条执行过的命令。为了方便，通过以下方式实现 UP 键和 DOWN 键切换历史命令。\n\n<!-- more -->\n\n- 设置 UP 键\n\n    M+x global-set-key\n    按 UP 键\n    comint-previous-input\n\n- 设置 DOWN 键\n\n    M+x global-set-key\n    按 DOWN 键\n    comint-nex-input\n\n","source":"_posts/Emacs-Shell-上下键切换历史命令.md","raw":"title: Emacs Shell 上下键切换历史命令\ntags:\n  - Emacs\ncategories:\n  - 开发工具\n  - Emacs\ndate: 2017-03-09 23:51:27\n---\n\nEmacs 开启 Shell 模式，UP 键默认是上一行，而不是上一条执行过的命令。为了方便，通过以下方式实现 UP 键和 DOWN 键切换历史命令。\n\n<!-- more -->\n\n- 设置 UP 键\n\n    M+x global-set-key\n    按 UP 键\n    comint-previous-input\n\n- 设置 DOWN 键\n\n    M+x global-set-key\n    按 DOWN 键\n    comint-nex-input\n\n","slug":"Emacs-Shell-上下键切换历史命令","published":1,"updated":"2021-07-19T16:28:00.244Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphlv001ditd37gji5252","content":"<p>Emacs 开启 Shell 模式，UP 键默认是上一行，而不是上一条执行过的命令。为了方便，通过以下方式实现 UP 键和 DOWN 键切换历史命令。</p>\n<span id=\"more\"></span>\n\n<ul>\n<li><p>设置 UP 键</p>\n<p>  M+x global-set-key<br>  按 UP 键<br>  comint-previous-input</p>\n</li>\n<li><p>设置 DOWN 键</p>\n<p>  M+x global-set-key<br>  按 DOWN 键<br>  comint-nex-input</p>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>Emacs 开启 Shell 模式，UP 键默认是上一行，而不是上一条执行过的命令。为了方便，通过以下方式实现 UP 键和 DOWN 键切换历史命令。</p>","more":"<ul>\n<li><p>设置 UP 键</p>\n<p>  M+x global-set-key<br>  按 UP 键<br>  comint-previous-input</p>\n</li>\n<li><p>设置 DOWN 键</p>\n<p>  M+x global-set-key<br>  按 DOWN 键<br>  comint-nex-input</p>\n</li>\n</ul>"},{"title":"Emacs org 文档 Table 行和列操作","date":"2019-04-17T06:13:35.000Z","_content":"**M+x org-table-insert-row** 在当前行的上面插入一个新行。\n**M+x org-table-insert-column** 在当前列前插入一个新列。\n**M+x org-table-delete-column** 删除当前列。","source":"_posts/Emacs-org-文档-Table-行和列操作.md","raw":"title: Emacs org 文档 Table 行和列操作\ndate: 2019-04-17 14:13:35\ntags:\n- Emacs\ncategories:\n- 开发工具\n- Emacs\n---\n**M+x org-table-insert-row** 在当前行的上面插入一个新行。\n**M+x org-table-insert-column** 在当前列前插入一个新列。\n**M+x org-table-delete-column** 删除当前列。","slug":"Emacs-org-文档-Table-行和列操作","published":1,"updated":"2021-07-19T16:28:00.244Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphlw001eitd3d7flb8as","content":"<p><strong>M+x org-table-insert-row</strong> 在当前行的上面插入一个新行。<br><strong>M+x org-table-insert-column</strong> 在当前列前插入一个新列。<br><strong>M+x org-table-delete-column</strong> 删除当前列。</p>\n","site":{"data":{}},"excerpt":"","more":"<p><strong>M+x org-table-insert-row</strong> 在当前行的上面插入一个新行。<br><strong>M+x org-table-insert-column</strong> 在当前列前插入一个新列。<br><strong>M+x org-table-delete-column</strong> 删除当前列。</p>\n"},{"title":"Emacs org 设置自动换行","date":"2019-11-02T08:58:44.000Z","_content":"\n在 Emacs 配置文件 .emacs 中添加以下配置：\n\n    (add-hook 'org-mode-hook (lambda () (setq truncate-lines nil)))\n","source":"_posts/Emacs-org-设置自动换行.md","raw":"title: Emacs org 设置自动换行\ndate: 2019-11-02 16:58:44\ntags:\n- Emacs\ncategories:\n- 开发工具\n- Emacs\n---\n\n在 Emacs 配置文件 .emacs 中添加以下配置：\n\n    (add-hook 'org-mode-hook (lambda () (setq truncate-lines nil)))\n","slug":"Emacs-org-设置自动换行","published":1,"updated":"2021-07-19T16:28:00.328Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphly001iitd312n48w93","content":"<p>在 Emacs 配置文件 .emacs 中添加以下配置：</p>\n<pre><code>(add-hook &#39;org-mode-hook (lambda () (setq truncate-lines nil)))\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>在 Emacs 配置文件 .emacs 中添加以下配置：</p>\n<pre><code>(add-hook &#39;org-mode-hook (lambda () (setq truncate-lines nil)))\n</code></pre>\n"},{"title":"Emacs 恢复自动保存文件","date":"2016-09-04T14:53:49.000Z","_content":"\n\n先打开没有来得及保存的文件，然后用下面的命令恢复自动保存的文件内容：\n\n    M-x recover file\n    ","source":"_posts/Emacs-恢复自动保存文件.md","raw":"title: Emacs 恢复自动保存文件\ntags:\n  - Emacs\ncategories:\n  - 开发工具\n  - Emacs\ndate: 2016-09-04 22:53:49\n---\n\n\n先打开没有来得及保存的文件，然后用下面的命令恢复自动保存的文件内容：\n\n    M-x recover file\n    ","slug":"Emacs-恢复自动保存文件","published":1,"updated":"2021-07-19T16:28:00.244Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphlz001kitd38c4re4d8","content":"<p>先打开没有来得及保存的文件，然后用下面的命令恢复自动保存的文件内容：</p>\n<pre><code>M-x recover file\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>先打开没有来得及保存的文件，然后用下面的命令恢复自动保存的文件内容：</p>\n<pre><code>M-x recover file\n</code></pre>\n"},{"title":"Emacs 禁用自动备份及修改备份目录","date":"2016-07-12T07:17:48.000Z","_content":"\n\n### Emacs 禁用自动备份\n\n在 ~/.emacs 文件中添加配置：\n\n    ;; disable auto backup\n\t(setq make-backup-files nil)\n\n### 修改备份目录\n\n在 ~/.emacs 文件中添加配置：\n\n    ;; set backup directory to ~/.backups\n    (setq backup-directory-alist (quote ((\".\" . \"~/.backups\"))))\n","source":"_posts/Emacs-禁用自动备份及修改备份目录.md","raw":"title: Emacs 禁用自动备份及修改备份目录\ntags:\n  - Emacs\ncategories:\n  - 开发工具\n  - Emacs\ndate: 2016-07-12 15:17:48\n---\n\n\n### Emacs 禁用自动备份\n\n在 ~/.emacs 文件中添加配置：\n\n    ;; disable auto backup\n\t(setq make-backup-files nil)\n\n### 修改备份目录\n\n在 ~/.emacs 文件中添加配置：\n\n    ;; set backup directory to ~/.backups\n    (setq backup-directory-alist (quote ((\".\" . \"~/.backups\"))))\n","slug":"Emacs-禁用自动备份及修改备份目录","published":1,"updated":"2021-07-19T16:28:00.244Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphm1001nitd3hd8qfvzd","content":"<h3 id=\"Emacs-禁用自动备份\"><a href=\"#Emacs-禁用自动备份\" class=\"headerlink\" title=\"Emacs 禁用自动备份\"></a>Emacs 禁用自动备份</h3><p>在 ~/.emacs 文件中添加配置：</p>\n<pre><code>;; disable auto backup\n(setq make-backup-files nil)\n</code></pre>\n<h3 id=\"修改备份目录\"><a href=\"#修改备份目录\" class=\"headerlink\" title=\"修改备份目录\"></a>修改备份目录</h3><p>在 ~/.emacs 文件中添加配置：</p>\n<pre><code>;; set backup directory to ~/.backups\n(setq backup-directory-alist (quote ((&quot;.&quot; . &quot;~/.backups&quot;))))\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"Emacs-禁用自动备份\"><a href=\"#Emacs-禁用自动备份\" class=\"headerlink\" title=\"Emacs 禁用自动备份\"></a>Emacs 禁用自动备份</h3><p>在 ~/.emacs 文件中添加配置：</p>\n<pre><code>;; disable auto backup\n(setq make-backup-files nil)\n</code></pre>\n<h3 id=\"修改备份目录\"><a href=\"#修改备份目录\" class=\"headerlink\" title=\"修改备份目录\"></a>修改备份目录</h3><p>在 ~/.emacs 文件中添加配置：</p>\n<pre><code>;; set backup directory to ~/.backups\n(setq backup-directory-alist (quote ((&quot;.&quot; . &quot;~/.backups&quot;))))\n</code></pre>\n"},{"title":"Flume1.9支持Kafka Record的Header信息读取","date":"2021-05-20T03:46:23.000Z","_content":"\n在新版本Kafka中为了更好的支持业务扩展，消息格式增加了消息级别的Header信息。但是Flume从Kafka中读取数据时只处理了Avro格式中的Header信息。解决这一问题修改Flume源代码如下：\n\norg.apache.flume.source.kafka.KafkaSource.java类中的doProcess方法第280行处添加以下代码：\n\n```Java\n        Headers messageHeaders = message.headers();\n        for (Header header : messageHeaders) {\n          String key = header.key();\n          String value = new String(header.value());\n\n          if (!headers.containsKey(key)) {\n            headers.put(key, value);\n          }\n        }\n```\n\n[完整的Java文件](/uploads/20210520/KafkaSource.java)","source":"_posts/Flume1-9支持Kafka-Record的Header信息读取.md","raw":"title: Flume1.9支持Kafka Record的Header信息读取\ndate: 2021-05-20 11:46:23\ntags:\n- Flume\n- Kafka\ncategories:\n- 大数据\n- Flume\n---\n\n在新版本Kafka中为了更好的支持业务扩展，消息格式增加了消息级别的Header信息。但是Flume从Kafka中读取数据时只处理了Avro格式中的Header信息。解决这一问题修改Flume源代码如下：\n\norg.apache.flume.source.kafka.KafkaSource.java类中的doProcess方法第280行处添加以下代码：\n\n```Java\n        Headers messageHeaders = message.headers();\n        for (Header header : messageHeaders) {\n          String key = header.key();\n          String value = new String(header.value());\n\n          if (!headers.containsKey(key)) {\n            headers.put(key, value);\n          }\n        }\n```\n\n[完整的Java文件](/uploads/20210520/KafkaSource.java)","slug":"Flume1-9支持Kafka-Record的Header信息读取","published":1,"updated":"2021-07-19T16:28:00.168Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphm2001pitd361sch2at","content":"<p>在新版本Kafka中为了更好的支持业务扩展，消息格式增加了消息级别的Header信息。但是Flume从Kafka中读取数据时只处理了Avro格式中的Header信息。解决这一问题修改Flume源代码如下：</p>\n<p>org.apache.flume.source.kafka.KafkaSource.java类中的doProcess方法第280行处添加以下代码：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Headers messageHeaders = message.headers();</span><br><span class=\"line\"><span class=\"keyword\">for</span> (Header header : messageHeaders) &#123;</span><br><span class=\"line\">  String key = header.key();</span><br><span class=\"line\">  String value = <span class=\"keyword\">new</span> String(header.value());</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">if</span> (!headers.containsKey(key)) &#123;</span><br><span class=\"line\">    headers.put(key, value);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p><a href=\"/uploads/20210520/KafkaSource.java\">完整的Java文件</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>在新版本Kafka中为了更好的支持业务扩展，消息格式增加了消息级别的Header信息。但是Flume从Kafka中读取数据时只处理了Avro格式中的Header信息。解决这一问题修改Flume源代码如下：</p>\n<p>org.apache.flume.source.kafka.KafkaSource.java类中的doProcess方法第280行处添加以下代码：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Headers messageHeaders = message.headers();</span><br><span class=\"line\"><span class=\"keyword\">for</span> (Header header : messageHeaders) &#123;</span><br><span class=\"line\">  String key = header.key();</span><br><span class=\"line\">  String value = <span class=\"keyword\">new</span> String(header.value());</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">if</span> (!headers.containsKey(key)) &#123;</span><br><span class=\"line\">    headers.put(key, value);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p><a href=\"/uploads/20210520/KafkaSource.java\">完整的Java文件</a></p>\n"},{"title":"GitLab + Jenkins + Nginx + Lua 实现代码自动分发","date":"2019-03-13T14:48:30.000Z","_content":"在大数据 Hive 数据仓库的开发中，主要是 shell + HQL 的脚本开发。脚本的测试需要放到适当的大数据环境中进行。如果大家共用 GitLab 项目上的同一个 Branch 进行测试需要代码频繁合并，影响效率。我的思路是每个 Developer 在自己的 Branch 开发并经过测试后发起 Merge Request。  \n在我的开发场景中 GitLab 在内网中通过端口映射后允许从外网访问，GitLab 中的 IP 都是内网的，GitLab 上的项目链接地址都是内网的，如（ssh://git@192.168.1.10:50022/test/test.git）。并且我的 Jenkins 也是在内网中的，所以需要通过一层代理接收外部的 REST API 请求。这一点给最终的实现带来一些难点。\n\n<!-- more -->\n\n#### 实现架构\n\n![数据仓库开发测试代码分发流程](/uploads/20190314/数据仓库开发测试代码分发流程.png)\n\n#### GitLab 项目配置\n\n在 GitLab 项目上配置 WebHook：![GitLabWebHook](/uploads/20190314/GitLabWebHook.png)\n\n这样每次项目的 push 操作都会向这个配置的 URL 发送一个 Post 请求。每次 WebHook 的执行情况可以点击 WebHook 后面的 Edit 按钮在编辑页面中查看。\n\n#### Nginx + Lua\n\n##### OpenResty 安装\n\n直接使用推荐的预编译 OpenResty 安装。Centos7.3 执行以下命令：\n\n    sudo yum install yum-utils\n    sudo yum-config-manager --add-repo https://openresty.org/package/centos/openresty.repo\n    sudo yum install openresty\n\n##### 安装 OpenResty Lua HTTP 客户端模块\n\n下载 GitHub 项目代码：[lua-resty-http](https://github.com/ledgetech/lua-resty-http)\n\n将 lib/resty 目录下的两个文件 http.lua 和 http_headers.lua 放到 OpenResty 安装对应的目录下（如：/usr/local/openresty/lualib/resty）。\n\n##### Nginx 配置\n\nnginx.conf 配置内容如下。主要是使用 Lua 解析从 GitLab WebHook 发送来的数据，并解析需要的参数后通过 Jenkins 的 REST API 发送给 Jenkins。\n\n    server {\n        listen       8080;\n        server_name  localhost;\n\n        #charset koi8-r;\n\n        #access_log  logs/host.access.log  main;\n\n        location / {\n            content_by_lua_block {\n                local cjson = require \"cjson\"\n                local http = require \"resty.http\"\n                local httpc = http.new()\n\n                ngx.req.read_body()\n                local data = ngx.req.get_body_data()\n                local json = cjson.decode(data)\n                local after = json[\"after\"]\n                if ( after == \"0000000000000000000000000000000000000000\" )\n                then\n                    ngx.status = 200\n                    ngx.say(\"delete branch do not need build\")\n                    return\n                end\n\n                local projectName = json[\"project\"][\"name\"]\n                local userName = json[\"user_name\"]\n                local ref = json[\"ref\"]\n                local branchName = string.sub(ref, 12)\n                if ( branchName == \"master\" )\n                then\n                    ngx.status = 200\n                    ngx.say(\"master branch do not need build\")\n                    return\n                end\n\n                local uri = \"http://172.16.72.200:8080/job/DataWarehouse/buildWithParameters?userName=\"\n                uri = uri..userName..\"&branchName=\"..branchName..\"&projectName=\"..projectName\n                local res, err = httpc:request_uri(uri, {\n                    method = \"POST\"\n                })\n\n                ngx.status = res.status\n                ngx.say(res.body)\n            }\n        }\n    }\n\n#### Jenkins 项目配置\n\n##### 配置参数化构建的项目\n\n注意参数名称与 Lua 发送请求的参数名称要对应，如下图：![](/uploads/20190314/JenkinsProject1.png)\n\n##### 代发分发逻辑\n\n在构建的 Shell 中实现代码分发到对应 Developer 的个人目录下：![](/uploads/20190314/JenkinsProject2.png)\n","source":"_posts/GitLab-Jenkins-Nginx-Lua-实现代码自动分发.md","raw":"title: GitLab + Jenkins + Nginx + Lua 实现代码自动分发\ndate: 2019-03-13 22:48:30\ntags:\n- GitLab\n- Jenkins\n- Nginx\n- Lua\n- OpenResty\ncategories:\n- 开发\n- CI\n---\n在大数据 Hive 数据仓库的开发中，主要是 shell + HQL 的脚本开发。脚本的测试需要放到适当的大数据环境中进行。如果大家共用 GitLab 项目上的同一个 Branch 进行测试需要代码频繁合并，影响效率。我的思路是每个 Developer 在自己的 Branch 开发并经过测试后发起 Merge Request。  \n在我的开发场景中 GitLab 在内网中通过端口映射后允许从外网访问，GitLab 中的 IP 都是内网的，GitLab 上的项目链接地址都是内网的，如（ssh://git@192.168.1.10:50022/test/test.git）。并且我的 Jenkins 也是在内网中的，所以需要通过一层代理接收外部的 REST API 请求。这一点给最终的实现带来一些难点。\n\n<!-- more -->\n\n#### 实现架构\n\n![数据仓库开发测试代码分发流程](/uploads/20190314/数据仓库开发测试代码分发流程.png)\n\n#### GitLab 项目配置\n\n在 GitLab 项目上配置 WebHook：![GitLabWebHook](/uploads/20190314/GitLabWebHook.png)\n\n这样每次项目的 push 操作都会向这个配置的 URL 发送一个 Post 请求。每次 WebHook 的执行情况可以点击 WebHook 后面的 Edit 按钮在编辑页面中查看。\n\n#### Nginx + Lua\n\n##### OpenResty 安装\n\n直接使用推荐的预编译 OpenResty 安装。Centos7.3 执行以下命令：\n\n    sudo yum install yum-utils\n    sudo yum-config-manager --add-repo https://openresty.org/package/centos/openresty.repo\n    sudo yum install openresty\n\n##### 安装 OpenResty Lua HTTP 客户端模块\n\n下载 GitHub 项目代码：[lua-resty-http](https://github.com/ledgetech/lua-resty-http)\n\n将 lib/resty 目录下的两个文件 http.lua 和 http_headers.lua 放到 OpenResty 安装对应的目录下（如：/usr/local/openresty/lualib/resty）。\n\n##### Nginx 配置\n\nnginx.conf 配置内容如下。主要是使用 Lua 解析从 GitLab WebHook 发送来的数据，并解析需要的参数后通过 Jenkins 的 REST API 发送给 Jenkins。\n\n    server {\n        listen       8080;\n        server_name  localhost;\n\n        #charset koi8-r;\n\n        #access_log  logs/host.access.log  main;\n\n        location / {\n            content_by_lua_block {\n                local cjson = require \"cjson\"\n                local http = require \"resty.http\"\n                local httpc = http.new()\n\n                ngx.req.read_body()\n                local data = ngx.req.get_body_data()\n                local json = cjson.decode(data)\n                local after = json[\"after\"]\n                if ( after == \"0000000000000000000000000000000000000000\" )\n                then\n                    ngx.status = 200\n                    ngx.say(\"delete branch do not need build\")\n                    return\n                end\n\n                local projectName = json[\"project\"][\"name\"]\n                local userName = json[\"user_name\"]\n                local ref = json[\"ref\"]\n                local branchName = string.sub(ref, 12)\n                if ( branchName == \"master\" )\n                then\n                    ngx.status = 200\n                    ngx.say(\"master branch do not need build\")\n                    return\n                end\n\n                local uri = \"http://172.16.72.200:8080/job/DataWarehouse/buildWithParameters?userName=\"\n                uri = uri..userName..\"&branchName=\"..branchName..\"&projectName=\"..projectName\n                local res, err = httpc:request_uri(uri, {\n                    method = \"POST\"\n                })\n\n                ngx.status = res.status\n                ngx.say(res.body)\n            }\n        }\n    }\n\n#### Jenkins 项目配置\n\n##### 配置参数化构建的项目\n\n注意参数名称与 Lua 发送请求的参数名称要对应，如下图：![](/uploads/20190314/JenkinsProject1.png)\n\n##### 代发分发逻辑\n\n在构建的 Shell 中实现代码分发到对应 Developer 的个人目录下：![](/uploads/20190314/JenkinsProject2.png)\n","slug":"GitLab-Jenkins-Nginx-Lua-实现代码自动分发","published":1,"updated":"2021-07-19T16:28:00.244Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphm4001titd3hl5i9gk8","content":"<p>在大数据 Hive 数据仓库的开发中，主要是 shell + HQL 的脚本开发。脚本的测试需要放到适当的大数据环境中进行。如果大家共用 GitLab 项目上的同一个 Branch 进行测试需要代码频繁合并，影响效率。我的思路是每个 Developer 在自己的 Branch 开发并经过测试后发起 Merge Request。<br>在我的开发场景中 GitLab 在内网中通过端口映射后允许从外网访问，GitLab 中的 IP 都是内网的，GitLab 上的项目链接地址都是内网的，如（ssh://<a href=\"mailto:&#103;&#105;&#x74;&#64;&#49;&#x39;&#x32;&#46;&#49;&#54;&#56;&#x2e;&#49;&#x2e;&#49;&#x30;\">&#103;&#105;&#x74;&#64;&#49;&#x39;&#x32;&#46;&#49;&#54;&#56;&#x2e;&#49;&#x2e;&#49;&#x30;</a>:50022/test/test.git）。并且我的 Jenkins 也是在内网中的，所以需要通过一层代理接收外部的 REST API 请求。这一点给最终的实现带来一些难点。</p>\n<span id=\"more\"></span>\n\n<h4 id=\"实现架构\"><a href=\"#实现架构\" class=\"headerlink\" title=\"实现架构\"></a>实现架构</h4><p><img src=\"/uploads/20190314/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%BC%80%E5%8F%91%E6%B5%8B%E8%AF%95%E4%BB%A3%E7%A0%81%E5%88%86%E5%8F%91%E6%B5%81%E7%A8%8B.png\" alt=\"数据仓库开发测试代码分发流程\"></p>\n<h4 id=\"GitLab-项目配置\"><a href=\"#GitLab-项目配置\" class=\"headerlink\" title=\"GitLab 项目配置\"></a>GitLab 项目配置</h4><p>在 GitLab 项目上配置 WebHook：<img src=\"/uploads/20190314/GitLabWebHook.png\" alt=\"GitLabWebHook\"></p>\n<p>这样每次项目的 push 操作都会向这个配置的 URL 发送一个 Post 请求。每次 WebHook 的执行情况可以点击 WebHook 后面的 Edit 按钮在编辑页面中查看。</p>\n<h4 id=\"Nginx-Lua\"><a href=\"#Nginx-Lua\" class=\"headerlink\" title=\"Nginx + Lua\"></a>Nginx + Lua</h4><h5 id=\"OpenResty-安装\"><a href=\"#OpenResty-安装\" class=\"headerlink\" title=\"OpenResty 安装\"></a>OpenResty 安装</h5><p>直接使用推荐的预编译 OpenResty 安装。Centos7.3 执行以下命令：</p>\n<pre><code>sudo yum install yum-utils\nsudo yum-config-manager --add-repo https://openresty.org/package/centos/openresty.repo\nsudo yum install openresty\n</code></pre>\n<h5 id=\"安装-OpenResty-Lua-HTTP-客户端模块\"><a href=\"#安装-OpenResty-Lua-HTTP-客户端模块\" class=\"headerlink\" title=\"安装 OpenResty Lua HTTP 客户端模块\"></a>安装 OpenResty Lua HTTP 客户端模块</h5><p>下载 GitHub 项目代码：<a href=\"https://github.com/ledgetech/lua-resty-http\">lua-resty-http</a></p>\n<p>将 lib/resty 目录下的两个文件 http.lua 和 http_headers.lua 放到 OpenResty 安装对应的目录下（如：/usr/local/openresty/lualib/resty）。</p>\n<h5 id=\"Nginx-配置\"><a href=\"#Nginx-配置\" class=\"headerlink\" title=\"Nginx 配置\"></a>Nginx 配置</h5><p>nginx.conf 配置内容如下。主要是使用 Lua 解析从 GitLab WebHook 发送来的数据，并解析需要的参数后通过 Jenkins 的 REST API 发送给 Jenkins。</p>\n<pre><code>server &#123;\n    listen       8080;\n    server_name  localhost;\n\n    #charset koi8-r;\n\n    #access_log  logs/host.access.log  main;\n\n    location / &#123;\n        content_by_lua_block &#123;\n            local cjson = require &quot;cjson&quot;\n            local http = require &quot;resty.http&quot;\n            local httpc = http.new()\n\n            ngx.req.read_body()\n            local data = ngx.req.get_body_data()\n            local json = cjson.decode(data)\n            local after = json[&quot;after&quot;]\n            if ( after == &quot;0000000000000000000000000000000000000000&quot; )\n            then\n                ngx.status = 200\n                ngx.say(&quot;delete branch do not need build&quot;)\n                return\n            end\n\n            local projectName = json[&quot;project&quot;][&quot;name&quot;]\n            local userName = json[&quot;user_name&quot;]\n            local ref = json[&quot;ref&quot;]\n            local branchName = string.sub(ref, 12)\n            if ( branchName == &quot;master&quot; )\n            then\n                ngx.status = 200\n                ngx.say(&quot;master branch do not need build&quot;)\n                return\n            end\n\n            local uri = &quot;http://172.16.72.200:8080/job/DataWarehouse/buildWithParameters?userName=&quot;\n            uri = uri..userName..&quot;&amp;branchName=&quot;..branchName..&quot;&amp;projectName=&quot;..projectName\n            local res, err = httpc:request_uri(uri, &#123;\n                method = &quot;POST&quot;\n            &#125;)\n\n            ngx.status = res.status\n            ngx.say(res.body)\n        &#125;\n    &#125;\n&#125;\n</code></pre>\n<h4 id=\"Jenkins-项目配置\"><a href=\"#Jenkins-项目配置\" class=\"headerlink\" title=\"Jenkins 项目配置\"></a>Jenkins 项目配置</h4><h5 id=\"配置参数化构建的项目\"><a href=\"#配置参数化构建的项目\" class=\"headerlink\" title=\"配置参数化构建的项目\"></a>配置参数化构建的项目</h5><p>注意参数名称与 Lua 发送请求的参数名称要对应，如下图：<img src=\"/uploads/20190314/JenkinsProject1.png\"></p>\n<h5 id=\"代发分发逻辑\"><a href=\"#代发分发逻辑\" class=\"headerlink\" title=\"代发分发逻辑\"></a>代发分发逻辑</h5><p>在构建的 Shell 中实现代码分发到对应 Developer 的个人目录下：<img src=\"/uploads/20190314/JenkinsProject2.png\"></p>\n","site":{"data":{}},"excerpt":"<p>在大数据 Hive 数据仓库的开发中，主要是 shell + HQL 的脚本开发。脚本的测试需要放到适当的大数据环境中进行。如果大家共用 GitLab 项目上的同一个 Branch 进行测试需要代码频繁合并，影响效率。我的思路是每个 Developer 在自己的 Branch 开发并经过测试后发起 Merge Request。<br>在我的开发场景中 GitLab 在内网中通过端口映射后允许从外网访问，GitLab 中的 IP 都是内网的，GitLab 上的项目链接地址都是内网的，如（ssh://<a href=\"mailto:&#103;&#105;&#x74;&#64;&#49;&#x39;&#x32;&#46;&#49;&#54;&#56;&#x2e;&#49;&#x2e;&#49;&#x30;\">&#103;&#105;&#x74;&#64;&#49;&#x39;&#x32;&#46;&#49;&#54;&#56;&#x2e;&#49;&#x2e;&#49;&#x30;</a>:50022/test/test.git）。并且我的 Jenkins 也是在内网中的，所以需要通过一层代理接收外部的 REST API 请求。这一点给最终的实现带来一些难点。</p>","more":"<h4 id=\"实现架构\"><a href=\"#实现架构\" class=\"headerlink\" title=\"实现架构\"></a>实现架构</h4><p><img src=\"/uploads/20190314/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%BC%80%E5%8F%91%E6%B5%8B%E8%AF%95%E4%BB%A3%E7%A0%81%E5%88%86%E5%8F%91%E6%B5%81%E7%A8%8B.png\" alt=\"数据仓库开发测试代码分发流程\"></p>\n<h4 id=\"GitLab-项目配置\"><a href=\"#GitLab-项目配置\" class=\"headerlink\" title=\"GitLab 项目配置\"></a>GitLab 项目配置</h4><p>在 GitLab 项目上配置 WebHook：<img src=\"/uploads/20190314/GitLabWebHook.png\" alt=\"GitLabWebHook\"></p>\n<p>这样每次项目的 push 操作都会向这个配置的 URL 发送一个 Post 请求。每次 WebHook 的执行情况可以点击 WebHook 后面的 Edit 按钮在编辑页面中查看。</p>\n<h4 id=\"Nginx-Lua\"><a href=\"#Nginx-Lua\" class=\"headerlink\" title=\"Nginx + Lua\"></a>Nginx + Lua</h4><h5 id=\"OpenResty-安装\"><a href=\"#OpenResty-安装\" class=\"headerlink\" title=\"OpenResty 安装\"></a>OpenResty 安装</h5><p>直接使用推荐的预编译 OpenResty 安装。Centos7.3 执行以下命令：</p>\n<pre><code>sudo yum install yum-utils\nsudo yum-config-manager --add-repo https://openresty.org/package/centos/openresty.repo\nsudo yum install openresty\n</code></pre>\n<h5 id=\"安装-OpenResty-Lua-HTTP-客户端模块\"><a href=\"#安装-OpenResty-Lua-HTTP-客户端模块\" class=\"headerlink\" title=\"安装 OpenResty Lua HTTP 客户端模块\"></a>安装 OpenResty Lua HTTP 客户端模块</h5><p>下载 GitHub 项目代码：<a href=\"https://github.com/ledgetech/lua-resty-http\">lua-resty-http</a></p>\n<p>将 lib/resty 目录下的两个文件 http.lua 和 http_headers.lua 放到 OpenResty 安装对应的目录下（如：/usr/local/openresty/lualib/resty）。</p>\n<h5 id=\"Nginx-配置\"><a href=\"#Nginx-配置\" class=\"headerlink\" title=\"Nginx 配置\"></a>Nginx 配置</h5><p>nginx.conf 配置内容如下。主要是使用 Lua 解析从 GitLab WebHook 发送来的数据，并解析需要的参数后通过 Jenkins 的 REST API 发送给 Jenkins。</p>\n<pre><code>server &#123;\n    listen       8080;\n    server_name  localhost;\n\n    #charset koi8-r;\n\n    #access_log  logs/host.access.log  main;\n\n    location / &#123;\n        content_by_lua_block &#123;\n            local cjson = require &quot;cjson&quot;\n            local http = require &quot;resty.http&quot;\n            local httpc = http.new()\n\n            ngx.req.read_body()\n            local data = ngx.req.get_body_data()\n            local json = cjson.decode(data)\n            local after = json[&quot;after&quot;]\n            if ( after == &quot;0000000000000000000000000000000000000000&quot; )\n            then\n                ngx.status = 200\n                ngx.say(&quot;delete branch do not need build&quot;)\n                return\n            end\n\n            local projectName = json[&quot;project&quot;][&quot;name&quot;]\n            local userName = json[&quot;user_name&quot;]\n            local ref = json[&quot;ref&quot;]\n            local branchName = string.sub(ref, 12)\n            if ( branchName == &quot;master&quot; )\n            then\n                ngx.status = 200\n                ngx.say(&quot;master branch do not need build&quot;)\n                return\n            end\n\n            local uri = &quot;http://172.16.72.200:8080/job/DataWarehouse/buildWithParameters?userName=&quot;\n            uri = uri..userName..&quot;&amp;branchName=&quot;..branchName..&quot;&amp;projectName=&quot;..projectName\n            local res, err = httpc:request_uri(uri, &#123;\n                method = &quot;POST&quot;\n            &#125;)\n\n            ngx.status = res.status\n            ngx.say(res.body)\n        &#125;\n    &#125;\n&#125;\n</code></pre>\n<h4 id=\"Jenkins-项目配置\"><a href=\"#Jenkins-项目配置\" class=\"headerlink\" title=\"Jenkins 项目配置\"></a>Jenkins 项目配置</h4><h5 id=\"配置参数化构建的项目\"><a href=\"#配置参数化构建的项目\" class=\"headerlink\" title=\"配置参数化构建的项目\"></a>配置参数化构建的项目</h5><p>注意参数名称与 Lua 发送请求的参数名称要对应，如下图：<img src=\"/uploads/20190314/JenkinsProject1.png\"></p>\n<h5 id=\"代发分发逻辑\"><a href=\"#代发分发逻辑\" class=\"headerlink\" title=\"代发分发逻辑\"></a>代发分发逻辑</h5><p>在构建的 Shell 中实现代码分发到对应 Developer 的个人目录下：<img src=\"/uploads/20190314/JenkinsProject2.png\"></p>"},{"title":"HDFS NameNode HA sshfence 端口","date":"2017-05-21T00:17:18.000Z","_content":"\n当 HDFS NameNode 发生 Failover 时，为了防止发生“脑裂”现象，需要对原 Active 的 NameNode 采用防护措施。当两个 NameNode 节点直接互相 SSH 使用非默认端口 22 时，需要在配置中指定使用的端口。例如，使用端口 2222，则配置如下：\n\n    <property>\n      <name>dfs.ha.fencing.methods</name>\n      <value>sshfence(hadoop:2222)</value>\n    </property>\n\n其中，小括号中的 hadoop 为 SSH 的用户名，2222 为端口号。采用 SSH 方式需要先打通两台 NameNode 间互相 SSH 的白名单。\n","source":"_posts/HDFS-NameNode-HA-sshfence-端口.md","raw":"title: HDFS NameNode HA sshfence 端口\ntags:\n  - Hadoop\n  - HDFS\ncategories:\n  - 大数据\n  - Hadoop\ndate: 2017-05-21 08:17:18\n---\n\n当 HDFS NameNode 发生 Failover 时，为了防止发生“脑裂”现象，需要对原 Active 的 NameNode 采用防护措施。当两个 NameNode 节点直接互相 SSH 使用非默认端口 22 时，需要在配置中指定使用的端口。例如，使用端口 2222，则配置如下：\n\n    <property>\n      <name>dfs.ha.fencing.methods</name>\n      <value>sshfence(hadoop:2222)</value>\n    </property>\n\n其中，小括号中的 hadoop 为 SSH 的用户名，2222 为端口号。采用 SSH 方式需要先打通两台 NameNode 间互相 SSH 的白名单。\n","slug":"HDFS-NameNode-HA-sshfence-端口","published":1,"updated":"2021-07-19T16:28:00.248Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphm5001xitd3ex3c3irx","content":"<p>当 HDFS NameNode 发生 Failover 时，为了防止发生“脑裂”现象，需要对原 Active 的 NameNode 采用防护措施。当两个 NameNode 节点直接互相 SSH 使用非默认端口 22 时，需要在配置中指定使用的端口。例如，使用端口 2222，则配置如下：</p>\n<pre><code>&lt;property&gt;\n  &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;\n  &lt;value&gt;sshfence(hadoop:2222)&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<p>其中，小括号中的 hadoop 为 SSH 的用户名，2222 为端口号。采用 SSH 方式需要先打通两台 NameNode 间互相 SSH 的白名单。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>当 HDFS NameNode 发生 Failover 时，为了防止发生“脑裂”现象，需要对原 Active 的 NameNode 采用防护措施。当两个 NameNode 节点直接互相 SSH 使用非默认端口 22 时，需要在配置中指定使用的端口。例如，使用端口 2222，则配置如下：</p>\n<pre><code>&lt;property&gt;\n  &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;\n  &lt;value&gt;sshfence(hadoop:2222)&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<p>其中，小括号中的 hadoop 为 SSH 的用户名，2222 为端口号。采用 SSH 方式需要先打通两台 NameNode 间互相 SSH 的白名单。</p>\n"},{"title":"HDFS Rebalance","date":"2018-03-18T09:29:03.000Z","_content":"\n\n### 相关配置项\n\n1. dfs.datanode.balance.bandwidthPerSec\n\nDataNode 用于数据 balance 的最大带宽，单位 byte / s。\n\n默认值：1048576（1M）\n\n<!-- more -->\n\n2. dfs.datanode.fsdataset.volume.choosing.policy\n\n从目录列表选择卷的策略的类名。默认是 org.apache.hadoop.hdfs.server.datanode.fsdataset.RoundRobinVolumeChoosingPolicy。如果考虑可用的磁盘空间，设置为“org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy”。\n\n> 2.7.0 版本文档中没有这个参数的解释，2.9.0 版本文档中有描述。Jira 链接：<https://issues.apache.org/jira/browse/HDFS-8356?jql=text%20~%20%22dfs.datanode.fsdataset.volume.choosing.policy%22>\n\n3. dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold\n\n只有当 dfs.datanode.fsdataset.volume.choosing.policy 设置为 org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy 时该参数才会被使用。这个设置控制 DN 卷之间的可用磁盘空间差异多少字节内认为它们是均衡的。如果所有卷可用空间的差异都在这个范围内，那么所有卷被认为是均衡的，并且块会以纯轮询的方式进行分配。\n\n默认值：10737418240（10G）\n\n4. dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction\n\n只有当 dfs.datanode.fsdataset.volume.choosing.policy 设置为 org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy 时该参数才会被使用。这个设置控制新块分配给有更多可用空间的卷的比率。这个设置应该在 0.0 - 1.0 之间，但是实践中应该设置为 0.5 - 1.0，因为没有原因应该让可用空间少的卷收到更多的块。 \n\n默认值：0.75f\n\n5. dfs.balancer.keytab.enabled\n\n设置为 true 为基于 Kerberos 认证的 Hadoop 启用使用 keytab 进行登陆。\n\n默认值：false\n\n6. dfs.balancer.address\n\n为基于 Kerberos 登陆的 keytab 设置 hostname。基于 keytab 登陆可以用 dfs.balancer.keytab.enabled 参数启用。\n\n默认值：0.0.0.0:0\n\n7. dfs.balancer.keytab.file\n\n作为 Balancer 服务登陆使用的 keytab 文件。用 dfs.balancer.kerberos.principal 配置服务主体的名称。基于 keytab 登陆可以用 dfs.balancer.keytab.enabled 参数启用。\n\n8. dfs.balancer.kerberos.principal\n\nBalancer proncipal。一般会设置为 balancer/\\_HOST@REALM.TLD。Balancer 启动时会用它自己的完全限定主机名替代 _HOST。\\_HOST 占位符允许在不同的服务器上使用相同的配置。基于 keytab 登陆可以用 dfs.balancer.keytab.enabled 参数启用。\n\n9. dfs.balancer.block-move.timeout\n\n移动一个块的最大时间毫秒数。如果这个参数设置大于 0，在这个时间后 Balancer 将停止等待一个块完成移动。在典型的集群中，3 到 5 分钟超时是合理的。如果块移动发生超时的概率比较大，这个参数需要提高。这可能是分派了太多的工作并且很多节点同时超过带宽限制的结果。这种情况，其他 Balancer 参数可能需要调整。默认该功能被禁用，参数值为 0。 \n\n默认值：0\n\n10. dfs.balancer.max-no-move-interval\n\n如果过了指定的时间，并且没有块从源 DataNode 移出，在当前 Balancer 循环中不会从这个 DataNode 移出块。\n\n默认值：60000\n\n### Balancer\n\n#### 用法\n\n    hdfs balancer\n          [-threshold <threshold>]\n          [-policy <policy>]\n          [-exclude [-f <hosts-file> | <comma-separated list of hosts>]]\n          [-include [-f <hosts-file> | <comma-separated list of hosts>]]\n          [-idleiterations <idleiterations>]\n\n#### 参数说明\n\n1. -policy <policy>：\n    datanode（默认）：如果每个 DataNode 是均衡的那么集群是均衡的。  \n    blockpool：如果每个 DataNode 上的 block pool 是均衡的那么集群是均衡的。\n2. -threshold <threshold>：\n    磁盘容量百分比。这个参数覆盖默认的阀值。\n3. -exclude -f <hosts-file> | <comma-separated list of hosts>：\n    从被 Balancer 均衡的节点中排除指定的 DataNode。\n4. -include -f <hosts-file> | <comma-separated list of hosts>：\n    Balancer 只均衡指定的 DataNode。\n5. -idleiterations <iterations>：\n    退出的最大空闲循环数。这个参数覆盖默认循环数（5）。\n\n#### 说明\n\n- 运行一个 Balancer 功能，管理员可以简单的按 Ctrl + C 停止均衡过程。参考 [Balancer](http://hadoop.apache.org/docs/r2.7.4/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html#Balancer) 获取更多详细说明。\n- blockpool 策略比 datanode 策略更严格。\n- 通过命令 hdfs dfsadmin -setBalancerBandwidth <bandwidth in bytes per second> 可以动态设置 Balance 的带宽。\n- 使用脚本 sbin/start-balancer.sh 将 balancer 进程放到后台运行；使用 sbin/stop-balancer.sh 停止后台进程。\n- 2.7.3 版本不支持按照磁盘进行 Balance，3.0 版本支持。 \n\n","source":"_posts/HDFS-Rebalance.md","raw":"title: HDFS Rebalance\ntags:\n  - Hadoop\n  - HDFS\ncategories:\n  - 大数据\n  - Hadoop\ndate: 2018-03-18 17:29:03\n---\n\n\n### 相关配置项\n\n1. dfs.datanode.balance.bandwidthPerSec\n\nDataNode 用于数据 balance 的最大带宽，单位 byte / s。\n\n默认值：1048576（1M）\n\n<!-- more -->\n\n2. dfs.datanode.fsdataset.volume.choosing.policy\n\n从目录列表选择卷的策略的类名。默认是 org.apache.hadoop.hdfs.server.datanode.fsdataset.RoundRobinVolumeChoosingPolicy。如果考虑可用的磁盘空间，设置为“org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy”。\n\n> 2.7.0 版本文档中没有这个参数的解释，2.9.0 版本文档中有描述。Jira 链接：<https://issues.apache.org/jira/browse/HDFS-8356?jql=text%20~%20%22dfs.datanode.fsdataset.volume.choosing.policy%22>\n\n3. dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold\n\n只有当 dfs.datanode.fsdataset.volume.choosing.policy 设置为 org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy 时该参数才会被使用。这个设置控制 DN 卷之间的可用磁盘空间差异多少字节内认为它们是均衡的。如果所有卷可用空间的差异都在这个范围内，那么所有卷被认为是均衡的，并且块会以纯轮询的方式进行分配。\n\n默认值：10737418240（10G）\n\n4. dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction\n\n只有当 dfs.datanode.fsdataset.volume.choosing.policy 设置为 org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy 时该参数才会被使用。这个设置控制新块分配给有更多可用空间的卷的比率。这个设置应该在 0.0 - 1.0 之间，但是实践中应该设置为 0.5 - 1.0，因为没有原因应该让可用空间少的卷收到更多的块。 \n\n默认值：0.75f\n\n5. dfs.balancer.keytab.enabled\n\n设置为 true 为基于 Kerberos 认证的 Hadoop 启用使用 keytab 进行登陆。\n\n默认值：false\n\n6. dfs.balancer.address\n\n为基于 Kerberos 登陆的 keytab 设置 hostname。基于 keytab 登陆可以用 dfs.balancer.keytab.enabled 参数启用。\n\n默认值：0.0.0.0:0\n\n7. dfs.balancer.keytab.file\n\n作为 Balancer 服务登陆使用的 keytab 文件。用 dfs.balancer.kerberos.principal 配置服务主体的名称。基于 keytab 登陆可以用 dfs.balancer.keytab.enabled 参数启用。\n\n8. dfs.balancer.kerberos.principal\n\nBalancer proncipal。一般会设置为 balancer/\\_HOST@REALM.TLD。Balancer 启动时会用它自己的完全限定主机名替代 _HOST。\\_HOST 占位符允许在不同的服务器上使用相同的配置。基于 keytab 登陆可以用 dfs.balancer.keytab.enabled 参数启用。\n\n9. dfs.balancer.block-move.timeout\n\n移动一个块的最大时间毫秒数。如果这个参数设置大于 0，在这个时间后 Balancer 将停止等待一个块完成移动。在典型的集群中，3 到 5 分钟超时是合理的。如果块移动发生超时的概率比较大，这个参数需要提高。这可能是分派了太多的工作并且很多节点同时超过带宽限制的结果。这种情况，其他 Balancer 参数可能需要调整。默认该功能被禁用，参数值为 0。 \n\n默认值：0\n\n10. dfs.balancer.max-no-move-interval\n\n如果过了指定的时间，并且没有块从源 DataNode 移出，在当前 Balancer 循环中不会从这个 DataNode 移出块。\n\n默认值：60000\n\n### Balancer\n\n#### 用法\n\n    hdfs balancer\n          [-threshold <threshold>]\n          [-policy <policy>]\n          [-exclude [-f <hosts-file> | <comma-separated list of hosts>]]\n          [-include [-f <hosts-file> | <comma-separated list of hosts>]]\n          [-idleiterations <idleiterations>]\n\n#### 参数说明\n\n1. -policy <policy>：\n    datanode（默认）：如果每个 DataNode 是均衡的那么集群是均衡的。  \n    blockpool：如果每个 DataNode 上的 block pool 是均衡的那么集群是均衡的。\n2. -threshold <threshold>：\n    磁盘容量百分比。这个参数覆盖默认的阀值。\n3. -exclude -f <hosts-file> | <comma-separated list of hosts>：\n    从被 Balancer 均衡的节点中排除指定的 DataNode。\n4. -include -f <hosts-file> | <comma-separated list of hosts>：\n    Balancer 只均衡指定的 DataNode。\n5. -idleiterations <iterations>：\n    退出的最大空闲循环数。这个参数覆盖默认循环数（5）。\n\n#### 说明\n\n- 运行一个 Balancer 功能，管理员可以简单的按 Ctrl + C 停止均衡过程。参考 [Balancer](http://hadoop.apache.org/docs/r2.7.4/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html#Balancer) 获取更多详细说明。\n- blockpool 策略比 datanode 策略更严格。\n- 通过命令 hdfs dfsadmin -setBalancerBandwidth <bandwidth in bytes per second> 可以动态设置 Balance 的带宽。\n- 使用脚本 sbin/start-balancer.sh 将 balancer 进程放到后台运行；使用 sbin/stop-balancer.sh 停止后台进程。\n- 2.7.3 版本不支持按照磁盘进行 Balance，3.0 版本支持。 \n\n","slug":"HDFS-Rebalance","published":1,"updated":"2021-07-19T16:28:00.248Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphm70020itd365zy6lvt","content":"<h3 id=\"相关配置项\"><a href=\"#相关配置项\" class=\"headerlink\" title=\"相关配置项\"></a>相关配置项</h3><ol>\n<li>dfs.datanode.balance.bandwidthPerSec</li>\n</ol>\n<p>DataNode 用于数据 balance 的最大带宽，单位 byte / s。</p>\n<p>默认值：1048576（1M）</p>\n<span id=\"more\"></span>\n\n<ol start=\"2\">\n<li>dfs.datanode.fsdataset.volume.choosing.policy</li>\n</ol>\n<p>从目录列表选择卷的策略的类名。默认是 org.apache.hadoop.hdfs.server.datanode.fsdataset.RoundRobinVolumeChoosingPolicy。如果考虑可用的磁盘空间，设置为“org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy”。</p>\n<blockquote>\n<p>2.7.0 版本文档中没有这个参数的解释，2.9.0 版本文档中有描述。Jira 链接：<a href=\"https://issues.apache.org/jira/browse/HDFS-8356?jql=text%20~%20%22dfs.datanode.fsdataset.volume.choosing.policy%22\">https://issues.apache.org/jira/browse/HDFS-8356?jql=text%20~%20%22dfs.datanode.fsdataset.volume.choosing.policy%22</a></p>\n</blockquote>\n<ol start=\"3\">\n<li>dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold</li>\n</ol>\n<p>只有当 dfs.datanode.fsdataset.volume.choosing.policy 设置为 org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy 时该参数才会被使用。这个设置控制 DN 卷之间的可用磁盘空间差异多少字节内认为它们是均衡的。如果所有卷可用空间的差异都在这个范围内，那么所有卷被认为是均衡的，并且块会以纯轮询的方式进行分配。</p>\n<p>默认值：10737418240（10G）</p>\n<ol start=\"4\">\n<li>dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction</li>\n</ol>\n<p>只有当 dfs.datanode.fsdataset.volume.choosing.policy 设置为 org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy 时该参数才会被使用。这个设置控制新块分配给有更多可用空间的卷的比率。这个设置应该在 0.0 - 1.0 之间，但是实践中应该设置为 0.5 - 1.0，因为没有原因应该让可用空间少的卷收到更多的块。 </p>\n<p>默认值：0.75f</p>\n<ol start=\"5\">\n<li>dfs.balancer.keytab.enabled</li>\n</ol>\n<p>设置为 true 为基于 Kerberos 认证的 Hadoop 启用使用 keytab 进行登陆。</p>\n<p>默认值：false</p>\n<ol start=\"6\">\n<li>dfs.balancer.address</li>\n</ol>\n<p>为基于 Kerberos 登陆的 keytab 设置 hostname。基于 keytab 登陆可以用 dfs.balancer.keytab.enabled 参数启用。</p>\n<p>默认值：0.0.0.0:0</p>\n<ol start=\"7\">\n<li>dfs.balancer.keytab.file</li>\n</ol>\n<p>作为 Balancer 服务登陆使用的 keytab 文件。用 dfs.balancer.kerberos.principal 配置服务主体的名称。基于 keytab 登陆可以用 dfs.balancer.keytab.enabled 参数启用。</p>\n<ol start=\"8\">\n<li>dfs.balancer.kerberos.principal</li>\n</ol>\n<p>Balancer proncipal。一般会设置为 balancer/_<a href=\"mailto:&#x48;&#x4f;&#83;&#x54;&#64;&#82;&#69;&#x41;&#x4c;&#x4d;&#46;&#84;&#76;&#x44;\">&#x48;&#x4f;&#83;&#x54;&#64;&#82;&#69;&#x41;&#x4c;&#x4d;&#46;&#84;&#76;&#x44;</a>。Balancer 启动时会用它自己的完全限定主机名替代 _HOST。_HOST 占位符允许在不同的服务器上使用相同的配置。基于 keytab 登陆可以用 dfs.balancer.keytab.enabled 参数启用。</p>\n<ol start=\"9\">\n<li>dfs.balancer.block-move.timeout</li>\n</ol>\n<p>移动一个块的最大时间毫秒数。如果这个参数设置大于 0，在这个时间后 Balancer 将停止等待一个块完成移动。在典型的集群中，3 到 5 分钟超时是合理的。如果块移动发生超时的概率比较大，这个参数需要提高。这可能是分派了太多的工作并且很多节点同时超过带宽限制的结果。这种情况，其他 Balancer 参数可能需要调整。默认该功能被禁用，参数值为 0。 </p>\n<p>默认值：0</p>\n<ol start=\"10\">\n<li>dfs.balancer.max-no-move-interval</li>\n</ol>\n<p>如果过了指定的时间，并且没有块从源 DataNode 移出，在当前 Balancer 循环中不会从这个 DataNode 移出块。</p>\n<p>默认值：60000</p>\n<h3 id=\"Balancer\"><a href=\"#Balancer\" class=\"headerlink\" title=\"Balancer\"></a>Balancer</h3><h4 id=\"用法\"><a href=\"#用法\" class=\"headerlink\" title=\"用法\"></a>用法</h4><pre><code>hdfs balancer\n      [-threshold &lt;threshold&gt;]\n      [-policy &lt;policy&gt;]\n      [-exclude [-f &lt;hosts-file&gt; | &lt;comma-separated list of hosts&gt;]]\n      [-include [-f &lt;hosts-file&gt; | &lt;comma-separated list of hosts&gt;]]\n      [-idleiterations &lt;idleiterations&gt;]\n</code></pre>\n<h4 id=\"参数说明\"><a href=\"#参数说明\" class=\"headerlink\" title=\"参数说明\"></a>参数说明</h4><ol>\n<li>-policy <policy>：<br> datanode（默认）：如果每个 DataNode 是均衡的那么集群是均衡的。<br> blockpool：如果每个 DataNode 上的 block pool 是均衡的那么集群是均衡的。</li>\n<li>-threshold <threshold>：<br> 磁盘容量百分比。这个参数覆盖默认的阀值。</li>\n<li>-exclude -f <hosts-file> | <comma-separated list of hosts>：<br> 从被 Balancer 均衡的节点中排除指定的 DataNode。</li>\n<li>-include -f <hosts-file> | <comma-separated list of hosts>：<br> Balancer 只均衡指定的 DataNode。</li>\n<li>-idleiterations <iterations>：<br> 退出的最大空闲循环数。这个参数覆盖默认循环数（5）。</li>\n</ol>\n<h4 id=\"说明\"><a href=\"#说明\" class=\"headerlink\" title=\"说明\"></a>说明</h4><ul>\n<li>运行一个 Balancer 功能，管理员可以简单的按 Ctrl + C 停止均衡过程。参考 <a href=\"http://hadoop.apache.org/docs/r2.7.4/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html#Balancer\">Balancer</a> 获取更多详细说明。</li>\n<li>blockpool 策略比 datanode 策略更严格。</li>\n<li>通过命令 hdfs dfsadmin -setBalancerBandwidth <bandwidth in bytes per second> 可以动态设置 Balance 的带宽。</li>\n<li>使用脚本 sbin/start-balancer.sh 将 balancer 进程放到后台运行；使用 sbin/stop-balancer.sh 停止后台进程。</li>\n<li>2.7.3 版本不支持按照磁盘进行 Balance，3.0 版本支持。 </li>\n</ul>\n","site":{"data":{}},"excerpt":"<h3 id=\"相关配置项\"><a href=\"#相关配置项\" class=\"headerlink\" title=\"相关配置项\"></a>相关配置项</h3><ol>\n<li>dfs.datanode.balance.bandwidthPerSec</li>\n</ol>\n<p>DataNode 用于数据 balance 的最大带宽，单位 byte / s。</p>\n<p>默认值：1048576（1M）</p>","more":"<ol start=\"2\">\n<li>dfs.datanode.fsdataset.volume.choosing.policy</li>\n</ol>\n<p>从目录列表选择卷的策略的类名。默认是 org.apache.hadoop.hdfs.server.datanode.fsdataset.RoundRobinVolumeChoosingPolicy。如果考虑可用的磁盘空间，设置为“org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy”。</p>\n<blockquote>\n<p>2.7.0 版本文档中没有这个参数的解释，2.9.0 版本文档中有描述。Jira 链接：<a href=\"https://issues.apache.org/jira/browse/HDFS-8356?jql=text%20~%20%22dfs.datanode.fsdataset.volume.choosing.policy%22\">https://issues.apache.org/jira/browse/HDFS-8356?jql=text%20~%20%22dfs.datanode.fsdataset.volume.choosing.policy%22</a></p>\n</blockquote>\n<ol start=\"3\">\n<li>dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold</li>\n</ol>\n<p>只有当 dfs.datanode.fsdataset.volume.choosing.policy 设置为 org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy 时该参数才会被使用。这个设置控制 DN 卷之间的可用磁盘空间差异多少字节内认为它们是均衡的。如果所有卷可用空间的差异都在这个范围内，那么所有卷被认为是均衡的，并且块会以纯轮询的方式进行分配。</p>\n<p>默认值：10737418240（10G）</p>\n<ol start=\"4\">\n<li>dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction</li>\n</ol>\n<p>只有当 dfs.datanode.fsdataset.volume.choosing.policy 设置为 org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy 时该参数才会被使用。这个设置控制新块分配给有更多可用空间的卷的比率。这个设置应该在 0.0 - 1.0 之间，但是实践中应该设置为 0.5 - 1.0，因为没有原因应该让可用空间少的卷收到更多的块。 </p>\n<p>默认值：0.75f</p>\n<ol start=\"5\">\n<li>dfs.balancer.keytab.enabled</li>\n</ol>\n<p>设置为 true 为基于 Kerberos 认证的 Hadoop 启用使用 keytab 进行登陆。</p>\n<p>默认值：false</p>\n<ol start=\"6\">\n<li>dfs.balancer.address</li>\n</ol>\n<p>为基于 Kerberos 登陆的 keytab 设置 hostname。基于 keytab 登陆可以用 dfs.balancer.keytab.enabled 参数启用。</p>\n<p>默认值：0.0.0.0:0</p>\n<ol start=\"7\">\n<li>dfs.balancer.keytab.file</li>\n</ol>\n<p>作为 Balancer 服务登陆使用的 keytab 文件。用 dfs.balancer.kerberos.principal 配置服务主体的名称。基于 keytab 登陆可以用 dfs.balancer.keytab.enabled 参数启用。</p>\n<ol start=\"8\">\n<li>dfs.balancer.kerberos.principal</li>\n</ol>\n<p>Balancer proncipal。一般会设置为 balancer/_<a href=\"mailto:&#x48;&#x4f;&#83;&#x54;&#64;&#82;&#69;&#x41;&#x4c;&#x4d;&#46;&#84;&#76;&#x44;\">&#x48;&#x4f;&#83;&#x54;&#64;&#82;&#69;&#x41;&#x4c;&#x4d;&#46;&#84;&#76;&#x44;</a>。Balancer 启动时会用它自己的完全限定主机名替代 _HOST。_HOST 占位符允许在不同的服务器上使用相同的配置。基于 keytab 登陆可以用 dfs.balancer.keytab.enabled 参数启用。</p>\n<ol start=\"9\">\n<li>dfs.balancer.block-move.timeout</li>\n</ol>\n<p>移动一个块的最大时间毫秒数。如果这个参数设置大于 0，在这个时间后 Balancer 将停止等待一个块完成移动。在典型的集群中，3 到 5 分钟超时是合理的。如果块移动发生超时的概率比较大，这个参数需要提高。这可能是分派了太多的工作并且很多节点同时超过带宽限制的结果。这种情况，其他 Balancer 参数可能需要调整。默认该功能被禁用，参数值为 0。 </p>\n<p>默认值：0</p>\n<ol start=\"10\">\n<li>dfs.balancer.max-no-move-interval</li>\n</ol>\n<p>如果过了指定的时间，并且没有块从源 DataNode 移出，在当前 Balancer 循环中不会从这个 DataNode 移出块。</p>\n<p>默认值：60000</p>\n<h3 id=\"Balancer\"><a href=\"#Balancer\" class=\"headerlink\" title=\"Balancer\"></a>Balancer</h3><h4 id=\"用法\"><a href=\"#用法\" class=\"headerlink\" title=\"用法\"></a>用法</h4><pre><code>hdfs balancer\n      [-threshold &lt;threshold&gt;]\n      [-policy &lt;policy&gt;]\n      [-exclude [-f &lt;hosts-file&gt; | &lt;comma-separated list of hosts&gt;]]\n      [-include [-f &lt;hosts-file&gt; | &lt;comma-separated list of hosts&gt;]]\n      [-idleiterations &lt;idleiterations&gt;]\n</code></pre>\n<h4 id=\"参数说明\"><a href=\"#参数说明\" class=\"headerlink\" title=\"参数说明\"></a>参数说明</h4><ol>\n<li>-policy <policy>：<br> datanode（默认）：如果每个 DataNode 是均衡的那么集群是均衡的。<br> blockpool：如果每个 DataNode 上的 block pool 是均衡的那么集群是均衡的。</li>\n<li>-threshold <threshold>：<br> 磁盘容量百分比。这个参数覆盖默认的阀值。</li>\n<li>-exclude -f <hosts-file> | <comma-separated list of hosts>：<br> 从被 Balancer 均衡的节点中排除指定的 DataNode。</li>\n<li>-include -f <hosts-file> | <comma-separated list of hosts>：<br> Balancer 只均衡指定的 DataNode。</li>\n<li>-idleiterations <iterations>：<br> 退出的最大空闲循环数。这个参数覆盖默认循环数（5）。</li>\n</ol>\n<h4 id=\"说明\"><a href=\"#说明\" class=\"headerlink\" title=\"说明\"></a>说明</h4><ul>\n<li>运行一个 Balancer 功能，管理员可以简单的按 Ctrl + C 停止均衡过程。参考 <a href=\"http://hadoop.apache.org/docs/r2.7.4/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html#Balancer\">Balancer</a> 获取更多详细说明。</li>\n<li>blockpool 策略比 datanode 策略更严格。</li>\n<li>通过命令 hdfs dfsadmin -setBalancerBandwidth <bandwidth in bytes per second> 可以动态设置 Balance 的带宽。</li>\n<li>使用脚本 sbin/start-balancer.sh 将 balancer 进程放到后台运行；使用 sbin/stop-balancer.sh 停止后台进程。</li>\n<li>2.7.3 版本不支持按照磁盘进行 Balance，3.0 版本支持。 </li>\n</ul>"},{"title":"HDFS Trash 特性","date":"2017-03-06T09:22:20.000Z","_content":"\n### HDFS Trash 特性\n\n如果启用 Trash 配置，通过 [FS Shell](http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/FileSystemShell.html#rm) 删除的文件不会立即从 HDFS 上移除。HDFS 将这些文件移动到一个回收站目录（每个用户在 /user/&lt;username&gt;/.Trash 下都有他自己的回收站目录）。只要这些文件还在回收站中，可以很快地进行恢复。\n\n<!-- more -->\n\n最近删除的文件移动到当前的回收站目录（/user/&lt;username&gt;/.Trash/Current），在一个可配置的时间间隔，HDFS 对当前回收站目录中的文件创建检查点（在 /user/&lt;username&gt;/.Trash/&lt;date&gt;下），并删除过期的检查点。参见关于回收站检查点的 [FS Shell  expunge 命令](http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/FileSystemShell.html#expunge)。\n\n当文件在回收站中过期后，NameNode 会从 HDFS 命名空间删除这些文件。一个文件的删除会引起跟这个文件相关联的块被释放。注意，在文件被用户删除的时间和 HDFS 中与之相关的可用空间增长时间之间存在一个可感知的时间延迟。\n\n### [expunge 命令](http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/FileSystemShell.html#expunge)\n\n用法：hadoop fs -expunge\n\n从回收站目录中永久删除比保留阀值旧的检查点中的文件，并创建新的检查点。\n\n当检查点被创建，当前被删除的在回收站中的文件被移动到该检查点下。比 fs.trash.checkpoint.interval 配置旧的检查点中的文件在下一次调用 -expunge 命令时将被永久的删除。\n\n如果文件系统支持这个特性，用户可以通过参数 fs.trash.checkpoint.interval（在 core-site.xml 中）来配置周期性创建和删除检查点。这个值应该比 fs.trash.interval 小或者相等。\n\n### fs.trash.interval 参数\n\n分钟数，当超过这个分钟数后检查点会被删除。如果为 0，Trash 特性被禁用。这个选项可以在服务器和客户端都设置。如果服务端 Trash 被禁用，那么会检查使用客户端配置。如果启用服务端 Trash，那么使用服务器配置的参数值，并忽略客户端配置的参数值。\n\n### fs.trash.checkpoint.interval 参数\n\n检查点之间的间隔分钟数。应该比 fs.trash.interval 小或者相等。如果为 0，该参数的值被设置为 fs.trash.interval 的参数值。每次检查点程序运行时，它会从当前回收站创建一个新的检查点，并移除超过 fs.trash.interval 配置参数分钟数以前的检查点。\n\n### 示例\n\n（1）创建两个目录：\n\n    $ hadoop fs -mkdir -p /user/hadoop/delete/test1\n    $ hadoop fs -mkdir -p /user/hadoop/delete/test2\n    $ hadoop fs -ls /user/hadoop/delete/\n    Found 2 items\n    drwxr-xr-x   - hadoop supergroup          0 2017-03-06 18:43 /user/hadoop/delete/test1\n    drwxr-xr-x   - hadoop supergroup          0 2017-03-06 18:46 /user/hadoop/delete/test2\n\n（2）删除目录 test1，提示信息显示目录被移动到回收站目录：\n\n    $ hadoop fs -rm -r /user/hadoop/delete/test1\n    17/03/06 18:49:02 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 4320 minutes, Emptier interval = 1440 minutes.\n    Moved: 'hdfs://frin-namenode1:9000/user/hadoop/delete/test1' to trash at: hdfs://frin-namenode1:9000/user/hadoop/.Trash/Current\n\n（3）使用 skipTrash 选项删除目录 test2，目录不会移动到回收站。它会从从 HDFS 被彻底删除。\n\n    $ hadoop fs -rm -r -skipTrash /user/hadoop/delete/test2\n    Deleted /user/hadoop/delete/test2\n\n（4）查看回收站目录可以看到只包含 test1 目录：\n\n    $ hadoop fs -ls /user/hadoop/.Trash/Current/user/hadoop/delete/\n    Found 1 items\n    drwxr-xr-x   - hadoop supergroup          0 2017-03-06 18:43 /user/hadoop/.Trash/Current/user/hadoop/delete/test1\n\n### 特别说明\n\n1. 回收站用的数据保存的理论最长时间约等于 fs.trash.interval 参数的 2 倍；\n\n2. 如果 NameNode 重启，会重新开始计时以确定检查点程序的执行时间。","source":"_posts/HDFS-Trash-特性.md","raw":"title: HDFS Trash 特性\ntags:\n  - Hadoop\n  - HDFS\ncategories:\n  - 大数据\n  - Hadoop\ndate: 2017-03-06 17:22:20\n---\n\n### HDFS Trash 特性\n\n如果启用 Trash 配置，通过 [FS Shell](http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/FileSystemShell.html#rm) 删除的文件不会立即从 HDFS 上移除。HDFS 将这些文件移动到一个回收站目录（每个用户在 /user/&lt;username&gt;/.Trash 下都有他自己的回收站目录）。只要这些文件还在回收站中，可以很快地进行恢复。\n\n<!-- more -->\n\n最近删除的文件移动到当前的回收站目录（/user/&lt;username&gt;/.Trash/Current），在一个可配置的时间间隔，HDFS 对当前回收站目录中的文件创建检查点（在 /user/&lt;username&gt;/.Trash/&lt;date&gt;下），并删除过期的检查点。参见关于回收站检查点的 [FS Shell  expunge 命令](http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/FileSystemShell.html#expunge)。\n\n当文件在回收站中过期后，NameNode 会从 HDFS 命名空间删除这些文件。一个文件的删除会引起跟这个文件相关联的块被释放。注意，在文件被用户删除的时间和 HDFS 中与之相关的可用空间增长时间之间存在一个可感知的时间延迟。\n\n### [expunge 命令](http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/FileSystemShell.html#expunge)\n\n用法：hadoop fs -expunge\n\n从回收站目录中永久删除比保留阀值旧的检查点中的文件，并创建新的检查点。\n\n当检查点被创建，当前被删除的在回收站中的文件被移动到该检查点下。比 fs.trash.checkpoint.interval 配置旧的检查点中的文件在下一次调用 -expunge 命令时将被永久的删除。\n\n如果文件系统支持这个特性，用户可以通过参数 fs.trash.checkpoint.interval（在 core-site.xml 中）来配置周期性创建和删除检查点。这个值应该比 fs.trash.interval 小或者相等。\n\n### fs.trash.interval 参数\n\n分钟数，当超过这个分钟数后检查点会被删除。如果为 0，Trash 特性被禁用。这个选项可以在服务器和客户端都设置。如果服务端 Trash 被禁用，那么会检查使用客户端配置。如果启用服务端 Trash，那么使用服务器配置的参数值，并忽略客户端配置的参数值。\n\n### fs.trash.checkpoint.interval 参数\n\n检查点之间的间隔分钟数。应该比 fs.trash.interval 小或者相等。如果为 0，该参数的值被设置为 fs.trash.interval 的参数值。每次检查点程序运行时，它会从当前回收站创建一个新的检查点，并移除超过 fs.trash.interval 配置参数分钟数以前的检查点。\n\n### 示例\n\n（1）创建两个目录：\n\n    $ hadoop fs -mkdir -p /user/hadoop/delete/test1\n    $ hadoop fs -mkdir -p /user/hadoop/delete/test2\n    $ hadoop fs -ls /user/hadoop/delete/\n    Found 2 items\n    drwxr-xr-x   - hadoop supergroup          0 2017-03-06 18:43 /user/hadoop/delete/test1\n    drwxr-xr-x   - hadoop supergroup          0 2017-03-06 18:46 /user/hadoop/delete/test2\n\n（2）删除目录 test1，提示信息显示目录被移动到回收站目录：\n\n    $ hadoop fs -rm -r /user/hadoop/delete/test1\n    17/03/06 18:49:02 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 4320 minutes, Emptier interval = 1440 minutes.\n    Moved: 'hdfs://frin-namenode1:9000/user/hadoop/delete/test1' to trash at: hdfs://frin-namenode1:9000/user/hadoop/.Trash/Current\n\n（3）使用 skipTrash 选项删除目录 test2，目录不会移动到回收站。它会从从 HDFS 被彻底删除。\n\n    $ hadoop fs -rm -r -skipTrash /user/hadoop/delete/test2\n    Deleted /user/hadoop/delete/test2\n\n（4）查看回收站目录可以看到只包含 test1 目录：\n\n    $ hadoop fs -ls /user/hadoop/.Trash/Current/user/hadoop/delete/\n    Found 1 items\n    drwxr-xr-x   - hadoop supergroup          0 2017-03-06 18:43 /user/hadoop/.Trash/Current/user/hadoop/delete/test1\n\n### 特别说明\n\n1. 回收站用的数据保存的理论最长时间约等于 fs.trash.interval 参数的 2 倍；\n\n2. 如果 NameNode 重启，会重新开始计时以确定检查点程序的执行时间。","slug":"HDFS-Trash-特性","published":1,"updated":"2021-07-19T16:28:00.248Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphm80023itd3471t8tsz","content":"<h3 id=\"HDFS-Trash-特性\"><a href=\"#HDFS-Trash-特性\" class=\"headerlink\" title=\"HDFS Trash 特性\"></a>HDFS Trash 特性</h3><p>如果启用 Trash 配置，通过 <a href=\"http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/FileSystemShell.html#rm\">FS Shell</a> 删除的文件不会立即从 HDFS 上移除。HDFS 将这些文件移动到一个回收站目录（每个用户在 /user/&lt;username&gt;/.Trash 下都有他自己的回收站目录）。只要这些文件还在回收站中，可以很快地进行恢复。</p>\n<span id=\"more\"></span>\n\n<p>最近删除的文件移动到当前的回收站目录（/user/&lt;username&gt;/.Trash/Current），在一个可配置的时间间隔，HDFS 对当前回收站目录中的文件创建检查点（在 /user/&lt;username&gt;/.Trash/&lt;date&gt;下），并删除过期的检查点。参见关于回收站检查点的 <a href=\"http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/FileSystemShell.html#expunge\">FS Shell  expunge 命令</a>。</p>\n<p>当文件在回收站中过期后，NameNode 会从 HDFS 命名空间删除这些文件。一个文件的删除会引起跟这个文件相关联的块被释放。注意，在文件被用户删除的时间和 HDFS 中与之相关的可用空间增长时间之间存在一个可感知的时间延迟。</p>\n<h3 id=\"expunge-命令\"><a href=\"#expunge-命令\" class=\"headerlink\" title=\"expunge 命令\"></a><a href=\"http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/FileSystemShell.html#expunge\">expunge 命令</a></h3><p>用法：hadoop fs -expunge</p>\n<p>从回收站目录中永久删除比保留阀值旧的检查点中的文件，并创建新的检查点。</p>\n<p>当检查点被创建，当前被删除的在回收站中的文件被移动到该检查点下。比 fs.trash.checkpoint.interval 配置旧的检查点中的文件在下一次调用 -expunge 命令时将被永久的删除。</p>\n<p>如果文件系统支持这个特性，用户可以通过参数 fs.trash.checkpoint.interval（在 core-site.xml 中）来配置周期性创建和删除检查点。这个值应该比 fs.trash.interval 小或者相等。</p>\n<h3 id=\"fs-trash-interval-参数\"><a href=\"#fs-trash-interval-参数\" class=\"headerlink\" title=\"fs.trash.interval 参数\"></a>fs.trash.interval 参数</h3><p>分钟数，当超过这个分钟数后检查点会被删除。如果为 0，Trash 特性被禁用。这个选项可以在服务器和客户端都设置。如果服务端 Trash 被禁用，那么会检查使用客户端配置。如果启用服务端 Trash，那么使用服务器配置的参数值，并忽略客户端配置的参数值。</p>\n<h3 id=\"fs-trash-checkpoint-interval-参数\"><a href=\"#fs-trash-checkpoint-interval-参数\" class=\"headerlink\" title=\"fs.trash.checkpoint.interval 参数\"></a>fs.trash.checkpoint.interval 参数</h3><p>检查点之间的间隔分钟数。应该比 fs.trash.interval 小或者相等。如果为 0，该参数的值被设置为 fs.trash.interval 的参数值。每次检查点程序运行时，它会从当前回收站创建一个新的检查点，并移除超过 fs.trash.interval 配置参数分钟数以前的检查点。</p>\n<h3 id=\"示例\"><a href=\"#示例\" class=\"headerlink\" title=\"示例\"></a>示例</h3><p>（1）创建两个目录：</p>\n<pre><code>$ hadoop fs -mkdir -p /user/hadoop/delete/test1\n$ hadoop fs -mkdir -p /user/hadoop/delete/test2\n$ hadoop fs -ls /user/hadoop/delete/\nFound 2 items\ndrwxr-xr-x   - hadoop supergroup          0 2017-03-06 18:43 /user/hadoop/delete/test1\ndrwxr-xr-x   - hadoop supergroup          0 2017-03-06 18:46 /user/hadoop/delete/test2\n</code></pre>\n<p>（2）删除目录 test1，提示信息显示目录被移动到回收站目录：</p>\n<pre><code>$ hadoop fs -rm -r /user/hadoop/delete/test1\n17/03/06 18:49:02 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 4320 minutes, Emptier interval = 1440 minutes.\nMoved: &#39;hdfs://frin-namenode1:9000/user/hadoop/delete/test1&#39; to trash at: hdfs://frin-namenode1:9000/user/hadoop/.Trash/Current\n</code></pre>\n<p>（3）使用 skipTrash 选项删除目录 test2，目录不会移动到回收站。它会从从 HDFS 被彻底删除。</p>\n<pre><code>$ hadoop fs -rm -r -skipTrash /user/hadoop/delete/test2\nDeleted /user/hadoop/delete/test2\n</code></pre>\n<p>（4）查看回收站目录可以看到只包含 test1 目录：</p>\n<pre><code>$ hadoop fs -ls /user/hadoop/.Trash/Current/user/hadoop/delete/\nFound 1 items\ndrwxr-xr-x   - hadoop supergroup          0 2017-03-06 18:43 /user/hadoop/.Trash/Current/user/hadoop/delete/test1\n</code></pre>\n<h3 id=\"特别说明\"><a href=\"#特别说明\" class=\"headerlink\" title=\"特别说明\"></a>特别说明</h3><ol>\n<li><p>回收站用的数据保存的理论最长时间约等于 fs.trash.interval 参数的 2 倍；</p>\n</li>\n<li><p>如果 NameNode 重启，会重新开始计时以确定检查点程序的执行时间。</p>\n</li>\n</ol>\n","site":{"data":{}},"excerpt":"<h3 id=\"HDFS-Trash-特性\"><a href=\"#HDFS-Trash-特性\" class=\"headerlink\" title=\"HDFS Trash 特性\"></a>HDFS Trash 特性</h3><p>如果启用 Trash 配置，通过 <a href=\"http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/FileSystemShell.html#rm\">FS Shell</a> 删除的文件不会立即从 HDFS 上移除。HDFS 将这些文件移动到一个回收站目录（每个用户在 /user/&lt;username&gt;/.Trash 下都有他自己的回收站目录）。只要这些文件还在回收站中，可以很快地进行恢复。</p>","more":"<p>最近删除的文件移动到当前的回收站目录（/user/&lt;username&gt;/.Trash/Current），在一个可配置的时间间隔，HDFS 对当前回收站目录中的文件创建检查点（在 /user/&lt;username&gt;/.Trash/&lt;date&gt;下），并删除过期的检查点。参见关于回收站检查点的 <a href=\"http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/FileSystemShell.html#expunge\">FS Shell  expunge 命令</a>。</p>\n<p>当文件在回收站中过期后，NameNode 会从 HDFS 命名空间删除这些文件。一个文件的删除会引起跟这个文件相关联的块被释放。注意，在文件被用户删除的时间和 HDFS 中与之相关的可用空间增长时间之间存在一个可感知的时间延迟。</p>\n<h3 id=\"expunge-命令\"><a href=\"#expunge-命令\" class=\"headerlink\" title=\"expunge 命令\"></a><a href=\"http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/FileSystemShell.html#expunge\">expunge 命令</a></h3><p>用法：hadoop fs -expunge</p>\n<p>从回收站目录中永久删除比保留阀值旧的检查点中的文件，并创建新的检查点。</p>\n<p>当检查点被创建，当前被删除的在回收站中的文件被移动到该检查点下。比 fs.trash.checkpoint.interval 配置旧的检查点中的文件在下一次调用 -expunge 命令时将被永久的删除。</p>\n<p>如果文件系统支持这个特性，用户可以通过参数 fs.trash.checkpoint.interval（在 core-site.xml 中）来配置周期性创建和删除检查点。这个值应该比 fs.trash.interval 小或者相等。</p>\n<h3 id=\"fs-trash-interval-参数\"><a href=\"#fs-trash-interval-参数\" class=\"headerlink\" title=\"fs.trash.interval 参数\"></a>fs.trash.interval 参数</h3><p>分钟数，当超过这个分钟数后检查点会被删除。如果为 0，Trash 特性被禁用。这个选项可以在服务器和客户端都设置。如果服务端 Trash 被禁用，那么会检查使用客户端配置。如果启用服务端 Trash，那么使用服务器配置的参数值，并忽略客户端配置的参数值。</p>\n<h3 id=\"fs-trash-checkpoint-interval-参数\"><a href=\"#fs-trash-checkpoint-interval-参数\" class=\"headerlink\" title=\"fs.trash.checkpoint.interval 参数\"></a>fs.trash.checkpoint.interval 参数</h3><p>检查点之间的间隔分钟数。应该比 fs.trash.interval 小或者相等。如果为 0，该参数的值被设置为 fs.trash.interval 的参数值。每次检查点程序运行时，它会从当前回收站创建一个新的检查点，并移除超过 fs.trash.interval 配置参数分钟数以前的检查点。</p>\n<h3 id=\"示例\"><a href=\"#示例\" class=\"headerlink\" title=\"示例\"></a>示例</h3><p>（1）创建两个目录：</p>\n<pre><code>$ hadoop fs -mkdir -p /user/hadoop/delete/test1\n$ hadoop fs -mkdir -p /user/hadoop/delete/test2\n$ hadoop fs -ls /user/hadoop/delete/\nFound 2 items\ndrwxr-xr-x   - hadoop supergroup          0 2017-03-06 18:43 /user/hadoop/delete/test1\ndrwxr-xr-x   - hadoop supergroup          0 2017-03-06 18:46 /user/hadoop/delete/test2\n</code></pre>\n<p>（2）删除目录 test1，提示信息显示目录被移动到回收站目录：</p>\n<pre><code>$ hadoop fs -rm -r /user/hadoop/delete/test1\n17/03/06 18:49:02 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 4320 minutes, Emptier interval = 1440 minutes.\nMoved: &#39;hdfs://frin-namenode1:9000/user/hadoop/delete/test1&#39; to trash at: hdfs://frin-namenode1:9000/user/hadoop/.Trash/Current\n</code></pre>\n<p>（3）使用 skipTrash 选项删除目录 test2，目录不会移动到回收站。它会从从 HDFS 被彻底删除。</p>\n<pre><code>$ hadoop fs -rm -r -skipTrash /user/hadoop/delete/test2\nDeleted /user/hadoop/delete/test2\n</code></pre>\n<p>（4）查看回收站目录可以看到只包含 test1 目录：</p>\n<pre><code>$ hadoop fs -ls /user/hadoop/.Trash/Current/user/hadoop/delete/\nFound 1 items\ndrwxr-xr-x   - hadoop supergroup          0 2017-03-06 18:43 /user/hadoop/.Trash/Current/user/hadoop/delete/test1\n</code></pre>\n<h3 id=\"特别说明\"><a href=\"#特别说明\" class=\"headerlink\" title=\"特别说明\"></a>特别说明</h3><ol>\n<li><p>回收站用的数据保存的理论最长时间约等于 fs.trash.interval 参数的 2 倍；</p>\n</li>\n<li><p>如果 NameNode 重启，会重新开始计时以确定检查点程序的执行时间。</p>\n</li>\n</ol>"},{"title":"HDFS getmerge: Operation not permitted","date":"2017-05-31T13:16:50.000Z","_content":"\n\n在 hadoop fs -getmerge 时报了不被允许的错误。检查发现是因为权限条件不满足。getmerge 操作会对应数据文件在本地目录下生成一个隐藏的以 crc 结尾的文件。我遇到的场景是，用户 A 首先执行了以下命令：\n\n    hadoop fs -getmerge /data/log_001.txt /data/logs/log_001.txt\n    \n此时，在本地目录 /data/logs/ 下会对应生成以下文件：\n\n    -rw-r--r-- 1 userA data-platform    0 May 31 21:00 log_001.txt\n    -rw-r--r-- 1 userA data-platform   16 May 31 20:56 .log_001.txt.crc\n    \n然而，当用户 B 再执行相同的命令时就会报 Operation not permitted 错误。原因是用户 B 没有删除 .log_001.txt.crc 文件的权限。\n","source":"_posts/HDFS-getmerge-Operation-not-permitted.md","raw":"title: 'HDFS getmerge: Operation not permitted'\ntags:\n  - Hadoop\n  - HDFS\ncategories:\n  - 大数据\n  - Hadoop\ndate: 2017-05-31 21:16:50\n---\n\n\n在 hadoop fs -getmerge 时报了不被允许的错误。检查发现是因为权限条件不满足。getmerge 操作会对应数据文件在本地目录下生成一个隐藏的以 crc 结尾的文件。我遇到的场景是，用户 A 首先执行了以下命令：\n\n    hadoop fs -getmerge /data/log_001.txt /data/logs/log_001.txt\n    \n此时，在本地目录 /data/logs/ 下会对应生成以下文件：\n\n    -rw-r--r-- 1 userA data-platform    0 May 31 21:00 log_001.txt\n    -rw-r--r-- 1 userA data-platform   16 May 31 20:56 .log_001.txt.crc\n    \n然而，当用户 B 再执行相同的命令时就会报 Operation not permitted 错误。原因是用户 B 没有删除 .log_001.txt.crc 文件的权限。\n","slug":"HDFS-getmerge-Operation-not-permitted","published":1,"updated":"2021-07-19T16:28:00.248Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphm90026itd3b8xa6ypq","content":"<p>在 hadoop fs -getmerge 时报了不被允许的错误。检查发现是因为权限条件不满足。getmerge 操作会对应数据文件在本地目录下生成一个隐藏的以 crc 结尾的文件。我遇到的场景是，用户 A 首先执行了以下命令：</p>\n<pre><code>hadoop fs -getmerge /data/log_001.txt /data/logs/log_001.txt\n</code></pre>\n<p>此时，在本地目录 /data/logs/ 下会对应生成以下文件：</p>\n<pre><code>-rw-r--r-- 1 userA data-platform    0 May 31 21:00 log_001.txt\n-rw-r--r-- 1 userA data-platform   16 May 31 20:56 .log_001.txt.crc\n</code></pre>\n<p>然而，当用户 B 再执行相同的命令时就会报 Operation not permitted 错误。原因是用户 B 没有删除 .log_001.txt.crc 文件的权限。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>在 hadoop fs -getmerge 时报了不被允许的错误。检查发现是因为权限条件不满足。getmerge 操作会对应数据文件在本地目录下生成一个隐藏的以 crc 结尾的文件。我遇到的场景是，用户 A 首先执行了以下命令：</p>\n<pre><code>hadoop fs -getmerge /data/log_001.txt /data/logs/log_001.txt\n</code></pre>\n<p>此时，在本地目录 /data/logs/ 下会对应生成以下文件：</p>\n<pre><code>-rw-r--r-- 1 userA data-platform    0 May 31 21:00 log_001.txt\n-rw-r--r-- 1 userA data-platform   16 May 31 20:56 .log_001.txt.crc\n</code></pre>\n<p>然而，当用户 B 再执行相同的命令时就会报 Operation not permitted 错误。原因是用户 B 没有删除 .log_001.txt.crc 文件的权限。</p>\n"},{"title":"HDFS ls 操作 OOM问题解决","date":"2018-03-19T15:37:24.000Z","_content":"\n\n如果一个目录下文件数量特别多（如 100W+），在执行 ls 操作时会出现 OOM 的错误，如下：\n\n<!-- more -->\n\n    Exception in thread \"IPC Client connection to namenode/11.11.11.111:1111\" java.lang.OutOfMemoryError: GC overhead limit exceeded\n    at java.util.regex.Pattern.compile(Pattern.java:846)\n    at java.lang.String.replace(String.java:2208)\n    at org.apache.hadoop.fs.Path.normalizePath(Path.java:147)\n    at org.apache.hadoop.fs.Path.initialize(Path.java:137)\n    at org.apache.hadoop.fs.Path.<init>(Path.java:126)\n    at org.apache.hadoop.dfs.DFSFileInfo.readFields(DFSFileInfo.java:141)\n    at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:230)\n    at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:166)\n    at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:214)\n    at org.apache.hadoop.io.ObjectWritable.readFields(ObjectWritable.java:61)\n    at org.apache.hadoop.ipc.Client$Connection.run(Client.java:273)\n    Exception in thread \"main\" java.lang.OutOfMemoryError: GC overhead limit exceeded\n    at java.util.Arrays.copyOfRange(Arrays.java:3209)\n    at java.lang.String.<init>(String.java:216)\n    at java.lang.StringBuffer.toString(StringBuffer.java:585)\n    at java.net.URI.toString(URI.java:1907)\n    at java.net.URI.<init>(URI.java:732)\n    at org.apache.hadoop.fs.Path.initialize(Path.java:137)\n    at org.apache.hadoop.fs.Path.<init>(Path.java:126)\n    at org.apache.hadoop.fs.Path.makeQualified(Path.java:296)\n    at org.apache.hadoop.dfs.DfsPath.<init>(DfsPath.java:35)\n    at org.apache.hadoop.dfs.DistributedFileSystem.listPaths(DistributedFileSystem.java:181)\n    at org.apache.hadoop.fs.FsShell.ls(FsShell.java:405)\n    at org.apache.hadoop.fs.FsShell.ls(FsShell.java:423)\n    at org.apache.hadoop.fs.FsShell.ls(FsShell.java:423)\n    at org.apache.hadoop.fs.FsShell.ls(FsShell.java:423)\n    at org.apache.hadoop.fs.FsShell.ls(FsShell.java:399)\n    at org.apache.hadoop.fs.FsShell.doall(FsShell.java:1054)\n    at org.apache.hadoop.fs.FsShell.run(FsShell.java:1244)\n    at org.apache.hadoop.util.ToolBase.doMain(ToolBase.java:187)\n    at org.apache.hadoop.fs.FsShell.main(FsShell.java:1333)\n\n这是因为 HDFS Client 进程的 heapsize  默认为 1G，如果目录下文件太多就会出现 OOM。这并不是一个 Bug，解决这个问题只需要将 HDFS Client 进程的 heapsize 设置大一些就可以。设置方法如下：\n\n    $ export HADOOP_CLIENT_OPTS=\"-XX:-UseGCOverheadLimit -Xmx4096m\"\n    $ hadoop fs -ls /path/\n","source":"_posts/HDFS-ls-操作-OOM问题解决.md","raw":"title: HDFS ls 操作 OOM问题解决\ntags:\n  - Hadoop\n  - HDFS\ncategories:\n  - 大数据\n  - Hadoop\ndate: 2018-03-19 23:37:24\n---\n\n\n如果一个目录下文件数量特别多（如 100W+），在执行 ls 操作时会出现 OOM 的错误，如下：\n\n<!-- more -->\n\n    Exception in thread \"IPC Client connection to namenode/11.11.11.111:1111\" java.lang.OutOfMemoryError: GC overhead limit exceeded\n    at java.util.regex.Pattern.compile(Pattern.java:846)\n    at java.lang.String.replace(String.java:2208)\n    at org.apache.hadoop.fs.Path.normalizePath(Path.java:147)\n    at org.apache.hadoop.fs.Path.initialize(Path.java:137)\n    at org.apache.hadoop.fs.Path.<init>(Path.java:126)\n    at org.apache.hadoop.dfs.DFSFileInfo.readFields(DFSFileInfo.java:141)\n    at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:230)\n    at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:166)\n    at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:214)\n    at org.apache.hadoop.io.ObjectWritable.readFields(ObjectWritable.java:61)\n    at org.apache.hadoop.ipc.Client$Connection.run(Client.java:273)\n    Exception in thread \"main\" java.lang.OutOfMemoryError: GC overhead limit exceeded\n    at java.util.Arrays.copyOfRange(Arrays.java:3209)\n    at java.lang.String.<init>(String.java:216)\n    at java.lang.StringBuffer.toString(StringBuffer.java:585)\n    at java.net.URI.toString(URI.java:1907)\n    at java.net.URI.<init>(URI.java:732)\n    at org.apache.hadoop.fs.Path.initialize(Path.java:137)\n    at org.apache.hadoop.fs.Path.<init>(Path.java:126)\n    at org.apache.hadoop.fs.Path.makeQualified(Path.java:296)\n    at org.apache.hadoop.dfs.DfsPath.<init>(DfsPath.java:35)\n    at org.apache.hadoop.dfs.DistributedFileSystem.listPaths(DistributedFileSystem.java:181)\n    at org.apache.hadoop.fs.FsShell.ls(FsShell.java:405)\n    at org.apache.hadoop.fs.FsShell.ls(FsShell.java:423)\n    at org.apache.hadoop.fs.FsShell.ls(FsShell.java:423)\n    at org.apache.hadoop.fs.FsShell.ls(FsShell.java:423)\n    at org.apache.hadoop.fs.FsShell.ls(FsShell.java:399)\n    at org.apache.hadoop.fs.FsShell.doall(FsShell.java:1054)\n    at org.apache.hadoop.fs.FsShell.run(FsShell.java:1244)\n    at org.apache.hadoop.util.ToolBase.doMain(ToolBase.java:187)\n    at org.apache.hadoop.fs.FsShell.main(FsShell.java:1333)\n\n这是因为 HDFS Client 进程的 heapsize  默认为 1G，如果目录下文件太多就会出现 OOM。这并不是一个 Bug，解决这个问题只需要将 HDFS Client 进程的 heapsize 设置大一些就可以。设置方法如下：\n\n    $ export HADOOP_CLIENT_OPTS=\"-XX:-UseGCOverheadLimit -Xmx4096m\"\n    $ hadoop fs -ls /path/\n","slug":"HDFS-ls-操作-OOM问题解决","published":1,"updated":"2021-07-19T16:28:00.248Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphma0028itd3ek65drxp","content":"<p>如果一个目录下文件数量特别多（如 100W+），在执行 ls 操作时会出现 OOM 的错误，如下：</p>\n<span id=\"more\"></span>\n\n<pre><code>Exception in thread &quot;IPC Client connection to namenode/11.11.11.111:1111&quot; java.lang.OutOfMemoryError: GC overhead limit exceeded\nat java.util.regex.Pattern.compile(Pattern.java:846)\nat java.lang.String.replace(String.java:2208)\nat org.apache.hadoop.fs.Path.normalizePath(Path.java:147)\nat org.apache.hadoop.fs.Path.initialize(Path.java:137)\nat org.apache.hadoop.fs.Path.&lt;init&gt;(Path.java:126)\nat org.apache.hadoop.dfs.DFSFileInfo.readFields(DFSFileInfo.java:141)\nat org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:230)\nat org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:166)\nat org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:214)\nat org.apache.hadoop.io.ObjectWritable.readFields(ObjectWritable.java:61)\nat org.apache.hadoop.ipc.Client$Connection.run(Client.java:273)\nException in thread &quot;main&quot; java.lang.OutOfMemoryError: GC overhead limit exceeded\nat java.util.Arrays.copyOfRange(Arrays.java:3209)\nat java.lang.String.&lt;init&gt;(String.java:216)\nat java.lang.StringBuffer.toString(StringBuffer.java:585)\nat java.net.URI.toString(URI.java:1907)\nat java.net.URI.&lt;init&gt;(URI.java:732)\nat org.apache.hadoop.fs.Path.initialize(Path.java:137)\nat org.apache.hadoop.fs.Path.&lt;init&gt;(Path.java:126)\nat org.apache.hadoop.fs.Path.makeQualified(Path.java:296)\nat org.apache.hadoop.dfs.DfsPath.&lt;init&gt;(DfsPath.java:35)\nat org.apache.hadoop.dfs.DistributedFileSystem.listPaths(DistributedFileSystem.java:181)\nat org.apache.hadoop.fs.FsShell.ls(FsShell.java:405)\nat org.apache.hadoop.fs.FsShell.ls(FsShell.java:423)\nat org.apache.hadoop.fs.FsShell.ls(FsShell.java:423)\nat org.apache.hadoop.fs.FsShell.ls(FsShell.java:423)\nat org.apache.hadoop.fs.FsShell.ls(FsShell.java:399)\nat org.apache.hadoop.fs.FsShell.doall(FsShell.java:1054)\nat org.apache.hadoop.fs.FsShell.run(FsShell.java:1244)\nat org.apache.hadoop.util.ToolBase.doMain(ToolBase.java:187)\nat org.apache.hadoop.fs.FsShell.main(FsShell.java:1333)\n</code></pre>\n<p>这是因为 HDFS Client 进程的 heapsize  默认为 1G，如果目录下文件太多就会出现 OOM。这并不是一个 Bug，解决这个问题只需要将 HDFS Client 进程的 heapsize 设置大一些就可以。设置方法如下：</p>\n<pre><code>$ export HADOOP_CLIENT_OPTS=&quot;-XX:-UseGCOverheadLimit -Xmx4096m&quot;\n$ hadoop fs -ls /path/\n</code></pre>\n","site":{"data":{}},"excerpt":"<p>如果一个目录下文件数量特别多（如 100W+），在执行 ls 操作时会出现 OOM 的错误，如下：</p>","more":"<pre><code>Exception in thread &quot;IPC Client connection to namenode/11.11.11.111:1111&quot; java.lang.OutOfMemoryError: GC overhead limit exceeded\nat java.util.regex.Pattern.compile(Pattern.java:846)\nat java.lang.String.replace(String.java:2208)\nat org.apache.hadoop.fs.Path.normalizePath(Path.java:147)\nat org.apache.hadoop.fs.Path.initialize(Path.java:137)\nat org.apache.hadoop.fs.Path.&lt;init&gt;(Path.java:126)\nat org.apache.hadoop.dfs.DFSFileInfo.readFields(DFSFileInfo.java:141)\nat org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:230)\nat org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:166)\nat org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:214)\nat org.apache.hadoop.io.ObjectWritable.readFields(ObjectWritable.java:61)\nat org.apache.hadoop.ipc.Client$Connection.run(Client.java:273)\nException in thread &quot;main&quot; java.lang.OutOfMemoryError: GC overhead limit exceeded\nat java.util.Arrays.copyOfRange(Arrays.java:3209)\nat java.lang.String.&lt;init&gt;(String.java:216)\nat java.lang.StringBuffer.toString(StringBuffer.java:585)\nat java.net.URI.toString(URI.java:1907)\nat java.net.URI.&lt;init&gt;(URI.java:732)\nat org.apache.hadoop.fs.Path.initialize(Path.java:137)\nat org.apache.hadoop.fs.Path.&lt;init&gt;(Path.java:126)\nat org.apache.hadoop.fs.Path.makeQualified(Path.java:296)\nat org.apache.hadoop.dfs.DfsPath.&lt;init&gt;(DfsPath.java:35)\nat org.apache.hadoop.dfs.DistributedFileSystem.listPaths(DistributedFileSystem.java:181)\nat org.apache.hadoop.fs.FsShell.ls(FsShell.java:405)\nat org.apache.hadoop.fs.FsShell.ls(FsShell.java:423)\nat org.apache.hadoop.fs.FsShell.ls(FsShell.java:423)\nat org.apache.hadoop.fs.FsShell.ls(FsShell.java:423)\nat org.apache.hadoop.fs.FsShell.ls(FsShell.java:399)\nat org.apache.hadoop.fs.FsShell.doall(FsShell.java:1054)\nat org.apache.hadoop.fs.FsShell.run(FsShell.java:1244)\nat org.apache.hadoop.util.ToolBase.doMain(ToolBase.java:187)\nat org.apache.hadoop.fs.FsShell.main(FsShell.java:1333)\n</code></pre>\n<p>这是因为 HDFS Client 进程的 heapsize  默认为 1G，如果目录下文件太多就会出现 OOM。这并不是一个 Bug，解决这个问题只需要将 HDFS Client 进程的 heapsize 设置大一些就可以。设置方法如下：</p>\n<pre><code>$ export HADOOP_CLIENT_OPTS=&quot;-XX:-UseGCOverheadLimit -Xmx4096m&quot;\n$ hadoop fs -ls /path/\n</code></pre>"},{"title":"HDFS 快照","date":"2017-04-12T10:07:27.000Z","_content":"\n### 概述\n\nHDFS 快照是只读的某个时间点的文件系统拷贝。可以在一个子树或者整个文件系统上执行快照。一般使用快照的场景有数据备份、防止用户错误及灾难恢复。\n\n<!-- more -->\n\nHDFS 快照的实现是很有效的：\n\n- 快照的创建是瞬间的：代价是 O(1)，除了索引点（inode）查找时间。\n- 只有发生跟快照相关的修改时才需要额外的内存：内存使用是 O(M)，M 是修改的文件/目录的数目。\n- DataNode 上的块不会拷贝：快照文件记录块列表及文件的大小。不会拷贝数据。\n- 快照不会对常规的 HDFS 操作有不利的影响：修改是以时间倒序排列的，因此当前的数据可以直接访问。快照数据是通过从当前数据减去修改来计算的。\n\n#### 快照表目录\n\n快照表可以在任何目录上创建，只要目录被设置为快照表。一个快照表目录可以同时容纳 65536 个快照。快照表目录的数量没有限制。管理员可以设置任何目录为快照表。如果一个快照表目录下存在快照，那么这个目录在所有的快照被删除前不能被删除也不能重命名。嵌套的快照表目录当前是不允许的。就是说，如果一个目录的一个上级目录/下级目录是快照表目录，那么这个目录不能被设置为快照表。\n\n#### 快照路径\n\n对于一个快照表目录，路径组件“.snapshot”用来访问它的快照。假设 /foo 是一个快照表目录，/foo/bar 是 /foo 中的一个文件/目录，/foo 有一个快照 s0。那么，路径\n\n    /foo/.snapshot/s0/bar\n\n指向 /foo/bar 的快照拷贝。普通 API 和 CLI 可以用“.snapshot”路径工作。下面是一些例子。\n\n- 列出一个快照表目录下所有的快照：\n\n    hdfs dfs -ls /foo/.snapshot\n\n- 列出快照 s0 中的文件：\n\n    hdfs dfs -ls /foo/.snapshot/s0\n\n- 从快照 s0 中拷贝一个文件：\n\n    hdfs dfs -cp -ptopax /foo/.snapshot/s0/bar /tmp\n\n注意，这个例子使用了保持选项保持时间戳、属主关系、权限、ACL 和 XAttr。\n\n### 用快照升级 HDFS 到某个版本\n\nHDFS 快照特性引入了一个新的保留路径名用来与快照交互：.snapshot。当从 HDFS 的一个旧版本升级时，存在的 .snapshot 路径名称需要先重命名或删除以避免与保留路径冲突。参见 [HDFS 用户指南](http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html#Upgrade_and_Rollback)升级一节获取更多信息。\n\n#### 快照选项\n\n##### 管理员选项\n\n这一节描述的操作需要超级用户权限。\n\n###### 允许快照\n\n允许创建一个目录的快照。如果操作成功完成，这个目录变为快照表。\n\n- 命令：\n\n    hdfs dfsadmin -allowSnapshot &lt;path&gt;\n\n- 参数：\n\npath：快照表目录的路径。\n\n也可以参见 HdfsAdmin 中对应的 Java API void allowSnapshot(Path path)。\n\n###### 禁止快照\n\n禁止一个目录创建快照。在禁止前这个目录的所有快照必须删除。\n\n- 命令：\n\n    hdfs dfsadmin -disallowSnapshot &lt;path&gt;\n\n- 参数：\n\npath：快照表目录的路径。\n\n也可以参见 HdfsAdmin 中对应的 Java API void disallowSnapshot(Path path)。\n\n##### 用户操作\n\n这一节描述用户操作。注意，HDFS 超级用户可以执行所有操作而不需要各个操作中要求的权限需求。\n\n###### 创建快照\n\n创建一个快照表目录的快照。这个操作需要快照表目录属主的权限。\n\n- 命令：\n\n    hdfs dfs -createSnapshot &lt;path&gt; [&lt;snapshotName&gt;]\n\n- 参数：\n\npath：快照表目录的路径。\n\nsnapshotName：快照名字，这是个可选参数。当省略这个参数时，会用时间戳以格式“'s'yyyyMMdd-HHmmss.SSS”生成默认名称，例如：s20130412-151029.033。\n\n也可以参见 [FileSystem](http://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/FileSystem.html) 中对应的 Java API Path createSnapshot(Path path) 和 Path createSnapshot(Path path, String snapshotName)。这些方法返回快照的路径。\n\n###### 删除快照\n\n从一个快照表目录中删除快照。这个操作需要快照表目录属主的权限。\n\n- 命令：\n\n    hdfs dfs -deleteSnapshot &lt;path&gt; &lt;snapshotName&gt;\n\n- 参数：\n\npath：快照表目录的路径。\n\nsnapshotName：快照名称。\n\n也可以参见 [FileSystem](http://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/FileSystem.html) 中对应的 Java API  void deleteSnapshot(Path path, String snapshotName)。\n\n###### 重命名快照\n\n重命名一个快照。这个操作需要快照表目录属主的权限。\n\n- 命令：\n\n    hdfs dfs -renameSnapshot &lt;path&gt; &lt;oldName&gt; &lt;newName&gt;\n\n- 参数：\n\npath：快照表目录的路径。\n\noldName：原快照名称。\n\nnewName：新快照名称。\n\n也可以参见 [FileSystem](http://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/FileSystem.html) 中对应的 Java API  void renameSnapshot(Path path, String oldName, String newName)。\n\n###### 获取快照表目录列表\n\n获取当前用户有权限的所有快照表目录。\n\n- 命令：\n\n    hdfs lsSnapshottableDir\n\n- 参数：无\n\n也可以参见 DistributedFileSystem 中对应的 Java API SnapshottableDirectoryStatus[] getSnapshottableDirectoryListing()。\n\n###### 获取快照差异报告\n\n获取两个快照间的差异。这个操作需要两个快照中所有文件/目录读访问权限。\n\n- 命令：\n\n    hdfs snapshotDiff &lt;path&gt; &lt;fromSnapshot&gt; &lt;toSnapshot&gt;\n\n- 参数：\n\npath：快照表目录的路径。\n\nfromSnapshot：开始快照的名称。\n\ntoSnapshot：结束快照的名称。\n\n- 结果：\n\n&#43;：文件/目录被创建。  \n&minus;：文件/目录被删除。  \nM：文件/目录被修改。\nR：文件/目录被重命名。\n\n重命名实体表明一个文件/目录被重命名但依然在同一个快照表目录中。如果一个文件/目录被重命名到快照表目录的外部，那么这个文件/目录报告为被删除。一个文件/目录从外部重命名到快照表中会报告为新创建。\n\n快照差异报告不保证相同的操作序列。例如，重命名目录“/foo”为“/foo2”，然后向文件“/foo2/bar”追加数据，差异报告将是：\n\n    R. /foo -> /foo2\n    M. /foo/bar\n\n换言之，被重命名目录下的文件/目录的修改会用被重名前原来的路径报告（上面例子中的“/foo/bar”）。\n\n也可以参见 DistributedFileSystem 中对应的 Java API SnapshotDiffReport getSnapshotDiffReport(Path path, String fromSnapshot, String toSnapshot)。","source":"_posts/HDFS-快照.md","raw":"title: HDFS 快照\ntags:\n  - Hadoop\n  - HDFS\ncategories:\n  - 大数据\n  - Hadoop\ndate: 2017-04-12 18:07:27\n---\n\n### 概述\n\nHDFS 快照是只读的某个时间点的文件系统拷贝。可以在一个子树或者整个文件系统上执行快照。一般使用快照的场景有数据备份、防止用户错误及灾难恢复。\n\n<!-- more -->\n\nHDFS 快照的实现是很有效的：\n\n- 快照的创建是瞬间的：代价是 O(1)，除了索引点（inode）查找时间。\n- 只有发生跟快照相关的修改时才需要额外的内存：内存使用是 O(M)，M 是修改的文件/目录的数目。\n- DataNode 上的块不会拷贝：快照文件记录块列表及文件的大小。不会拷贝数据。\n- 快照不会对常规的 HDFS 操作有不利的影响：修改是以时间倒序排列的，因此当前的数据可以直接访问。快照数据是通过从当前数据减去修改来计算的。\n\n#### 快照表目录\n\n快照表可以在任何目录上创建，只要目录被设置为快照表。一个快照表目录可以同时容纳 65536 个快照。快照表目录的数量没有限制。管理员可以设置任何目录为快照表。如果一个快照表目录下存在快照，那么这个目录在所有的快照被删除前不能被删除也不能重命名。嵌套的快照表目录当前是不允许的。就是说，如果一个目录的一个上级目录/下级目录是快照表目录，那么这个目录不能被设置为快照表。\n\n#### 快照路径\n\n对于一个快照表目录，路径组件“.snapshot”用来访问它的快照。假设 /foo 是一个快照表目录，/foo/bar 是 /foo 中的一个文件/目录，/foo 有一个快照 s0。那么，路径\n\n    /foo/.snapshot/s0/bar\n\n指向 /foo/bar 的快照拷贝。普通 API 和 CLI 可以用“.snapshot”路径工作。下面是一些例子。\n\n- 列出一个快照表目录下所有的快照：\n\n    hdfs dfs -ls /foo/.snapshot\n\n- 列出快照 s0 中的文件：\n\n    hdfs dfs -ls /foo/.snapshot/s0\n\n- 从快照 s0 中拷贝一个文件：\n\n    hdfs dfs -cp -ptopax /foo/.snapshot/s0/bar /tmp\n\n注意，这个例子使用了保持选项保持时间戳、属主关系、权限、ACL 和 XAttr。\n\n### 用快照升级 HDFS 到某个版本\n\nHDFS 快照特性引入了一个新的保留路径名用来与快照交互：.snapshot。当从 HDFS 的一个旧版本升级时，存在的 .snapshot 路径名称需要先重命名或删除以避免与保留路径冲突。参见 [HDFS 用户指南](http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html#Upgrade_and_Rollback)升级一节获取更多信息。\n\n#### 快照选项\n\n##### 管理员选项\n\n这一节描述的操作需要超级用户权限。\n\n###### 允许快照\n\n允许创建一个目录的快照。如果操作成功完成，这个目录变为快照表。\n\n- 命令：\n\n    hdfs dfsadmin -allowSnapshot &lt;path&gt;\n\n- 参数：\n\npath：快照表目录的路径。\n\n也可以参见 HdfsAdmin 中对应的 Java API void allowSnapshot(Path path)。\n\n###### 禁止快照\n\n禁止一个目录创建快照。在禁止前这个目录的所有快照必须删除。\n\n- 命令：\n\n    hdfs dfsadmin -disallowSnapshot &lt;path&gt;\n\n- 参数：\n\npath：快照表目录的路径。\n\n也可以参见 HdfsAdmin 中对应的 Java API void disallowSnapshot(Path path)。\n\n##### 用户操作\n\n这一节描述用户操作。注意，HDFS 超级用户可以执行所有操作而不需要各个操作中要求的权限需求。\n\n###### 创建快照\n\n创建一个快照表目录的快照。这个操作需要快照表目录属主的权限。\n\n- 命令：\n\n    hdfs dfs -createSnapshot &lt;path&gt; [&lt;snapshotName&gt;]\n\n- 参数：\n\npath：快照表目录的路径。\n\nsnapshotName：快照名字，这是个可选参数。当省略这个参数时，会用时间戳以格式“'s'yyyyMMdd-HHmmss.SSS”生成默认名称，例如：s20130412-151029.033。\n\n也可以参见 [FileSystem](http://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/FileSystem.html) 中对应的 Java API Path createSnapshot(Path path) 和 Path createSnapshot(Path path, String snapshotName)。这些方法返回快照的路径。\n\n###### 删除快照\n\n从一个快照表目录中删除快照。这个操作需要快照表目录属主的权限。\n\n- 命令：\n\n    hdfs dfs -deleteSnapshot &lt;path&gt; &lt;snapshotName&gt;\n\n- 参数：\n\npath：快照表目录的路径。\n\nsnapshotName：快照名称。\n\n也可以参见 [FileSystem](http://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/FileSystem.html) 中对应的 Java API  void deleteSnapshot(Path path, String snapshotName)。\n\n###### 重命名快照\n\n重命名一个快照。这个操作需要快照表目录属主的权限。\n\n- 命令：\n\n    hdfs dfs -renameSnapshot &lt;path&gt; &lt;oldName&gt; &lt;newName&gt;\n\n- 参数：\n\npath：快照表目录的路径。\n\noldName：原快照名称。\n\nnewName：新快照名称。\n\n也可以参见 [FileSystem](http://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/FileSystem.html) 中对应的 Java API  void renameSnapshot(Path path, String oldName, String newName)。\n\n###### 获取快照表目录列表\n\n获取当前用户有权限的所有快照表目录。\n\n- 命令：\n\n    hdfs lsSnapshottableDir\n\n- 参数：无\n\n也可以参见 DistributedFileSystem 中对应的 Java API SnapshottableDirectoryStatus[] getSnapshottableDirectoryListing()。\n\n###### 获取快照差异报告\n\n获取两个快照间的差异。这个操作需要两个快照中所有文件/目录读访问权限。\n\n- 命令：\n\n    hdfs snapshotDiff &lt;path&gt; &lt;fromSnapshot&gt; &lt;toSnapshot&gt;\n\n- 参数：\n\npath：快照表目录的路径。\n\nfromSnapshot：开始快照的名称。\n\ntoSnapshot：结束快照的名称。\n\n- 结果：\n\n&#43;：文件/目录被创建。  \n&minus;：文件/目录被删除。  \nM：文件/目录被修改。\nR：文件/目录被重命名。\n\n重命名实体表明一个文件/目录被重命名但依然在同一个快照表目录中。如果一个文件/目录被重命名到快照表目录的外部，那么这个文件/目录报告为被删除。一个文件/目录从外部重命名到快照表中会报告为新创建。\n\n快照差异报告不保证相同的操作序列。例如，重命名目录“/foo”为“/foo2”，然后向文件“/foo2/bar”追加数据，差异报告将是：\n\n    R. /foo -> /foo2\n    M. /foo/bar\n\n换言之，被重命名目录下的文件/目录的修改会用被重名前原来的路径报告（上面例子中的“/foo/bar”）。\n\n也可以参见 DistributedFileSystem 中对应的 Java API SnapshotDiffReport getSnapshotDiffReport(Path path, String fromSnapshot, String toSnapshot)。","slug":"HDFS-快照","published":1,"updated":"2021-07-19T16:28:00.248Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphmb0029itd3d8ib3rmp","content":"<h3 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h3><p>HDFS 快照是只读的某个时间点的文件系统拷贝。可以在一个子树或者整个文件系统上执行快照。一般使用快照的场景有数据备份、防止用户错误及灾难恢复。</p>\n<span id=\"more\"></span>\n\n<p>HDFS 快照的实现是很有效的：</p>\n<ul>\n<li>快照的创建是瞬间的：代价是 O(1)，除了索引点（inode）查找时间。</li>\n<li>只有发生跟快照相关的修改时才需要额外的内存：内存使用是 O(M)，M 是修改的文件/目录的数目。</li>\n<li>DataNode 上的块不会拷贝：快照文件记录块列表及文件的大小。不会拷贝数据。</li>\n<li>快照不会对常规的 HDFS 操作有不利的影响：修改是以时间倒序排列的，因此当前的数据可以直接访问。快照数据是通过从当前数据减去修改来计算的。</li>\n</ul>\n<h4 id=\"快照表目录\"><a href=\"#快照表目录\" class=\"headerlink\" title=\"快照表目录\"></a>快照表目录</h4><p>快照表可以在任何目录上创建，只要目录被设置为快照表。一个快照表目录可以同时容纳 65536 个快照。快照表目录的数量没有限制。管理员可以设置任何目录为快照表。如果一个快照表目录下存在快照，那么这个目录在所有的快照被删除前不能被删除也不能重命名。嵌套的快照表目录当前是不允许的。就是说，如果一个目录的一个上级目录/下级目录是快照表目录，那么这个目录不能被设置为快照表。</p>\n<h4 id=\"快照路径\"><a href=\"#快照路径\" class=\"headerlink\" title=\"快照路径\"></a>快照路径</h4><p>对于一个快照表目录，路径组件“.snapshot”用来访问它的快照。假设 /foo 是一个快照表目录，/foo/bar 是 /foo 中的一个文件/目录，/foo 有一个快照 s0。那么，路径</p>\n<pre><code>/foo/.snapshot/s0/bar\n</code></pre>\n<p>指向 /foo/bar 的快照拷贝。普通 API 和 CLI 可以用“.snapshot”路径工作。下面是一些例子。</p>\n<ul>\n<li><p>列出一个快照表目录下所有的快照：</p>\n<p>  hdfs dfs -ls /foo/.snapshot</p>\n</li>\n<li><p>列出快照 s0 中的文件：</p>\n<p>  hdfs dfs -ls /foo/.snapshot/s0</p>\n</li>\n<li><p>从快照 s0 中拷贝一个文件：</p>\n<p>  hdfs dfs -cp -ptopax /foo/.snapshot/s0/bar /tmp</p>\n</li>\n</ul>\n<p>注意，这个例子使用了保持选项保持时间戳、属主关系、权限、ACL 和 XAttr。</p>\n<h3 id=\"用快照升级-HDFS-到某个版本\"><a href=\"#用快照升级-HDFS-到某个版本\" class=\"headerlink\" title=\"用快照升级 HDFS 到某个版本\"></a>用快照升级 HDFS 到某个版本</h3><p>HDFS 快照特性引入了一个新的保留路径名用来与快照交互：.snapshot。当从 HDFS 的一个旧版本升级时，存在的 .snapshot 路径名称需要先重命名或删除以避免与保留路径冲突。参见 <a href=\"http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html#Upgrade_and_Rollback\">HDFS 用户指南</a>升级一节获取更多信息。</p>\n<h4 id=\"快照选项\"><a href=\"#快照选项\" class=\"headerlink\" title=\"快照选项\"></a>快照选项</h4><h5 id=\"管理员选项\"><a href=\"#管理员选项\" class=\"headerlink\" title=\"管理员选项\"></a>管理员选项</h5><p>这一节描述的操作需要超级用户权限。</p>\n<h6 id=\"允许快照\"><a href=\"#允许快照\" class=\"headerlink\" title=\"允许快照\"></a>允许快照</h6><p>允许创建一个目录的快照。如果操作成功完成，这个目录变为快照表。</p>\n<ul>\n<li><p>命令：</p>\n<p>  hdfs dfsadmin -allowSnapshot &lt;path&gt;</p>\n</li>\n<li><p>参数：</p>\n</li>\n</ul>\n<p>path：快照表目录的路径。</p>\n<p>也可以参见 HdfsAdmin 中对应的 Java API void allowSnapshot(Path path)。</p>\n<h6 id=\"禁止快照\"><a href=\"#禁止快照\" class=\"headerlink\" title=\"禁止快照\"></a>禁止快照</h6><p>禁止一个目录创建快照。在禁止前这个目录的所有快照必须删除。</p>\n<ul>\n<li><p>命令：</p>\n<p>  hdfs dfsadmin -disallowSnapshot &lt;path&gt;</p>\n</li>\n<li><p>参数：</p>\n</li>\n</ul>\n<p>path：快照表目录的路径。</p>\n<p>也可以参见 HdfsAdmin 中对应的 Java API void disallowSnapshot(Path path)。</p>\n<h5 id=\"用户操作\"><a href=\"#用户操作\" class=\"headerlink\" title=\"用户操作\"></a>用户操作</h5><p>这一节描述用户操作。注意，HDFS 超级用户可以执行所有操作而不需要各个操作中要求的权限需求。</p>\n<h6 id=\"创建快照\"><a href=\"#创建快照\" class=\"headerlink\" title=\"创建快照\"></a>创建快照</h6><p>创建一个快照表目录的快照。这个操作需要快照表目录属主的权限。</p>\n<ul>\n<li><p>命令：</p>\n<p>  hdfs dfs -createSnapshot &lt;path&gt; [&lt;snapshotName&gt;]</p>\n</li>\n<li><p>参数：</p>\n</li>\n</ul>\n<p>path：快照表目录的路径。</p>\n<p>snapshotName：快照名字，这是个可选参数。当省略这个参数时，会用时间戳以格式“’s’yyyyMMdd-HHmmss.SSS”生成默认名称，例如：s20130412-151029.033。</p>\n<p>也可以参见 <a href=\"http://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/FileSystem.html\">FileSystem</a> 中对应的 Java API Path createSnapshot(Path path) 和 Path createSnapshot(Path path, String snapshotName)。这些方法返回快照的路径。</p>\n<h6 id=\"删除快照\"><a href=\"#删除快照\" class=\"headerlink\" title=\"删除快照\"></a>删除快照</h6><p>从一个快照表目录中删除快照。这个操作需要快照表目录属主的权限。</p>\n<ul>\n<li><p>命令：</p>\n<p>  hdfs dfs -deleteSnapshot &lt;path&gt; &lt;snapshotName&gt;</p>\n</li>\n<li><p>参数：</p>\n</li>\n</ul>\n<p>path：快照表目录的路径。</p>\n<p>snapshotName：快照名称。</p>\n<p>也可以参见 <a href=\"http://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/FileSystem.html\">FileSystem</a> 中对应的 Java API  void deleteSnapshot(Path path, String snapshotName)。</p>\n<h6 id=\"重命名快照\"><a href=\"#重命名快照\" class=\"headerlink\" title=\"重命名快照\"></a>重命名快照</h6><p>重命名一个快照。这个操作需要快照表目录属主的权限。</p>\n<ul>\n<li><p>命令：</p>\n<p>  hdfs dfs -renameSnapshot &lt;path&gt; &lt;oldName&gt; &lt;newName&gt;</p>\n</li>\n<li><p>参数：</p>\n</li>\n</ul>\n<p>path：快照表目录的路径。</p>\n<p>oldName：原快照名称。</p>\n<p>newName：新快照名称。</p>\n<p>也可以参见 <a href=\"http://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/FileSystem.html\">FileSystem</a> 中对应的 Java API  void renameSnapshot(Path path, String oldName, String newName)。</p>\n<h6 id=\"获取快照表目录列表\"><a href=\"#获取快照表目录列表\" class=\"headerlink\" title=\"获取快照表目录列表\"></a>获取快照表目录列表</h6><p>获取当前用户有权限的所有快照表目录。</p>\n<ul>\n<li><p>命令：</p>\n<p>  hdfs lsSnapshottableDir</p>\n</li>\n<li><p>参数：无</p>\n</li>\n</ul>\n<p>也可以参见 DistributedFileSystem 中对应的 Java API SnapshottableDirectoryStatus[] getSnapshottableDirectoryListing()。</p>\n<h6 id=\"获取快照差异报告\"><a href=\"#获取快照差异报告\" class=\"headerlink\" title=\"获取快照差异报告\"></a>获取快照差异报告</h6><p>获取两个快照间的差异。这个操作需要两个快照中所有文件/目录读访问权限。</p>\n<ul>\n<li><p>命令：</p>\n<p>  hdfs snapshotDiff &lt;path&gt; &lt;fromSnapshot&gt; &lt;toSnapshot&gt;</p>\n</li>\n<li><p>参数：</p>\n</li>\n</ul>\n<p>path：快照表目录的路径。</p>\n<p>fromSnapshot：开始快照的名称。</p>\n<p>toSnapshot：结束快照的名称。</p>\n<ul>\n<li>结果：</li>\n</ul>\n<p>&#43;：文件/目录被创建。<br>&minus;：文件/目录被删除。<br>M：文件/目录被修改。<br>R：文件/目录被重命名。</p>\n<p>重命名实体表明一个文件/目录被重命名但依然在同一个快照表目录中。如果一个文件/目录被重命名到快照表目录的外部，那么这个文件/目录报告为被删除。一个文件/目录从外部重命名到快照表中会报告为新创建。</p>\n<p>快照差异报告不保证相同的操作序列。例如，重命名目录“/foo”为“/foo2”，然后向文件“/foo2/bar”追加数据，差异报告将是：</p>\n<pre><code>R. /foo -&gt; /foo2\nM. /foo/bar\n</code></pre>\n<p>换言之，被重命名目录下的文件/目录的修改会用被重名前原来的路径报告（上面例子中的“/foo/bar”）。</p>\n<p>也可以参见 DistributedFileSystem 中对应的 Java API SnapshotDiffReport getSnapshotDiffReport(Path path, String fromSnapshot, String toSnapshot)。</p>\n","site":{"data":{}},"excerpt":"<h3 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h3><p>HDFS 快照是只读的某个时间点的文件系统拷贝。可以在一个子树或者整个文件系统上执行快照。一般使用快照的场景有数据备份、防止用户错误及灾难恢复。</p>","more":"<p>HDFS 快照的实现是很有效的：</p>\n<ul>\n<li>快照的创建是瞬间的：代价是 O(1)，除了索引点（inode）查找时间。</li>\n<li>只有发生跟快照相关的修改时才需要额外的内存：内存使用是 O(M)，M 是修改的文件/目录的数目。</li>\n<li>DataNode 上的块不会拷贝：快照文件记录块列表及文件的大小。不会拷贝数据。</li>\n<li>快照不会对常规的 HDFS 操作有不利的影响：修改是以时间倒序排列的，因此当前的数据可以直接访问。快照数据是通过从当前数据减去修改来计算的。</li>\n</ul>\n<h4 id=\"快照表目录\"><a href=\"#快照表目录\" class=\"headerlink\" title=\"快照表目录\"></a>快照表目录</h4><p>快照表可以在任何目录上创建，只要目录被设置为快照表。一个快照表目录可以同时容纳 65536 个快照。快照表目录的数量没有限制。管理员可以设置任何目录为快照表。如果一个快照表目录下存在快照，那么这个目录在所有的快照被删除前不能被删除也不能重命名。嵌套的快照表目录当前是不允许的。就是说，如果一个目录的一个上级目录/下级目录是快照表目录，那么这个目录不能被设置为快照表。</p>\n<h4 id=\"快照路径\"><a href=\"#快照路径\" class=\"headerlink\" title=\"快照路径\"></a>快照路径</h4><p>对于一个快照表目录，路径组件“.snapshot”用来访问它的快照。假设 /foo 是一个快照表目录，/foo/bar 是 /foo 中的一个文件/目录，/foo 有一个快照 s0。那么，路径</p>\n<pre><code>/foo/.snapshot/s0/bar\n</code></pre>\n<p>指向 /foo/bar 的快照拷贝。普通 API 和 CLI 可以用“.snapshot”路径工作。下面是一些例子。</p>\n<ul>\n<li><p>列出一个快照表目录下所有的快照：</p>\n<p>  hdfs dfs -ls /foo/.snapshot</p>\n</li>\n<li><p>列出快照 s0 中的文件：</p>\n<p>  hdfs dfs -ls /foo/.snapshot/s0</p>\n</li>\n<li><p>从快照 s0 中拷贝一个文件：</p>\n<p>  hdfs dfs -cp -ptopax /foo/.snapshot/s0/bar /tmp</p>\n</li>\n</ul>\n<p>注意，这个例子使用了保持选项保持时间戳、属主关系、权限、ACL 和 XAttr。</p>\n<h3 id=\"用快照升级-HDFS-到某个版本\"><a href=\"#用快照升级-HDFS-到某个版本\" class=\"headerlink\" title=\"用快照升级 HDFS 到某个版本\"></a>用快照升级 HDFS 到某个版本</h3><p>HDFS 快照特性引入了一个新的保留路径名用来与快照交互：.snapshot。当从 HDFS 的一个旧版本升级时，存在的 .snapshot 路径名称需要先重命名或删除以避免与保留路径冲突。参见 <a href=\"http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html#Upgrade_and_Rollback\">HDFS 用户指南</a>升级一节获取更多信息。</p>\n<h4 id=\"快照选项\"><a href=\"#快照选项\" class=\"headerlink\" title=\"快照选项\"></a>快照选项</h4><h5 id=\"管理员选项\"><a href=\"#管理员选项\" class=\"headerlink\" title=\"管理员选项\"></a>管理员选项</h5><p>这一节描述的操作需要超级用户权限。</p>\n<h6 id=\"允许快照\"><a href=\"#允许快照\" class=\"headerlink\" title=\"允许快照\"></a>允许快照</h6><p>允许创建一个目录的快照。如果操作成功完成，这个目录变为快照表。</p>\n<ul>\n<li><p>命令：</p>\n<p>  hdfs dfsadmin -allowSnapshot &lt;path&gt;</p>\n</li>\n<li><p>参数：</p>\n</li>\n</ul>\n<p>path：快照表目录的路径。</p>\n<p>也可以参见 HdfsAdmin 中对应的 Java API void allowSnapshot(Path path)。</p>\n<h6 id=\"禁止快照\"><a href=\"#禁止快照\" class=\"headerlink\" title=\"禁止快照\"></a>禁止快照</h6><p>禁止一个目录创建快照。在禁止前这个目录的所有快照必须删除。</p>\n<ul>\n<li><p>命令：</p>\n<p>  hdfs dfsadmin -disallowSnapshot &lt;path&gt;</p>\n</li>\n<li><p>参数：</p>\n</li>\n</ul>\n<p>path：快照表目录的路径。</p>\n<p>也可以参见 HdfsAdmin 中对应的 Java API void disallowSnapshot(Path path)。</p>\n<h5 id=\"用户操作\"><a href=\"#用户操作\" class=\"headerlink\" title=\"用户操作\"></a>用户操作</h5><p>这一节描述用户操作。注意，HDFS 超级用户可以执行所有操作而不需要各个操作中要求的权限需求。</p>\n<h6 id=\"创建快照\"><a href=\"#创建快照\" class=\"headerlink\" title=\"创建快照\"></a>创建快照</h6><p>创建一个快照表目录的快照。这个操作需要快照表目录属主的权限。</p>\n<ul>\n<li><p>命令：</p>\n<p>  hdfs dfs -createSnapshot &lt;path&gt; [&lt;snapshotName&gt;]</p>\n</li>\n<li><p>参数：</p>\n</li>\n</ul>\n<p>path：快照表目录的路径。</p>\n<p>snapshotName：快照名字，这是个可选参数。当省略这个参数时，会用时间戳以格式“’s’yyyyMMdd-HHmmss.SSS”生成默认名称，例如：s20130412-151029.033。</p>\n<p>也可以参见 <a href=\"http://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/FileSystem.html\">FileSystem</a> 中对应的 Java API Path createSnapshot(Path path) 和 Path createSnapshot(Path path, String snapshotName)。这些方法返回快照的路径。</p>\n<h6 id=\"删除快照\"><a href=\"#删除快照\" class=\"headerlink\" title=\"删除快照\"></a>删除快照</h6><p>从一个快照表目录中删除快照。这个操作需要快照表目录属主的权限。</p>\n<ul>\n<li><p>命令：</p>\n<p>  hdfs dfs -deleteSnapshot &lt;path&gt; &lt;snapshotName&gt;</p>\n</li>\n<li><p>参数：</p>\n</li>\n</ul>\n<p>path：快照表目录的路径。</p>\n<p>snapshotName：快照名称。</p>\n<p>也可以参见 <a href=\"http://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/FileSystem.html\">FileSystem</a> 中对应的 Java API  void deleteSnapshot(Path path, String snapshotName)。</p>\n<h6 id=\"重命名快照\"><a href=\"#重命名快照\" class=\"headerlink\" title=\"重命名快照\"></a>重命名快照</h6><p>重命名一个快照。这个操作需要快照表目录属主的权限。</p>\n<ul>\n<li><p>命令：</p>\n<p>  hdfs dfs -renameSnapshot &lt;path&gt; &lt;oldName&gt; &lt;newName&gt;</p>\n</li>\n<li><p>参数：</p>\n</li>\n</ul>\n<p>path：快照表目录的路径。</p>\n<p>oldName：原快照名称。</p>\n<p>newName：新快照名称。</p>\n<p>也可以参见 <a href=\"http://hadoop.apache.org/docs/r2.7.3/api/org/apache/hadoop/fs/FileSystem.html\">FileSystem</a> 中对应的 Java API  void renameSnapshot(Path path, String oldName, String newName)。</p>\n<h6 id=\"获取快照表目录列表\"><a href=\"#获取快照表目录列表\" class=\"headerlink\" title=\"获取快照表目录列表\"></a>获取快照表目录列表</h6><p>获取当前用户有权限的所有快照表目录。</p>\n<ul>\n<li><p>命令：</p>\n<p>  hdfs lsSnapshottableDir</p>\n</li>\n<li><p>参数：无</p>\n</li>\n</ul>\n<p>也可以参见 DistributedFileSystem 中对应的 Java API SnapshottableDirectoryStatus[] getSnapshottableDirectoryListing()。</p>\n<h6 id=\"获取快照差异报告\"><a href=\"#获取快照差异报告\" class=\"headerlink\" title=\"获取快照差异报告\"></a>获取快照差异报告</h6><p>获取两个快照间的差异。这个操作需要两个快照中所有文件/目录读访问权限。</p>\n<ul>\n<li><p>命令：</p>\n<p>  hdfs snapshotDiff &lt;path&gt; &lt;fromSnapshot&gt; &lt;toSnapshot&gt;</p>\n</li>\n<li><p>参数：</p>\n</li>\n</ul>\n<p>path：快照表目录的路径。</p>\n<p>fromSnapshot：开始快照的名称。</p>\n<p>toSnapshot：结束快照的名称。</p>\n<ul>\n<li>结果：</li>\n</ul>\n<p>&#43;：文件/目录被创建。<br>&minus;：文件/目录被删除。<br>M：文件/目录被修改。<br>R：文件/目录被重命名。</p>\n<p>重命名实体表明一个文件/目录被重命名但依然在同一个快照表目录中。如果一个文件/目录被重命名到快照表目录的外部，那么这个文件/目录报告为被删除。一个文件/目录从外部重命名到快照表中会报告为新创建。</p>\n<p>快照差异报告不保证相同的操作序列。例如，重命名目录“/foo”为“/foo2”，然后向文件“/foo2/bar”追加数据，差异报告将是：</p>\n<pre><code>R. /foo -&gt; /foo2\nM. /foo/bar\n</code></pre>\n<p>换言之，被重命名目录下的文件/目录的修改会用被重名前原来的路径报告（上面例子中的“/foo/bar”）。</p>\n<p>也可以参见 DistributedFileSystem 中对应的 Java API SnapshotDiffReport getSnapshotDiffReport(Path path, String fromSnapshot, String toSnapshot)。</p>"},{"title":"HDFS 权限","date":"2016-11-13T13:41:34.000Z","_content":"\n### 概述\n\nHDFS 实现文件和目录权限的模式拥有许多 POSIX 模式。每个文件和目录都关联一个属主和组。文件或者目录针对属主用户、组中的其他用户以及其他用户有单独的权限。要读取文件内容必须有 r 权限，写入或者追加文件内容必须有 w 权限。要列出目录的内容必须有 r 权限，创建或者删除文件或者目录必须有 w 权限，访问目录的子目录必须有 x 权限。\n\n<!-- more -->\n\n与 POSIX 模式相比，因为没有可执行文件的概念所以没有文件的 setuid 和 setgid 标识位。作为简化也没有目录的 setuid 和 setgid 标识位。在目录上设置粘滞位（Sticky bit）防止除了超级用户、目录属主或文件属主外的其他用户删除或者移动目录下的文件。给文件设置粘滞位没有任何效果。总的来说，文件和目录的权限都是这种模式。通常，使用 Unix 惯例来表现和显示模式，包括使用 8 进制数字。当创建文件或目录时，它的属主是客户端进程的用户，它的组是父目录的组（BSD 规则）。\n\nHDFS 也提供支持 POSIX ACL（访问控制列表）来增强针对特定用户或组细粒度的权限控制。ACL 稍后在本文档详细讨论。\n\n访问 HDFS 的每个客户端进程，包含用户名和组列表两部分身份。每当必须对一个客户端进程访问一个名字为 foo 的文件或者目录做鉴权时：\n\n- 如果用户名与 foo 的属主匹配，那么属主的权限被测试；\n- 否则如果 foo 的组匹配任意一个组列表中的组，那么组权限被测试；\n- 否则 foo 的其他权限被测试。\n\n如果权限检查失败，客户端操作也失败。\n\n### 用户身份\n\n从 Hadoop 0.22 版本，Hadoop 支持两种识别用户身份的方式，通过 hadoop.security.authentication 属性指定：\n\n#### 简单模式\n\n用这种操作模式，通过主机操作系统确定客户端进程的身份。在类 Unix 系统上，用户名与 `whoami` 是等效的。\n\n#### kerberos\n\n用基于 Kerberos 操作模式，通过 Kerberos 证书确定客户端进程的身份。例如，在一个 Kerberos 环境，一个用户可以使用 kinit 工具获取 Kerberos 的授予票据（TGT）并且使用 klist 确定他们当前的当事人。当映射一个 Kerberos 当事人到一个 HDFS 用户名，除了主要的组件外所有组件都被丢弃。例如，当事人 todd/foobar@CORP.COMPANY.COM 将在 HDFS 上作为简单的 todd 用户名。\n\n不管什么操作模式，用户身份机制是 HDFS 本身外部的。HDFS 没有创建用户身份、建立组或处理用户凭据的的支持。\n\n### 组映射\n\n一旦一个用户像上面描述的那样被确定，组列表被通过 hadoop.security.group.mapping 属性配置的组映射服务确定。如果 Java 本地接口（JNI）是可用的，默认实现 org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback 将确定。如果 JNI 是可用的，实现将使用 Hadoop 的 API 决定一个用户的组列表。如果 JNI 不可用，那么会使用 Shell 实现 org.apache.hadoop.security.ShellBasedUnixGroupsMapping。这个实现 Shell 使用 bash -c groups 命令（Linux/Unix 环境）或者 net group 命令（Windows 环境）来决定一个用户的组列表。一个备选的实现是通过 org.apache.hadoop.security.LdapGroupsMapping 实现，直接连接 LDAP 服务器决定组列表。但是，这种方式只能在用户组唯一的在 LDAP 上且不在 Unix 服务器上具体化时使用。关于配置组映射服务更多的信息可以在 Javadoc 中获取。\n\n对于 HDFS，在 NameNode 上执行用户到组的映射。因此，NameNode 主机系统配置确定用户到组的映射。\n\n注意，HDFS 用字符串存储一个文件或者目录的用户和组；不像 Unix 那样需要从用户和组的身份编号转换。\n\n### 理解具体实现\n\n每个文件或者目录的操作都传送全路径名称给 NameNode，NameNode 对每个操作的路径应用权限检查。客户端框架隐式的将用户身份与到 NameNode 的连接联系在一起，减少已存在的客户端 API 的修改。总是存在这样的场景，当对一个文件的操作成功后，因为文件或者在路径中的一些目录不存在了当重复这个操作时可能会失败。例如，当客户端开始读一个文件的时候，会首先发送第一个请求给 NameNode 来找出文件的第一个块所在的位置。第二个找出其他块的请求可能会失败。另一方面，正在删除文件的操作不能撤销一个已经知道文件块的客户端的访问。随着权限的添加，客户端对一个文件的访问可能在请求间被撤回。再者，变更权限不会撤销一个已经知道文件块的客户端的访问。\n\n### 文件系统 API 的变化\n\n如果权限检查失败，所有使用路径参数的方法都会抛出 AccessControlException 异常。\n\n新方法：\n\n- public FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress) throws IOException;\n- public boolean mkdirs(Path f, FsPermission permission) throws IOException;\n- public void setPermission(Path p, FsPermission permission) throws IOException;\n- public void setOwner(Path p, String username, String groupname) throws IOException;\n- public FileStatus getFileStatus(Path f) throws IOException; 还将返回与路径相关联的用户、组和模式。\n\n一个新文件或者目录的模式通过掩码组配置参数被限制。当现有的 create(path, …) 方法（不带权限参数）被使用时，新文件的模式是 0666 & ^umask。当新的 create(path, permission, …) 方法（带权限参数 P）被使用时，新文件的模式是 P & ^umask & 0666。当使用现有的 mkdirs(path) 方法（不带权限参数）创建一个新目录时，新目录的模式是 0777 & ^umask。当新的 mkdirs(path, permission) 方法（带权限参数 P）被使用时，新目录的模式是 P & ^umask & 0777。\n\n### 应用程序 Shell 的变化\n\n新操作：\n\n- chmod [-R] mode file ...\n  只有文件属主或者超级用户才有权限修改文件的模式。\n- chgrp [-R] group file ...\n  调用 chgrp 的用户必须属于特定的组并且是文件的属主或者超级用户。\n- chown [-R] [owner][:[group]] file ...\n  一个文件的所有者只能由超级用户修改。\n- ls file ...\n- lsr file ...\n  输出被重新格式化来展示属主、组和模式。\n\n### 超级用户\n\n超级用户是与 NameNode 进程自己有相同身份的用户。简单的，如果你启动 NameNode，那么你就是超级用户。超级用户可以做任何事情，超级用户的权限检查从不会失败。谁是超级用户没有不变的概念，当 NameNode 启动后进程的身份决定了现在的超级用户是谁。HDFS 的超级用户不必是 NameNode 主机的超级用户，也没有必要，所有的集群有相同的超级用户。同时，一个试验者在个人工作站上运行 HDFS 便可以不需要任何配置很方便的成为安装的超级用户。\n\n此外，使用配置参数区别一个管理员组。如果设置，这个组的成员都是超级用户。\n\n### Web 服务器\n\n默认的，Web 服务器的身份是一个配置参数。也就是，NameNode 没有真实用户的概念，但是，Web 服务器表现为管理员选择的有身份（用户和组）的用户。除非选择与超级用户匹配的身份，否则部分名称空间 Web 服务器是无法访问的。\n\n### ACL（访问控制列表）\n\n除了传统的 POSIX 权限模型，HDFS 也支持 POSIX ACL（访问控制列表）。对于实现不同于用户和组自然组织层次的权限需求，ACL 是很有帮助的。一个 ACL 提供了一种对指定的用户和组设置不同权限的方法，不仅仅是文件的属主和文件的组。\n\n默认的，对 ACL 的支持是禁用的，并且 NameNode 不允许创建 ACL。要启用 ACL 支持，在 NameNode 配置中设置 dfs.namenode.acls.enabled 为 true。\n\n一个 ACL 由一组 ACL 条目组成。每条 ACL 条目确定一个用户或组并为该用户或组授权或拒绝读、写和执行权限。例如：\n\n    user::rw-\n    user:bruce:rwx                  #effective:r--\n    group::r-x                      #effective:r--\n    group:sales:rwx                 #effective:r--\n    mask::r--\n    other::r--\n\nACL 条目包含一个类型、一个可选的名称和一个权限字符串。为了显示目的，‘:’用作每个域的分隔符。在这个示例 ACL 中，文件的属主有读写访问权限，文件的组有读和执行访问权限，其他用户有读访问权限。目前为止，这相当于设置文件的权限位为 654。\n\n另外，有两个针对名称为 bruce 的用户和名称为 sales 的组的扩展 ACL 条目，两个都赋予了全部访问权限。掩码是一个特殊的 ACL 条目，用来过滤所有赋予命名的用户条目和命名的组条目的权限，以及未命名的组条目。在这个示例中，掩码只有读权限，我们可以看到几个 ACL 条目的有效权限因此被过滤掉了。\n\n每个 ACL 都必须有掩码。如果一个用户在设置 ACL 时没有应用掩码，那么通过计算过滤所有条目权限的掩码来自动插入一个掩码。\n\n在一个有 ACL 的文件上运行 chmod，实际是改变了掩码的权限。因为掩码作为一个过滤器，这有效的约束所有扩展 ACL 条目的权限，而不仅是改变组条目并且遗漏其他扩展的 ACL 条目。\n\n这个模型也区分“访问 ACL”和“默认 ACL”，“访问 ACL”定义在权限检查时强制执行的规则，“默认 ACL”定义子文件或者子目录创建时自动接收到的 ACL 条目。例如：\n\n    user::rwx\n    group::r-x\n    other::r-x\n    default:user::rwx\n    default:user:bruce:rwx          #effective:r-x\n    default:group::r-x\n    default:group:sales:rwx         #effective:r-x\n    default:mask::r-x\n    default:other::r-x\n\n只有目录才有默认 ACL。当一个新文件或者子目录创建时，会自动拷贝它的父目录的默认 ACL 到它自己的访问 ACL 中。一个新子目录也拷贝它到自己的默认 ACL 中。用这种方式，默认 ACL 在新子目录创建时被拷贝到文件系统目录树任意级别的深度。\n\n在新子文件的访问 ACL 中确切的权限值是服从模式参数过滤的。就默认 022 掩码而言，这是典型的新目录的 755 和 新文件的 644。模式参数对没有命名的用户（文件属主）进行过滤拷贝的权限值、掩码和其他。使用这个特定的 ACL 示例，并创建一个模式为 755 的新子目录，这个模式对最终结果没有任何过滤作用。然而，如果我们考虑创建一个模式为 644 的文件，那么模式过滤使新文件的 ACL 针对未命名用户（文件属主）接收到读写权限，mask 为读权限且其他用户为读权限。这个掩码意味着对命名为 bruce 的用户和命名为 sales 的组的有效权限只有读。\n\n注意，拷贝发生在创建新文件或者子目录的时候。之后对父目录默认 ACL 的改变不会改变已经存在的子目录。\n\n默认的 ACL 必须有所有最小要求的 ACL 条目，包含未命名用户（文件属主）、未命名的组（文件组）和其他用户条目。如果当设置一个默认 ACL 条目时用户没有应用这些条目中的一条，那么这么条目会通过从访问 ACL 中拷贝相应的条目被自动插入，或者没有访问 ACL 时拷贝权限位。默认 ACL 也必须要有掩码。像上面描述的，如果没有指定掩码，那么通过计算过滤所有条目权限的掩码来自动插入一个掩码。\n\n当考虑一个有 ACL 的文件时，权限检查的算法变为：\n\n- 如果用户名与文件属主匹配，那么属主权限将被测试；\n- 否则如果用户名与命名用户条目中的一个用户名匹配，那么这些权限将被测试，并被掩码权限过滤；\n- 否则如果文件组与组列表任何一个组匹配，并且如果这些权限被掩码授权访问过滤，那么这些权限将被使用；\n- 否则如果有一个命名的组条目与一个组列表中的组匹配，并且如果这些权限被掩码授权访问过滤，那么这些权限将被使用；\n- 否则如果文件组或者任何一个命名组条目与组列表中的一个组匹配，但是没有被那些权限赋予访问权限，那么访问被拒绝；\n- 否则文件的其他权限将被测试。\n\n最佳实践是基于传统的权限位实现大部分权限需求，定义小数量的 ACL 作为例外规则来增强权限位。一个有 ACL 的文件比一个只有权限位的文件会引起 NameNode 额外的内存消耗。\n\n### ACL 文件系统 API\n\n新方法：\n\n- public void modifyAclEntries(Path path, List<AclEntry> aclSpec) throws IOException;\n- public void removeAclEntries(Path path, List<AclEntry> aclSpec) throws IOException;\n- public void public void removeDefaultAcl(Path path) throws IOException;\n- public void removeAcl(Path path) throws IOException;\n- public void setAcl(Path path, List<AclEntry> aclSpec) throws IOException;\n- public AclStatus getAclStatus(Path path) throws IOException;\n\n### ACL Shell 命令\n\n- hdfs dfs -getfacl [-R] <path>\n  展示文件和目录的访问控制列表（ACL）。如果一个目录有默认 ACL，那么 getfacl 也会展示默认 ACL。\n- hdfs dfs -setfacl [-R] [-b |-k -m |-x <acl_spec> <path>] |[--set <acl_spec> <path>]\n  设置文件和目录的访问控制列表（ACL）。\n- hdfs dfs -ls <args>\n  ls 的输出将在任何有 ACL 的文件和目录的权限字符串后追加一个 ‘+’ 字符。\n\n查看 [File System Shell](http://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-common/FileSystemShell.html) 文档获取这些命令的全部内容。\n\n### 配置参数\n\n- dfs.permissions.enabled = true\n  如果是则用这里描述的权限系统。如果否，权限检查被关闭，但所有其他行为不会改变。从一个参数值切换到另外一个参数值不会改变文件或目录的模式、属主或组。不管权限是开还是关，chmod、chgrp、chown 和 setfacl 总是检查权限。这些功能只在权限上下文中有用，因此没有向后兼容的问题。此外，这允许管理员在打开常规权限检查之前可靠的设置属主和权限。\n- dfs.web.ugi = webuser,webgroup\n  Web 服务器使用的用户名。设置这个为超级用户的名称允许所有 Web 客户端查看所有内容。修改这个为一个别的未用的身份允许 Web 客户端只能查看那些使用 “other” 权限可以访问的内容。另外，组可以添加到逗号分割的列表中。\n- dfs.permissions.superusergroup = supergroup\n  超级用户组的名称。\n- fs.permissions.umask-mode = 0022\n  创建文件和目录时使用的掩码。对于配置文件，可能使用十进制数值 18。\n- dfs.cluster.administrators = ACL-for-admins\n  为集群指定为 ACL 的管理员。控制谁可以在 HDFS 中访问默认 servlet 等。\n- dfs.namenode.acls.enabled = true\n  设置为 true 来启用 HDFS ACL（访问控制列表）支持。默认的，ACL 是禁用的。当 ACL 禁用时，NameNode 拒绝所有设置一个 ACL 的尝试。","source":"_posts/HDFS-权限.md","raw":"title: HDFS 权限\ntags:\n  - Hadoop\n  - HDFS\ncategories:\n  - 大数据\n  - Hadoop\ndate: 2016-11-13 21:41:34\n---\n\n### 概述\n\nHDFS 实现文件和目录权限的模式拥有许多 POSIX 模式。每个文件和目录都关联一个属主和组。文件或者目录针对属主用户、组中的其他用户以及其他用户有单独的权限。要读取文件内容必须有 r 权限，写入或者追加文件内容必须有 w 权限。要列出目录的内容必须有 r 权限，创建或者删除文件或者目录必须有 w 权限，访问目录的子目录必须有 x 权限。\n\n<!-- more -->\n\n与 POSIX 模式相比，因为没有可执行文件的概念所以没有文件的 setuid 和 setgid 标识位。作为简化也没有目录的 setuid 和 setgid 标识位。在目录上设置粘滞位（Sticky bit）防止除了超级用户、目录属主或文件属主外的其他用户删除或者移动目录下的文件。给文件设置粘滞位没有任何效果。总的来说，文件和目录的权限都是这种模式。通常，使用 Unix 惯例来表现和显示模式，包括使用 8 进制数字。当创建文件或目录时，它的属主是客户端进程的用户，它的组是父目录的组（BSD 规则）。\n\nHDFS 也提供支持 POSIX ACL（访问控制列表）来增强针对特定用户或组细粒度的权限控制。ACL 稍后在本文档详细讨论。\n\n访问 HDFS 的每个客户端进程，包含用户名和组列表两部分身份。每当必须对一个客户端进程访问一个名字为 foo 的文件或者目录做鉴权时：\n\n- 如果用户名与 foo 的属主匹配，那么属主的权限被测试；\n- 否则如果 foo 的组匹配任意一个组列表中的组，那么组权限被测试；\n- 否则 foo 的其他权限被测试。\n\n如果权限检查失败，客户端操作也失败。\n\n### 用户身份\n\n从 Hadoop 0.22 版本，Hadoop 支持两种识别用户身份的方式，通过 hadoop.security.authentication 属性指定：\n\n#### 简单模式\n\n用这种操作模式，通过主机操作系统确定客户端进程的身份。在类 Unix 系统上，用户名与 `whoami` 是等效的。\n\n#### kerberos\n\n用基于 Kerberos 操作模式，通过 Kerberos 证书确定客户端进程的身份。例如，在一个 Kerberos 环境，一个用户可以使用 kinit 工具获取 Kerberos 的授予票据（TGT）并且使用 klist 确定他们当前的当事人。当映射一个 Kerberos 当事人到一个 HDFS 用户名，除了主要的组件外所有组件都被丢弃。例如，当事人 todd/foobar@CORP.COMPANY.COM 将在 HDFS 上作为简单的 todd 用户名。\n\n不管什么操作模式，用户身份机制是 HDFS 本身外部的。HDFS 没有创建用户身份、建立组或处理用户凭据的的支持。\n\n### 组映射\n\n一旦一个用户像上面描述的那样被确定，组列表被通过 hadoop.security.group.mapping 属性配置的组映射服务确定。如果 Java 本地接口（JNI）是可用的，默认实现 org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback 将确定。如果 JNI 是可用的，实现将使用 Hadoop 的 API 决定一个用户的组列表。如果 JNI 不可用，那么会使用 Shell 实现 org.apache.hadoop.security.ShellBasedUnixGroupsMapping。这个实现 Shell 使用 bash -c groups 命令（Linux/Unix 环境）或者 net group 命令（Windows 环境）来决定一个用户的组列表。一个备选的实现是通过 org.apache.hadoop.security.LdapGroupsMapping 实现，直接连接 LDAP 服务器决定组列表。但是，这种方式只能在用户组唯一的在 LDAP 上且不在 Unix 服务器上具体化时使用。关于配置组映射服务更多的信息可以在 Javadoc 中获取。\n\n对于 HDFS，在 NameNode 上执行用户到组的映射。因此，NameNode 主机系统配置确定用户到组的映射。\n\n注意，HDFS 用字符串存储一个文件或者目录的用户和组；不像 Unix 那样需要从用户和组的身份编号转换。\n\n### 理解具体实现\n\n每个文件或者目录的操作都传送全路径名称给 NameNode，NameNode 对每个操作的路径应用权限检查。客户端框架隐式的将用户身份与到 NameNode 的连接联系在一起，减少已存在的客户端 API 的修改。总是存在这样的场景，当对一个文件的操作成功后，因为文件或者在路径中的一些目录不存在了当重复这个操作时可能会失败。例如，当客户端开始读一个文件的时候，会首先发送第一个请求给 NameNode 来找出文件的第一个块所在的位置。第二个找出其他块的请求可能会失败。另一方面，正在删除文件的操作不能撤销一个已经知道文件块的客户端的访问。随着权限的添加，客户端对一个文件的访问可能在请求间被撤回。再者，变更权限不会撤销一个已经知道文件块的客户端的访问。\n\n### 文件系统 API 的变化\n\n如果权限检查失败，所有使用路径参数的方法都会抛出 AccessControlException 异常。\n\n新方法：\n\n- public FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress) throws IOException;\n- public boolean mkdirs(Path f, FsPermission permission) throws IOException;\n- public void setPermission(Path p, FsPermission permission) throws IOException;\n- public void setOwner(Path p, String username, String groupname) throws IOException;\n- public FileStatus getFileStatus(Path f) throws IOException; 还将返回与路径相关联的用户、组和模式。\n\n一个新文件或者目录的模式通过掩码组配置参数被限制。当现有的 create(path, …) 方法（不带权限参数）被使用时，新文件的模式是 0666 & ^umask。当新的 create(path, permission, …) 方法（带权限参数 P）被使用时，新文件的模式是 P & ^umask & 0666。当使用现有的 mkdirs(path) 方法（不带权限参数）创建一个新目录时，新目录的模式是 0777 & ^umask。当新的 mkdirs(path, permission) 方法（带权限参数 P）被使用时，新目录的模式是 P & ^umask & 0777。\n\n### 应用程序 Shell 的变化\n\n新操作：\n\n- chmod [-R] mode file ...\n  只有文件属主或者超级用户才有权限修改文件的模式。\n- chgrp [-R] group file ...\n  调用 chgrp 的用户必须属于特定的组并且是文件的属主或者超级用户。\n- chown [-R] [owner][:[group]] file ...\n  一个文件的所有者只能由超级用户修改。\n- ls file ...\n- lsr file ...\n  输出被重新格式化来展示属主、组和模式。\n\n### 超级用户\n\n超级用户是与 NameNode 进程自己有相同身份的用户。简单的，如果你启动 NameNode，那么你就是超级用户。超级用户可以做任何事情，超级用户的权限检查从不会失败。谁是超级用户没有不变的概念，当 NameNode 启动后进程的身份决定了现在的超级用户是谁。HDFS 的超级用户不必是 NameNode 主机的超级用户，也没有必要，所有的集群有相同的超级用户。同时，一个试验者在个人工作站上运行 HDFS 便可以不需要任何配置很方便的成为安装的超级用户。\n\n此外，使用配置参数区别一个管理员组。如果设置，这个组的成员都是超级用户。\n\n### Web 服务器\n\n默认的，Web 服务器的身份是一个配置参数。也就是，NameNode 没有真实用户的概念，但是，Web 服务器表现为管理员选择的有身份（用户和组）的用户。除非选择与超级用户匹配的身份，否则部分名称空间 Web 服务器是无法访问的。\n\n### ACL（访问控制列表）\n\n除了传统的 POSIX 权限模型，HDFS 也支持 POSIX ACL（访问控制列表）。对于实现不同于用户和组自然组织层次的权限需求，ACL 是很有帮助的。一个 ACL 提供了一种对指定的用户和组设置不同权限的方法，不仅仅是文件的属主和文件的组。\n\n默认的，对 ACL 的支持是禁用的，并且 NameNode 不允许创建 ACL。要启用 ACL 支持，在 NameNode 配置中设置 dfs.namenode.acls.enabled 为 true。\n\n一个 ACL 由一组 ACL 条目组成。每条 ACL 条目确定一个用户或组并为该用户或组授权或拒绝读、写和执行权限。例如：\n\n    user::rw-\n    user:bruce:rwx                  #effective:r--\n    group::r-x                      #effective:r--\n    group:sales:rwx                 #effective:r--\n    mask::r--\n    other::r--\n\nACL 条目包含一个类型、一个可选的名称和一个权限字符串。为了显示目的，‘:’用作每个域的分隔符。在这个示例 ACL 中，文件的属主有读写访问权限，文件的组有读和执行访问权限，其他用户有读访问权限。目前为止，这相当于设置文件的权限位为 654。\n\n另外，有两个针对名称为 bruce 的用户和名称为 sales 的组的扩展 ACL 条目，两个都赋予了全部访问权限。掩码是一个特殊的 ACL 条目，用来过滤所有赋予命名的用户条目和命名的组条目的权限，以及未命名的组条目。在这个示例中，掩码只有读权限，我们可以看到几个 ACL 条目的有效权限因此被过滤掉了。\n\n每个 ACL 都必须有掩码。如果一个用户在设置 ACL 时没有应用掩码，那么通过计算过滤所有条目权限的掩码来自动插入一个掩码。\n\n在一个有 ACL 的文件上运行 chmod，实际是改变了掩码的权限。因为掩码作为一个过滤器，这有效的约束所有扩展 ACL 条目的权限，而不仅是改变组条目并且遗漏其他扩展的 ACL 条目。\n\n这个模型也区分“访问 ACL”和“默认 ACL”，“访问 ACL”定义在权限检查时强制执行的规则，“默认 ACL”定义子文件或者子目录创建时自动接收到的 ACL 条目。例如：\n\n    user::rwx\n    group::r-x\n    other::r-x\n    default:user::rwx\n    default:user:bruce:rwx          #effective:r-x\n    default:group::r-x\n    default:group:sales:rwx         #effective:r-x\n    default:mask::r-x\n    default:other::r-x\n\n只有目录才有默认 ACL。当一个新文件或者子目录创建时，会自动拷贝它的父目录的默认 ACL 到它自己的访问 ACL 中。一个新子目录也拷贝它到自己的默认 ACL 中。用这种方式，默认 ACL 在新子目录创建时被拷贝到文件系统目录树任意级别的深度。\n\n在新子文件的访问 ACL 中确切的权限值是服从模式参数过滤的。就默认 022 掩码而言，这是典型的新目录的 755 和 新文件的 644。模式参数对没有命名的用户（文件属主）进行过滤拷贝的权限值、掩码和其他。使用这个特定的 ACL 示例，并创建一个模式为 755 的新子目录，这个模式对最终结果没有任何过滤作用。然而，如果我们考虑创建一个模式为 644 的文件，那么模式过滤使新文件的 ACL 针对未命名用户（文件属主）接收到读写权限，mask 为读权限且其他用户为读权限。这个掩码意味着对命名为 bruce 的用户和命名为 sales 的组的有效权限只有读。\n\n注意，拷贝发生在创建新文件或者子目录的时候。之后对父目录默认 ACL 的改变不会改变已经存在的子目录。\n\n默认的 ACL 必须有所有最小要求的 ACL 条目，包含未命名用户（文件属主）、未命名的组（文件组）和其他用户条目。如果当设置一个默认 ACL 条目时用户没有应用这些条目中的一条，那么这么条目会通过从访问 ACL 中拷贝相应的条目被自动插入，或者没有访问 ACL 时拷贝权限位。默认 ACL 也必须要有掩码。像上面描述的，如果没有指定掩码，那么通过计算过滤所有条目权限的掩码来自动插入一个掩码。\n\n当考虑一个有 ACL 的文件时，权限检查的算法变为：\n\n- 如果用户名与文件属主匹配，那么属主权限将被测试；\n- 否则如果用户名与命名用户条目中的一个用户名匹配，那么这些权限将被测试，并被掩码权限过滤；\n- 否则如果文件组与组列表任何一个组匹配，并且如果这些权限被掩码授权访问过滤，那么这些权限将被使用；\n- 否则如果有一个命名的组条目与一个组列表中的组匹配，并且如果这些权限被掩码授权访问过滤，那么这些权限将被使用；\n- 否则如果文件组或者任何一个命名组条目与组列表中的一个组匹配，但是没有被那些权限赋予访问权限，那么访问被拒绝；\n- 否则文件的其他权限将被测试。\n\n最佳实践是基于传统的权限位实现大部分权限需求，定义小数量的 ACL 作为例外规则来增强权限位。一个有 ACL 的文件比一个只有权限位的文件会引起 NameNode 额外的内存消耗。\n\n### ACL 文件系统 API\n\n新方法：\n\n- public void modifyAclEntries(Path path, List<AclEntry> aclSpec) throws IOException;\n- public void removeAclEntries(Path path, List<AclEntry> aclSpec) throws IOException;\n- public void public void removeDefaultAcl(Path path) throws IOException;\n- public void removeAcl(Path path) throws IOException;\n- public void setAcl(Path path, List<AclEntry> aclSpec) throws IOException;\n- public AclStatus getAclStatus(Path path) throws IOException;\n\n### ACL Shell 命令\n\n- hdfs dfs -getfacl [-R] <path>\n  展示文件和目录的访问控制列表（ACL）。如果一个目录有默认 ACL，那么 getfacl 也会展示默认 ACL。\n- hdfs dfs -setfacl [-R] [-b |-k -m |-x <acl_spec> <path>] |[--set <acl_spec> <path>]\n  设置文件和目录的访问控制列表（ACL）。\n- hdfs dfs -ls <args>\n  ls 的输出将在任何有 ACL 的文件和目录的权限字符串后追加一个 ‘+’ 字符。\n\n查看 [File System Shell](http://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-common/FileSystemShell.html) 文档获取这些命令的全部内容。\n\n### 配置参数\n\n- dfs.permissions.enabled = true\n  如果是则用这里描述的权限系统。如果否，权限检查被关闭，但所有其他行为不会改变。从一个参数值切换到另外一个参数值不会改变文件或目录的模式、属主或组。不管权限是开还是关，chmod、chgrp、chown 和 setfacl 总是检查权限。这些功能只在权限上下文中有用，因此没有向后兼容的问题。此外，这允许管理员在打开常规权限检查之前可靠的设置属主和权限。\n- dfs.web.ugi = webuser,webgroup\n  Web 服务器使用的用户名。设置这个为超级用户的名称允许所有 Web 客户端查看所有内容。修改这个为一个别的未用的身份允许 Web 客户端只能查看那些使用 “other” 权限可以访问的内容。另外，组可以添加到逗号分割的列表中。\n- dfs.permissions.superusergroup = supergroup\n  超级用户组的名称。\n- fs.permissions.umask-mode = 0022\n  创建文件和目录时使用的掩码。对于配置文件，可能使用十进制数值 18。\n- dfs.cluster.administrators = ACL-for-admins\n  为集群指定为 ACL 的管理员。控制谁可以在 HDFS 中访问默认 servlet 等。\n- dfs.namenode.acls.enabled = true\n  设置为 true 来启用 HDFS ACL（访问控制列表）支持。默认的，ACL 是禁用的。当 ACL 禁用时，NameNode 拒绝所有设置一个 ACL 的尝试。","slug":"HDFS-权限","published":1,"updated":"2021-07-19T16:28:00.248Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphmd002ditd39qf095sg","content":"<h3 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h3><p>HDFS 实现文件和目录权限的模式拥有许多 POSIX 模式。每个文件和目录都关联一个属主和组。文件或者目录针对属主用户、组中的其他用户以及其他用户有单独的权限。要读取文件内容必须有 r 权限，写入或者追加文件内容必须有 w 权限。要列出目录的内容必须有 r 权限，创建或者删除文件或者目录必须有 w 权限，访问目录的子目录必须有 x 权限。</p>\n<span id=\"more\"></span>\n\n<p>与 POSIX 模式相比，因为没有可执行文件的概念所以没有文件的 setuid 和 setgid 标识位。作为简化也没有目录的 setuid 和 setgid 标识位。在目录上设置粘滞位（Sticky bit）防止除了超级用户、目录属主或文件属主外的其他用户删除或者移动目录下的文件。给文件设置粘滞位没有任何效果。总的来说，文件和目录的权限都是这种模式。通常，使用 Unix 惯例来表现和显示模式，包括使用 8 进制数字。当创建文件或目录时，它的属主是客户端进程的用户，它的组是父目录的组（BSD 规则）。</p>\n<p>HDFS 也提供支持 POSIX ACL（访问控制列表）来增强针对特定用户或组细粒度的权限控制。ACL 稍后在本文档详细讨论。</p>\n<p>访问 HDFS 的每个客户端进程，包含用户名和组列表两部分身份。每当必须对一个客户端进程访问一个名字为 foo 的文件或者目录做鉴权时：</p>\n<ul>\n<li>如果用户名与 foo 的属主匹配，那么属主的权限被测试；</li>\n<li>否则如果 foo 的组匹配任意一个组列表中的组，那么组权限被测试；</li>\n<li>否则 foo 的其他权限被测试。</li>\n</ul>\n<p>如果权限检查失败，客户端操作也失败。</p>\n<h3 id=\"用户身份\"><a href=\"#用户身份\" class=\"headerlink\" title=\"用户身份\"></a>用户身份</h3><p>从 Hadoop 0.22 版本，Hadoop 支持两种识别用户身份的方式，通过 hadoop.security.authentication 属性指定：</p>\n<h4 id=\"简单模式\"><a href=\"#简单模式\" class=\"headerlink\" title=\"简单模式\"></a>简单模式</h4><p>用这种操作模式，通过主机操作系统确定客户端进程的身份。在类 Unix 系统上，用户名与 <code>whoami</code> 是等效的。</p>\n<h4 id=\"kerberos\"><a href=\"#kerberos\" class=\"headerlink\" title=\"kerberos\"></a>kerberos</h4><p>用基于 Kerberos 操作模式，通过 Kerberos 证书确定客户端进程的身份。例如，在一个 Kerberos 环境，一个用户可以使用 kinit 工具获取 Kerberos 的授予票据（TGT）并且使用 klist 确定他们当前的当事人。当映射一个 Kerberos 当事人到一个 HDFS 用户名，除了主要的组件外所有组件都被丢弃。例如，当事人 todd/<a href=\"mailto:&#102;&#x6f;&#x6f;&#98;&#97;&#114;&#64;&#67;&#x4f;&#82;&#x50;&#x2e;&#x43;&#79;&#77;&#80;&#65;&#78;&#89;&#46;&#x43;&#x4f;&#x4d;\">&#102;&#x6f;&#x6f;&#98;&#97;&#114;&#64;&#67;&#x4f;&#82;&#x50;&#x2e;&#x43;&#79;&#77;&#80;&#65;&#78;&#89;&#46;&#x43;&#x4f;&#x4d;</a> 将在 HDFS 上作为简单的 todd 用户名。</p>\n<p>不管什么操作模式，用户身份机制是 HDFS 本身外部的。HDFS 没有创建用户身份、建立组或处理用户凭据的的支持。</p>\n<h3 id=\"组映射\"><a href=\"#组映射\" class=\"headerlink\" title=\"组映射\"></a>组映射</h3><p>一旦一个用户像上面描述的那样被确定，组列表被通过 hadoop.security.group.mapping 属性配置的组映射服务确定。如果 Java 本地接口（JNI）是可用的，默认实现 org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback 将确定。如果 JNI 是可用的，实现将使用 Hadoop 的 API 决定一个用户的组列表。如果 JNI 不可用，那么会使用 Shell 实现 org.apache.hadoop.security.ShellBasedUnixGroupsMapping。这个实现 Shell 使用 bash -c groups 命令（Linux/Unix 环境）或者 net group 命令（Windows 环境）来决定一个用户的组列表。一个备选的实现是通过 org.apache.hadoop.security.LdapGroupsMapping 实现，直接连接 LDAP 服务器决定组列表。但是，这种方式只能在用户组唯一的在 LDAP 上且不在 Unix 服务器上具体化时使用。关于配置组映射服务更多的信息可以在 Javadoc 中获取。</p>\n<p>对于 HDFS，在 NameNode 上执行用户到组的映射。因此，NameNode 主机系统配置确定用户到组的映射。</p>\n<p>注意，HDFS 用字符串存储一个文件或者目录的用户和组；不像 Unix 那样需要从用户和组的身份编号转换。</p>\n<h3 id=\"理解具体实现\"><a href=\"#理解具体实现\" class=\"headerlink\" title=\"理解具体实现\"></a>理解具体实现</h3><p>每个文件或者目录的操作都传送全路径名称给 NameNode，NameNode 对每个操作的路径应用权限检查。客户端框架隐式的将用户身份与到 NameNode 的连接联系在一起，减少已存在的客户端 API 的修改。总是存在这样的场景，当对一个文件的操作成功后，因为文件或者在路径中的一些目录不存在了当重复这个操作时可能会失败。例如，当客户端开始读一个文件的时候，会首先发送第一个请求给 NameNode 来找出文件的第一个块所在的位置。第二个找出其他块的请求可能会失败。另一方面，正在删除文件的操作不能撤销一个已经知道文件块的客户端的访问。随着权限的添加，客户端对一个文件的访问可能在请求间被撤回。再者，变更权限不会撤销一个已经知道文件块的客户端的访问。</p>\n<h3 id=\"文件系统-API-的变化\"><a href=\"#文件系统-API-的变化\" class=\"headerlink\" title=\"文件系统 API 的变化\"></a>文件系统 API 的变化</h3><p>如果权限检查失败，所有使用路径参数的方法都会抛出 AccessControlException 异常。</p>\n<p>新方法：</p>\n<ul>\n<li>public FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress) throws IOException;</li>\n<li>public boolean mkdirs(Path f, FsPermission permission) throws IOException;</li>\n<li>public void setPermission(Path p, FsPermission permission) throws IOException;</li>\n<li>public void setOwner(Path p, String username, String groupname) throws IOException;</li>\n<li>public FileStatus getFileStatus(Path f) throws IOException; 还将返回与路径相关联的用户、组和模式。</li>\n</ul>\n<p>一个新文件或者目录的模式通过掩码组配置参数被限制。当现有的 create(path, …) 方法（不带权限参数）被使用时，新文件的模式是 0666 &amp; ^umask。当新的 create(path, permission, …) 方法（带权限参数 P）被使用时，新文件的模式是 P &amp; ^umask &amp; 0666。当使用现有的 mkdirs(path) 方法（不带权限参数）创建一个新目录时，新目录的模式是 0777 &amp; ^umask。当新的 mkdirs(path, permission) 方法（带权限参数 P）被使用时，新目录的模式是 P &amp; ^umask &amp; 0777。</p>\n<h3 id=\"应用程序-Shell-的变化\"><a href=\"#应用程序-Shell-的变化\" class=\"headerlink\" title=\"应用程序 Shell 的变化\"></a>应用程序 Shell 的变化</h3><p>新操作：</p>\n<ul>\n<li>chmod [-R] mode file …<br>只有文件属主或者超级用户才有权限修改文件的模式。</li>\n<li>chgrp [-R] group file …<br>调用 chgrp 的用户必须属于特定的组并且是文件的属主或者超级用户。</li>\n<li>chown [-R] [owner][:[group]] file …<br>一个文件的所有者只能由超级用户修改。</li>\n<li>ls file …</li>\n<li>lsr file …<br>输出被重新格式化来展示属主、组和模式。</li>\n</ul>\n<h3 id=\"超级用户\"><a href=\"#超级用户\" class=\"headerlink\" title=\"超级用户\"></a>超级用户</h3><p>超级用户是与 NameNode 进程自己有相同身份的用户。简单的，如果你启动 NameNode，那么你就是超级用户。超级用户可以做任何事情，超级用户的权限检查从不会失败。谁是超级用户没有不变的概念，当 NameNode 启动后进程的身份决定了现在的超级用户是谁。HDFS 的超级用户不必是 NameNode 主机的超级用户，也没有必要，所有的集群有相同的超级用户。同时，一个试验者在个人工作站上运行 HDFS 便可以不需要任何配置很方便的成为安装的超级用户。</p>\n<p>此外，使用配置参数区别一个管理员组。如果设置，这个组的成员都是超级用户。</p>\n<h3 id=\"Web-服务器\"><a href=\"#Web-服务器\" class=\"headerlink\" title=\"Web 服务器\"></a>Web 服务器</h3><p>默认的，Web 服务器的身份是一个配置参数。也就是，NameNode 没有真实用户的概念，但是，Web 服务器表现为管理员选择的有身份（用户和组）的用户。除非选择与超级用户匹配的身份，否则部分名称空间 Web 服务器是无法访问的。</p>\n<h3 id=\"ACL（访问控制列表）\"><a href=\"#ACL（访问控制列表）\" class=\"headerlink\" title=\"ACL（访问控制列表）\"></a>ACL（访问控制列表）</h3><p>除了传统的 POSIX 权限模型，HDFS 也支持 POSIX ACL（访问控制列表）。对于实现不同于用户和组自然组织层次的权限需求，ACL 是很有帮助的。一个 ACL 提供了一种对指定的用户和组设置不同权限的方法，不仅仅是文件的属主和文件的组。</p>\n<p>默认的，对 ACL 的支持是禁用的，并且 NameNode 不允许创建 ACL。要启用 ACL 支持，在 NameNode 配置中设置 dfs.namenode.acls.enabled 为 true。</p>\n<p>一个 ACL 由一组 ACL 条目组成。每条 ACL 条目确定一个用户或组并为该用户或组授权或拒绝读、写和执行权限。例如：</p>\n<pre><code>user::rw-\nuser:bruce:rwx                  #effective:r--\ngroup::r-x                      #effective:r--\ngroup:sales:rwx                 #effective:r--\nmask::r--\nother::r--\n</code></pre>\n<p>ACL 条目包含一个类型、一个可选的名称和一个权限字符串。为了显示目的，‘:’用作每个域的分隔符。在这个示例 ACL 中，文件的属主有读写访问权限，文件的组有读和执行访问权限，其他用户有读访问权限。目前为止，这相当于设置文件的权限位为 654。</p>\n<p>另外，有两个针对名称为 bruce 的用户和名称为 sales 的组的扩展 ACL 条目，两个都赋予了全部访问权限。掩码是一个特殊的 ACL 条目，用来过滤所有赋予命名的用户条目和命名的组条目的权限，以及未命名的组条目。在这个示例中，掩码只有读权限，我们可以看到几个 ACL 条目的有效权限因此被过滤掉了。</p>\n<p>每个 ACL 都必须有掩码。如果一个用户在设置 ACL 时没有应用掩码，那么通过计算过滤所有条目权限的掩码来自动插入一个掩码。</p>\n<p>在一个有 ACL 的文件上运行 chmod，实际是改变了掩码的权限。因为掩码作为一个过滤器，这有效的约束所有扩展 ACL 条目的权限，而不仅是改变组条目并且遗漏其他扩展的 ACL 条目。</p>\n<p>这个模型也区分“访问 ACL”和“默认 ACL”，“访问 ACL”定义在权限检查时强制执行的规则，“默认 ACL”定义子文件或者子目录创建时自动接收到的 ACL 条目。例如：</p>\n<pre><code>user::rwx\ngroup::r-x\nother::r-x\ndefault:user::rwx\ndefault:user:bruce:rwx          #effective:r-x\ndefault:group::r-x\ndefault:group:sales:rwx         #effective:r-x\ndefault:mask::r-x\ndefault:other::r-x\n</code></pre>\n<p>只有目录才有默认 ACL。当一个新文件或者子目录创建时，会自动拷贝它的父目录的默认 ACL 到它自己的访问 ACL 中。一个新子目录也拷贝它到自己的默认 ACL 中。用这种方式，默认 ACL 在新子目录创建时被拷贝到文件系统目录树任意级别的深度。</p>\n<p>在新子文件的访问 ACL 中确切的权限值是服从模式参数过滤的。就默认 022 掩码而言，这是典型的新目录的 755 和 新文件的 644。模式参数对没有命名的用户（文件属主）进行过滤拷贝的权限值、掩码和其他。使用这个特定的 ACL 示例，并创建一个模式为 755 的新子目录，这个模式对最终结果没有任何过滤作用。然而，如果我们考虑创建一个模式为 644 的文件，那么模式过滤使新文件的 ACL 针对未命名用户（文件属主）接收到读写权限，mask 为读权限且其他用户为读权限。这个掩码意味着对命名为 bruce 的用户和命名为 sales 的组的有效权限只有读。</p>\n<p>注意，拷贝发生在创建新文件或者子目录的时候。之后对父目录默认 ACL 的改变不会改变已经存在的子目录。</p>\n<p>默认的 ACL 必须有所有最小要求的 ACL 条目，包含未命名用户（文件属主）、未命名的组（文件组）和其他用户条目。如果当设置一个默认 ACL 条目时用户没有应用这些条目中的一条，那么这么条目会通过从访问 ACL 中拷贝相应的条目被自动插入，或者没有访问 ACL 时拷贝权限位。默认 ACL 也必须要有掩码。像上面描述的，如果没有指定掩码，那么通过计算过滤所有条目权限的掩码来自动插入一个掩码。</p>\n<p>当考虑一个有 ACL 的文件时，权限检查的算法变为：</p>\n<ul>\n<li>如果用户名与文件属主匹配，那么属主权限将被测试；</li>\n<li>否则如果用户名与命名用户条目中的一个用户名匹配，那么这些权限将被测试，并被掩码权限过滤；</li>\n<li>否则如果文件组与组列表任何一个组匹配，并且如果这些权限被掩码授权访问过滤，那么这些权限将被使用；</li>\n<li>否则如果有一个命名的组条目与一个组列表中的组匹配，并且如果这些权限被掩码授权访问过滤，那么这些权限将被使用；</li>\n<li>否则如果文件组或者任何一个命名组条目与组列表中的一个组匹配，但是没有被那些权限赋予访问权限，那么访问被拒绝；</li>\n<li>否则文件的其他权限将被测试。</li>\n</ul>\n<p>最佳实践是基于传统的权限位实现大部分权限需求，定义小数量的 ACL 作为例外规则来增强权限位。一个有 ACL 的文件比一个只有权限位的文件会引起 NameNode 额外的内存消耗。</p>\n<h3 id=\"ACL-文件系统-API\"><a href=\"#ACL-文件系统-API\" class=\"headerlink\" title=\"ACL 文件系统 API\"></a>ACL 文件系统 API</h3><p>新方法：</p>\n<ul>\n<li>public void modifyAclEntries(Path path, List<AclEntry> aclSpec) throws IOException;</li>\n<li>public void removeAclEntries(Path path, List<AclEntry> aclSpec) throws IOException;</li>\n<li>public void public void removeDefaultAcl(Path path) throws IOException;</li>\n<li>public void removeAcl(Path path) throws IOException;</li>\n<li>public void setAcl(Path path, List<AclEntry> aclSpec) throws IOException;</li>\n<li>public AclStatus getAclStatus(Path path) throws IOException;</li>\n</ul>\n<h3 id=\"ACL-Shell-命令\"><a href=\"#ACL-Shell-命令\" class=\"headerlink\" title=\"ACL Shell 命令\"></a>ACL Shell 命令</h3><ul>\n<li>hdfs dfs -getfacl [-R] <path><br>展示文件和目录的访问控制列表（ACL）。如果一个目录有默认 ACL，那么 getfacl 也会展示默认 ACL。</li>\n<li>hdfs dfs -setfacl [-R] [-b |-k -m |-x <acl_spec> <path>] |[–set <acl_spec> <path>]<br>设置文件和目录的访问控制列表（ACL）。</li>\n<li>hdfs dfs -ls <args><br>ls 的输出将在任何有 ACL 的文件和目录的权限字符串后追加一个 ‘+’ 字符。</li>\n</ul>\n<p>查看 <a href=\"http://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-common/FileSystemShell.html\">File System Shell</a> 文档获取这些命令的全部内容。</p>\n<h3 id=\"配置参数\"><a href=\"#配置参数\" class=\"headerlink\" title=\"配置参数\"></a>配置参数</h3><ul>\n<li>dfs.permissions.enabled = true<br>如果是则用这里描述的权限系统。如果否，权限检查被关闭，但所有其他行为不会改变。从一个参数值切换到另外一个参数值不会改变文件或目录的模式、属主或组。不管权限是开还是关，chmod、chgrp、chown 和 setfacl 总是检查权限。这些功能只在权限上下文中有用，因此没有向后兼容的问题。此外，这允许管理员在打开常规权限检查之前可靠的设置属主和权限。</li>\n<li>dfs.web.ugi = webuser,webgroup<br>Web 服务器使用的用户名。设置这个为超级用户的名称允许所有 Web 客户端查看所有内容。修改这个为一个别的未用的身份允许 Web 客户端只能查看那些使用 “other” 权限可以访问的内容。另外，组可以添加到逗号分割的列表中。</li>\n<li>dfs.permissions.superusergroup = supergroup<br>超级用户组的名称。</li>\n<li>fs.permissions.umask-mode = 0022<br>创建文件和目录时使用的掩码。对于配置文件，可能使用十进制数值 18。</li>\n<li>dfs.cluster.administrators = ACL-for-admins<br>为集群指定为 ACL 的管理员。控制谁可以在 HDFS 中访问默认 servlet 等。</li>\n<li>dfs.namenode.acls.enabled = true<br>设置为 true 来启用 HDFS ACL（访问控制列表）支持。默认的，ACL 是禁用的。当 ACL 禁用时，NameNode 拒绝所有设置一个 ACL 的尝试。</li>\n</ul>\n","site":{"data":{}},"excerpt":"<h3 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h3><p>HDFS 实现文件和目录权限的模式拥有许多 POSIX 模式。每个文件和目录都关联一个属主和组。文件或者目录针对属主用户、组中的其他用户以及其他用户有单独的权限。要读取文件内容必须有 r 权限，写入或者追加文件内容必须有 w 权限。要列出目录的内容必须有 r 权限，创建或者删除文件或者目录必须有 w 权限，访问目录的子目录必须有 x 权限。</p>","more":"<p>与 POSIX 模式相比，因为没有可执行文件的概念所以没有文件的 setuid 和 setgid 标识位。作为简化也没有目录的 setuid 和 setgid 标识位。在目录上设置粘滞位（Sticky bit）防止除了超级用户、目录属主或文件属主外的其他用户删除或者移动目录下的文件。给文件设置粘滞位没有任何效果。总的来说，文件和目录的权限都是这种模式。通常，使用 Unix 惯例来表现和显示模式，包括使用 8 进制数字。当创建文件或目录时，它的属主是客户端进程的用户，它的组是父目录的组（BSD 规则）。</p>\n<p>HDFS 也提供支持 POSIX ACL（访问控制列表）来增强针对特定用户或组细粒度的权限控制。ACL 稍后在本文档详细讨论。</p>\n<p>访问 HDFS 的每个客户端进程，包含用户名和组列表两部分身份。每当必须对一个客户端进程访问一个名字为 foo 的文件或者目录做鉴权时：</p>\n<ul>\n<li>如果用户名与 foo 的属主匹配，那么属主的权限被测试；</li>\n<li>否则如果 foo 的组匹配任意一个组列表中的组，那么组权限被测试；</li>\n<li>否则 foo 的其他权限被测试。</li>\n</ul>\n<p>如果权限检查失败，客户端操作也失败。</p>\n<h3 id=\"用户身份\"><a href=\"#用户身份\" class=\"headerlink\" title=\"用户身份\"></a>用户身份</h3><p>从 Hadoop 0.22 版本，Hadoop 支持两种识别用户身份的方式，通过 hadoop.security.authentication 属性指定：</p>\n<h4 id=\"简单模式\"><a href=\"#简单模式\" class=\"headerlink\" title=\"简单模式\"></a>简单模式</h4><p>用这种操作模式，通过主机操作系统确定客户端进程的身份。在类 Unix 系统上，用户名与 <code>whoami</code> 是等效的。</p>\n<h4 id=\"kerberos\"><a href=\"#kerberos\" class=\"headerlink\" title=\"kerberos\"></a>kerberos</h4><p>用基于 Kerberos 操作模式，通过 Kerberos 证书确定客户端进程的身份。例如，在一个 Kerberos 环境，一个用户可以使用 kinit 工具获取 Kerberos 的授予票据（TGT）并且使用 klist 确定他们当前的当事人。当映射一个 Kerberos 当事人到一个 HDFS 用户名，除了主要的组件外所有组件都被丢弃。例如，当事人 todd/<a href=\"mailto:&#102;&#x6f;&#x6f;&#98;&#97;&#114;&#64;&#67;&#x4f;&#82;&#x50;&#x2e;&#x43;&#79;&#77;&#80;&#65;&#78;&#89;&#46;&#x43;&#x4f;&#x4d;\">&#102;&#x6f;&#x6f;&#98;&#97;&#114;&#64;&#67;&#x4f;&#82;&#x50;&#x2e;&#x43;&#79;&#77;&#80;&#65;&#78;&#89;&#46;&#x43;&#x4f;&#x4d;</a> 将在 HDFS 上作为简单的 todd 用户名。</p>\n<p>不管什么操作模式，用户身份机制是 HDFS 本身外部的。HDFS 没有创建用户身份、建立组或处理用户凭据的的支持。</p>\n<h3 id=\"组映射\"><a href=\"#组映射\" class=\"headerlink\" title=\"组映射\"></a>组映射</h3><p>一旦一个用户像上面描述的那样被确定，组列表被通过 hadoop.security.group.mapping 属性配置的组映射服务确定。如果 Java 本地接口（JNI）是可用的，默认实现 org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback 将确定。如果 JNI 是可用的，实现将使用 Hadoop 的 API 决定一个用户的组列表。如果 JNI 不可用，那么会使用 Shell 实现 org.apache.hadoop.security.ShellBasedUnixGroupsMapping。这个实现 Shell 使用 bash -c groups 命令（Linux/Unix 环境）或者 net group 命令（Windows 环境）来决定一个用户的组列表。一个备选的实现是通过 org.apache.hadoop.security.LdapGroupsMapping 实现，直接连接 LDAP 服务器决定组列表。但是，这种方式只能在用户组唯一的在 LDAP 上且不在 Unix 服务器上具体化时使用。关于配置组映射服务更多的信息可以在 Javadoc 中获取。</p>\n<p>对于 HDFS，在 NameNode 上执行用户到组的映射。因此，NameNode 主机系统配置确定用户到组的映射。</p>\n<p>注意，HDFS 用字符串存储一个文件或者目录的用户和组；不像 Unix 那样需要从用户和组的身份编号转换。</p>\n<h3 id=\"理解具体实现\"><a href=\"#理解具体实现\" class=\"headerlink\" title=\"理解具体实现\"></a>理解具体实现</h3><p>每个文件或者目录的操作都传送全路径名称给 NameNode，NameNode 对每个操作的路径应用权限检查。客户端框架隐式的将用户身份与到 NameNode 的连接联系在一起，减少已存在的客户端 API 的修改。总是存在这样的场景，当对一个文件的操作成功后，因为文件或者在路径中的一些目录不存在了当重复这个操作时可能会失败。例如，当客户端开始读一个文件的时候，会首先发送第一个请求给 NameNode 来找出文件的第一个块所在的位置。第二个找出其他块的请求可能会失败。另一方面，正在删除文件的操作不能撤销一个已经知道文件块的客户端的访问。随着权限的添加，客户端对一个文件的访问可能在请求间被撤回。再者，变更权限不会撤销一个已经知道文件块的客户端的访问。</p>\n<h3 id=\"文件系统-API-的变化\"><a href=\"#文件系统-API-的变化\" class=\"headerlink\" title=\"文件系统 API 的变化\"></a>文件系统 API 的变化</h3><p>如果权限检查失败，所有使用路径参数的方法都会抛出 AccessControlException 异常。</p>\n<p>新方法：</p>\n<ul>\n<li>public FSDataOutputStream create(Path f, FsPermission permission, boolean overwrite, int bufferSize, short replication, long blockSize, Progressable progress) throws IOException;</li>\n<li>public boolean mkdirs(Path f, FsPermission permission) throws IOException;</li>\n<li>public void setPermission(Path p, FsPermission permission) throws IOException;</li>\n<li>public void setOwner(Path p, String username, String groupname) throws IOException;</li>\n<li>public FileStatus getFileStatus(Path f) throws IOException; 还将返回与路径相关联的用户、组和模式。</li>\n</ul>\n<p>一个新文件或者目录的模式通过掩码组配置参数被限制。当现有的 create(path, …) 方法（不带权限参数）被使用时，新文件的模式是 0666 &amp; ^umask。当新的 create(path, permission, …) 方法（带权限参数 P）被使用时，新文件的模式是 P &amp; ^umask &amp; 0666。当使用现有的 mkdirs(path) 方法（不带权限参数）创建一个新目录时，新目录的模式是 0777 &amp; ^umask。当新的 mkdirs(path, permission) 方法（带权限参数 P）被使用时，新目录的模式是 P &amp; ^umask &amp; 0777。</p>\n<h3 id=\"应用程序-Shell-的变化\"><a href=\"#应用程序-Shell-的变化\" class=\"headerlink\" title=\"应用程序 Shell 的变化\"></a>应用程序 Shell 的变化</h3><p>新操作：</p>\n<ul>\n<li>chmod [-R] mode file …<br>只有文件属主或者超级用户才有权限修改文件的模式。</li>\n<li>chgrp [-R] group file …<br>调用 chgrp 的用户必须属于特定的组并且是文件的属主或者超级用户。</li>\n<li>chown [-R] [owner][:[group]] file …<br>一个文件的所有者只能由超级用户修改。</li>\n<li>ls file …</li>\n<li>lsr file …<br>输出被重新格式化来展示属主、组和模式。</li>\n</ul>\n<h3 id=\"超级用户\"><a href=\"#超级用户\" class=\"headerlink\" title=\"超级用户\"></a>超级用户</h3><p>超级用户是与 NameNode 进程自己有相同身份的用户。简单的，如果你启动 NameNode，那么你就是超级用户。超级用户可以做任何事情，超级用户的权限检查从不会失败。谁是超级用户没有不变的概念，当 NameNode 启动后进程的身份决定了现在的超级用户是谁。HDFS 的超级用户不必是 NameNode 主机的超级用户，也没有必要，所有的集群有相同的超级用户。同时，一个试验者在个人工作站上运行 HDFS 便可以不需要任何配置很方便的成为安装的超级用户。</p>\n<p>此外，使用配置参数区别一个管理员组。如果设置，这个组的成员都是超级用户。</p>\n<h3 id=\"Web-服务器\"><a href=\"#Web-服务器\" class=\"headerlink\" title=\"Web 服务器\"></a>Web 服务器</h3><p>默认的，Web 服务器的身份是一个配置参数。也就是，NameNode 没有真实用户的概念，但是，Web 服务器表现为管理员选择的有身份（用户和组）的用户。除非选择与超级用户匹配的身份，否则部分名称空间 Web 服务器是无法访问的。</p>\n<h3 id=\"ACL（访问控制列表）\"><a href=\"#ACL（访问控制列表）\" class=\"headerlink\" title=\"ACL（访问控制列表）\"></a>ACL（访问控制列表）</h3><p>除了传统的 POSIX 权限模型，HDFS 也支持 POSIX ACL（访问控制列表）。对于实现不同于用户和组自然组织层次的权限需求，ACL 是很有帮助的。一个 ACL 提供了一种对指定的用户和组设置不同权限的方法，不仅仅是文件的属主和文件的组。</p>\n<p>默认的，对 ACL 的支持是禁用的，并且 NameNode 不允许创建 ACL。要启用 ACL 支持，在 NameNode 配置中设置 dfs.namenode.acls.enabled 为 true。</p>\n<p>一个 ACL 由一组 ACL 条目组成。每条 ACL 条目确定一个用户或组并为该用户或组授权或拒绝读、写和执行权限。例如：</p>\n<pre><code>user::rw-\nuser:bruce:rwx                  #effective:r--\ngroup::r-x                      #effective:r--\ngroup:sales:rwx                 #effective:r--\nmask::r--\nother::r--\n</code></pre>\n<p>ACL 条目包含一个类型、一个可选的名称和一个权限字符串。为了显示目的，‘:’用作每个域的分隔符。在这个示例 ACL 中，文件的属主有读写访问权限，文件的组有读和执行访问权限，其他用户有读访问权限。目前为止，这相当于设置文件的权限位为 654。</p>\n<p>另外，有两个针对名称为 bruce 的用户和名称为 sales 的组的扩展 ACL 条目，两个都赋予了全部访问权限。掩码是一个特殊的 ACL 条目，用来过滤所有赋予命名的用户条目和命名的组条目的权限，以及未命名的组条目。在这个示例中，掩码只有读权限，我们可以看到几个 ACL 条目的有效权限因此被过滤掉了。</p>\n<p>每个 ACL 都必须有掩码。如果一个用户在设置 ACL 时没有应用掩码，那么通过计算过滤所有条目权限的掩码来自动插入一个掩码。</p>\n<p>在一个有 ACL 的文件上运行 chmod，实际是改变了掩码的权限。因为掩码作为一个过滤器，这有效的约束所有扩展 ACL 条目的权限，而不仅是改变组条目并且遗漏其他扩展的 ACL 条目。</p>\n<p>这个模型也区分“访问 ACL”和“默认 ACL”，“访问 ACL”定义在权限检查时强制执行的规则，“默认 ACL”定义子文件或者子目录创建时自动接收到的 ACL 条目。例如：</p>\n<pre><code>user::rwx\ngroup::r-x\nother::r-x\ndefault:user::rwx\ndefault:user:bruce:rwx          #effective:r-x\ndefault:group::r-x\ndefault:group:sales:rwx         #effective:r-x\ndefault:mask::r-x\ndefault:other::r-x\n</code></pre>\n<p>只有目录才有默认 ACL。当一个新文件或者子目录创建时，会自动拷贝它的父目录的默认 ACL 到它自己的访问 ACL 中。一个新子目录也拷贝它到自己的默认 ACL 中。用这种方式，默认 ACL 在新子目录创建时被拷贝到文件系统目录树任意级别的深度。</p>\n<p>在新子文件的访问 ACL 中确切的权限值是服从模式参数过滤的。就默认 022 掩码而言，这是典型的新目录的 755 和 新文件的 644。模式参数对没有命名的用户（文件属主）进行过滤拷贝的权限值、掩码和其他。使用这个特定的 ACL 示例，并创建一个模式为 755 的新子目录，这个模式对最终结果没有任何过滤作用。然而，如果我们考虑创建一个模式为 644 的文件，那么模式过滤使新文件的 ACL 针对未命名用户（文件属主）接收到读写权限，mask 为读权限且其他用户为读权限。这个掩码意味着对命名为 bruce 的用户和命名为 sales 的组的有效权限只有读。</p>\n<p>注意，拷贝发生在创建新文件或者子目录的时候。之后对父目录默认 ACL 的改变不会改变已经存在的子目录。</p>\n<p>默认的 ACL 必须有所有最小要求的 ACL 条目，包含未命名用户（文件属主）、未命名的组（文件组）和其他用户条目。如果当设置一个默认 ACL 条目时用户没有应用这些条目中的一条，那么这么条目会通过从访问 ACL 中拷贝相应的条目被自动插入，或者没有访问 ACL 时拷贝权限位。默认 ACL 也必须要有掩码。像上面描述的，如果没有指定掩码，那么通过计算过滤所有条目权限的掩码来自动插入一个掩码。</p>\n<p>当考虑一个有 ACL 的文件时，权限检查的算法变为：</p>\n<ul>\n<li>如果用户名与文件属主匹配，那么属主权限将被测试；</li>\n<li>否则如果用户名与命名用户条目中的一个用户名匹配，那么这些权限将被测试，并被掩码权限过滤；</li>\n<li>否则如果文件组与组列表任何一个组匹配，并且如果这些权限被掩码授权访问过滤，那么这些权限将被使用；</li>\n<li>否则如果有一个命名的组条目与一个组列表中的组匹配，并且如果这些权限被掩码授权访问过滤，那么这些权限将被使用；</li>\n<li>否则如果文件组或者任何一个命名组条目与组列表中的一个组匹配，但是没有被那些权限赋予访问权限，那么访问被拒绝；</li>\n<li>否则文件的其他权限将被测试。</li>\n</ul>\n<p>最佳实践是基于传统的权限位实现大部分权限需求，定义小数量的 ACL 作为例外规则来增强权限位。一个有 ACL 的文件比一个只有权限位的文件会引起 NameNode 额外的内存消耗。</p>\n<h3 id=\"ACL-文件系统-API\"><a href=\"#ACL-文件系统-API\" class=\"headerlink\" title=\"ACL 文件系统 API\"></a>ACL 文件系统 API</h3><p>新方法：</p>\n<ul>\n<li>public void modifyAclEntries(Path path, List<AclEntry> aclSpec) throws IOException;</li>\n<li>public void removeAclEntries(Path path, List<AclEntry> aclSpec) throws IOException;</li>\n<li>public void public void removeDefaultAcl(Path path) throws IOException;</li>\n<li>public void removeAcl(Path path) throws IOException;</li>\n<li>public void setAcl(Path path, List<AclEntry> aclSpec) throws IOException;</li>\n<li>public AclStatus getAclStatus(Path path) throws IOException;</li>\n</ul>\n<h3 id=\"ACL-Shell-命令\"><a href=\"#ACL-Shell-命令\" class=\"headerlink\" title=\"ACL Shell 命令\"></a>ACL Shell 命令</h3><ul>\n<li>hdfs dfs -getfacl [-R] <path><br>展示文件和目录的访问控制列表（ACL）。如果一个目录有默认 ACL，那么 getfacl 也会展示默认 ACL。</li>\n<li>hdfs dfs -setfacl [-R] [-b |-k -m |-x <acl_spec> <path>] |[–set <acl_spec> <path>]<br>设置文件和目录的访问控制列表（ACL）。</li>\n<li>hdfs dfs -ls <args><br>ls 的输出将在任何有 ACL 的文件和目录的权限字符串后追加一个 ‘+’ 字符。</li>\n</ul>\n<p>查看 <a href=\"http://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-common/FileSystemShell.html\">File System Shell</a> 文档获取这些命令的全部内容。</p>\n<h3 id=\"配置参数\"><a href=\"#配置参数\" class=\"headerlink\" title=\"配置参数\"></a>配置参数</h3><ul>\n<li>dfs.permissions.enabled = true<br>如果是则用这里描述的权限系统。如果否，权限检查被关闭，但所有其他行为不会改变。从一个参数值切换到另外一个参数值不会改变文件或目录的模式、属主或组。不管权限是开还是关，chmod、chgrp、chown 和 setfacl 总是检查权限。这些功能只在权限上下文中有用，因此没有向后兼容的问题。此外，这允许管理员在打开常规权限检查之前可靠的设置属主和权限。</li>\n<li>dfs.web.ugi = webuser,webgroup<br>Web 服务器使用的用户名。设置这个为超级用户的名称允许所有 Web 客户端查看所有内容。修改这个为一个别的未用的身份允许 Web 客户端只能查看那些使用 “other” 权限可以访问的内容。另外，组可以添加到逗号分割的列表中。</li>\n<li>dfs.permissions.superusergroup = supergroup<br>超级用户组的名称。</li>\n<li>fs.permissions.umask-mode = 0022<br>创建文件和目录时使用的掩码。对于配置文件，可能使用十进制数值 18。</li>\n<li>dfs.cluster.administrators = ACL-for-admins<br>为集群指定为 ACL 的管理员。控制谁可以在 HDFS 中访问默认 servlet 等。</li>\n<li>dfs.namenode.acls.enabled = true<br>设置为 true 来启用 HDFS ACL（访问控制列表）支持。默认的，ACL 是禁用的。当 ACL 禁用时，NameNode 拒绝所有设置一个 ACL 的尝试。</li>\n</ul>"},{"title":"Hadoop 2.7.3 文档 expunge 命令排版错误","date":"2017-03-11T22:57:58.000Z","_content":"\n在 Hadoop 2.7.3 版本的官方文档中 File System Shell 中的 expunge 命令描述中以下位置存在错误：\n\nWhen checkpoint is created, recently deleted files in trash are moved under the checkpoint. Files in checkpoints older than <span style=\"color:red;\">**fs.trash.checkpoint.interval**</span> will be permanently deleted on the next invocation of -expunge command.\n\n<!-- more -->\n\n红色字体标注的参数应该是 <span style=\"color:red;\">**fs.trash.interval**</span>。这个排版错误已经在  2.8.0 和 3.0.0-alpha1 中修复，参见 Jira Issue：<https://issues.apache.org/jira/browse/HADOOP-12675>","source":"_posts/Hadoop-2-7-3-文档-expunge-命令排版错误.md","raw":"title: Hadoop 2.7.3 文档 expunge 命令排版错误\ntags:\n  - Hadoop\ncategories:\n  - 大数据\n  - Hadoop\ndate: 2017-03-12 06:57:58\n---\n\n在 Hadoop 2.7.3 版本的官方文档中 File System Shell 中的 expunge 命令描述中以下位置存在错误：\n\nWhen checkpoint is created, recently deleted files in trash are moved under the checkpoint. Files in checkpoints older than <span style=\"color:red;\">**fs.trash.checkpoint.interval**</span> will be permanently deleted on the next invocation of -expunge command.\n\n<!-- more -->\n\n红色字体标注的参数应该是 <span style=\"color:red;\">**fs.trash.interval**</span>。这个排版错误已经在  2.8.0 和 3.0.0-alpha1 中修复，参见 Jira Issue：<https://issues.apache.org/jira/browse/HADOOP-12675>","slug":"Hadoop-2-7-3-文档-expunge-命令排版错误","published":1,"updated":"2021-07-19T16:28:00.248Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphme002fitd30pfzb3m1","content":"<p>在 Hadoop 2.7.3 版本的官方文档中 File System Shell 中的 expunge 命令描述中以下位置存在错误：</p>\n<p>When checkpoint is created, recently deleted files in trash are moved under the checkpoint. Files in checkpoints older than <span style=\"color:red;\"><strong>fs.trash.checkpoint.interval</strong></span> will be permanently deleted on the next invocation of -expunge command.</p>\n<span id=\"more\"></span>\n\n<p>红色字体标注的参数应该是 <span style=\"color:red;\"><strong>fs.trash.interval</strong></span>。这个排版错误已经在  2.8.0 和 3.0.0-alpha1 中修复，参见 Jira Issue：<a href=\"https://issues.apache.org/jira/browse/HADOOP-12675\">https://issues.apache.org/jira/browse/HADOOP-12675</a></p>\n","site":{"data":{}},"excerpt":"<p>在 Hadoop 2.7.3 版本的官方文档中 File System Shell 中的 expunge 命令描述中以下位置存在错误：</p>\n<p>When checkpoint is created, recently deleted files in trash are moved under the checkpoint. Files in checkpoints older than <span style=\"color:red;\"><strong>fs.trash.checkpoint.interval</strong></span> will be permanently deleted on the next invocation of -expunge command.</p>","more":"<p>红色字体标注的参数应该是 <span style=\"color:red;\"><strong>fs.trash.interval</strong></span>。这个排版错误已经在  2.8.0 和 3.0.0-alpha1 中修复，参见 Jira Issue：<a href=\"https://issues.apache.org/jira/browse/HADOOP-12675\">https://issues.apache.org/jira/browse/HADOOP-12675</a></p>"},{"title":"Hadoop DataNode 磁盘扩容","date":"2018-03-18T03:06:56.000Z","_content":"\n\nDataNode 是支持热插拔磁盘的，所以磁盘扩容比较简单。磁盘格式化后挂载完毕。\n\n<!-- more -->\n\n修改 hdfs-site.xml 配置文件，将新增的磁盘目录加到配置中：\n\n    <property>\n      <name>dfs.datanode.data.dir</name>\n      <value>/data0/hadoop/dfs/data,/data1/hadoop/dfs/data</value>\n    </property>\n\n修改 yarn-site.xml 配置文件，将新增的磁盘目录加到配置中：\n\n    <property>\n      <name>yarn.nodemanager.local-dirs</name>\n      <value>/data0/hadoop/yarn/data,/data1/hadoop/yarn/data</value>\n    </property>\n    <property>\n      <name>yarn.nodemanager.log-dirs</name>\n      <value>/data0/hadoop/yarn/log,/data1/hadoop/yarn/log</value>\n    </property>\n\n配置修改完成后将配置文件分发到所有的节点。依次重启 DataNode 和 NodeManager。\n\n如果只为 DataNode 新增磁盘，可以不用重启 DataNode，使用 reconfig 刷新配置即可：\n\n    $ hdfs dfsadmin -reconfig datanode 192.168.72.2:50020 start\n    Started reconfiguration task on DataNode 192.168.72.2:50020\n\n查看重新配置执行状态的时候会有误报的错误信息，这个问题的跟踪地址：<https://issues.apache.org/jira/browse/HDFS-8582>。错误信息如下：\n\n    $ hdfs dfsadmin -reconfig datanode 192.168.72.2:50020 status\n    Reconfiguring status for DataNode[192.168.72.2:50020]: started at Thu Mar 08 17:56:10 CST 2018 and finished at Thu Mar 08 17:56:11 CST 2018.\n    FAILED: Change property yarn.nodemanager.log-dirs\n\tFrom: \"/data0/hadoop/yarn/log\"\n\tTo: \"/data0/hadoop/yarn/log,/data1/hadoop/yarn/log\"\n\tError: Property yarn.nodemanager.log-dirs is not reconfigurable.\n    FAILED: Change property rpc.engine.org.apache.hadoop.ipc.ProtocolMetaInfoPB\n\tFrom: \"org.apache.hadoop.ipc.ProtobufRpcEngine\"\n\tTo: \"\"\n\tError: Property rpc.engine.org.apache.hadoop.ipc.ProtocolMetaInfoPB is not reconfigurable.\n    FAILED: Change property dfs.datanode.startup\n\tFrom: \"REGULAR\"\n\tTo: \"\"\n\tError: Property dfs.datanode.startup is not reconfigurable.\n","source":"_posts/Hadoop-DataNode-磁盘扩容.md","raw":"title: Hadoop DataNode 磁盘扩容\ntags:\n  - Hadoop\n  - HDFS\ncategories:\n  - 大数据\n  - Hadoop\ndate: 2018-03-18 11:06:56\n---\n\n\nDataNode 是支持热插拔磁盘的，所以磁盘扩容比较简单。磁盘格式化后挂载完毕。\n\n<!-- more -->\n\n修改 hdfs-site.xml 配置文件，将新增的磁盘目录加到配置中：\n\n    <property>\n      <name>dfs.datanode.data.dir</name>\n      <value>/data0/hadoop/dfs/data,/data1/hadoop/dfs/data</value>\n    </property>\n\n修改 yarn-site.xml 配置文件，将新增的磁盘目录加到配置中：\n\n    <property>\n      <name>yarn.nodemanager.local-dirs</name>\n      <value>/data0/hadoop/yarn/data,/data1/hadoop/yarn/data</value>\n    </property>\n    <property>\n      <name>yarn.nodemanager.log-dirs</name>\n      <value>/data0/hadoop/yarn/log,/data1/hadoop/yarn/log</value>\n    </property>\n\n配置修改完成后将配置文件分发到所有的节点。依次重启 DataNode 和 NodeManager。\n\n如果只为 DataNode 新增磁盘，可以不用重启 DataNode，使用 reconfig 刷新配置即可：\n\n    $ hdfs dfsadmin -reconfig datanode 192.168.72.2:50020 start\n    Started reconfiguration task on DataNode 192.168.72.2:50020\n\n查看重新配置执行状态的时候会有误报的错误信息，这个问题的跟踪地址：<https://issues.apache.org/jira/browse/HDFS-8582>。错误信息如下：\n\n    $ hdfs dfsadmin -reconfig datanode 192.168.72.2:50020 status\n    Reconfiguring status for DataNode[192.168.72.2:50020]: started at Thu Mar 08 17:56:10 CST 2018 and finished at Thu Mar 08 17:56:11 CST 2018.\n    FAILED: Change property yarn.nodemanager.log-dirs\n\tFrom: \"/data0/hadoop/yarn/log\"\n\tTo: \"/data0/hadoop/yarn/log,/data1/hadoop/yarn/log\"\n\tError: Property yarn.nodemanager.log-dirs is not reconfigurable.\n    FAILED: Change property rpc.engine.org.apache.hadoop.ipc.ProtocolMetaInfoPB\n\tFrom: \"org.apache.hadoop.ipc.ProtobufRpcEngine\"\n\tTo: \"\"\n\tError: Property rpc.engine.org.apache.hadoop.ipc.ProtocolMetaInfoPB is not reconfigurable.\n    FAILED: Change property dfs.datanode.startup\n\tFrom: \"REGULAR\"\n\tTo: \"\"\n\tError: Property dfs.datanode.startup is not reconfigurable.\n","slug":"Hadoop-DataNode-磁盘扩容","published":1,"updated":"2021-07-19T16:28:00.248Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphmf002kitd32jhrg8hd","content":"<p>DataNode 是支持热插拔磁盘的，所以磁盘扩容比较简单。磁盘格式化后挂载完毕。</p>\n<span id=\"more\"></span>\n\n<p>修改 hdfs-site.xml 配置文件，将新增的磁盘目录加到配置中：</p>\n<pre><code>&lt;property&gt;\n  &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;\n  &lt;value&gt;/data0/hadoop/dfs/data,/data1/hadoop/dfs/data&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<p>修改 yarn-site.xml 配置文件，将新增的磁盘目录加到配置中：</p>\n<pre><code>&lt;property&gt;\n  &lt;name&gt;yarn.nodemanager.local-dirs&lt;/name&gt;\n  &lt;value&gt;/data0/hadoop/yarn/data,/data1/hadoop/yarn/data&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;yarn.nodemanager.log-dirs&lt;/name&gt;\n  &lt;value&gt;/data0/hadoop/yarn/log,/data1/hadoop/yarn/log&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<p>配置修改完成后将配置文件分发到所有的节点。依次重启 DataNode 和 NodeManager。</p>\n<p>如果只为 DataNode 新增磁盘，可以不用重启 DataNode，使用 reconfig 刷新配置即可：</p>\n<pre><code>$ hdfs dfsadmin -reconfig datanode 192.168.72.2:50020 start\nStarted reconfiguration task on DataNode 192.168.72.2:50020\n</code></pre>\n<p>查看重新配置执行状态的时候会有误报的错误信息，这个问题的跟踪地址：<a href=\"https://issues.apache.org/jira/browse/HDFS-8582\">https://issues.apache.org/jira/browse/HDFS-8582</a>。错误信息如下：</p>\n<pre><code>$ hdfs dfsadmin -reconfig datanode 192.168.72.2:50020 status\nReconfiguring status for DataNode[192.168.72.2:50020]: started at Thu Mar 08 17:56:10 CST 2018 and finished at Thu Mar 08 17:56:11 CST 2018.\nFAILED: Change property yarn.nodemanager.log-dirs\nFrom: &quot;/data0/hadoop/yarn/log&quot;\nTo: &quot;/data0/hadoop/yarn/log,/data1/hadoop/yarn/log&quot;\nError: Property yarn.nodemanager.log-dirs is not reconfigurable.\nFAILED: Change property rpc.engine.org.apache.hadoop.ipc.ProtocolMetaInfoPB\nFrom: &quot;org.apache.hadoop.ipc.ProtobufRpcEngine&quot;\nTo: &quot;&quot;\nError: Property rpc.engine.org.apache.hadoop.ipc.ProtocolMetaInfoPB is not reconfigurable.\nFAILED: Change property dfs.datanode.startup\nFrom: &quot;REGULAR&quot;\nTo: &quot;&quot;\nError: Property dfs.datanode.startup is not reconfigurable.\n</code></pre>\n","site":{"data":{}},"excerpt":"<p>DataNode 是支持热插拔磁盘的，所以磁盘扩容比较简单。磁盘格式化后挂载完毕。</p>","more":"<p>修改 hdfs-site.xml 配置文件，将新增的磁盘目录加到配置中：</p>\n<pre><code>&lt;property&gt;\n  &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;\n  &lt;value&gt;/data0/hadoop/dfs/data,/data1/hadoop/dfs/data&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<p>修改 yarn-site.xml 配置文件，将新增的磁盘目录加到配置中：</p>\n<pre><code>&lt;property&gt;\n  &lt;name&gt;yarn.nodemanager.local-dirs&lt;/name&gt;\n  &lt;value&gt;/data0/hadoop/yarn/data,/data1/hadoop/yarn/data&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;yarn.nodemanager.log-dirs&lt;/name&gt;\n  &lt;value&gt;/data0/hadoop/yarn/log,/data1/hadoop/yarn/log&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<p>配置修改完成后将配置文件分发到所有的节点。依次重启 DataNode 和 NodeManager。</p>\n<p>如果只为 DataNode 新增磁盘，可以不用重启 DataNode，使用 reconfig 刷新配置即可：</p>\n<pre><code>$ hdfs dfsadmin -reconfig datanode 192.168.72.2:50020 start\nStarted reconfiguration task on DataNode 192.168.72.2:50020\n</code></pre>\n<p>查看重新配置执行状态的时候会有误报的错误信息，这个问题的跟踪地址：<a href=\"https://issues.apache.org/jira/browse/HDFS-8582\">https://issues.apache.org/jira/browse/HDFS-8582</a>。错误信息如下：</p>\n<pre><code>$ hdfs dfsadmin -reconfig datanode 192.168.72.2:50020 status\nReconfiguring status for DataNode[192.168.72.2:50020]: started at Thu Mar 08 17:56:10 CST 2018 and finished at Thu Mar 08 17:56:11 CST 2018.\nFAILED: Change property yarn.nodemanager.log-dirs\nFrom: &quot;/data0/hadoop/yarn/log&quot;\nTo: &quot;/data0/hadoop/yarn/log,/data1/hadoop/yarn/log&quot;\nError: Property yarn.nodemanager.log-dirs is not reconfigurable.\nFAILED: Change property rpc.engine.org.apache.hadoop.ipc.ProtocolMetaInfoPB\nFrom: &quot;org.apache.hadoop.ipc.ProtobufRpcEngine&quot;\nTo: &quot;&quot;\nError: Property rpc.engine.org.apache.hadoop.ipc.ProtocolMetaInfoPB is not reconfigurable.\nFAILED: Change property dfs.datanode.startup\nFrom: &quot;REGULAR&quot;\nTo: &quot;&quot;\nError: Property dfs.datanode.startup is not reconfigurable.\n</code></pre>"},{"title":"Hadoop Eclipse 插件","date":"2016-09-10T12:13:02.000Z","_content":"\n### 环境\n\n- Mac OS X EI Capitan 10.11.6\n- java version “1.7.0_80”\n- Hadoop 2.7.3：安装路径 /Users/ling/frin/work/hadoop/hadoop-2.7.3\n- Eclipse Mars.2 Release (4.5.2)：安装路径 /Users/ling/frin/apps/Eclipse.app/Contents/Eclipse。\n\n<!-- more -->\n\n### 安装 Hadoop\n\n按照官方文档 [Hadoop: Setting up a Single Node Cluster.](http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/SingleCluster.html) 安装并配置伪分布式模式，并启动 hdfs。\n\n### 插件安装\n\nHadoop Eclipse 插件项目 Github 地址：<https://github.com/winghc/hadoop2x-eclipse-plugin>。项目的 release 目录下有已经编译好的 jar 包，此处使用 [hadoop-eclipse-plugin-2.6.0.jar](https://github.com/winghc/hadoop2x-eclipse-plugin/blob/master/release/hadoop-eclipse-plugin-2.6.0.jar)。\n\n下载 [hadoop-eclipse-plugin-2.6.0.jar](https://github.com/winghc/hadoop2x-eclipse-plugin/blob/master/release/hadoop-eclipse-plugin-2.6.0.jar)，然后将下载的 jar 包放到 ${ECLIPSE_HOME}/plugins 目录下，重启 Eclipse 安装完毕。\n\n### 配置插件\n#### Map/Reduce Locations 配置\n\n- 在菜单栏中选中 Window -> Show View -> Other...，在对话框中选中 MapReduce Tools -> Map/Reduce Locations，在 Eclipse 中会展示 Map/Reduce Locations 视图，如下图：![Map/Reduce Locations](/uploads/20160910/map-reduce-locations-view.png)\n- 在 Map/Reduce Locations 视图中点击右上方小象图标会弹出参数配置窗口，如下图：![Map/Reduce Locations 设置](/uploads/20160910/map-reduce-locations-conf.png)\n\n参数配置介绍：\n\n- Location name：自定义名称\n- Map/Reduce(V2) Master:\n  - Host：Map/Reduce 运行的 Host，此处为 localhost\n  - Port：该参数在新版本 Hadoop 中不需要，所以保持默认\n- DFS Master：\n  - 选中“Use M/R Master host“选项\n  - Host：上面的选项选中后，该参数默认\n  - Port：需要跟 Hadoop 配置文件 ${HADOOP_HOME}/etc/hadoop/core-site.xml 中 fs.defaultFS 参数中配置的端口一致。\n- User name：运行 Hadoop 的系统用户名\n\n#### Hadoop 安装目录配置\n\n在菜单栏中选中 Eclipse -> 偏好设置。在弹出的窗口中选中“Hadoop Map/Reduce“，在右侧设置 Hadoop 安装目录，如下图：![hadoop installation directory](/uploads/20160910/hadoop-installation-directory.png)\n\n#### 验证\n\n配置完成后点击 finish 按钮完成配置。此时在 Map/Reduce Location View 中会显示刚配置的 Location name；在 Project Explore 视图中会显示 DFS Locations，下面也会有刚配置的 Location nam，如下图：![dfs-locations](/uploads/20160910/dfs-locations.png)\n\n> 通过 DFS Location 可以管理 HDFS 目录。\n\n### 运行 MapReduce 作业\n\n#### 新建 MapReduce 工程\n\n在菜单栏中选择 File -> New -> Other -> MapReduce Project，然后点击 Next 进入项目配置对话框，如下图配置后点击 Finish 完成。![word-count](/uploads/20160910/word-count.png)\n\n#### 准备 MapReduce 作业\n\n直接将 Hadoop 源代码中的 /hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/WordCount.java 复制到新建的 MapReduce 工程中。\n\n#### 准备数据\n\n在 HDFS 上创建目录 /user/frin/input，并上传 example.txt 文件，内容如下：\n\n    hello world\n    hadoop example\n    word count\n    hadoop map reduce\n    hadoop hdfs\n\n#### 配置输入/输出\n\n在 WordCount.java 代码中点击右键，在弹出的菜单中选中 Run As -> Run Configurations...。在弹出的对话框中双击 Java Application 选项，在 (x)=Arguments 选项卡中配置程序参数，如下图：![argument.png](/uploads/20160910/argument.png)\n\n配置后点击 Apply 完成配置。\n\n#### 运行作业\n\n在 WordCount.java 代码中点击右键，在弹出的菜单中选中 Run As -> Run On Hadoop。在 Console 中会打印程序执行信息。最后执行结果如下图：![WordCount Result](/uploads/20160910/word-count-result.png)\n\n#### Debug MapReduce 作业\n\n在 WordCount.java 中设置断点，然后点击右键，在弹出的菜单中选择 Debug As -> Debug Configurations... 弹出 Debug 配置窗口，选中 Java Application -> WordCount（WordCount 是刚才配置的运行环境），如下图：![Debug Configuration](/uploads/20160910/debug-configuration.png)\n\n点击 Debug 按钮就可以对 MapReduce 进行调试了，调试界面如下图：![Debug View](/uploads/20160910/debug-view.png)\n\n### 错误\n#### Map/Reduce Locations 错误\n\nMap/Reduce Locations 配置完成后，展开 Location 时会报如下错误，参考了网上的信息也没有解决，但是并不影响 HDFS 管理以及 MapReduce 作业的运行。![Map/Reduce Locations Error](/uploads/20160910/map-reduce-locations-error.png)\n\n#### 输入路径不存在\n\n报错信息：\n\n    Exception in thread \"main\" org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: file:/user/frin/input\n\n如下图：![input path not exists](/uploads/20160910/input-path-not-exists.png)\n\n这个错误是因为程序默认从本地磁盘查找路径而非 HDFS，解决方法是将 Hadoop 配置文件目录加入到项目的 CLASSPATH 中。在 WordCount 项目上点击右键，然后选择 Properties。在弹出的对话框中选择 Java Build Path -> Libraries -> Add External Class Folder...，并选择 $HADOOP_HOME/etc/hadoop 目录。配置完成后重新运行作业。\n\n","source":"_posts/Hadoop-Eclipse-插件.md","raw":"title: Hadoop Eclipse 插件\ntags:\n  - Hadoop\n  - Eclipse\ncategories:\n  - 大数据\n  - Hadoop\ndate: 2016-09-10 20:13:02\n---\n\n### 环境\n\n- Mac OS X EI Capitan 10.11.6\n- java version “1.7.0_80”\n- Hadoop 2.7.3：安装路径 /Users/ling/frin/work/hadoop/hadoop-2.7.3\n- Eclipse Mars.2 Release (4.5.2)：安装路径 /Users/ling/frin/apps/Eclipse.app/Contents/Eclipse。\n\n<!-- more -->\n\n### 安装 Hadoop\n\n按照官方文档 [Hadoop: Setting up a Single Node Cluster.](http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/SingleCluster.html) 安装并配置伪分布式模式，并启动 hdfs。\n\n### 插件安装\n\nHadoop Eclipse 插件项目 Github 地址：<https://github.com/winghc/hadoop2x-eclipse-plugin>。项目的 release 目录下有已经编译好的 jar 包，此处使用 [hadoop-eclipse-plugin-2.6.0.jar](https://github.com/winghc/hadoop2x-eclipse-plugin/blob/master/release/hadoop-eclipse-plugin-2.6.0.jar)。\n\n下载 [hadoop-eclipse-plugin-2.6.0.jar](https://github.com/winghc/hadoop2x-eclipse-plugin/blob/master/release/hadoop-eclipse-plugin-2.6.0.jar)，然后将下载的 jar 包放到 ${ECLIPSE_HOME}/plugins 目录下，重启 Eclipse 安装完毕。\n\n### 配置插件\n#### Map/Reduce Locations 配置\n\n- 在菜单栏中选中 Window -> Show View -> Other...，在对话框中选中 MapReduce Tools -> Map/Reduce Locations，在 Eclipse 中会展示 Map/Reduce Locations 视图，如下图：![Map/Reduce Locations](/uploads/20160910/map-reduce-locations-view.png)\n- 在 Map/Reduce Locations 视图中点击右上方小象图标会弹出参数配置窗口，如下图：![Map/Reduce Locations 设置](/uploads/20160910/map-reduce-locations-conf.png)\n\n参数配置介绍：\n\n- Location name：自定义名称\n- Map/Reduce(V2) Master:\n  - Host：Map/Reduce 运行的 Host，此处为 localhost\n  - Port：该参数在新版本 Hadoop 中不需要，所以保持默认\n- DFS Master：\n  - 选中“Use M/R Master host“选项\n  - Host：上面的选项选中后，该参数默认\n  - Port：需要跟 Hadoop 配置文件 ${HADOOP_HOME}/etc/hadoop/core-site.xml 中 fs.defaultFS 参数中配置的端口一致。\n- User name：运行 Hadoop 的系统用户名\n\n#### Hadoop 安装目录配置\n\n在菜单栏中选中 Eclipse -> 偏好设置。在弹出的窗口中选中“Hadoop Map/Reduce“，在右侧设置 Hadoop 安装目录，如下图：![hadoop installation directory](/uploads/20160910/hadoop-installation-directory.png)\n\n#### 验证\n\n配置完成后点击 finish 按钮完成配置。此时在 Map/Reduce Location View 中会显示刚配置的 Location name；在 Project Explore 视图中会显示 DFS Locations，下面也会有刚配置的 Location nam，如下图：![dfs-locations](/uploads/20160910/dfs-locations.png)\n\n> 通过 DFS Location 可以管理 HDFS 目录。\n\n### 运行 MapReduce 作业\n\n#### 新建 MapReduce 工程\n\n在菜单栏中选择 File -> New -> Other -> MapReduce Project，然后点击 Next 进入项目配置对话框，如下图配置后点击 Finish 完成。![word-count](/uploads/20160910/word-count.png)\n\n#### 准备 MapReduce 作业\n\n直接将 Hadoop 源代码中的 /hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/WordCount.java 复制到新建的 MapReduce 工程中。\n\n#### 准备数据\n\n在 HDFS 上创建目录 /user/frin/input，并上传 example.txt 文件，内容如下：\n\n    hello world\n    hadoop example\n    word count\n    hadoop map reduce\n    hadoop hdfs\n\n#### 配置输入/输出\n\n在 WordCount.java 代码中点击右键，在弹出的菜单中选中 Run As -> Run Configurations...。在弹出的对话框中双击 Java Application 选项，在 (x)=Arguments 选项卡中配置程序参数，如下图：![argument.png](/uploads/20160910/argument.png)\n\n配置后点击 Apply 完成配置。\n\n#### 运行作业\n\n在 WordCount.java 代码中点击右键，在弹出的菜单中选中 Run As -> Run On Hadoop。在 Console 中会打印程序执行信息。最后执行结果如下图：![WordCount Result](/uploads/20160910/word-count-result.png)\n\n#### Debug MapReduce 作业\n\n在 WordCount.java 中设置断点，然后点击右键，在弹出的菜单中选择 Debug As -> Debug Configurations... 弹出 Debug 配置窗口，选中 Java Application -> WordCount（WordCount 是刚才配置的运行环境），如下图：![Debug Configuration](/uploads/20160910/debug-configuration.png)\n\n点击 Debug 按钮就可以对 MapReduce 进行调试了，调试界面如下图：![Debug View](/uploads/20160910/debug-view.png)\n\n### 错误\n#### Map/Reduce Locations 错误\n\nMap/Reduce Locations 配置完成后，展开 Location 时会报如下错误，参考了网上的信息也没有解决，但是并不影响 HDFS 管理以及 MapReduce 作业的运行。![Map/Reduce Locations Error](/uploads/20160910/map-reduce-locations-error.png)\n\n#### 输入路径不存在\n\n报错信息：\n\n    Exception in thread \"main\" org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: file:/user/frin/input\n\n如下图：![input path not exists](/uploads/20160910/input-path-not-exists.png)\n\n这个错误是因为程序默认从本地磁盘查找路径而非 HDFS，解决方法是将 Hadoop 配置文件目录加入到项目的 CLASSPATH 中。在 WordCount 项目上点击右键，然后选择 Properties。在弹出的对话框中选择 Java Build Path -> Libraries -> Add External Class Folder...，并选择 $HADOOP_HOME/etc/hadoop 目录。配置完成后重新运行作业。\n\n","slug":"Hadoop-Eclipse-插件","published":1,"updated":"2021-07-19T16:28:00.248Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphmg002nitd30f3a2hbd","content":"<h3 id=\"环境\"><a href=\"#环境\" class=\"headerlink\" title=\"环境\"></a>环境</h3><ul>\n<li>Mac OS X EI Capitan 10.11.6</li>\n<li>java version “1.7.0_80”</li>\n<li>Hadoop 2.7.3：安装路径 /Users/ling/frin/work/hadoop/hadoop-2.7.3</li>\n<li>Eclipse Mars.2 Release (4.5.2)：安装路径 /Users/ling/frin/apps/Eclipse.app/Contents/Eclipse。</li>\n</ul>\n<span id=\"more\"></span>\n\n<h3 id=\"安装-Hadoop\"><a href=\"#安装-Hadoop\" class=\"headerlink\" title=\"安装 Hadoop\"></a>安装 Hadoop</h3><p>按照官方文档 <a href=\"http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/SingleCluster.html\">Hadoop: Setting up a Single Node Cluster.</a> 安装并配置伪分布式模式，并启动 hdfs。</p>\n<h3 id=\"插件安装\"><a href=\"#插件安装\" class=\"headerlink\" title=\"插件安装\"></a>插件安装</h3><p>Hadoop Eclipse 插件项目 Github 地址：<a href=\"https://github.com/winghc/hadoop2x-eclipse-plugin\">https://github.com/winghc/hadoop2x-eclipse-plugin</a>。项目的 release 目录下有已经编译好的 jar 包，此处使用 <a href=\"https://github.com/winghc/hadoop2x-eclipse-plugin/blob/master/release/hadoop-eclipse-plugin-2.6.0.jar\">hadoop-eclipse-plugin-2.6.0.jar</a>。</p>\n<p>下载 <a href=\"https://github.com/winghc/hadoop2x-eclipse-plugin/blob/master/release/hadoop-eclipse-plugin-2.6.0.jar\">hadoop-eclipse-plugin-2.6.0.jar</a>，然后将下载的 jar 包放到 ${ECLIPSE_HOME}/plugins 目录下，重启 Eclipse 安装完毕。</p>\n<h3 id=\"配置插件\"><a href=\"#配置插件\" class=\"headerlink\" title=\"配置插件\"></a>配置插件</h3><h4 id=\"Map-Reduce-Locations-配置\"><a href=\"#Map-Reduce-Locations-配置\" class=\"headerlink\" title=\"Map/Reduce Locations 配置\"></a>Map/Reduce Locations 配置</h4><ul>\n<li>在菜单栏中选中 Window -&gt; Show View -&gt; Other…，在对话框中选中 MapReduce Tools -&gt; Map/Reduce Locations，在 Eclipse 中会展示 Map/Reduce Locations 视图，如下图：<img src=\"/uploads/20160910/map-reduce-locations-view.png\" alt=\"Map/Reduce Locations\"></li>\n<li>在 Map/Reduce Locations 视图中点击右上方小象图标会弹出参数配置窗口，如下图：<img src=\"/uploads/20160910/map-reduce-locations-conf.png\" alt=\"Map/Reduce Locations 设置\"></li>\n</ul>\n<p>参数配置介绍：</p>\n<ul>\n<li>Location name：自定义名称</li>\n<li>Map/Reduce(V2) Master:<ul>\n<li>Host：Map/Reduce 运行的 Host，此处为 localhost</li>\n<li>Port：该参数在新版本 Hadoop 中不需要，所以保持默认</li>\n</ul>\n</li>\n<li>DFS Master：<ul>\n<li>选中“Use M/R Master host“选项</li>\n<li>Host：上面的选项选中后，该参数默认</li>\n<li>Port：需要跟 Hadoop 配置文件 ${HADOOP_HOME}/etc/hadoop/core-site.xml 中 fs.defaultFS 参数中配置的端口一致。</li>\n</ul>\n</li>\n<li>User name：运行 Hadoop 的系统用户名</li>\n</ul>\n<h4 id=\"Hadoop-安装目录配置\"><a href=\"#Hadoop-安装目录配置\" class=\"headerlink\" title=\"Hadoop 安装目录配置\"></a>Hadoop 安装目录配置</h4><p>在菜单栏中选中 Eclipse -&gt; 偏好设置。在弹出的窗口中选中“Hadoop Map/Reduce“，在右侧设置 Hadoop 安装目录，如下图：<img src=\"/uploads/20160910/hadoop-installation-directory.png\" alt=\"hadoop installation directory\"></p>\n<h4 id=\"验证\"><a href=\"#验证\" class=\"headerlink\" title=\"验证\"></a>验证</h4><p>配置完成后点击 finish 按钮完成配置。此时在 Map/Reduce Location View 中会显示刚配置的 Location name；在 Project Explore 视图中会显示 DFS Locations，下面也会有刚配置的 Location nam，如下图：<img src=\"/uploads/20160910/dfs-locations.png\" alt=\"dfs-locations\"></p>\n<blockquote>\n<p>通过 DFS Location 可以管理 HDFS 目录。</p>\n</blockquote>\n<h3 id=\"运行-MapReduce-作业\"><a href=\"#运行-MapReduce-作业\" class=\"headerlink\" title=\"运行 MapReduce 作业\"></a>运行 MapReduce 作业</h3><h4 id=\"新建-MapReduce-工程\"><a href=\"#新建-MapReduce-工程\" class=\"headerlink\" title=\"新建 MapReduce 工程\"></a>新建 MapReduce 工程</h4><p>在菜单栏中选择 File -&gt; New -&gt; Other -&gt; MapReduce Project，然后点击 Next 进入项目配置对话框，如下图配置后点击 Finish 完成。<img src=\"/uploads/20160910/word-count.png\" alt=\"word-count\"></p>\n<h4 id=\"准备-MapReduce-作业\"><a href=\"#准备-MapReduce-作业\" class=\"headerlink\" title=\"准备 MapReduce 作业\"></a>准备 MapReduce 作业</h4><p>直接将 Hadoop 源代码中的 /hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/WordCount.java 复制到新建的 MapReduce 工程中。</p>\n<h4 id=\"准备数据\"><a href=\"#准备数据\" class=\"headerlink\" title=\"准备数据\"></a>准备数据</h4><p>在 HDFS 上创建目录 /user/frin/input，并上传 example.txt 文件，内容如下：</p>\n<pre><code>hello world\nhadoop example\nword count\nhadoop map reduce\nhadoop hdfs\n</code></pre>\n<h4 id=\"配置输入-输出\"><a href=\"#配置输入-输出\" class=\"headerlink\" title=\"配置输入/输出\"></a>配置输入/输出</h4><p>在 WordCount.java 代码中点击右键，在弹出的菜单中选中 Run As -&gt; Run Configurations…。在弹出的对话框中双击 Java Application 选项，在 (x)=Arguments 选项卡中配置程序参数，如下图：<img src=\"/uploads/20160910/argument.png\" alt=\"argument.png\"></p>\n<p>配置后点击 Apply 完成配置。</p>\n<h4 id=\"运行作业\"><a href=\"#运行作业\" class=\"headerlink\" title=\"运行作业\"></a>运行作业</h4><p>在 WordCount.java 代码中点击右键，在弹出的菜单中选中 Run As -&gt; Run On Hadoop。在 Console 中会打印程序执行信息。最后执行结果如下图：<img src=\"/uploads/20160910/word-count-result.png\" alt=\"WordCount Result\"></p>\n<h4 id=\"Debug-MapReduce-作业\"><a href=\"#Debug-MapReduce-作业\" class=\"headerlink\" title=\"Debug MapReduce 作业\"></a>Debug MapReduce 作业</h4><p>在 WordCount.java 中设置断点，然后点击右键，在弹出的菜单中选择 Debug As -&gt; Debug Configurations… 弹出 Debug 配置窗口，选中 Java Application -&gt; WordCount（WordCount 是刚才配置的运行环境），如下图：<img src=\"/uploads/20160910/debug-configuration.png\" alt=\"Debug Configuration\"></p>\n<p>点击 Debug 按钮就可以对 MapReduce 进行调试了，调试界面如下图：<img src=\"/uploads/20160910/debug-view.png\" alt=\"Debug View\"></p>\n<h3 id=\"错误\"><a href=\"#错误\" class=\"headerlink\" title=\"错误\"></a>错误</h3><h4 id=\"Map-Reduce-Locations-错误\"><a href=\"#Map-Reduce-Locations-错误\" class=\"headerlink\" title=\"Map/Reduce Locations 错误\"></a>Map/Reduce Locations 错误</h4><p>Map/Reduce Locations 配置完成后，展开 Location 时会报如下错误，参考了网上的信息也没有解决，但是并不影响 HDFS 管理以及 MapReduce 作业的运行。<img src=\"/uploads/20160910/map-reduce-locations-error.png\" alt=\"Map/Reduce Locations Error\"></p>\n<h4 id=\"输入路径不存在\"><a href=\"#输入路径不存在\" class=\"headerlink\" title=\"输入路径不存在\"></a>输入路径不存在</h4><p>报错信息：</p>\n<pre><code>Exception in thread &quot;main&quot; org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: file:/user/frin/input\n</code></pre>\n<p>如下图：<img src=\"/uploads/20160910/input-path-not-exists.png\" alt=\"input path not exists\"></p>\n<p>这个错误是因为程序默认从本地磁盘查找路径而非 HDFS，解决方法是将 Hadoop 配置文件目录加入到项目的 CLASSPATH 中。在 WordCount 项目上点击右键，然后选择 Properties。在弹出的对话框中选择 Java Build Path -&gt; Libraries -&gt; Add External Class Folder…，并选择 $HADOOP_HOME/etc/hadoop 目录。配置完成后重新运行作业。</p>\n","site":{"data":{}},"excerpt":"<h3 id=\"环境\"><a href=\"#环境\" class=\"headerlink\" title=\"环境\"></a>环境</h3><ul>\n<li>Mac OS X EI Capitan 10.11.6</li>\n<li>java version “1.7.0_80”</li>\n<li>Hadoop 2.7.3：安装路径 /Users/ling/frin/work/hadoop/hadoop-2.7.3</li>\n<li>Eclipse Mars.2 Release (4.5.2)：安装路径 /Users/ling/frin/apps/Eclipse.app/Contents/Eclipse。</li>\n</ul>","more":"<h3 id=\"安装-Hadoop\"><a href=\"#安装-Hadoop\" class=\"headerlink\" title=\"安装 Hadoop\"></a>安装 Hadoop</h3><p>按照官方文档 <a href=\"http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/SingleCluster.html\">Hadoop: Setting up a Single Node Cluster.</a> 安装并配置伪分布式模式，并启动 hdfs。</p>\n<h3 id=\"插件安装\"><a href=\"#插件安装\" class=\"headerlink\" title=\"插件安装\"></a>插件安装</h3><p>Hadoop Eclipse 插件项目 Github 地址：<a href=\"https://github.com/winghc/hadoop2x-eclipse-plugin\">https://github.com/winghc/hadoop2x-eclipse-plugin</a>。项目的 release 目录下有已经编译好的 jar 包，此处使用 <a href=\"https://github.com/winghc/hadoop2x-eclipse-plugin/blob/master/release/hadoop-eclipse-plugin-2.6.0.jar\">hadoop-eclipse-plugin-2.6.0.jar</a>。</p>\n<p>下载 <a href=\"https://github.com/winghc/hadoop2x-eclipse-plugin/blob/master/release/hadoop-eclipse-plugin-2.6.0.jar\">hadoop-eclipse-plugin-2.6.0.jar</a>，然后将下载的 jar 包放到 ${ECLIPSE_HOME}/plugins 目录下，重启 Eclipse 安装完毕。</p>\n<h3 id=\"配置插件\"><a href=\"#配置插件\" class=\"headerlink\" title=\"配置插件\"></a>配置插件</h3><h4 id=\"Map-Reduce-Locations-配置\"><a href=\"#Map-Reduce-Locations-配置\" class=\"headerlink\" title=\"Map/Reduce Locations 配置\"></a>Map/Reduce Locations 配置</h4><ul>\n<li>在菜单栏中选中 Window -&gt; Show View -&gt; Other…，在对话框中选中 MapReduce Tools -&gt; Map/Reduce Locations，在 Eclipse 中会展示 Map/Reduce Locations 视图，如下图：<img src=\"/uploads/20160910/map-reduce-locations-view.png\" alt=\"Map/Reduce Locations\"></li>\n<li>在 Map/Reduce Locations 视图中点击右上方小象图标会弹出参数配置窗口，如下图：<img src=\"/uploads/20160910/map-reduce-locations-conf.png\" alt=\"Map/Reduce Locations 设置\"></li>\n</ul>\n<p>参数配置介绍：</p>\n<ul>\n<li>Location name：自定义名称</li>\n<li>Map/Reduce(V2) Master:<ul>\n<li>Host：Map/Reduce 运行的 Host，此处为 localhost</li>\n<li>Port：该参数在新版本 Hadoop 中不需要，所以保持默认</li>\n</ul>\n</li>\n<li>DFS Master：<ul>\n<li>选中“Use M/R Master host“选项</li>\n<li>Host：上面的选项选中后，该参数默认</li>\n<li>Port：需要跟 Hadoop 配置文件 ${HADOOP_HOME}/etc/hadoop/core-site.xml 中 fs.defaultFS 参数中配置的端口一致。</li>\n</ul>\n</li>\n<li>User name：运行 Hadoop 的系统用户名</li>\n</ul>\n<h4 id=\"Hadoop-安装目录配置\"><a href=\"#Hadoop-安装目录配置\" class=\"headerlink\" title=\"Hadoop 安装目录配置\"></a>Hadoop 安装目录配置</h4><p>在菜单栏中选中 Eclipse -&gt; 偏好设置。在弹出的窗口中选中“Hadoop Map/Reduce“，在右侧设置 Hadoop 安装目录，如下图：<img src=\"/uploads/20160910/hadoop-installation-directory.png\" alt=\"hadoop installation directory\"></p>\n<h4 id=\"验证\"><a href=\"#验证\" class=\"headerlink\" title=\"验证\"></a>验证</h4><p>配置完成后点击 finish 按钮完成配置。此时在 Map/Reduce Location View 中会显示刚配置的 Location name；在 Project Explore 视图中会显示 DFS Locations，下面也会有刚配置的 Location nam，如下图：<img src=\"/uploads/20160910/dfs-locations.png\" alt=\"dfs-locations\"></p>\n<blockquote>\n<p>通过 DFS Location 可以管理 HDFS 目录。</p>\n</blockquote>\n<h3 id=\"运行-MapReduce-作业\"><a href=\"#运行-MapReduce-作业\" class=\"headerlink\" title=\"运行 MapReduce 作业\"></a>运行 MapReduce 作业</h3><h4 id=\"新建-MapReduce-工程\"><a href=\"#新建-MapReduce-工程\" class=\"headerlink\" title=\"新建 MapReduce 工程\"></a>新建 MapReduce 工程</h4><p>在菜单栏中选择 File -&gt; New -&gt; Other -&gt; MapReduce Project，然后点击 Next 进入项目配置对话框，如下图配置后点击 Finish 完成。<img src=\"/uploads/20160910/word-count.png\" alt=\"word-count\"></p>\n<h4 id=\"准备-MapReduce-作业\"><a href=\"#准备-MapReduce-作业\" class=\"headerlink\" title=\"准备 MapReduce 作业\"></a>准备 MapReduce 作业</h4><p>直接将 Hadoop 源代码中的 /hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/WordCount.java 复制到新建的 MapReduce 工程中。</p>\n<h4 id=\"准备数据\"><a href=\"#准备数据\" class=\"headerlink\" title=\"准备数据\"></a>准备数据</h4><p>在 HDFS 上创建目录 /user/frin/input，并上传 example.txt 文件，内容如下：</p>\n<pre><code>hello world\nhadoop example\nword count\nhadoop map reduce\nhadoop hdfs\n</code></pre>\n<h4 id=\"配置输入-输出\"><a href=\"#配置输入-输出\" class=\"headerlink\" title=\"配置输入/输出\"></a>配置输入/输出</h4><p>在 WordCount.java 代码中点击右键，在弹出的菜单中选中 Run As -&gt; Run Configurations…。在弹出的对话框中双击 Java Application 选项，在 (x)=Arguments 选项卡中配置程序参数，如下图：<img src=\"/uploads/20160910/argument.png\" alt=\"argument.png\"></p>\n<p>配置后点击 Apply 完成配置。</p>\n<h4 id=\"运行作业\"><a href=\"#运行作业\" class=\"headerlink\" title=\"运行作业\"></a>运行作业</h4><p>在 WordCount.java 代码中点击右键，在弹出的菜单中选中 Run As -&gt; Run On Hadoop。在 Console 中会打印程序执行信息。最后执行结果如下图：<img src=\"/uploads/20160910/word-count-result.png\" alt=\"WordCount Result\"></p>\n<h4 id=\"Debug-MapReduce-作业\"><a href=\"#Debug-MapReduce-作业\" class=\"headerlink\" title=\"Debug MapReduce 作业\"></a>Debug MapReduce 作业</h4><p>在 WordCount.java 中设置断点，然后点击右键，在弹出的菜单中选择 Debug As -&gt; Debug Configurations… 弹出 Debug 配置窗口，选中 Java Application -&gt; WordCount（WordCount 是刚才配置的运行环境），如下图：<img src=\"/uploads/20160910/debug-configuration.png\" alt=\"Debug Configuration\"></p>\n<p>点击 Debug 按钮就可以对 MapReduce 进行调试了，调试界面如下图：<img src=\"/uploads/20160910/debug-view.png\" alt=\"Debug View\"></p>\n<h3 id=\"错误\"><a href=\"#错误\" class=\"headerlink\" title=\"错误\"></a>错误</h3><h4 id=\"Map-Reduce-Locations-错误\"><a href=\"#Map-Reduce-Locations-错误\" class=\"headerlink\" title=\"Map/Reduce Locations 错误\"></a>Map/Reduce Locations 错误</h4><p>Map/Reduce Locations 配置完成后，展开 Location 时会报如下错误，参考了网上的信息也没有解决，但是并不影响 HDFS 管理以及 MapReduce 作业的运行。<img src=\"/uploads/20160910/map-reduce-locations-error.png\" alt=\"Map/Reduce Locations Error\"></p>\n<h4 id=\"输入路径不存在\"><a href=\"#输入路径不存在\" class=\"headerlink\" title=\"输入路径不存在\"></a>输入路径不存在</h4><p>报错信息：</p>\n<pre><code>Exception in thread &quot;main&quot; org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: file:/user/frin/input\n</code></pre>\n<p>如下图：<img src=\"/uploads/20160910/input-path-not-exists.png\" alt=\"input path not exists\"></p>\n<p>这个错误是因为程序默认从本地磁盘查找路径而非 HDFS，解决方法是将 Hadoop 配置文件目录加入到项目的 CLASSPATH 中。在 WordCount 项目上点击右键，然后选择 Properties。在弹出的对话框中选择 Java Build Path -&gt; Libraries -&gt; Add External Class Folder…，并选择 $HADOOP_HOME/etc/hadoop 目录。配置完成后重新运行作业。</p>"},{"title":"Hadoop distcp 数据同步","date":"2017-10-19T02:18:10.000Z","_content":"\n\n### distcp 介绍\n\ndistcp 是 hadoop 集群间数据同步的工具。基于 MapReduce 进行分布式、错误处理、恢复和报告。\n\n一般 distcp 使用如下：\n\n    hadoop distcp -bandwidth 10 -m 50 -overwrite -strategy dynamic webhdfs://hadoop219:50070/code hdfs://ycluster/data/data-platform/cluster219/code\n\n<!-- more -->\n\n### 参数说明\n\n- -p[rbugpcaxt]：禁止：r（副本数量）；b（块大小）；u（用户）；g（组）；p（权限）；c（checksum类型）；a（ACL）；x（xAttr）；t（时间戳）。\n- -m：同时拷贝的最大并发数。指定拷贝数据的 MapTask 数量。\n- -overwrite：覆盖目标数据。如果 map 失败，并且没有指定 -i 忽略失败，则分片中的文件都会重新拷贝。\n- -strategy{dynamic|uniformsize}：选择 distcp 使用的策略。默认使用 uniformsize（每个 map 会平衡文件大小）。如果指定 dynamic，则会使用 DynamicInputFormat。\n- -bandwidth：指定每个 map 的带宽，单位 MB/s。\n\n### 常见问题\n\n#### 版本问题\n\n不同版本间使用 hdfs 协议时可能会发生问题，如：\n\n    $ hadoop fs -ls hdfs://hadoop219:50070/\n    17/10/17 14:45:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n    ls: Failed on local exception: com.google.protobuf.InvalidProtocolBufferException: Protocol message end-group tag did not match expected tag.; Host Details : local host is: \"192-168-72-200/192.168.72.200\"; destination host is: \"hadoop219\":50070;\n    \n建议使用兼容性更好的 webhdfs 协议：\n\n    $ hadoop fs -ls webhdfs://hadoop219:50070/\n    17/10/17 14:45:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n    Found 9 items\n    drwxr-xr-x   - root          supergroup          0 2017-05-24 10:16 webhdfs://hadoop219:50070/code\n    drwxr-xr-x   - hadoop        supergroup          0 2017-07-25 12:07 webhdfs://hadoop219:50070/data\n    drwxr-xr-x   - root          supergroup          0 2017-07-11 23:59 webhdfs://hadoop219:50070/data_log\n    drwxr-xr-x   - Administrator supergroup          0 2015-07-27 20:40 webhdfs://hadoop219:50070/output\n    drwxr-xr-x   - root          supergroup          0 2016-11-14 14:02 webhdfs://hadoop219:50070/recommend_v2\n    drwxr-xr-x   - hadoop        supergroup          0 2016-07-06 08:29 webhdfs://hadoop219:50070/system\n    drwx-wx-wx   - root          supergroup          0 2016-09-12 20:17 webhdfs://hadoop219:50070/tmp\n    drwxr-xr-x   - root          supergroup          0 2016-07-21 17:09 webhdfs://hadoop219:50070/user\n    drwxr-xr-x   - root          supergroup          0 2017-05-11 18:37 webhdfs://hadoop219:50070/yl_test\n\n#### Check-sum mismatch\n\n异常信息如下：\n\n    Caused by: java.io.IOException: Check-sum mismatch between webhdfs://hadoop219:50070/code/bss_city_device/2017-07-31/part-m-00002 and hdfs://ycluster/data/data-platform/cluster219/code/.distcp.tmp.attempt_1506582668997_345304_m_000011_1. Source and target differ in block-size. Use -pb to preserve block-sizes during copy. Alternatively, skip checksum-checks altogether, using -skipCrc. (NOTE: By skipping checksums, one runs the risk of masking data-corruption during file-transfer.)\n    \tat org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.compareCheckSums(RetriableFileCopyCommand.java:210)\n    \tat org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doCopy(RetriableFileCopyCommand.java:130)\n    \tat org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doExecute(RetriableFileCopyCommand.java:99)\n    \tat org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:87)\n    \t... 11 more\n\n根据异常信息可以看到是两个集群之间 block size 不一致引起的。增加 -pb 参数禁止块大小检查。\n","source":"_posts/Hadoop-distcp-数据同步.md","raw":"title: Hadoop distcp 数据同步\ntags:\n  - Hadoop\n  - HDFS\ncategories:\n  - 大数据\n  - Hadoop\ndate: 2017-10-19 10:18:10\n---\n\n\n### distcp 介绍\n\ndistcp 是 hadoop 集群间数据同步的工具。基于 MapReduce 进行分布式、错误处理、恢复和报告。\n\n一般 distcp 使用如下：\n\n    hadoop distcp -bandwidth 10 -m 50 -overwrite -strategy dynamic webhdfs://hadoop219:50070/code hdfs://ycluster/data/data-platform/cluster219/code\n\n<!-- more -->\n\n### 参数说明\n\n- -p[rbugpcaxt]：禁止：r（副本数量）；b（块大小）；u（用户）；g（组）；p（权限）；c（checksum类型）；a（ACL）；x（xAttr）；t（时间戳）。\n- -m：同时拷贝的最大并发数。指定拷贝数据的 MapTask 数量。\n- -overwrite：覆盖目标数据。如果 map 失败，并且没有指定 -i 忽略失败，则分片中的文件都会重新拷贝。\n- -strategy{dynamic|uniformsize}：选择 distcp 使用的策略。默认使用 uniformsize（每个 map 会平衡文件大小）。如果指定 dynamic，则会使用 DynamicInputFormat。\n- -bandwidth：指定每个 map 的带宽，单位 MB/s。\n\n### 常见问题\n\n#### 版本问题\n\n不同版本间使用 hdfs 协议时可能会发生问题，如：\n\n    $ hadoop fs -ls hdfs://hadoop219:50070/\n    17/10/17 14:45:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n    ls: Failed on local exception: com.google.protobuf.InvalidProtocolBufferException: Protocol message end-group tag did not match expected tag.; Host Details : local host is: \"192-168-72-200/192.168.72.200\"; destination host is: \"hadoop219\":50070;\n    \n建议使用兼容性更好的 webhdfs 协议：\n\n    $ hadoop fs -ls webhdfs://hadoop219:50070/\n    17/10/17 14:45:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n    Found 9 items\n    drwxr-xr-x   - root          supergroup          0 2017-05-24 10:16 webhdfs://hadoop219:50070/code\n    drwxr-xr-x   - hadoop        supergroup          0 2017-07-25 12:07 webhdfs://hadoop219:50070/data\n    drwxr-xr-x   - root          supergroup          0 2017-07-11 23:59 webhdfs://hadoop219:50070/data_log\n    drwxr-xr-x   - Administrator supergroup          0 2015-07-27 20:40 webhdfs://hadoop219:50070/output\n    drwxr-xr-x   - root          supergroup          0 2016-11-14 14:02 webhdfs://hadoop219:50070/recommend_v2\n    drwxr-xr-x   - hadoop        supergroup          0 2016-07-06 08:29 webhdfs://hadoop219:50070/system\n    drwx-wx-wx   - root          supergroup          0 2016-09-12 20:17 webhdfs://hadoop219:50070/tmp\n    drwxr-xr-x   - root          supergroup          0 2016-07-21 17:09 webhdfs://hadoop219:50070/user\n    drwxr-xr-x   - root          supergroup          0 2017-05-11 18:37 webhdfs://hadoop219:50070/yl_test\n\n#### Check-sum mismatch\n\n异常信息如下：\n\n    Caused by: java.io.IOException: Check-sum mismatch between webhdfs://hadoop219:50070/code/bss_city_device/2017-07-31/part-m-00002 and hdfs://ycluster/data/data-platform/cluster219/code/.distcp.tmp.attempt_1506582668997_345304_m_000011_1. Source and target differ in block-size. Use -pb to preserve block-sizes during copy. Alternatively, skip checksum-checks altogether, using -skipCrc. (NOTE: By skipping checksums, one runs the risk of masking data-corruption during file-transfer.)\n    \tat org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.compareCheckSums(RetriableFileCopyCommand.java:210)\n    \tat org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doCopy(RetriableFileCopyCommand.java:130)\n    \tat org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doExecute(RetriableFileCopyCommand.java:99)\n    \tat org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:87)\n    \t... 11 more\n\n根据异常信息可以看到是两个集群之间 block size 不一致引起的。增加 -pb 参数禁止块大小检查。\n","slug":"Hadoop-distcp-数据同步","published":1,"updated":"2021-07-19T16:28:00.252Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphmi002qitd34obx8llc","content":"<h3 id=\"distcp-介绍\"><a href=\"#distcp-介绍\" class=\"headerlink\" title=\"distcp 介绍\"></a>distcp 介绍</h3><p>distcp 是 hadoop 集群间数据同步的工具。基于 MapReduce 进行分布式、错误处理、恢复和报告。</p>\n<p>一般 distcp 使用如下：</p>\n<pre><code>hadoop distcp -bandwidth 10 -m 50 -overwrite -strategy dynamic webhdfs://hadoop219:50070/code hdfs://ycluster/data/data-platform/cluster219/code\n</code></pre>\n<span id=\"more\"></span>\n\n<h3 id=\"参数说明\"><a href=\"#参数说明\" class=\"headerlink\" title=\"参数说明\"></a>参数说明</h3><ul>\n<li>-p[rbugpcaxt]：禁止：r（副本数量）；b（块大小）；u（用户）；g（组）；p（权限）；c（checksum类型）；a（ACL）；x（xAttr）；t（时间戳）。</li>\n<li>-m：同时拷贝的最大并发数。指定拷贝数据的 MapTask 数量。</li>\n<li>-overwrite：覆盖目标数据。如果 map 失败，并且没有指定 -i 忽略失败，则分片中的文件都会重新拷贝。</li>\n<li>-strategy{dynamic|uniformsize}：选择 distcp 使用的策略。默认使用 uniformsize（每个 map 会平衡文件大小）。如果指定 dynamic，则会使用 DynamicInputFormat。</li>\n<li>-bandwidth：指定每个 map 的带宽，单位 MB/s。</li>\n</ul>\n<h3 id=\"常见问题\"><a href=\"#常见问题\" class=\"headerlink\" title=\"常见问题\"></a>常见问题</h3><h4 id=\"版本问题\"><a href=\"#版本问题\" class=\"headerlink\" title=\"版本问题\"></a>版本问题</h4><p>不同版本间使用 hdfs 协议时可能会发生问题，如：</p>\n<pre><code>$ hadoop fs -ls hdfs://hadoop219:50070/\n17/10/17 14:45:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nls: Failed on local exception: com.google.protobuf.InvalidProtocolBufferException: Protocol message end-group tag did not match expected tag.; Host Details : local host is: &quot;192-168-72-200/192.168.72.200&quot;; destination host is: &quot;hadoop219&quot;:50070;\n</code></pre>\n<p>建议使用兼容性更好的 webhdfs 协议：</p>\n<pre><code>$ hadoop fs -ls webhdfs://hadoop219:50070/\n17/10/17 14:45:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nFound 9 items\ndrwxr-xr-x   - root          supergroup          0 2017-05-24 10:16 webhdfs://hadoop219:50070/code\ndrwxr-xr-x   - hadoop        supergroup          0 2017-07-25 12:07 webhdfs://hadoop219:50070/data\ndrwxr-xr-x   - root          supergroup          0 2017-07-11 23:59 webhdfs://hadoop219:50070/data_log\ndrwxr-xr-x   - Administrator supergroup          0 2015-07-27 20:40 webhdfs://hadoop219:50070/output\ndrwxr-xr-x   - root          supergroup          0 2016-11-14 14:02 webhdfs://hadoop219:50070/recommend_v2\ndrwxr-xr-x   - hadoop        supergroup          0 2016-07-06 08:29 webhdfs://hadoop219:50070/system\ndrwx-wx-wx   - root          supergroup          0 2016-09-12 20:17 webhdfs://hadoop219:50070/tmp\ndrwxr-xr-x   - root          supergroup          0 2016-07-21 17:09 webhdfs://hadoop219:50070/user\ndrwxr-xr-x   - root          supergroup          0 2017-05-11 18:37 webhdfs://hadoop219:50070/yl_test\n</code></pre>\n<h4 id=\"Check-sum-mismatch\"><a href=\"#Check-sum-mismatch\" class=\"headerlink\" title=\"Check-sum mismatch\"></a>Check-sum mismatch</h4><p>异常信息如下：</p>\n<pre><code>Caused by: java.io.IOException: Check-sum mismatch between webhdfs://hadoop219:50070/code/bss_city_device/2017-07-31/part-m-00002 and hdfs://ycluster/data/data-platform/cluster219/code/.distcp.tmp.attempt_1506582668997_345304_m_000011_1. Source and target differ in block-size. Use -pb to preserve block-sizes during copy. Alternatively, skip checksum-checks altogether, using -skipCrc. (NOTE: By skipping checksums, one runs the risk of masking data-corruption during file-transfer.)\n    at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.compareCheckSums(RetriableFileCopyCommand.java:210)\n    at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doCopy(RetriableFileCopyCommand.java:130)\n    at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doExecute(RetriableFileCopyCommand.java:99)\n    at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:87)\n    ... 11 more\n</code></pre>\n<p>根据异常信息可以看到是两个集群之间 block size 不一致引起的。增加 -pb 参数禁止块大小检查。</p>\n","site":{"data":{}},"excerpt":"<h3 id=\"distcp-介绍\"><a href=\"#distcp-介绍\" class=\"headerlink\" title=\"distcp 介绍\"></a>distcp 介绍</h3><p>distcp 是 hadoop 集群间数据同步的工具。基于 MapReduce 进行分布式、错误处理、恢复和报告。</p>\n<p>一般 distcp 使用如下：</p>\n<pre><code>hadoop distcp -bandwidth 10 -m 50 -overwrite -strategy dynamic webhdfs://hadoop219:50070/code hdfs://ycluster/data/data-platform/cluster219/code\n</code></pre>","more":"<h3 id=\"参数说明\"><a href=\"#参数说明\" class=\"headerlink\" title=\"参数说明\"></a>参数说明</h3><ul>\n<li>-p[rbugpcaxt]：禁止：r（副本数量）；b（块大小）；u（用户）；g（组）；p（权限）；c（checksum类型）；a（ACL）；x（xAttr）；t（时间戳）。</li>\n<li>-m：同时拷贝的最大并发数。指定拷贝数据的 MapTask 数量。</li>\n<li>-overwrite：覆盖目标数据。如果 map 失败，并且没有指定 -i 忽略失败，则分片中的文件都会重新拷贝。</li>\n<li>-strategy{dynamic|uniformsize}：选择 distcp 使用的策略。默认使用 uniformsize（每个 map 会平衡文件大小）。如果指定 dynamic，则会使用 DynamicInputFormat。</li>\n<li>-bandwidth：指定每个 map 的带宽，单位 MB/s。</li>\n</ul>\n<h3 id=\"常见问题\"><a href=\"#常见问题\" class=\"headerlink\" title=\"常见问题\"></a>常见问题</h3><h4 id=\"版本问题\"><a href=\"#版本问题\" class=\"headerlink\" title=\"版本问题\"></a>版本问题</h4><p>不同版本间使用 hdfs 协议时可能会发生问题，如：</p>\n<pre><code>$ hadoop fs -ls hdfs://hadoop219:50070/\n17/10/17 14:45:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nls: Failed on local exception: com.google.protobuf.InvalidProtocolBufferException: Protocol message end-group tag did not match expected tag.; Host Details : local host is: &quot;192-168-72-200/192.168.72.200&quot;; destination host is: &quot;hadoop219&quot;:50070;\n</code></pre>\n<p>建议使用兼容性更好的 webhdfs 协议：</p>\n<pre><code>$ hadoop fs -ls webhdfs://hadoop219:50070/\n17/10/17 14:45:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nFound 9 items\ndrwxr-xr-x   - root          supergroup          0 2017-05-24 10:16 webhdfs://hadoop219:50070/code\ndrwxr-xr-x   - hadoop        supergroup          0 2017-07-25 12:07 webhdfs://hadoop219:50070/data\ndrwxr-xr-x   - root          supergroup          0 2017-07-11 23:59 webhdfs://hadoop219:50070/data_log\ndrwxr-xr-x   - Administrator supergroup          0 2015-07-27 20:40 webhdfs://hadoop219:50070/output\ndrwxr-xr-x   - root          supergroup          0 2016-11-14 14:02 webhdfs://hadoop219:50070/recommend_v2\ndrwxr-xr-x   - hadoop        supergroup          0 2016-07-06 08:29 webhdfs://hadoop219:50070/system\ndrwx-wx-wx   - root          supergroup          0 2016-09-12 20:17 webhdfs://hadoop219:50070/tmp\ndrwxr-xr-x   - root          supergroup          0 2016-07-21 17:09 webhdfs://hadoop219:50070/user\ndrwxr-xr-x   - root          supergroup          0 2017-05-11 18:37 webhdfs://hadoop219:50070/yl_test\n</code></pre>\n<h4 id=\"Check-sum-mismatch\"><a href=\"#Check-sum-mismatch\" class=\"headerlink\" title=\"Check-sum mismatch\"></a>Check-sum mismatch</h4><p>异常信息如下：</p>\n<pre><code>Caused by: java.io.IOException: Check-sum mismatch between webhdfs://hadoop219:50070/code/bss_city_device/2017-07-31/part-m-00002 and hdfs://ycluster/data/data-platform/cluster219/code/.distcp.tmp.attempt_1506582668997_345304_m_000011_1. Source and target differ in block-size. Use -pb to preserve block-sizes during copy. Alternatively, skip checksum-checks altogether, using -skipCrc. (NOTE: By skipping checksums, one runs the risk of masking data-corruption during file-transfer.)\n    at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.compareCheckSums(RetriableFileCopyCommand.java:210)\n    at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doCopy(RetriableFileCopyCommand.java:130)\n    at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doExecute(RetriableFileCopyCommand.java:99)\n    at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:87)\n    ... 11 more\n</code></pre>\n<p>根据异常信息可以看到是两个集群之间 block size 不一致引起的。增加 -pb 参数禁止块大小检查。</p>"},{"title":"Hadoop 进程关闭时报 no 进程名 to stop","date":"2017-02-24T05:49:45.000Z","_content":"\n### 问题描述\n\n在停止 Hadoop 的 resourcemanager 时遇到 no resourcemanager to stop 的问题。查看 resourcemanager 进程确实在运行。![no resourcemanager to stop](/uploads/20170224/no-resourcemanager-to-stop.png)\n\n<!-- more -->\n\n查看 yarn-daemon.sh 中 stop 部分的代码：\n\n    (stop)\n\n      if [ -f $pid ]; then\n        TARGET_PID=`cat $pid`\n        if kill -0 $TARGET_PID > /dev/null 2>&1; then\n          echo stopping $command\n          kill $TARGET_PID\n          sleep $YARN_STOP_TIMEOUT\n          if kill -0 $TARGET_PID > /dev/null 2>&1; then\n            echo \"$command did not stop gracefully after $YARN_STOP_TIMEOUT seconds: killing with kill -9\"\n            kill -9 $TARGET_PID\n          fi\n        else\n          echo no $command to stop\n        fi\n        rm -f $pid\n      else\n        echo no $command to stop\n      fi\n      ;;\n\n在停止 resourcemanager 的时候会先从 pid 文件中获取进程的 pid（TARGET_PID=`cat $pid`）。之后的操作都是对这个 pid 进行。默认情况下，这个 pid 的文件会放在 /tmp/ 目录下。到该目录下查看，确实没有进程的 pid 文件。\n\n### 为什么 pid 文件不见了？\n\n是因为系统安装了 tmpwatch。从 /etc/cron.daily/ 下可以看到 tmpwatch，内容如下：\n\n    #! /bin/sh\n    flags=-umc\n    /usr/sbin/tmpwatch \"$flags\" -x /tmp/.X11-unix -x /tmp/.XIM-unix \\\n            -x /tmp/.font-unix -x /tmp/.ICE-unix -x /tmp/.Test-unix \\\n            -X '/tmp/hsperfdata_*' 10d /tmp\n    /usr/sbin/tmpwatch \"$flags\" 30d /var/tmp\n    for d in /var/{cache/man,catman}/{cat?,X11R6/cat?,local/cat?}; do\n        if [ -d \"$d\" ]; then\n            /usr/sbin/tmpwatch \"$flags\" -f 30d \"$d\"\n        fi\n    done\n\n可以看到，/tmp 下超过 10 天的文件会被清理掉。\n\n### 如何停止进程\n\n从 yarn-daemon.sh 中可以看到，只要有 pid，执行 kill pid 结束进程。kill 是一种安全结束进程的方式，如果 kill 不起作用可以用 kill -9 强制结束进程。\n\n### 设置 pid 文件位置\n\n在 yarn-daemon.sh 中设置 pid 文件目录位置，如下：\n\n    export YARN_PID_DIR=$HADOOP_YARN_HOME/pids\n    if [ \"$YARN_PID_DIR\" = \"\" ]; then\n      YARN_PID_DIR=/tmp\n    fi\n","source":"_posts/Hadoop-进程关闭时报-no-进程名-to-stop.md","raw":"title: Hadoop 进程关闭时报 no 进程名 to stop\ntags:\n  - Hadoop\ncategories:\n  - 大数据\n  - Hadoop\ndate: 2017-02-24 13:49:45\n---\n\n### 问题描述\n\n在停止 Hadoop 的 resourcemanager 时遇到 no resourcemanager to stop 的问题。查看 resourcemanager 进程确实在运行。![no resourcemanager to stop](/uploads/20170224/no-resourcemanager-to-stop.png)\n\n<!-- more -->\n\n查看 yarn-daemon.sh 中 stop 部分的代码：\n\n    (stop)\n\n      if [ -f $pid ]; then\n        TARGET_PID=`cat $pid`\n        if kill -0 $TARGET_PID > /dev/null 2>&1; then\n          echo stopping $command\n          kill $TARGET_PID\n          sleep $YARN_STOP_TIMEOUT\n          if kill -0 $TARGET_PID > /dev/null 2>&1; then\n            echo \"$command did not stop gracefully after $YARN_STOP_TIMEOUT seconds: killing with kill -9\"\n            kill -9 $TARGET_PID\n          fi\n        else\n          echo no $command to stop\n        fi\n        rm -f $pid\n      else\n        echo no $command to stop\n      fi\n      ;;\n\n在停止 resourcemanager 的时候会先从 pid 文件中获取进程的 pid（TARGET_PID=`cat $pid`）。之后的操作都是对这个 pid 进行。默认情况下，这个 pid 的文件会放在 /tmp/ 目录下。到该目录下查看，确实没有进程的 pid 文件。\n\n### 为什么 pid 文件不见了？\n\n是因为系统安装了 tmpwatch。从 /etc/cron.daily/ 下可以看到 tmpwatch，内容如下：\n\n    #! /bin/sh\n    flags=-umc\n    /usr/sbin/tmpwatch \"$flags\" -x /tmp/.X11-unix -x /tmp/.XIM-unix \\\n            -x /tmp/.font-unix -x /tmp/.ICE-unix -x /tmp/.Test-unix \\\n            -X '/tmp/hsperfdata_*' 10d /tmp\n    /usr/sbin/tmpwatch \"$flags\" 30d /var/tmp\n    for d in /var/{cache/man,catman}/{cat?,X11R6/cat?,local/cat?}; do\n        if [ -d \"$d\" ]; then\n            /usr/sbin/tmpwatch \"$flags\" -f 30d \"$d\"\n        fi\n    done\n\n可以看到，/tmp 下超过 10 天的文件会被清理掉。\n\n### 如何停止进程\n\n从 yarn-daemon.sh 中可以看到，只要有 pid，执行 kill pid 结束进程。kill 是一种安全结束进程的方式，如果 kill 不起作用可以用 kill -9 强制结束进程。\n\n### 设置 pid 文件位置\n\n在 yarn-daemon.sh 中设置 pid 文件目录位置，如下：\n\n    export YARN_PID_DIR=$HADOOP_YARN_HOME/pids\n    if [ \"$YARN_PID_DIR\" = \"\" ]; then\n      YARN_PID_DIR=/tmp\n    fi\n","slug":"Hadoop-进程关闭时报-no-进程名-to-stop","published":1,"updated":"2021-07-19T16:28:00.252Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphmj002ritd3crvp62qa","content":"<h3 id=\"问题描述\"><a href=\"#问题描述\" class=\"headerlink\" title=\"问题描述\"></a>问题描述</h3><p>在停止 Hadoop 的 resourcemanager 时遇到 no resourcemanager to stop 的问题。查看 resourcemanager 进程确实在运行。<img src=\"/uploads/20170224/no-resourcemanager-to-stop.png\" alt=\"no resourcemanager to stop\"></p>\n<span id=\"more\"></span>\n\n<p>查看 yarn-daemon.sh 中 stop 部分的代码：</p>\n<pre><code>(stop)\n\n  if [ -f $pid ]; then\n    TARGET_PID=`cat $pid`\n    if kill -0 $TARGET_PID &gt; /dev/null 2&gt;&amp;1; then\n      echo stopping $command\n      kill $TARGET_PID\n      sleep $YARN_STOP_TIMEOUT\n      if kill -0 $TARGET_PID &gt; /dev/null 2&gt;&amp;1; then\n        echo &quot;$command did not stop gracefully after $YARN_STOP_TIMEOUT seconds: killing with kill -9&quot;\n        kill -9 $TARGET_PID\n      fi\n    else\n      echo no $command to stop\n    fi\n    rm -f $pid\n  else\n    echo no $command to stop\n  fi\n  ;;\n</code></pre>\n<p>在停止 resourcemanager 的时候会先从 pid 文件中获取进程的 pid（TARGET_PID=<code>cat $pid</code>）。之后的操作都是对这个 pid 进行。默认情况下，这个 pid 的文件会放在 /tmp/ 目录下。到该目录下查看，确实没有进程的 pid 文件。</p>\n<h3 id=\"为什么-pid-文件不见了？\"><a href=\"#为什么-pid-文件不见了？\" class=\"headerlink\" title=\"为什么 pid 文件不见了？\"></a>为什么 pid 文件不见了？</h3><p>是因为系统安装了 tmpwatch。从 /etc/cron.daily/ 下可以看到 tmpwatch，内容如下：</p>\n<pre><code>#! /bin/sh\nflags=-umc\n/usr/sbin/tmpwatch &quot;$flags&quot; -x /tmp/.X11-unix -x /tmp/.XIM-unix \\\n        -x /tmp/.font-unix -x /tmp/.ICE-unix -x /tmp/.Test-unix \\\n        -X &#39;/tmp/hsperfdata_*&#39; 10d /tmp\n/usr/sbin/tmpwatch &quot;$flags&quot; 30d /var/tmp\nfor d in /var/&#123;cache/man,catman&#125;/&#123;cat?,X11R6/cat?,local/cat?&#125;; do\n    if [ -d &quot;$d&quot; ]; then\n        /usr/sbin/tmpwatch &quot;$flags&quot; -f 30d &quot;$d&quot;\n    fi\ndone\n</code></pre>\n<p>可以看到，/tmp 下超过 10 天的文件会被清理掉。</p>\n<h3 id=\"如何停止进程\"><a href=\"#如何停止进程\" class=\"headerlink\" title=\"如何停止进程\"></a>如何停止进程</h3><p>从 yarn-daemon.sh 中可以看到，只要有 pid，执行 kill pid 结束进程。kill 是一种安全结束进程的方式，如果 kill 不起作用可以用 kill -9 强制结束进程。</p>\n<h3 id=\"设置-pid-文件位置\"><a href=\"#设置-pid-文件位置\" class=\"headerlink\" title=\"设置 pid 文件位置\"></a>设置 pid 文件位置</h3><p>在 yarn-daemon.sh 中设置 pid 文件目录位置，如下：</p>\n<pre><code>export YARN_PID_DIR=$HADOOP_YARN_HOME/pids\nif [ &quot;$YARN_PID_DIR&quot; = &quot;&quot; ]; then\n  YARN_PID_DIR=/tmp\nfi\n</code></pre>\n","site":{"data":{}},"excerpt":"<h3 id=\"问题描述\"><a href=\"#问题描述\" class=\"headerlink\" title=\"问题描述\"></a>问题描述</h3><p>在停止 Hadoop 的 resourcemanager 时遇到 no resourcemanager to stop 的问题。查看 resourcemanager 进程确实在运行。<img src=\"/uploads/20170224/no-resourcemanager-to-stop.png\" alt=\"no resourcemanager to stop\"></p>","more":"<p>查看 yarn-daemon.sh 中 stop 部分的代码：</p>\n<pre><code>(stop)\n\n  if [ -f $pid ]; then\n    TARGET_PID=`cat $pid`\n    if kill -0 $TARGET_PID &gt; /dev/null 2&gt;&amp;1; then\n      echo stopping $command\n      kill $TARGET_PID\n      sleep $YARN_STOP_TIMEOUT\n      if kill -0 $TARGET_PID &gt; /dev/null 2&gt;&amp;1; then\n        echo &quot;$command did not stop gracefully after $YARN_STOP_TIMEOUT seconds: killing with kill -9&quot;\n        kill -9 $TARGET_PID\n      fi\n    else\n      echo no $command to stop\n    fi\n    rm -f $pid\n  else\n    echo no $command to stop\n  fi\n  ;;\n</code></pre>\n<p>在停止 resourcemanager 的时候会先从 pid 文件中获取进程的 pid（TARGET_PID=<code>cat $pid</code>）。之后的操作都是对这个 pid 进行。默认情况下，这个 pid 的文件会放在 /tmp/ 目录下。到该目录下查看，确实没有进程的 pid 文件。</p>\n<h3 id=\"为什么-pid-文件不见了？\"><a href=\"#为什么-pid-文件不见了？\" class=\"headerlink\" title=\"为什么 pid 文件不见了？\"></a>为什么 pid 文件不见了？</h3><p>是因为系统安装了 tmpwatch。从 /etc/cron.daily/ 下可以看到 tmpwatch，内容如下：</p>\n<pre><code>#! /bin/sh\nflags=-umc\n/usr/sbin/tmpwatch &quot;$flags&quot; -x /tmp/.X11-unix -x /tmp/.XIM-unix \\\n        -x /tmp/.font-unix -x /tmp/.ICE-unix -x /tmp/.Test-unix \\\n        -X &#39;/tmp/hsperfdata_*&#39; 10d /tmp\n/usr/sbin/tmpwatch &quot;$flags&quot; 30d /var/tmp\nfor d in /var/&#123;cache/man,catman&#125;/&#123;cat?,X11R6/cat?,local/cat?&#125;; do\n    if [ -d &quot;$d&quot; ]; then\n        /usr/sbin/tmpwatch &quot;$flags&quot; -f 30d &quot;$d&quot;\n    fi\ndone\n</code></pre>\n<p>可以看到，/tmp 下超过 10 天的文件会被清理掉。</p>\n<h3 id=\"如何停止进程\"><a href=\"#如何停止进程\" class=\"headerlink\" title=\"如何停止进程\"></a>如何停止进程</h3><p>从 yarn-daemon.sh 中可以看到，只要有 pid，执行 kill pid 结束进程。kill 是一种安全结束进程的方式，如果 kill 不起作用可以用 kill -9 强制结束进程。</p>\n<h3 id=\"设置-pid-文件位置\"><a href=\"#设置-pid-文件位置\" class=\"headerlink\" title=\"设置 pid 文件位置\"></a>设置 pid 文件位置</h3><p>在 yarn-daemon.sh 中设置 pid 文件目录位置，如下：</p>\n<pre><code>export YARN_PID_DIR=$HADOOP_YARN_HOME/pids\nif [ &quot;$YARN_PID_DIR&quot; = &quot;&quot; ]; then\n  YARN_PID_DIR=/tmp\nfi\n</code></pre>"},{"title":"Hadoop 集群修改服务绑定地址","date":"2017-05-21T03:54:35.000Z","_content":"\n有时集群服务器会有多个网卡，例如一个网卡用户服务器管理，一个网卡用于数据交换。一般这种情况下，数据交换网是不对终端用户开放的。所以，如果服务进程监听地址绑定到这个地址的话，通过管理地址是无法访问到该服务的。Hadoop 提供了以下配置项，可以将服务监听地址绑定到 0.0.0.0，这样通过任一地址都可以访问到服务了。如下：\n\n**hdfs-site.xml 中的配置项**\n\n- dfs.namenode.rpc-bind-host\n- dfs.namenode.http-bind-host\n\n**yarn-site.xml 中的配置项**\n\n- yarn.resourcemanager.bind-host\n- yarn.nodemanager.bind-host\n\n**mapred-site.xml 中的配置项**\n\n- mapreduce.jobhistory.webapp.address\n","source":"_posts/Hadoop-集群修改服务绑定地址.md","raw":"title: Hadoop 集群修改服务绑定地址\ntags:\n  - Hadoop\ncategories:\n  - 大数据\n  - Hadoop\ndate: 2017-05-21 11:54:35\n---\n\n有时集群服务器会有多个网卡，例如一个网卡用户服务器管理，一个网卡用于数据交换。一般这种情况下，数据交换网是不对终端用户开放的。所以，如果服务进程监听地址绑定到这个地址的话，通过管理地址是无法访问到该服务的。Hadoop 提供了以下配置项，可以将服务监听地址绑定到 0.0.0.0，这样通过任一地址都可以访问到服务了。如下：\n\n**hdfs-site.xml 中的配置项**\n\n- dfs.namenode.rpc-bind-host\n- dfs.namenode.http-bind-host\n\n**yarn-site.xml 中的配置项**\n\n- yarn.resourcemanager.bind-host\n- yarn.nodemanager.bind-host\n\n**mapred-site.xml 中的配置项**\n\n- mapreduce.jobhistory.webapp.address\n","slug":"Hadoop-集群修改服务绑定地址","published":1,"updated":"2021-07-19T16:28:00.252Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphml002vitd3e6nc5615","content":"<p>有时集群服务器会有多个网卡，例如一个网卡用户服务器管理，一个网卡用于数据交换。一般这种情况下，数据交换网是不对终端用户开放的。所以，如果服务进程监听地址绑定到这个地址的话，通过管理地址是无法访问到该服务的。Hadoop 提供了以下配置项，可以将服务监听地址绑定到 0.0.0.0，这样通过任一地址都可以访问到服务了。如下：</p>\n<p><strong>hdfs-site.xml 中的配置项</strong></p>\n<ul>\n<li>dfs.namenode.rpc-bind-host</li>\n<li>dfs.namenode.http-bind-host</li>\n</ul>\n<p><strong>yarn-site.xml 中的配置项</strong></p>\n<ul>\n<li>yarn.resourcemanager.bind-host</li>\n<li>yarn.nodemanager.bind-host</li>\n</ul>\n<p><strong>mapred-site.xml 中的配置项</strong></p>\n<ul>\n<li>mapreduce.jobhistory.webapp.address</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>有时集群服务器会有多个网卡，例如一个网卡用户服务器管理，一个网卡用于数据交换。一般这种情况下，数据交换网是不对终端用户开放的。所以，如果服务进程监听地址绑定到这个地址的话，通过管理地址是无法访问到该服务的。Hadoop 提供了以下配置项，可以将服务监听地址绑定到 0.0.0.0，这样通过任一地址都可以访问到服务了。如下：</p>\n<p><strong>hdfs-site.xml 中的配置项</strong></p>\n<ul>\n<li>dfs.namenode.rpc-bind-host</li>\n<li>dfs.namenode.http-bind-host</li>\n</ul>\n<p><strong>yarn-site.xml 中的配置项</strong></p>\n<ul>\n<li>yarn.resourcemanager.bind-host</li>\n<li>yarn.nodemanager.bind-host</li>\n</ul>\n<p><strong>mapred-site.xml 中的配置项</strong></p>\n<ul>\n<li>mapreduce.jobhistory.webapp.address</li>\n</ul>\n"},{"title":"Hadoop 集群各服务进程 PID 文件位置","date":"2017-03-14T07:36:34.000Z","_content":"\nPID 文件位置默认在 /tmp 下，这个目录中的文件可能被清理，导致 PID 找不到，参见[Hadoop 进程关闭时报 no 进程名 to stop](http://zhang-jc.github.io/2017/02/24/Hadoop-%E8%BF%9B%E7%A8%8B%E5%85%B3%E9%97%AD%E6%97%B6%E6%8A%A5-no-%E8%BF%9B%E7%A8%8B%E5%90%8D-to-stop/)。为了避免这个问题，需要修改 Hadoop 各服务进程 PID 文件的位置：\n\n<!-- more -->\n\n- namenode、datanode、journalnode\n\netc/hadoop/hadoop-env.sh 中增加：\n\n    export HADOOP_PID_DIR=${HADOOP_HOME}/pids\n\n- resourcemanager、timelineserver\n\nsbin/yarn-daemon.sh 中增加：\n\n    export YARN_PID_DIR=${HADOOP_HOME}/pids\n\n- MR jobhistoryserver\n\nsbin/mr-jobhistory-daemon.sh 中添加：\n\n    export HADOOP_MAPRED_PID_DIR=${HADOOP_HOME}/pids","source":"_posts/Hadoop-集群各服务进程-PID-文件位置.md","raw":"title: Hadoop 集群各服务进程 PID 文件位置\ntags:\n  - Hadoop\ncategories:\n  - 大数据\n  - Hadoop\ndate: 2017-03-14 15:36:34\n---\n\nPID 文件位置默认在 /tmp 下，这个目录中的文件可能被清理，导致 PID 找不到，参见[Hadoop 进程关闭时报 no 进程名 to stop](http://zhang-jc.github.io/2017/02/24/Hadoop-%E8%BF%9B%E7%A8%8B%E5%85%B3%E9%97%AD%E6%97%B6%E6%8A%A5-no-%E8%BF%9B%E7%A8%8B%E5%90%8D-to-stop/)。为了避免这个问题，需要修改 Hadoop 各服务进程 PID 文件的位置：\n\n<!-- more -->\n\n- namenode、datanode、journalnode\n\netc/hadoop/hadoop-env.sh 中增加：\n\n    export HADOOP_PID_DIR=${HADOOP_HOME}/pids\n\n- resourcemanager、timelineserver\n\nsbin/yarn-daemon.sh 中增加：\n\n    export YARN_PID_DIR=${HADOOP_HOME}/pids\n\n- MR jobhistoryserver\n\nsbin/mr-jobhistory-daemon.sh 中添加：\n\n    export HADOOP_MAPRED_PID_DIR=${HADOOP_HOME}/pids","slug":"Hadoop-集群各服务进程-PID-文件位置","published":1,"updated":"2021-07-19T16:28:00.252Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphmm002xitd34dt51kmg","content":"<p>PID 文件位置默认在 /tmp 下，这个目录中的文件可能被清理，导致 PID 找不到，参见<a href=\"http://zhang-jc.github.io/2017/02/24/Hadoop-%E8%BF%9B%E7%A8%8B%E5%85%B3%E9%97%AD%E6%97%B6%E6%8A%A5-no-%E8%BF%9B%E7%A8%8B%E5%90%8D-to-stop/\">Hadoop 进程关闭时报 no 进程名 to stop</a>。为了避免这个问题，需要修改 Hadoop 各服务进程 PID 文件的位置：</p>\n<span id=\"more\"></span>\n\n<ul>\n<li>namenode、datanode、journalnode</li>\n</ul>\n<p>etc/hadoop/hadoop-env.sh 中增加：</p>\n<pre><code>export HADOOP_PID_DIR=$&#123;HADOOP_HOME&#125;/pids\n</code></pre>\n<ul>\n<li>resourcemanager、timelineserver</li>\n</ul>\n<p>sbin/yarn-daemon.sh 中增加：</p>\n<pre><code>export YARN_PID_DIR=$&#123;HADOOP_HOME&#125;/pids\n</code></pre>\n<ul>\n<li>MR jobhistoryserver</li>\n</ul>\n<p>sbin/mr-jobhistory-daemon.sh 中添加：</p>\n<pre><code>export HADOOP_MAPRED_PID_DIR=$&#123;HADOOP_HOME&#125;/pids\n</code></pre>\n","site":{"data":{}},"excerpt":"<p>PID 文件位置默认在 /tmp 下，这个目录中的文件可能被清理，导致 PID 找不到，参见<a href=\"http://zhang-jc.github.io/2017/02/24/Hadoop-%E8%BF%9B%E7%A8%8B%E5%85%B3%E9%97%AD%E6%97%B6%E6%8A%A5-no-%E8%BF%9B%E7%A8%8B%E5%90%8D-to-stop/\">Hadoop 进程关闭时报 no 进程名 to stop</a>。为了避免这个问题，需要修改 Hadoop 各服务进程 PID 文件的位置：</p>","more":"<ul>\n<li>namenode、datanode、journalnode</li>\n</ul>\n<p>etc/hadoop/hadoop-env.sh 中增加：</p>\n<pre><code>export HADOOP_PID_DIR=$&#123;HADOOP_HOME&#125;/pids\n</code></pre>\n<ul>\n<li>resourcemanager、timelineserver</li>\n</ul>\n<p>sbin/yarn-daemon.sh 中增加：</p>\n<pre><code>export YARN_PID_DIR=$&#123;HADOOP_HOME&#125;/pids\n</code></pre>\n<ul>\n<li>MR jobhistoryserver</li>\n</ul>\n<p>sbin/mr-jobhistory-daemon.sh 中添加：</p>\n<pre><code>export HADOOP_MAPRED_PID_DIR=$&#123;HADOOP_HOME&#125;/pids\n</code></pre>"},{"title":"Hadoop 集群安装及配置实战","date":"2016-07-12T14:46:10.000Z","_content":"\n### 目的\n\n在三台机器上安装 Hadoop 集群，并配置启动，且成功运行 WordCount 示例程序。本文只设置了必要的配置。\n\n<!-- more -->\n\n### 准备工作\n\n#### 机器列表\n\n三台相同配置的虚拟机：192.168.1.133、192.168.1.134、192.168.1.139\n\n三台虚拟机配置互相免密码 SSH 登陆。\n\n#### 机器配置\n\n三台虚拟机配置如下：\n\n- 操作系统：Ubuntu Server 16.04 LTS\n- 内存：2G\n- 磁盘：15G\n\n#### 架构\n\n- 192.168.1.133：Master 主机，运行 NameNode、ResourceManager、MapReduce 作业历史服务器\n- 192.168.1.134、192.168.1.139：Slave 主机，运行 DataNode、NodeManager\n\n### 配置过程\n\n#### 配置 /etc/hosts\n\n在三台机器的 /etc/hosts 中添加以下配置：\n\n    192.168.1.133 master-hadoop\n    192.168.1.134 slave1-hadoop\n    192.168.1.139 slave2-hadoop\n\n#### 配置 slaves\n\n编辑 etc/hadoop/slaves 文件，内容如下：\n\n    192.168.1.134\n    192.168.1.139\n\n#### 设置 JAVA_HOME\n\n修改 etc/hadoop/hadoop-env.sh、etc/hadoop/mapred-env.sh、etc/hadoop/yarn-env.sh 三个配置文件中的 JAVA_HOME 参数。\n\n#### 配置 etc/hadoop/core-site.xml\n\n    <configuration>\n      <property>\n        <name>fs.defaultFS</name>\n        <value>hdfs://master-hadoop:9000</value>\n      </property>\n    </configuration>\n\n#### 配置 etc/hadoop/hdfs-site.xml\n\n    <configuration>\n      <property>\n        <name>dfs.replication</name>\n        <value>1</value>\n      </property>\n    </configuration>\n\n#### 配置 etc/hadoop/mapred-site.xml\n\n    <configuration>\n      <property>\n        <name>mapreduce.framework.name</name>\n        <value>yarn</value>\n      </property>\n      <property>\n        <name>mapreduce.jobhistory.address</name>\n        <value>master-hadoop:10020</value>\n      </property>\n      <property>\n        <name>mapreduce.jobhistory.webapp.address</name>\n        <value>master-hadoop:19888</value>\n      </property>\n    </configuration>\n\n#### 配置 etc/hadoop/yarn-site.xml\n\n    <configuration>\n      <property>\n        <name>yarn.resourcemanager.address</name>\n        <value>master-hadoop:8032</value>\n      </property>\n      <property>\n        <name>yarn.resourcemanager.scheduler.address</name>\n        <value>master-hadoop:8030</value>\n      </property>\n      <property>\n        <name>yarn.resourcemanager.resource-tracker.address</name>\n        <value>master-hadoop:8031</value>\n      </property>\n      <property>\n        <name>yarn.resourcemanager.admin.address</name>\n        <value>master-hadoop:18033</value>\n      </property>\n      <property>\n        <name>yarn.resourcemanager.webapp.address</name>\n        <value>master-hadoop:8088</value>\n      </property>\n      <property>\n        <name>yarn.nodemanager.aux-services</name>\n        <value>mapreduce_shuffle</value>\n      </property>\n    </configuration>\n\n#### 分发文件\n\n将配置完成的 hadoop 文件包分发到三台虚拟机的 /root 目录下（可以根据自己的情况选择）。\n\n### 启动 Hadoop\n\n#### 格式化 HDFS\n\n在 NameNode 主机上执行以下命令格式化 HDFS：\n\n    hdfs namenode -format\n\n#### 启动 NameNode\n\n登陆 192.168.1.133 节点，执行以下命令启动 NameNode：\n\n    # sbin/hadoop-daemon.sh --script hdfs start namenode\n\n#### 启动 DataNode\n\n在每个 Slave 节点上用以下命令启动 DataNode：\n\n    # sbin/hadoop-daemons.sh --script hdfs start datanode\n\n> 注：可以直接使用下面的命令启动 NameNode 和 DataNode，因为我们已经配置了 slaves 和 免密码登陆。\n\n    # sbin/start-dfs.sh\n\n#### 启动 ResourceManager\n\n登陆 192.168.1.133 ，执行以下命令启动 ResourceManager：\n\n    # sbin/yarn-daemon.sh start resourcemanager\n\n#### 启动 NodeManager\n\n在每个 Slave 节点上启动 NodeManager：\n\n    # sbin/yarn-daemons.sh start nodemanager\n\n> 注：可以直接使用下面的命令启动 NameNode 和 DataNode，因为我们已经配置了 slaves 和 免密码登陆。\n\n    # sbin/start-yarn.sh\n\n#### 启动 MapReduce 作业历史服务器\n\n登陆 192.168.1.133，执行以下命令启动 MapReduce 作业历史服务器：\n\n### 检查\n\n#### Web 界面\n\n分别打开下面三个 Web 界面检查启动正确性：\n\n| 守护进程 | Web 用户界面 |\n| :------ | :---------- |\n| NameNode | http://192.168.1.133:50070/ |\n| ResourceManager | http://192.168.1.133::8088/ |\n| MapReduce JobHistory Server | http://192.168.1.133:19888/ |\n\n#### 遇到的问题\n\n在检查 NameNode 的 Web 界面时，发现 live node 数量为 0，检查及解决步骤如下：\n\n1. 检查了 Slave 主机上的 DataNode 都已经启动；\n2. 检查 192.168.1.134 Slave 主机上 DataNode 启动 log，发现下面的警告信息，说明 DataNode 节点无法连接 NameNode 的 9000 端口：\n\n    2016-07-10 22:41:17,213 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: master-hadoop/192.168.1.133:9000\n    2016-07-10 22:41:23,216 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n    2016-07-10 22:41:24,217 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n    2016-07-10 22:41:25,219 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n    2016-07-10 22:41:26,220 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n    2016-07-10 22:41:27,222 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n    2016-07-10 22:41:28,223 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n    2016-07-10 22:41:29,225 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n    2016-07-10 22:41:30,227 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n    2016-07-10 22:41:31,229 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n    2016-07-10 22:41:32,231 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n\n3. 检查 NameNode 端口 9000 情况，信息如下所示，说明 NameNode 进程只监听了 127.0.1.1:9000 端口，除了自己之外其他主机都不能连接：\n\n    $ netstat -apn|grep 9000  \n    tcp        0      0 127.0.1.1:9000          0.0.0.0:*               LISTEN      4964/java\n\n4. 检查 /etc/hosts 配置文件，发现有如下配置：\n\n    127.0.1.1 master-hadoop\n\n5. 从 /etc/hosts 中删除上面一条配置，重启 NameNode，再次检查 NameNode 端口 9000 情况：\n\n    $ netstat -apn|grep 9000  \n    tcp        0      0 192.168.1.133:9000      0.0.0.0:*               LISTEN      1772/java        \n    tcp        0      0 192.168.1.133:9000      192.168.1.139:56002     ESTABLISHED 1772/java  \n    tcp        0      0 192.168.1.133:9000      192.168.1.134:35168     ESTABLISHED 1772/java\n\n6. 再次检查 NameNode 的 Web 界面发现 live node 数量为 2。\n\n### 运行 WordCount 示例程序\n\n    # hdfs dfs -mkdir -p /user/root/input\n    # hdfs dfs -put etc/hadoop/*.xml /user/root/input\n    # hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep /user/root/input /user/root/output 'dfs[a-z.]+'\n    # hdfs dfs -cat /user/root/output/*\n    1\tdfsadmin\n    1\tdfs.replication\n","source":"_posts/Hadoop-集群安装及配置实战.md","raw":"title: Hadoop 集群安装及配置实战\ntags:\n  - 大数据\n  - Hadoop\ncategories:\n  - 大数据\n  - Hadoop\ndate: 2016-07-12 22:46:10\n---\n\n### 目的\n\n在三台机器上安装 Hadoop 集群，并配置启动，且成功运行 WordCount 示例程序。本文只设置了必要的配置。\n\n<!-- more -->\n\n### 准备工作\n\n#### 机器列表\n\n三台相同配置的虚拟机：192.168.1.133、192.168.1.134、192.168.1.139\n\n三台虚拟机配置互相免密码 SSH 登陆。\n\n#### 机器配置\n\n三台虚拟机配置如下：\n\n- 操作系统：Ubuntu Server 16.04 LTS\n- 内存：2G\n- 磁盘：15G\n\n#### 架构\n\n- 192.168.1.133：Master 主机，运行 NameNode、ResourceManager、MapReduce 作业历史服务器\n- 192.168.1.134、192.168.1.139：Slave 主机，运行 DataNode、NodeManager\n\n### 配置过程\n\n#### 配置 /etc/hosts\n\n在三台机器的 /etc/hosts 中添加以下配置：\n\n    192.168.1.133 master-hadoop\n    192.168.1.134 slave1-hadoop\n    192.168.1.139 slave2-hadoop\n\n#### 配置 slaves\n\n编辑 etc/hadoop/slaves 文件，内容如下：\n\n    192.168.1.134\n    192.168.1.139\n\n#### 设置 JAVA_HOME\n\n修改 etc/hadoop/hadoop-env.sh、etc/hadoop/mapred-env.sh、etc/hadoop/yarn-env.sh 三个配置文件中的 JAVA_HOME 参数。\n\n#### 配置 etc/hadoop/core-site.xml\n\n    <configuration>\n      <property>\n        <name>fs.defaultFS</name>\n        <value>hdfs://master-hadoop:9000</value>\n      </property>\n    </configuration>\n\n#### 配置 etc/hadoop/hdfs-site.xml\n\n    <configuration>\n      <property>\n        <name>dfs.replication</name>\n        <value>1</value>\n      </property>\n    </configuration>\n\n#### 配置 etc/hadoop/mapred-site.xml\n\n    <configuration>\n      <property>\n        <name>mapreduce.framework.name</name>\n        <value>yarn</value>\n      </property>\n      <property>\n        <name>mapreduce.jobhistory.address</name>\n        <value>master-hadoop:10020</value>\n      </property>\n      <property>\n        <name>mapreduce.jobhistory.webapp.address</name>\n        <value>master-hadoop:19888</value>\n      </property>\n    </configuration>\n\n#### 配置 etc/hadoop/yarn-site.xml\n\n    <configuration>\n      <property>\n        <name>yarn.resourcemanager.address</name>\n        <value>master-hadoop:8032</value>\n      </property>\n      <property>\n        <name>yarn.resourcemanager.scheduler.address</name>\n        <value>master-hadoop:8030</value>\n      </property>\n      <property>\n        <name>yarn.resourcemanager.resource-tracker.address</name>\n        <value>master-hadoop:8031</value>\n      </property>\n      <property>\n        <name>yarn.resourcemanager.admin.address</name>\n        <value>master-hadoop:18033</value>\n      </property>\n      <property>\n        <name>yarn.resourcemanager.webapp.address</name>\n        <value>master-hadoop:8088</value>\n      </property>\n      <property>\n        <name>yarn.nodemanager.aux-services</name>\n        <value>mapreduce_shuffle</value>\n      </property>\n    </configuration>\n\n#### 分发文件\n\n将配置完成的 hadoop 文件包分发到三台虚拟机的 /root 目录下（可以根据自己的情况选择）。\n\n### 启动 Hadoop\n\n#### 格式化 HDFS\n\n在 NameNode 主机上执行以下命令格式化 HDFS：\n\n    hdfs namenode -format\n\n#### 启动 NameNode\n\n登陆 192.168.1.133 节点，执行以下命令启动 NameNode：\n\n    # sbin/hadoop-daemon.sh --script hdfs start namenode\n\n#### 启动 DataNode\n\n在每个 Slave 节点上用以下命令启动 DataNode：\n\n    # sbin/hadoop-daemons.sh --script hdfs start datanode\n\n> 注：可以直接使用下面的命令启动 NameNode 和 DataNode，因为我们已经配置了 slaves 和 免密码登陆。\n\n    # sbin/start-dfs.sh\n\n#### 启动 ResourceManager\n\n登陆 192.168.1.133 ，执行以下命令启动 ResourceManager：\n\n    # sbin/yarn-daemon.sh start resourcemanager\n\n#### 启动 NodeManager\n\n在每个 Slave 节点上启动 NodeManager：\n\n    # sbin/yarn-daemons.sh start nodemanager\n\n> 注：可以直接使用下面的命令启动 NameNode 和 DataNode，因为我们已经配置了 slaves 和 免密码登陆。\n\n    # sbin/start-yarn.sh\n\n#### 启动 MapReduce 作业历史服务器\n\n登陆 192.168.1.133，执行以下命令启动 MapReduce 作业历史服务器：\n\n### 检查\n\n#### Web 界面\n\n分别打开下面三个 Web 界面检查启动正确性：\n\n| 守护进程 | Web 用户界面 |\n| :------ | :---------- |\n| NameNode | http://192.168.1.133:50070/ |\n| ResourceManager | http://192.168.1.133::8088/ |\n| MapReduce JobHistory Server | http://192.168.1.133:19888/ |\n\n#### 遇到的问题\n\n在检查 NameNode 的 Web 界面时，发现 live node 数量为 0，检查及解决步骤如下：\n\n1. 检查了 Slave 主机上的 DataNode 都已经启动；\n2. 检查 192.168.1.134 Slave 主机上 DataNode 启动 log，发现下面的警告信息，说明 DataNode 节点无法连接 NameNode 的 9000 端口：\n\n    2016-07-10 22:41:17,213 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: master-hadoop/192.168.1.133:9000\n    2016-07-10 22:41:23,216 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n    2016-07-10 22:41:24,217 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n    2016-07-10 22:41:25,219 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n    2016-07-10 22:41:26,220 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n    2016-07-10 22:41:27,222 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n    2016-07-10 22:41:28,223 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n    2016-07-10 22:41:29,225 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n    2016-07-10 22:41:30,227 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n    2016-07-10 22:41:31,229 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n    2016-07-10 22:41:32,231 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n\n3. 检查 NameNode 端口 9000 情况，信息如下所示，说明 NameNode 进程只监听了 127.0.1.1:9000 端口，除了自己之外其他主机都不能连接：\n\n    $ netstat -apn|grep 9000  \n    tcp        0      0 127.0.1.1:9000          0.0.0.0:*               LISTEN      4964/java\n\n4. 检查 /etc/hosts 配置文件，发现有如下配置：\n\n    127.0.1.1 master-hadoop\n\n5. 从 /etc/hosts 中删除上面一条配置，重启 NameNode，再次检查 NameNode 端口 9000 情况：\n\n    $ netstat -apn|grep 9000  \n    tcp        0      0 192.168.1.133:9000      0.0.0.0:*               LISTEN      1772/java        \n    tcp        0      0 192.168.1.133:9000      192.168.1.139:56002     ESTABLISHED 1772/java  \n    tcp        0      0 192.168.1.133:9000      192.168.1.134:35168     ESTABLISHED 1772/java\n\n6. 再次检查 NameNode 的 Web 界面发现 live node 数量为 2。\n\n### 运行 WordCount 示例程序\n\n    # hdfs dfs -mkdir -p /user/root/input\n    # hdfs dfs -put etc/hadoop/*.xml /user/root/input\n    # hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep /user/root/input /user/root/output 'dfs[a-z.]+'\n    # hdfs dfs -cat /user/root/output/*\n    1\tdfsadmin\n    1\tdfs.replication\n","slug":"Hadoop-集群安装及配置实战","published":1,"updated":"2021-07-19T16:28:00.252Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphmn0030itd3h1hb350l","content":"<h3 id=\"目的\"><a href=\"#目的\" class=\"headerlink\" title=\"目的\"></a>目的</h3><p>在三台机器上安装 Hadoop 集群，并配置启动，且成功运行 WordCount 示例程序。本文只设置了必要的配置。</p>\n<span id=\"more\"></span>\n\n<h3 id=\"准备工作\"><a href=\"#准备工作\" class=\"headerlink\" title=\"准备工作\"></a>准备工作</h3><h4 id=\"机器列表\"><a href=\"#机器列表\" class=\"headerlink\" title=\"机器列表\"></a>机器列表</h4><p>三台相同配置的虚拟机：192.168.1.133、192.168.1.134、192.168.1.139</p>\n<p>三台虚拟机配置互相免密码 SSH 登陆。</p>\n<h4 id=\"机器配置\"><a href=\"#机器配置\" class=\"headerlink\" title=\"机器配置\"></a>机器配置</h4><p>三台虚拟机配置如下：</p>\n<ul>\n<li>操作系统：Ubuntu Server 16.04 LTS</li>\n<li>内存：2G</li>\n<li>磁盘：15G</li>\n</ul>\n<h4 id=\"架构\"><a href=\"#架构\" class=\"headerlink\" title=\"架构\"></a>架构</h4><ul>\n<li>192.168.1.133：Master 主机，运行 NameNode、ResourceManager、MapReduce 作业历史服务器</li>\n<li>192.168.1.134、192.168.1.139：Slave 主机，运行 DataNode、NodeManager</li>\n</ul>\n<h3 id=\"配置过程\"><a href=\"#配置过程\" class=\"headerlink\" title=\"配置过程\"></a>配置过程</h3><h4 id=\"配置-etc-hosts\"><a href=\"#配置-etc-hosts\" class=\"headerlink\" title=\"配置 /etc/hosts\"></a>配置 /etc/hosts</h4><p>在三台机器的 /etc/hosts 中添加以下配置：</p>\n<pre><code>192.168.1.133 master-hadoop\n192.168.1.134 slave1-hadoop\n192.168.1.139 slave2-hadoop\n</code></pre>\n<h4 id=\"配置-slaves\"><a href=\"#配置-slaves\" class=\"headerlink\" title=\"配置 slaves\"></a>配置 slaves</h4><p>编辑 etc/hadoop/slaves 文件，内容如下：</p>\n<pre><code>192.168.1.134\n192.168.1.139\n</code></pre>\n<h4 id=\"设置-JAVA-HOME\"><a href=\"#设置-JAVA-HOME\" class=\"headerlink\" title=\"设置 JAVA_HOME\"></a>设置 JAVA_HOME</h4><p>修改 etc/hadoop/hadoop-env.sh、etc/hadoop/mapred-env.sh、etc/hadoop/yarn-env.sh 三个配置文件中的 JAVA_HOME 参数。</p>\n<h4 id=\"配置-etc-hadoop-core-site-xml\"><a href=\"#配置-etc-hadoop-core-site-xml\" class=\"headerlink\" title=\"配置 etc/hadoop/core-site.xml\"></a>配置 etc/hadoop/core-site.xml</h4><pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;fs.defaultFS&lt;/name&gt;\n    &lt;value&gt;hdfs://master-hadoop:9000&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<h4 id=\"配置-etc-hadoop-hdfs-site-xml\"><a href=\"#配置-etc-hadoop-hdfs-site-xml\" class=\"headerlink\" title=\"配置 etc/hadoop/hdfs-site.xml\"></a>配置 etc/hadoop/hdfs-site.xml</h4><pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.replication&lt;/name&gt;\n    &lt;value&gt;1&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<h4 id=\"配置-etc-hadoop-mapred-site-xml\"><a href=\"#配置-etc-hadoop-mapred-site-xml\" class=\"headerlink\" title=\"配置 etc/hadoop/mapred-site.xml\"></a>配置 etc/hadoop/mapred-site.xml</h4><pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;\n    &lt;value&gt;yarn&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;\n    &lt;value&gt;master-hadoop:10020&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;\n    &lt;value&gt;master-hadoop:19888&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<h4 id=\"配置-etc-hadoop-yarn-site-xml\"><a href=\"#配置-etc-hadoop-yarn-site-xml\" class=\"headerlink\" title=\"配置 etc/hadoop/yarn-site.xml\"></a>配置 etc/hadoop/yarn-site.xml</h4><pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;\n    &lt;value&gt;master-hadoop:8032&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;\n    &lt;value&gt;master-hadoop:8030&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;\n    &lt;value&gt;master-hadoop:8031&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt;\n    &lt;value&gt;master-hadoop:18033&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;\n    &lt;value&gt;master-hadoop:8088&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<h4 id=\"分发文件\"><a href=\"#分发文件\" class=\"headerlink\" title=\"分发文件\"></a>分发文件</h4><p>将配置完成的 hadoop 文件包分发到三台虚拟机的 /root 目录下（可以根据自己的情况选择）。</p>\n<h3 id=\"启动-Hadoop\"><a href=\"#启动-Hadoop\" class=\"headerlink\" title=\"启动 Hadoop\"></a>启动 Hadoop</h3><h4 id=\"格式化-HDFS\"><a href=\"#格式化-HDFS\" class=\"headerlink\" title=\"格式化 HDFS\"></a>格式化 HDFS</h4><p>在 NameNode 主机上执行以下命令格式化 HDFS：</p>\n<pre><code>hdfs namenode -format\n</code></pre>\n<h4 id=\"启动-NameNode\"><a href=\"#启动-NameNode\" class=\"headerlink\" title=\"启动 NameNode\"></a>启动 NameNode</h4><p>登陆 192.168.1.133 节点，执行以下命令启动 NameNode：</p>\n<pre><code># sbin/hadoop-daemon.sh --script hdfs start namenode\n</code></pre>\n<h4 id=\"启动-DataNode\"><a href=\"#启动-DataNode\" class=\"headerlink\" title=\"启动 DataNode\"></a>启动 DataNode</h4><p>在每个 Slave 节点上用以下命令启动 DataNode：</p>\n<pre><code># sbin/hadoop-daemons.sh --script hdfs start datanode\n</code></pre>\n<blockquote>\n<p>注：可以直接使用下面的命令启动 NameNode 和 DataNode，因为我们已经配置了 slaves 和 免密码登陆。</p>\n</blockquote>\n<pre><code># sbin/start-dfs.sh\n</code></pre>\n<h4 id=\"启动-ResourceManager\"><a href=\"#启动-ResourceManager\" class=\"headerlink\" title=\"启动 ResourceManager\"></a>启动 ResourceManager</h4><p>登陆 192.168.1.133 ，执行以下命令启动 ResourceManager：</p>\n<pre><code># sbin/yarn-daemon.sh start resourcemanager\n</code></pre>\n<h4 id=\"启动-NodeManager\"><a href=\"#启动-NodeManager\" class=\"headerlink\" title=\"启动 NodeManager\"></a>启动 NodeManager</h4><p>在每个 Slave 节点上启动 NodeManager：</p>\n<pre><code># sbin/yarn-daemons.sh start nodemanager\n</code></pre>\n<blockquote>\n<p>注：可以直接使用下面的命令启动 NameNode 和 DataNode，因为我们已经配置了 slaves 和 免密码登陆。</p>\n</blockquote>\n<pre><code># sbin/start-yarn.sh\n</code></pre>\n<h4 id=\"启动-MapReduce-作业历史服务器\"><a href=\"#启动-MapReduce-作业历史服务器\" class=\"headerlink\" title=\"启动 MapReduce 作业历史服务器\"></a>启动 MapReduce 作业历史服务器</h4><p>登陆 192.168.1.133，执行以下命令启动 MapReduce 作业历史服务器：</p>\n<h3 id=\"检查\"><a href=\"#检查\" class=\"headerlink\" title=\"检查\"></a>检查</h3><h4 id=\"Web-界面\"><a href=\"#Web-界面\" class=\"headerlink\" title=\"Web 界面\"></a>Web 界面</h4><p>分别打开下面三个 Web 界面检查启动正确性：</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">守护进程</th>\n<th align=\"left\">Web 用户界面</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">NameNode</td>\n<td align=\"left\"><a href=\"http://192.168.1.133:50070/\">http://192.168.1.133:50070/</a></td>\n</tr>\n<tr>\n<td align=\"left\">ResourceManager</td>\n<td align=\"left\"><a href=\"http://192.168.1.133::8088/\">http://192.168.1.133::8088/</a></td>\n</tr>\n<tr>\n<td align=\"left\">MapReduce JobHistory Server</td>\n<td align=\"left\"><a href=\"http://192.168.1.133:19888/\">http://192.168.1.133:19888/</a></td>\n</tr>\n</tbody></table>\n<h4 id=\"遇到的问题\"><a href=\"#遇到的问题\" class=\"headerlink\" title=\"遇到的问题\"></a>遇到的问题</h4><p>在检查 NameNode 的 Web 界面时，发现 live node 数量为 0，检查及解决步骤如下：</p>\n<ol>\n<li><p>检查了 Slave 主机上的 DataNode 都已经启动；</p>\n</li>\n<li><p>检查 192.168.1.134 Slave 主机上 DataNode 启动 log，发现下面的警告信息，说明 DataNode 节点无法连接 NameNode 的 9000 端口：</p>\n<p> 2016-07-10 22:41:17,213 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: master-hadoop/192.168.1.133:9000<br> 2016-07-10 22:41:23,216 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)<br> 2016-07-10 22:41:24,217 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)<br> 2016-07-10 22:41:25,219 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)<br> 2016-07-10 22:41:26,220 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)<br> 2016-07-10 22:41:27,222 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)<br> 2016-07-10 22:41:28,223 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)<br> 2016-07-10 22:41:29,225 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)<br> 2016-07-10 22:41:30,227 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)<br> 2016-07-10 22:41:31,229 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)<br> 2016-07-10 22:41:32,231 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)</p>\n</li>\n<li><p>检查 NameNode 端口 9000 情况，信息如下所示，说明 NameNode 进程只监听了 127.0.1.1:9000 端口，除了自己之外其他主机都不能连接：</p>\n<p> $ netstat -apn|grep 9000<br> tcp        0      0 127.0.1.1:9000          0.0.0.0:*               LISTEN      4964/java</p>\n</li>\n<li><p>检查 /etc/hosts 配置文件，发现有如下配置：</p>\n<p> 127.0.1.1 master-hadoop</p>\n</li>\n<li><p>从 /etc/hosts 中删除上面一条配置，重启 NameNode，再次检查 NameNode 端口 9000 情况：</p>\n<p> $ netstat -apn|grep 9000<br> tcp        0      0 192.168.1.133:9000      0.0.0.0:*               LISTEN      1772/java<br> tcp        0      0 192.168.1.133:9000      192.168.1.139:56002     ESTABLISHED 1772/java<br> tcp        0      0 192.168.1.133:9000      192.168.1.134:35168     ESTABLISHED 1772/java</p>\n</li>\n<li><p>再次检查 NameNode 的 Web 界面发现 live node 数量为 2。</p>\n</li>\n</ol>\n<h3 id=\"运行-WordCount-示例程序\"><a href=\"#运行-WordCount-示例程序\" class=\"headerlink\" title=\"运行 WordCount 示例程序\"></a>运行 WordCount 示例程序</h3><pre><code># hdfs dfs -mkdir -p /user/root/input\n# hdfs dfs -put etc/hadoop/*.xml /user/root/input\n# hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep /user/root/input /user/root/output &#39;dfs[a-z.]+&#39;\n# hdfs dfs -cat /user/root/output/*\n1    dfsadmin\n1    dfs.replication\n</code></pre>\n","site":{"data":{}},"excerpt":"<h3 id=\"目的\"><a href=\"#目的\" class=\"headerlink\" title=\"目的\"></a>目的</h3><p>在三台机器上安装 Hadoop 集群，并配置启动，且成功运行 WordCount 示例程序。本文只设置了必要的配置。</p>","more":"<h3 id=\"准备工作\"><a href=\"#准备工作\" class=\"headerlink\" title=\"准备工作\"></a>准备工作</h3><h4 id=\"机器列表\"><a href=\"#机器列表\" class=\"headerlink\" title=\"机器列表\"></a>机器列表</h4><p>三台相同配置的虚拟机：192.168.1.133、192.168.1.134、192.168.1.139</p>\n<p>三台虚拟机配置互相免密码 SSH 登陆。</p>\n<h4 id=\"机器配置\"><a href=\"#机器配置\" class=\"headerlink\" title=\"机器配置\"></a>机器配置</h4><p>三台虚拟机配置如下：</p>\n<ul>\n<li>操作系统：Ubuntu Server 16.04 LTS</li>\n<li>内存：2G</li>\n<li>磁盘：15G</li>\n</ul>\n<h4 id=\"架构\"><a href=\"#架构\" class=\"headerlink\" title=\"架构\"></a>架构</h4><ul>\n<li>192.168.1.133：Master 主机，运行 NameNode、ResourceManager、MapReduce 作业历史服务器</li>\n<li>192.168.1.134、192.168.1.139：Slave 主机，运行 DataNode、NodeManager</li>\n</ul>\n<h3 id=\"配置过程\"><a href=\"#配置过程\" class=\"headerlink\" title=\"配置过程\"></a>配置过程</h3><h4 id=\"配置-etc-hosts\"><a href=\"#配置-etc-hosts\" class=\"headerlink\" title=\"配置 /etc/hosts\"></a>配置 /etc/hosts</h4><p>在三台机器的 /etc/hosts 中添加以下配置：</p>\n<pre><code>192.168.1.133 master-hadoop\n192.168.1.134 slave1-hadoop\n192.168.1.139 slave2-hadoop\n</code></pre>\n<h4 id=\"配置-slaves\"><a href=\"#配置-slaves\" class=\"headerlink\" title=\"配置 slaves\"></a>配置 slaves</h4><p>编辑 etc/hadoop/slaves 文件，内容如下：</p>\n<pre><code>192.168.1.134\n192.168.1.139\n</code></pre>\n<h4 id=\"设置-JAVA-HOME\"><a href=\"#设置-JAVA-HOME\" class=\"headerlink\" title=\"设置 JAVA_HOME\"></a>设置 JAVA_HOME</h4><p>修改 etc/hadoop/hadoop-env.sh、etc/hadoop/mapred-env.sh、etc/hadoop/yarn-env.sh 三个配置文件中的 JAVA_HOME 参数。</p>\n<h4 id=\"配置-etc-hadoop-core-site-xml\"><a href=\"#配置-etc-hadoop-core-site-xml\" class=\"headerlink\" title=\"配置 etc/hadoop/core-site.xml\"></a>配置 etc/hadoop/core-site.xml</h4><pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;fs.defaultFS&lt;/name&gt;\n    &lt;value&gt;hdfs://master-hadoop:9000&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<h4 id=\"配置-etc-hadoop-hdfs-site-xml\"><a href=\"#配置-etc-hadoop-hdfs-site-xml\" class=\"headerlink\" title=\"配置 etc/hadoop/hdfs-site.xml\"></a>配置 etc/hadoop/hdfs-site.xml</h4><pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.replication&lt;/name&gt;\n    &lt;value&gt;1&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<h4 id=\"配置-etc-hadoop-mapred-site-xml\"><a href=\"#配置-etc-hadoop-mapred-site-xml\" class=\"headerlink\" title=\"配置 etc/hadoop/mapred-site.xml\"></a>配置 etc/hadoop/mapred-site.xml</h4><pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;\n    &lt;value&gt;yarn&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;\n    &lt;value&gt;master-hadoop:10020&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;\n    &lt;value&gt;master-hadoop:19888&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<h4 id=\"配置-etc-hadoop-yarn-site-xml\"><a href=\"#配置-etc-hadoop-yarn-site-xml\" class=\"headerlink\" title=\"配置 etc/hadoop/yarn-site.xml\"></a>配置 etc/hadoop/yarn-site.xml</h4><pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;\n    &lt;value&gt;master-hadoop:8032&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;\n    &lt;value&gt;master-hadoop:8030&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;\n    &lt;value&gt;master-hadoop:8031&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt;\n    &lt;value&gt;master-hadoop:18033&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;\n    &lt;value&gt;master-hadoop:8088&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<h4 id=\"分发文件\"><a href=\"#分发文件\" class=\"headerlink\" title=\"分发文件\"></a>分发文件</h4><p>将配置完成的 hadoop 文件包分发到三台虚拟机的 /root 目录下（可以根据自己的情况选择）。</p>\n<h3 id=\"启动-Hadoop\"><a href=\"#启动-Hadoop\" class=\"headerlink\" title=\"启动 Hadoop\"></a>启动 Hadoop</h3><h4 id=\"格式化-HDFS\"><a href=\"#格式化-HDFS\" class=\"headerlink\" title=\"格式化 HDFS\"></a>格式化 HDFS</h4><p>在 NameNode 主机上执行以下命令格式化 HDFS：</p>\n<pre><code>hdfs namenode -format\n</code></pre>\n<h4 id=\"启动-NameNode\"><a href=\"#启动-NameNode\" class=\"headerlink\" title=\"启动 NameNode\"></a>启动 NameNode</h4><p>登陆 192.168.1.133 节点，执行以下命令启动 NameNode：</p>\n<pre><code># sbin/hadoop-daemon.sh --script hdfs start namenode\n</code></pre>\n<h4 id=\"启动-DataNode\"><a href=\"#启动-DataNode\" class=\"headerlink\" title=\"启动 DataNode\"></a>启动 DataNode</h4><p>在每个 Slave 节点上用以下命令启动 DataNode：</p>\n<pre><code># sbin/hadoop-daemons.sh --script hdfs start datanode\n</code></pre>\n<blockquote>\n<p>注：可以直接使用下面的命令启动 NameNode 和 DataNode，因为我们已经配置了 slaves 和 免密码登陆。</p>\n</blockquote>\n<pre><code># sbin/start-dfs.sh\n</code></pre>\n<h4 id=\"启动-ResourceManager\"><a href=\"#启动-ResourceManager\" class=\"headerlink\" title=\"启动 ResourceManager\"></a>启动 ResourceManager</h4><p>登陆 192.168.1.133 ，执行以下命令启动 ResourceManager：</p>\n<pre><code># sbin/yarn-daemon.sh start resourcemanager\n</code></pre>\n<h4 id=\"启动-NodeManager\"><a href=\"#启动-NodeManager\" class=\"headerlink\" title=\"启动 NodeManager\"></a>启动 NodeManager</h4><p>在每个 Slave 节点上启动 NodeManager：</p>\n<pre><code># sbin/yarn-daemons.sh start nodemanager\n</code></pre>\n<blockquote>\n<p>注：可以直接使用下面的命令启动 NameNode 和 DataNode，因为我们已经配置了 slaves 和 免密码登陆。</p>\n</blockquote>\n<pre><code># sbin/start-yarn.sh\n</code></pre>\n<h4 id=\"启动-MapReduce-作业历史服务器\"><a href=\"#启动-MapReduce-作业历史服务器\" class=\"headerlink\" title=\"启动 MapReduce 作业历史服务器\"></a>启动 MapReduce 作业历史服务器</h4><p>登陆 192.168.1.133，执行以下命令启动 MapReduce 作业历史服务器：</p>\n<h3 id=\"检查\"><a href=\"#检查\" class=\"headerlink\" title=\"检查\"></a>检查</h3><h4 id=\"Web-界面\"><a href=\"#Web-界面\" class=\"headerlink\" title=\"Web 界面\"></a>Web 界面</h4><p>分别打开下面三个 Web 界面检查启动正确性：</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">守护进程</th>\n<th align=\"left\">Web 用户界面</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">NameNode</td>\n<td align=\"left\"><a href=\"http://192.168.1.133:50070/\">http://192.168.1.133:50070/</a></td>\n</tr>\n<tr>\n<td align=\"left\">ResourceManager</td>\n<td align=\"left\"><a href=\"http://192.168.1.133::8088/\">http://192.168.1.133::8088/</a></td>\n</tr>\n<tr>\n<td align=\"left\">MapReduce JobHistory Server</td>\n<td align=\"left\"><a href=\"http://192.168.1.133:19888/\">http://192.168.1.133:19888/</a></td>\n</tr>\n</tbody></table>\n<h4 id=\"遇到的问题\"><a href=\"#遇到的问题\" class=\"headerlink\" title=\"遇到的问题\"></a>遇到的问题</h4><p>在检查 NameNode 的 Web 界面时，发现 live node 数量为 0，检查及解决步骤如下：</p>\n<ol>\n<li><p>检查了 Slave 主机上的 DataNode 都已经启动；</p>\n</li>\n<li><p>检查 192.168.1.134 Slave 主机上 DataNode 启动 log，发现下面的警告信息，说明 DataNode 节点无法连接 NameNode 的 9000 端口：</p>\n<p> 2016-07-10 22:41:17,213 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: master-hadoop/192.168.1.133:9000<br> 2016-07-10 22:41:23,216 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)<br> 2016-07-10 22:41:24,217 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)<br> 2016-07-10 22:41:25,219 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)<br> 2016-07-10 22:41:26,220 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)<br> 2016-07-10 22:41:27,222 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)<br> 2016-07-10 22:41:28,223 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)<br> 2016-07-10 22:41:29,225 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)<br> 2016-07-10 22:41:30,227 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)<br> 2016-07-10 22:41:31,229 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)<br> 2016-07-10 22:41:32,231 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master-hadoop/192.168.1.133:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)</p>\n</li>\n<li><p>检查 NameNode 端口 9000 情况，信息如下所示，说明 NameNode 进程只监听了 127.0.1.1:9000 端口，除了自己之外其他主机都不能连接：</p>\n<p> $ netstat -apn|grep 9000<br> tcp        0      0 127.0.1.1:9000          0.0.0.0:*               LISTEN      4964/java</p>\n</li>\n<li><p>检查 /etc/hosts 配置文件，发现有如下配置：</p>\n<p> 127.0.1.1 master-hadoop</p>\n</li>\n<li><p>从 /etc/hosts 中删除上面一条配置，重启 NameNode，再次检查 NameNode 端口 9000 情况：</p>\n<p> $ netstat -apn|grep 9000<br> tcp        0      0 192.168.1.133:9000      0.0.0.0:*               LISTEN      1772/java<br> tcp        0      0 192.168.1.133:9000      192.168.1.139:56002     ESTABLISHED 1772/java<br> tcp        0      0 192.168.1.133:9000      192.168.1.134:35168     ESTABLISHED 1772/java</p>\n</li>\n<li><p>再次检查 NameNode 的 Web 界面发现 live node 数量为 2。</p>\n</li>\n</ol>\n<h3 id=\"运行-WordCount-示例程序\"><a href=\"#运行-WordCount-示例程序\" class=\"headerlink\" title=\"运行 WordCount 示例程序\"></a>运行 WordCount 示例程序</h3><pre><code># hdfs dfs -mkdir -p /user/root/input\n# hdfs dfs -put etc/hadoop/*.xml /user/root/input\n# hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep /user/root/input /user/root/output &#39;dfs[a-z.]+&#39;\n# hdfs dfs -cat /user/root/output/*\n1    dfsadmin\n1    dfs.replication\n</code></pre>"},{"title":"Hadoop2.7.3 编译错误一例","date":"2021-01-26T08:39:01.000Z","_content":"\n错误信息如下：\n\n\t[ERROR] Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.7:run (dist) on project hadoop-kms: An Ant BuildException has occured: exec returned: 2\n\t[ERROR] around Ant part ...<exec failonerror=\"true\" dir=\"/data/jenkins/.jenkins/workspace/HttpFSProduct/hadoop-common-project/hadoop-kms/target\" executable=\"sh\">... @ 10:137 in /data/jenkins/.jenkins/workspace/HttpFSProduct/hadoop-common-project/hadoop-kms/target/antrun/build-main.xml\n\t[ERROR] -> [Help 1]\n\torg.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.7:run (dist) on project hadoop-kms: An Ant BuildException has occured: exec returned: 2\n\taround Ant part ...<exec failonerror=\"true\" dir=\"/data/jenkins/.jenkins/workspace/HttpFSProduct/hadoop-common-project/hadoop-kms/target\" executable=\"sh\">... @ 10:137 in /data/jenkins/.jenkins/workspace/HttpFSProduct/hadoop-common-project/hadoop-kms/target/antrun/build-main.xml\n\t    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:215)\n\t    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:156)\n\t    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:148)\n\t    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)\n\t    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)\n\t    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56)\n\t    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)\n\t    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:305)\n\t    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192)\n\t    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105)\n\t    at org.jvnet.hudson.maven3.launcher.Maven35Launcher.main (Maven35Launcher.java:130)\n\t    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\n\t    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\n\t    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\n\t    at java.lang.reflect.Method.invoke (Method.java:498)\n\t    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)\n\t    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)\n\t    at jenkins.maven3.agent.Maven35Main.launch (Maven35Main.java:176)\n\t    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\n\t    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\n\t    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\n\t    at java.lang.reflect.Method.invoke (Method.java:498)\n\t    at hudson.maven.Maven3Builder.call (Maven3Builder.java:139)\n\t    at hudson.maven.Maven3Builder.call (Maven3Builder.java:70)\n\t    at hudson.remoting.UserRequest.perform (UserRequest.java:212)\n\t    at hudson.remoting.UserRequest.perform (UserRequest.java:54)\n\t    at hudson.remoting.Request$2.run (Request.java:369)\n\t    at hudson.remoting.InterceptingExecutorService$1.call (InterceptingExecutorService.java:72)\n\t    at java.util.concurrent.FutureTask.run (FutureTask.java:266)\n\t    at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1149)\n\t    at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:624)\n\t    at java.lang.Thread.run (Thread.java:748)\n\tCaused by: org.apache.maven.plugin.MojoExecutionException: An Ant BuildException has occured: exec returned: 2\n\taround Ant part ...<exec failonerror=\"true\" dir=\"/data/jenkins/.jenkins/workspace/HttpFSProduct/hadoop-common-project/hadoop-kms/target\" executable=\"sh\">... @ 10:137 in /data/jenkins/.jenkins/workspace/HttpFSProduct/hadoop-common-project/hadoop-kms/target/antrun/build-main.xml\n\t    at org.apache.maven.plugin.antrun.AntRunMojo.execute (AntRunMojo.java:355)\n\t    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:137)\n\t    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:210)\n\t    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:156)\n\t    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:148)\n\t    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)\n\t    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)\n\t    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56)\n\t    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)\n\t    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:305)\n\t    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192)\n\t    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105)\n\t    at org.jvnet.hudson.maven3.launcher.Maven35Launcher.main (Maven35Launcher.java:130)\n\t    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\n\t    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\n\t    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\n\t    at java.lang.reflect.Method.invoke (Method.java:498)\n\t    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)\n\t    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)\n\t    at jenkins.maven3.agent.Maven35Main.launch (Maven35Main.java:176)\n\t    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\n\t    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\n\t    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\n\t    at java.lang.reflect.Method.invoke (Method.java:498)\n\t    at hudson.maven.Maven3Builder.call (Maven3Builder.java:139)\n\t    at hudson.maven.Maven3Builder.call (Maven3Builder.java:70)\n\t    at hudson.remoting.UserRequest.perform (UserRequest.java:212)\n\t    at hudson.remoting.UserRequest.perform (UserRequest.java:54)\n\t    at hudson.remoting.Request$2.run (Request.java:369)\n\t    at hudson.remoting.InterceptingExecutorService$1.call (InterceptingExecutorService.java:72)\n\t    at java.util.concurrent.FutureTask.run (FutureTask.java:266)\n\t    at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1149)\n\t    at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:624)\n\t    at java.lang.Thread.run (Thread.java:748)\n\tCaused by: org.apache.tools.ant.BuildException: exec returned: 2\n\t    at org.apache.tools.ant.taskdefs.ExecTask.runExecute (ExecTask.java:646)\n\t    at org.apache.tools.ant.taskdefs.ExecTask.runExec (ExecTask.java:672)\n\t    at org.apache.tools.ant.taskdefs.ExecTask.execute (ExecTask.java:498)\n\t    at org.apache.tools.ant.UnknownElement.execute (UnknownElement.java:291)\n\t    at sun.reflect.GeneratedMethodAccessor37.invoke (Unknown Source)\n\t    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\n\t    at java.lang.reflect.Method.invoke (Method.java:498)\n\t    at org.apache.tools.ant.dispatch.DispatchUtils.execute (DispatchUtils.java:106)\n\t    at org.apache.tools.ant.Task.perform (Task.java:348)\n\t    at org.apache.tools.ant.Target.execute (Target.java:390)\n\t    at org.apache.tools.ant.Target.performTasks (Target.java:411)\n\t    at org.apache.tools.ant.Project.executeSortedTargets (Project.java:1399)\n\t    at org.apache.tools.ant.Project.executeTarget (Project.java:1368)\n\t    at org.apache.maven.plugin.antrun.AntRunMojo.execute (AntRunMojo.java:327)\n\t    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:137)\n\t    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:210)\n\t    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:156)\n\t    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:148)\n\t    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)\n\t    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)\n\t    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56)\n\t    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)\n\t    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:305)\n\t    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192)\n\t    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105)\n\t    at org.jvnet.hudson.maven3.launcher.Maven35Launcher.main (Maven35Launcher.java:130)\n\t    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\n\t    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\n\t    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\n\t    at java.lang.reflect.Method.invoke (Method.java:498)\n\t    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)\n\t    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)\n\t    at jenkins.maven3.agent.Maven35Main.launch (Maven35Main.java:176)\n\t    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\n\t    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\n\t    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\n\t    at java.lang.reflect.Method.invoke (Method.java:498)\n\t    at hudson.maven.Maven3Builder.call (Maven3Builder.java:139)\n\t    at hudson.maven.Maven3Builder.call (Maven3Builder.java:70)\n\t    at hudson.remoting.UserRequest.perform (UserRequest.java:212)\n\t    at hudson.remoting.UserRequest.perform (UserRequest.java:54)\n\t    at hudson.remoting.Request$2.run (Request.java:369)\n\t    at hudson.remoting.InterceptingExecutorService$1.call (InterceptingExecutorService.java:72)\n\t    at java.util.concurrent.FutureTask.run (FutureTask.java:266)\n\t    at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1149)\n\t    at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:624)\n\t    at java.lang.Thread.run (Thread.java:748)\n\t[ERROR] \n\t[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n\t[ERROR] \n\t[ERROR] For more information about the errors and possible solutions, please read the following articles:\n\t[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException\n\t[ERROR] \n\t[ERROR] After correcting the problems, you can resume the build with the command\n\t[ERROR]   mvn <goals> -rf :hadoop-kms\n\n百度后发现是 tomcat 下载不完整导致的，只下载了2M。如下：\n\n\t# pwd\n\t/data/hadoop2.7.3/hadoop-common-project/hadoop-kms/downloads\n\t# ll\n\t总用量 1988\n\t-rw-rw-r-- 1 jenkins jenkins 2035659 1月  26 16:07 apache-tomcat-6.0.44.tar.gz\n\t# du -sh *\n\t2.0M\tapache-tomcat-6.0.44.tar.gz\n\t\n下载完整的包并放到上面的目录后重试解决。","source":"_posts/Hadoop2-7-3-编译错误一例.md","raw":"title: Hadoop2.7.3 编译错误一例\ndate: 2021-01-26 16:39:01\ntags:\n- Hadoop\n- 大数据\ncategories:\n- 大数据\n- Hadoop\n---\n\n错误信息如下：\n\n\t[ERROR] Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.7:run (dist) on project hadoop-kms: An Ant BuildException has occured: exec returned: 2\n\t[ERROR] around Ant part ...<exec failonerror=\"true\" dir=\"/data/jenkins/.jenkins/workspace/HttpFSProduct/hadoop-common-project/hadoop-kms/target\" executable=\"sh\">... @ 10:137 in /data/jenkins/.jenkins/workspace/HttpFSProduct/hadoop-common-project/hadoop-kms/target/antrun/build-main.xml\n\t[ERROR] -> [Help 1]\n\torg.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.7:run (dist) on project hadoop-kms: An Ant BuildException has occured: exec returned: 2\n\taround Ant part ...<exec failonerror=\"true\" dir=\"/data/jenkins/.jenkins/workspace/HttpFSProduct/hadoop-common-project/hadoop-kms/target\" executable=\"sh\">... @ 10:137 in /data/jenkins/.jenkins/workspace/HttpFSProduct/hadoop-common-project/hadoop-kms/target/antrun/build-main.xml\n\t    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:215)\n\t    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:156)\n\t    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:148)\n\t    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)\n\t    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)\n\t    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56)\n\t    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)\n\t    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:305)\n\t    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192)\n\t    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105)\n\t    at org.jvnet.hudson.maven3.launcher.Maven35Launcher.main (Maven35Launcher.java:130)\n\t    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\n\t    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\n\t    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\n\t    at java.lang.reflect.Method.invoke (Method.java:498)\n\t    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)\n\t    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)\n\t    at jenkins.maven3.agent.Maven35Main.launch (Maven35Main.java:176)\n\t    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\n\t    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\n\t    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\n\t    at java.lang.reflect.Method.invoke (Method.java:498)\n\t    at hudson.maven.Maven3Builder.call (Maven3Builder.java:139)\n\t    at hudson.maven.Maven3Builder.call (Maven3Builder.java:70)\n\t    at hudson.remoting.UserRequest.perform (UserRequest.java:212)\n\t    at hudson.remoting.UserRequest.perform (UserRequest.java:54)\n\t    at hudson.remoting.Request$2.run (Request.java:369)\n\t    at hudson.remoting.InterceptingExecutorService$1.call (InterceptingExecutorService.java:72)\n\t    at java.util.concurrent.FutureTask.run (FutureTask.java:266)\n\t    at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1149)\n\t    at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:624)\n\t    at java.lang.Thread.run (Thread.java:748)\n\tCaused by: org.apache.maven.plugin.MojoExecutionException: An Ant BuildException has occured: exec returned: 2\n\taround Ant part ...<exec failonerror=\"true\" dir=\"/data/jenkins/.jenkins/workspace/HttpFSProduct/hadoop-common-project/hadoop-kms/target\" executable=\"sh\">... @ 10:137 in /data/jenkins/.jenkins/workspace/HttpFSProduct/hadoop-common-project/hadoop-kms/target/antrun/build-main.xml\n\t    at org.apache.maven.plugin.antrun.AntRunMojo.execute (AntRunMojo.java:355)\n\t    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:137)\n\t    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:210)\n\t    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:156)\n\t    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:148)\n\t    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)\n\t    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)\n\t    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56)\n\t    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)\n\t    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:305)\n\t    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192)\n\t    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105)\n\t    at org.jvnet.hudson.maven3.launcher.Maven35Launcher.main (Maven35Launcher.java:130)\n\t    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\n\t    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\n\t    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\n\t    at java.lang.reflect.Method.invoke (Method.java:498)\n\t    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)\n\t    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)\n\t    at jenkins.maven3.agent.Maven35Main.launch (Maven35Main.java:176)\n\t    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\n\t    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\n\t    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\n\t    at java.lang.reflect.Method.invoke (Method.java:498)\n\t    at hudson.maven.Maven3Builder.call (Maven3Builder.java:139)\n\t    at hudson.maven.Maven3Builder.call (Maven3Builder.java:70)\n\t    at hudson.remoting.UserRequest.perform (UserRequest.java:212)\n\t    at hudson.remoting.UserRequest.perform (UserRequest.java:54)\n\t    at hudson.remoting.Request$2.run (Request.java:369)\n\t    at hudson.remoting.InterceptingExecutorService$1.call (InterceptingExecutorService.java:72)\n\t    at java.util.concurrent.FutureTask.run (FutureTask.java:266)\n\t    at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1149)\n\t    at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:624)\n\t    at java.lang.Thread.run (Thread.java:748)\n\tCaused by: org.apache.tools.ant.BuildException: exec returned: 2\n\t    at org.apache.tools.ant.taskdefs.ExecTask.runExecute (ExecTask.java:646)\n\t    at org.apache.tools.ant.taskdefs.ExecTask.runExec (ExecTask.java:672)\n\t    at org.apache.tools.ant.taskdefs.ExecTask.execute (ExecTask.java:498)\n\t    at org.apache.tools.ant.UnknownElement.execute (UnknownElement.java:291)\n\t    at sun.reflect.GeneratedMethodAccessor37.invoke (Unknown Source)\n\t    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\n\t    at java.lang.reflect.Method.invoke (Method.java:498)\n\t    at org.apache.tools.ant.dispatch.DispatchUtils.execute (DispatchUtils.java:106)\n\t    at org.apache.tools.ant.Task.perform (Task.java:348)\n\t    at org.apache.tools.ant.Target.execute (Target.java:390)\n\t    at org.apache.tools.ant.Target.performTasks (Target.java:411)\n\t    at org.apache.tools.ant.Project.executeSortedTargets (Project.java:1399)\n\t    at org.apache.tools.ant.Project.executeTarget (Project.java:1368)\n\t    at org.apache.maven.plugin.antrun.AntRunMojo.execute (AntRunMojo.java:327)\n\t    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:137)\n\t    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:210)\n\t    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:156)\n\t    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:148)\n\t    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)\n\t    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)\n\t    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56)\n\t    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)\n\t    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:305)\n\t    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192)\n\t    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105)\n\t    at org.jvnet.hudson.maven3.launcher.Maven35Launcher.main (Maven35Launcher.java:130)\n\t    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\n\t    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\n\t    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\n\t    at java.lang.reflect.Method.invoke (Method.java:498)\n\t    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)\n\t    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)\n\t    at jenkins.maven3.agent.Maven35Main.launch (Maven35Main.java:176)\n\t    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\n\t    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\n\t    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\n\t    at java.lang.reflect.Method.invoke (Method.java:498)\n\t    at hudson.maven.Maven3Builder.call (Maven3Builder.java:139)\n\t    at hudson.maven.Maven3Builder.call (Maven3Builder.java:70)\n\t    at hudson.remoting.UserRequest.perform (UserRequest.java:212)\n\t    at hudson.remoting.UserRequest.perform (UserRequest.java:54)\n\t    at hudson.remoting.Request$2.run (Request.java:369)\n\t    at hudson.remoting.InterceptingExecutorService$1.call (InterceptingExecutorService.java:72)\n\t    at java.util.concurrent.FutureTask.run (FutureTask.java:266)\n\t    at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1149)\n\t    at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:624)\n\t    at java.lang.Thread.run (Thread.java:748)\n\t[ERROR] \n\t[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n\t[ERROR] \n\t[ERROR] For more information about the errors and possible solutions, please read the following articles:\n\t[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException\n\t[ERROR] \n\t[ERROR] After correcting the problems, you can resume the build with the command\n\t[ERROR]   mvn <goals> -rf :hadoop-kms\n\n百度后发现是 tomcat 下载不完整导致的，只下载了2M。如下：\n\n\t# pwd\n\t/data/hadoop2.7.3/hadoop-common-project/hadoop-kms/downloads\n\t# ll\n\t总用量 1988\n\t-rw-rw-r-- 1 jenkins jenkins 2035659 1月  26 16:07 apache-tomcat-6.0.44.tar.gz\n\t# du -sh *\n\t2.0M\tapache-tomcat-6.0.44.tar.gz\n\t\n下载完整的包并放到上面的目录后重试解决。","slug":"Hadoop2-7-3-编译错误一例","published":1,"updated":"2021-07-19T16:28:00.116Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphmo0031itd3auueea6j","content":"<p>错误信息如下：</p>\n<pre><code>[ERROR] Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.7:run (dist) on project hadoop-kms: An Ant BuildException has occured: exec returned: 2\n[ERROR] around Ant part ...&lt;exec failonerror=&quot;true&quot; dir=&quot;/data/jenkins/.jenkins/workspace/HttpFSProduct/hadoop-common-project/hadoop-kms/target&quot; executable=&quot;sh&quot;&gt;... @ 10:137 in /data/jenkins/.jenkins/workspace/HttpFSProduct/hadoop-common-project/hadoop-kms/target/antrun/build-main.xml\n[ERROR] -&gt; [Help 1]\norg.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.7:run (dist) on project hadoop-kms: An Ant BuildException has occured: exec returned: 2\naround Ant part ...&lt;exec failonerror=&quot;true&quot; dir=&quot;/data/jenkins/.jenkins/workspace/HttpFSProduct/hadoop-common-project/hadoop-kms/target&quot; executable=&quot;sh&quot;&gt;... @ 10:137 in /data/jenkins/.jenkins/workspace/HttpFSProduct/hadoop-common-project/hadoop-kms/target/antrun/build-main.xml\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:215)\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:156)\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:148)\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)\n    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56)\n    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:305)\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192)\n    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105)\n    at org.jvnet.hudson.maven3.launcher.Maven35Launcher.main (Maven35Launcher.java:130)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke (Method.java:498)\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)\n    at jenkins.maven3.agent.Maven35Main.launch (Maven35Main.java:176)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke (Method.java:498)\n    at hudson.maven.Maven3Builder.call (Maven3Builder.java:139)\n    at hudson.maven.Maven3Builder.call (Maven3Builder.java:70)\n    at hudson.remoting.UserRequest.perform (UserRequest.java:212)\n    at hudson.remoting.UserRequest.perform (UserRequest.java:54)\n    at hudson.remoting.Request$2.run (Request.java:369)\n    at hudson.remoting.InterceptingExecutorService$1.call (InterceptingExecutorService.java:72)\n    at java.util.concurrent.FutureTask.run (FutureTask.java:266)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run (Thread.java:748)\nCaused by: org.apache.maven.plugin.MojoExecutionException: An Ant BuildException has occured: exec returned: 2\naround Ant part ...&lt;exec failonerror=&quot;true&quot; dir=&quot;/data/jenkins/.jenkins/workspace/HttpFSProduct/hadoop-common-project/hadoop-kms/target&quot; executable=&quot;sh&quot;&gt;... @ 10:137 in /data/jenkins/.jenkins/workspace/HttpFSProduct/hadoop-common-project/hadoop-kms/target/antrun/build-main.xml\n    at org.apache.maven.plugin.antrun.AntRunMojo.execute (AntRunMojo.java:355)\n    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:137)\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:210)\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:156)\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:148)\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)\n    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56)\n    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:305)\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192)\n    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105)\n    at org.jvnet.hudson.maven3.launcher.Maven35Launcher.main (Maven35Launcher.java:130)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke (Method.java:498)\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)\n    at jenkins.maven3.agent.Maven35Main.launch (Maven35Main.java:176)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke (Method.java:498)\n    at hudson.maven.Maven3Builder.call (Maven3Builder.java:139)\n    at hudson.maven.Maven3Builder.call (Maven3Builder.java:70)\n    at hudson.remoting.UserRequest.perform (UserRequest.java:212)\n    at hudson.remoting.UserRequest.perform (UserRequest.java:54)\n    at hudson.remoting.Request$2.run (Request.java:369)\n    at hudson.remoting.InterceptingExecutorService$1.call (InterceptingExecutorService.java:72)\n    at java.util.concurrent.FutureTask.run (FutureTask.java:266)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run (Thread.java:748)\nCaused by: org.apache.tools.ant.BuildException: exec returned: 2\n    at org.apache.tools.ant.taskdefs.ExecTask.runExecute (ExecTask.java:646)\n    at org.apache.tools.ant.taskdefs.ExecTask.runExec (ExecTask.java:672)\n    at org.apache.tools.ant.taskdefs.ExecTask.execute (ExecTask.java:498)\n    at org.apache.tools.ant.UnknownElement.execute (UnknownElement.java:291)\n    at sun.reflect.GeneratedMethodAccessor37.invoke (Unknown Source)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke (Method.java:498)\n    at org.apache.tools.ant.dispatch.DispatchUtils.execute (DispatchUtils.java:106)\n    at org.apache.tools.ant.Task.perform (Task.java:348)\n    at org.apache.tools.ant.Target.execute (Target.java:390)\n    at org.apache.tools.ant.Target.performTasks (Target.java:411)\n    at org.apache.tools.ant.Project.executeSortedTargets (Project.java:1399)\n    at org.apache.tools.ant.Project.executeTarget (Project.java:1368)\n    at org.apache.maven.plugin.antrun.AntRunMojo.execute (AntRunMojo.java:327)\n    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:137)\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:210)\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:156)\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:148)\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)\n    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56)\n    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:305)\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192)\n    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105)\n    at org.jvnet.hudson.maven3.launcher.Maven35Launcher.main (Maven35Launcher.java:130)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke (Method.java:498)\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)\n    at jenkins.maven3.agent.Maven35Main.launch (Maven35Main.java:176)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke (Method.java:498)\n    at hudson.maven.Maven3Builder.call (Maven3Builder.java:139)\n    at hudson.maven.Maven3Builder.call (Maven3Builder.java:70)\n    at hudson.remoting.UserRequest.perform (UserRequest.java:212)\n    at hudson.remoting.UserRequest.perform (UserRequest.java:54)\n    at hudson.remoting.Request$2.run (Request.java:369)\n    at hudson.remoting.InterceptingExecutorService$1.call (InterceptingExecutorService.java:72)\n    at java.util.concurrent.FutureTask.run (FutureTask.java:266)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run (Thread.java:748)\n[ERROR] \n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR] \n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException\n[ERROR] \n[ERROR] After correcting the problems, you can resume the build with the command\n[ERROR]   mvn &lt;goals&gt; -rf :hadoop-kms\n</code></pre>\n<p>百度后发现是 tomcat 下载不完整导致的，只下载了2M。如下：</p>\n<pre><code># pwd\n/data/hadoop2.7.3/hadoop-common-project/hadoop-kms/downloads\n# ll\n总用量 1988\n-rw-rw-r-- 1 jenkins jenkins 2035659 1月  26 16:07 apache-tomcat-6.0.44.tar.gz\n# du -sh *\n2.0M    apache-tomcat-6.0.44.tar.gz\n</code></pre>\n<p>下载完整的包并放到上面的目录后重试解决。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>错误信息如下：</p>\n<pre><code>[ERROR] Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.7:run (dist) on project hadoop-kms: An Ant BuildException has occured: exec returned: 2\n[ERROR] around Ant part ...&lt;exec failonerror=&quot;true&quot; dir=&quot;/data/jenkins/.jenkins/workspace/HttpFSProduct/hadoop-common-project/hadoop-kms/target&quot; executable=&quot;sh&quot;&gt;... @ 10:137 in /data/jenkins/.jenkins/workspace/HttpFSProduct/hadoop-common-project/hadoop-kms/target/antrun/build-main.xml\n[ERROR] -&gt; [Help 1]\norg.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.7:run (dist) on project hadoop-kms: An Ant BuildException has occured: exec returned: 2\naround Ant part ...&lt;exec failonerror=&quot;true&quot; dir=&quot;/data/jenkins/.jenkins/workspace/HttpFSProduct/hadoop-common-project/hadoop-kms/target&quot; executable=&quot;sh&quot;&gt;... @ 10:137 in /data/jenkins/.jenkins/workspace/HttpFSProduct/hadoop-common-project/hadoop-kms/target/antrun/build-main.xml\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:215)\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:156)\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:148)\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)\n    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56)\n    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:305)\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192)\n    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105)\n    at org.jvnet.hudson.maven3.launcher.Maven35Launcher.main (Maven35Launcher.java:130)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke (Method.java:498)\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)\n    at jenkins.maven3.agent.Maven35Main.launch (Maven35Main.java:176)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke (Method.java:498)\n    at hudson.maven.Maven3Builder.call (Maven3Builder.java:139)\n    at hudson.maven.Maven3Builder.call (Maven3Builder.java:70)\n    at hudson.remoting.UserRequest.perform (UserRequest.java:212)\n    at hudson.remoting.UserRequest.perform (UserRequest.java:54)\n    at hudson.remoting.Request$2.run (Request.java:369)\n    at hudson.remoting.InterceptingExecutorService$1.call (InterceptingExecutorService.java:72)\n    at java.util.concurrent.FutureTask.run (FutureTask.java:266)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run (Thread.java:748)\nCaused by: org.apache.maven.plugin.MojoExecutionException: An Ant BuildException has occured: exec returned: 2\naround Ant part ...&lt;exec failonerror=&quot;true&quot; dir=&quot;/data/jenkins/.jenkins/workspace/HttpFSProduct/hadoop-common-project/hadoop-kms/target&quot; executable=&quot;sh&quot;&gt;... @ 10:137 in /data/jenkins/.jenkins/workspace/HttpFSProduct/hadoop-common-project/hadoop-kms/target/antrun/build-main.xml\n    at org.apache.maven.plugin.antrun.AntRunMojo.execute (AntRunMojo.java:355)\n    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:137)\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:210)\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:156)\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:148)\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)\n    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56)\n    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:305)\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192)\n    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105)\n    at org.jvnet.hudson.maven3.launcher.Maven35Launcher.main (Maven35Launcher.java:130)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke (Method.java:498)\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)\n    at jenkins.maven3.agent.Maven35Main.launch (Maven35Main.java:176)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke (Method.java:498)\n    at hudson.maven.Maven3Builder.call (Maven3Builder.java:139)\n    at hudson.maven.Maven3Builder.call (Maven3Builder.java:70)\n    at hudson.remoting.UserRequest.perform (UserRequest.java:212)\n    at hudson.remoting.UserRequest.perform (UserRequest.java:54)\n    at hudson.remoting.Request$2.run (Request.java:369)\n    at hudson.remoting.InterceptingExecutorService$1.call (InterceptingExecutorService.java:72)\n    at java.util.concurrent.FutureTask.run (FutureTask.java:266)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run (Thread.java:748)\nCaused by: org.apache.tools.ant.BuildException: exec returned: 2\n    at org.apache.tools.ant.taskdefs.ExecTask.runExecute (ExecTask.java:646)\n    at org.apache.tools.ant.taskdefs.ExecTask.runExec (ExecTask.java:672)\n    at org.apache.tools.ant.taskdefs.ExecTask.execute (ExecTask.java:498)\n    at org.apache.tools.ant.UnknownElement.execute (UnknownElement.java:291)\n    at sun.reflect.GeneratedMethodAccessor37.invoke (Unknown Source)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke (Method.java:498)\n    at org.apache.tools.ant.dispatch.DispatchUtils.execute (DispatchUtils.java:106)\n    at org.apache.tools.ant.Task.perform (Task.java:348)\n    at org.apache.tools.ant.Target.execute (Target.java:390)\n    at org.apache.tools.ant.Target.performTasks (Target.java:411)\n    at org.apache.tools.ant.Project.executeSortedTargets (Project.java:1399)\n    at org.apache.tools.ant.Project.executeTarget (Project.java:1368)\n    at org.apache.maven.plugin.antrun.AntRunMojo.execute (AntRunMojo.java:327)\n    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:137)\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:210)\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:156)\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:148)\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)\n    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56)\n    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:305)\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192)\n    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105)\n    at org.jvnet.hudson.maven3.launcher.Maven35Launcher.main (Maven35Launcher.java:130)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke (Method.java:498)\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)\n    at jenkins.maven3.agent.Maven35Main.launch (Maven35Main.java:176)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke (Method.java:498)\n    at hudson.maven.Maven3Builder.call (Maven3Builder.java:139)\n    at hudson.maven.Maven3Builder.call (Maven3Builder.java:70)\n    at hudson.remoting.UserRequest.perform (UserRequest.java:212)\n    at hudson.remoting.UserRequest.perform (UserRequest.java:54)\n    at hudson.remoting.Request$2.run (Request.java:369)\n    at hudson.remoting.InterceptingExecutorService$1.call (InterceptingExecutorService.java:72)\n    at java.util.concurrent.FutureTask.run (FutureTask.java:266)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run (Thread.java:748)\n[ERROR] \n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR] \n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException\n[ERROR] \n[ERROR] After correcting the problems, you can resume the build with the command\n[ERROR]   mvn &lt;goals&gt; -rf :hadoop-kms\n</code></pre>\n<p>百度后发现是 tomcat 下载不完整导致的，只下载了2M。如下：</p>\n<pre><code># pwd\n/data/hadoop2.7.3/hadoop-common-project/hadoop-kms/downloads\n# ll\n总用量 1988\n-rw-rw-r-- 1 jenkins jenkins 2035659 1月  26 16:07 apache-tomcat-6.0.44.tar.gz\n# du -sh *\n2.0M    apache-tomcat-6.0.44.tar.gz\n</code></pre>\n<p>下载完整的包并放到上面的目录后重试解决。</p>\n"},{"title":"Hadoop：安装单个节点的集群","date":"2016-06-24T14:59:01.000Z","_content":"\n### 目的\n\n本文档描述如何安装并配置一个单节点 Hadoop，因此可以使用 Hadoop MapReduce 和分布式文件系统（HDFS）快速执行简单的操作。\n\n<!-- more -->\n\n### 前提\n\n#### 支持的平台\n\n- GNU/Linux 作为开发和产品平台都被支持。Hadoop 在 2000 台节点的集群上已经被证明。\n- Windows 也是一个被支持的平台，但下面的步骤只针对 Linux。在 Windows 上安装 Hadoop，参见 [wiki 页](http://wiki.apache.org/hadoop/Hadoop2OnWindows)\n\n#### 必须的软件\n\nLinux 必须的软件包括：\n\n1. 必须安装 Java™。[HadoopJavaVersions](http://wiki.apache.org/hadoop/HadoopJavaVersions) 中列出了推荐的 Java 版本。\n2. 必须安装 ssh 并且必须运行 sshd 来使用管理远程 Hadoop 守护进程的 Hadoop 脚本。\n\n#### 安装软件\n\n如果集群没有必需的软件则需要安装。在 Ubuntu Linux 上的示例：\n\n    $ sudo apt-get install ssh\n    $ sudo apt-get install rsync\n\n### 下载\n\n要获取一个 Hadoop 的发行包，从 [Apache 下载镜像列表](http://www.apache.org/dyn/closer.cgi/hadoop/common/)中的一个下载当前稳定发布版。\n\n### 准备启动 Hadoop 集群\n\n解包下载的 Hadoop 发行包。在发行包中，编辑文件 etc/hadoop/hadoop-env.sh 定义下面的一些参数：\n\n    # set to the root of your Java installation\n    export JAVA_HOME=/usr/java/latest\n\n尝试下面的命令：\n\n    $ bin/hadoop\n\n这个命令将展示 hadoop 脚本的使用文档。\n\n现在可以以支持的三种模式中的一种启动 Hadoop 集群：\n\n- 本地单例模式\n- 伪分布式模式\n- 完全分布式模式\n\n### 单例操作\n\n默认的，Hadoop 配置为以非分布式模式的单个 Java 进程运行。这对调试非常有帮助。\n\n下面的示例拷贝 Hadoop 安装目录下的配置文件作为输入，然后找出并展示所有匹配给定正则表达式的内容。输出内容写入指定的输出目录。\n\n    $ mkdir input\n    $ cp etc/hadoop/*.xml input\n    $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input output 'dfs[a-z.]+'\n    $ cat output/*\n\n### 伪分布式操作\n\nHadoop 也可以在单个节点上以伪分布式模式运行，每个 Hadoop 守护进程在一个独立的 Java 进程中运行。\n\n#### 配置\n\n使用以下配置：\n\netc/hadoop/core-site.xml:\n\n    <configuration>\n      <property>\n        <name>fs.defaultFS</name>\n        <value>hdfs://localhost:9000</value>\n      </property>\n    </configuration>\n\netc/hadoop/hdfs-site.xml:\n\n    <configuration>\n      <property>\n        <name>dfs.replication</name>\n        <value>1</value>\n      </property>\n    </configuration>\n\n#### 设置免密码 ssh\n\n检查是否可以无密码 ssh 到本地主机：\n\n    $ ssh localhost\n\n如果不能无密码 ssh 本地主机，则执行以下命令：\n\n> 官方手册中是用 dsa，但实际操作在 Ubuntu16.04 上没能通过。具体原因有待学习。\n> 使用 rsa 通过，下面也以 rsa 为例。\n\n    $ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\n    $ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n    $ chmod 0600 ~/.ssh/authorized_keys\n\n#### 执行\n\n下面的操作指南会执行一个本地 MapReduce 任务。如果想在 YARN 上执行一个任务，参见[单节点上的 YARN](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html#YARN_on_Single_Node)。\n\n1. 格式化文件系统：\n\n    $ bin/hdfs namenode -format\n\n2. 启动 NameNode 和 DataNode 后台进程：\n\n    $ sbin/start-dfs.sh\n\nHadoop 后台进程日志写入到 $HADOOP_LOG_DIR 目录（默认是 $HADOOP_HOME/logs）。\n\n3. 浏览 NameNode 的 Web 界面；默认是：\n\n- NameNode - http://localhost:50070/\n\n4. 创建执行 MapReduce 任务需要的 HDFS 目录：\n\n    $ bin/hdfs dfs -mkdir /user\n    $ bin/hdfs dfs -mkdir /user/<username>\n\n5. 拷贝输入文件到分布式文件系统：\n\n    $ bin/hdfs dfs -put etc/hadoop input\n\n6. 运行提供的例子：\n\n    $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input output 'dfs[a-z.]+'\n\n7. 检查输出文件：从分布式文件系统拷贝输出文件到本地文件系统并检查：\n\n    $ bin/hdfs dfs -get output output\n    $ cat output/*\n\n或者在分布式文件系统上查看输出文件：\n\n    $ bin/hdfs dfs -cat output/*\n\n8. 用下面的命令停止后台进程：\n\n    $ sbin/stop-dfs.sh\n\n#### 单个节点上的 YARN\n\n设置几个参数就可以在伪分布式模式下在 YARN 上运行 MapReduce 任务，并且运行另外的 ResourceManager 和 NodeManager 后台进程。\n\n下面的操作指南假设上面的 1.～4. 步已经执行。\n\n1. 配置参数如下：\n\netc/hadoop/mapred-site.xml：\n\n    <configuration>\n      <property>\n        <name>mapreduce.framework.name</name>\n        <value>yarn</value>\n      </property>\n    </configuration>\n\netc/hadoop/yarn-site.xml：\n\n    <configuration>\n      <property>\n        <name>yarn.nodemanager.aux-services</name>\n        <value>mapreduce_shuffle</value>\n      </property>\n    </configuration>\n\n2. 启动 ResourceManager 和 NodeManager 后台进程：\n\n    $ sbin/start-yarn.sh\n\n3. 浏览 ResourceManager 的 Web 界面；默认是：\n\n- ResourceManager - http://localhost:8088/\n\n4. 运行一个 MapReduce 任务。\n5. 用以下命令停止后台进程：\n\n    $ sbin/stop-yarn.sh\n","source":"_posts/Hadoop：安装单个节点的集群.md","raw":"title: Hadoop：安装单个节点的集群\ntags:\n  - 大数据\n  - Hadoop\ncategories:\n  - 大数据\n  - Hadoop\ndate: 2016-06-24 22:59:01\n---\n\n### 目的\n\n本文档描述如何安装并配置一个单节点 Hadoop，因此可以使用 Hadoop MapReduce 和分布式文件系统（HDFS）快速执行简单的操作。\n\n<!-- more -->\n\n### 前提\n\n#### 支持的平台\n\n- GNU/Linux 作为开发和产品平台都被支持。Hadoop 在 2000 台节点的集群上已经被证明。\n- Windows 也是一个被支持的平台，但下面的步骤只针对 Linux。在 Windows 上安装 Hadoop，参见 [wiki 页](http://wiki.apache.org/hadoop/Hadoop2OnWindows)\n\n#### 必须的软件\n\nLinux 必须的软件包括：\n\n1. 必须安装 Java™。[HadoopJavaVersions](http://wiki.apache.org/hadoop/HadoopJavaVersions) 中列出了推荐的 Java 版本。\n2. 必须安装 ssh 并且必须运行 sshd 来使用管理远程 Hadoop 守护进程的 Hadoop 脚本。\n\n#### 安装软件\n\n如果集群没有必需的软件则需要安装。在 Ubuntu Linux 上的示例：\n\n    $ sudo apt-get install ssh\n    $ sudo apt-get install rsync\n\n### 下载\n\n要获取一个 Hadoop 的发行包，从 [Apache 下载镜像列表](http://www.apache.org/dyn/closer.cgi/hadoop/common/)中的一个下载当前稳定发布版。\n\n### 准备启动 Hadoop 集群\n\n解包下载的 Hadoop 发行包。在发行包中，编辑文件 etc/hadoop/hadoop-env.sh 定义下面的一些参数：\n\n    # set to the root of your Java installation\n    export JAVA_HOME=/usr/java/latest\n\n尝试下面的命令：\n\n    $ bin/hadoop\n\n这个命令将展示 hadoop 脚本的使用文档。\n\n现在可以以支持的三种模式中的一种启动 Hadoop 集群：\n\n- 本地单例模式\n- 伪分布式模式\n- 完全分布式模式\n\n### 单例操作\n\n默认的，Hadoop 配置为以非分布式模式的单个 Java 进程运行。这对调试非常有帮助。\n\n下面的示例拷贝 Hadoop 安装目录下的配置文件作为输入，然后找出并展示所有匹配给定正则表达式的内容。输出内容写入指定的输出目录。\n\n    $ mkdir input\n    $ cp etc/hadoop/*.xml input\n    $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input output 'dfs[a-z.]+'\n    $ cat output/*\n\n### 伪分布式操作\n\nHadoop 也可以在单个节点上以伪分布式模式运行，每个 Hadoop 守护进程在一个独立的 Java 进程中运行。\n\n#### 配置\n\n使用以下配置：\n\netc/hadoop/core-site.xml:\n\n    <configuration>\n      <property>\n        <name>fs.defaultFS</name>\n        <value>hdfs://localhost:9000</value>\n      </property>\n    </configuration>\n\netc/hadoop/hdfs-site.xml:\n\n    <configuration>\n      <property>\n        <name>dfs.replication</name>\n        <value>1</value>\n      </property>\n    </configuration>\n\n#### 设置免密码 ssh\n\n检查是否可以无密码 ssh 到本地主机：\n\n    $ ssh localhost\n\n如果不能无密码 ssh 本地主机，则执行以下命令：\n\n> 官方手册中是用 dsa，但实际操作在 Ubuntu16.04 上没能通过。具体原因有待学习。\n> 使用 rsa 通过，下面也以 rsa 为例。\n\n    $ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\n    $ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n    $ chmod 0600 ~/.ssh/authorized_keys\n\n#### 执行\n\n下面的操作指南会执行一个本地 MapReduce 任务。如果想在 YARN 上执行一个任务，参见[单节点上的 YARN](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html#YARN_on_Single_Node)。\n\n1. 格式化文件系统：\n\n    $ bin/hdfs namenode -format\n\n2. 启动 NameNode 和 DataNode 后台进程：\n\n    $ sbin/start-dfs.sh\n\nHadoop 后台进程日志写入到 $HADOOP_LOG_DIR 目录（默认是 $HADOOP_HOME/logs）。\n\n3. 浏览 NameNode 的 Web 界面；默认是：\n\n- NameNode - http://localhost:50070/\n\n4. 创建执行 MapReduce 任务需要的 HDFS 目录：\n\n    $ bin/hdfs dfs -mkdir /user\n    $ bin/hdfs dfs -mkdir /user/<username>\n\n5. 拷贝输入文件到分布式文件系统：\n\n    $ bin/hdfs dfs -put etc/hadoop input\n\n6. 运行提供的例子：\n\n    $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input output 'dfs[a-z.]+'\n\n7. 检查输出文件：从分布式文件系统拷贝输出文件到本地文件系统并检查：\n\n    $ bin/hdfs dfs -get output output\n    $ cat output/*\n\n或者在分布式文件系统上查看输出文件：\n\n    $ bin/hdfs dfs -cat output/*\n\n8. 用下面的命令停止后台进程：\n\n    $ sbin/stop-dfs.sh\n\n#### 单个节点上的 YARN\n\n设置几个参数就可以在伪分布式模式下在 YARN 上运行 MapReduce 任务，并且运行另外的 ResourceManager 和 NodeManager 后台进程。\n\n下面的操作指南假设上面的 1.～4. 步已经执行。\n\n1. 配置参数如下：\n\netc/hadoop/mapred-site.xml：\n\n    <configuration>\n      <property>\n        <name>mapreduce.framework.name</name>\n        <value>yarn</value>\n      </property>\n    </configuration>\n\netc/hadoop/yarn-site.xml：\n\n    <configuration>\n      <property>\n        <name>yarn.nodemanager.aux-services</name>\n        <value>mapreduce_shuffle</value>\n      </property>\n    </configuration>\n\n2. 启动 ResourceManager 和 NodeManager 后台进程：\n\n    $ sbin/start-yarn.sh\n\n3. 浏览 ResourceManager 的 Web 界面；默认是：\n\n- ResourceManager - http://localhost:8088/\n\n4. 运行一个 MapReduce 任务。\n5. 用以下命令停止后台进程：\n\n    $ sbin/stop-yarn.sh\n","slug":"Hadoop：安装单个节点的集群","published":1,"updated":"2021-07-19T16:28:00.252Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphmp0035itd3ahdk2dmf","content":"<h3 id=\"目的\"><a href=\"#目的\" class=\"headerlink\" title=\"目的\"></a>目的</h3><p>本文档描述如何安装并配置一个单节点 Hadoop，因此可以使用 Hadoop MapReduce 和分布式文件系统（HDFS）快速执行简单的操作。</p>\n<span id=\"more\"></span>\n\n<h3 id=\"前提\"><a href=\"#前提\" class=\"headerlink\" title=\"前提\"></a>前提</h3><h4 id=\"支持的平台\"><a href=\"#支持的平台\" class=\"headerlink\" title=\"支持的平台\"></a>支持的平台</h4><ul>\n<li>GNU/Linux 作为开发和产品平台都被支持。Hadoop 在 2000 台节点的集群上已经被证明。</li>\n<li>Windows 也是一个被支持的平台，但下面的步骤只针对 Linux。在 Windows 上安装 Hadoop，参见 <a href=\"http://wiki.apache.org/hadoop/Hadoop2OnWindows\">wiki 页</a></li>\n</ul>\n<h4 id=\"必须的软件\"><a href=\"#必须的软件\" class=\"headerlink\" title=\"必须的软件\"></a>必须的软件</h4><p>Linux 必须的软件包括：</p>\n<ol>\n<li>必须安装 Java™。<a href=\"http://wiki.apache.org/hadoop/HadoopJavaVersions\">HadoopJavaVersions</a> 中列出了推荐的 Java 版本。</li>\n<li>必须安装 ssh 并且必须运行 sshd 来使用管理远程 Hadoop 守护进程的 Hadoop 脚本。</li>\n</ol>\n<h4 id=\"安装软件\"><a href=\"#安装软件\" class=\"headerlink\" title=\"安装软件\"></a>安装软件</h4><p>如果集群没有必需的软件则需要安装。在 Ubuntu Linux 上的示例：</p>\n<pre><code>$ sudo apt-get install ssh\n$ sudo apt-get install rsync\n</code></pre>\n<h3 id=\"下载\"><a href=\"#下载\" class=\"headerlink\" title=\"下载\"></a>下载</h3><p>要获取一个 Hadoop 的发行包，从 <a href=\"http://www.apache.org/dyn/closer.cgi/hadoop/common/\">Apache 下载镜像列表</a>中的一个下载当前稳定发布版。</p>\n<h3 id=\"准备启动-Hadoop-集群\"><a href=\"#准备启动-Hadoop-集群\" class=\"headerlink\" title=\"准备启动 Hadoop 集群\"></a>准备启动 Hadoop 集群</h3><p>解包下载的 Hadoop 发行包。在发行包中，编辑文件 etc/hadoop/hadoop-env.sh 定义下面的一些参数：</p>\n<pre><code># set to the root of your Java installation\nexport JAVA_HOME=/usr/java/latest\n</code></pre>\n<p>尝试下面的命令：</p>\n<pre><code>$ bin/hadoop\n</code></pre>\n<p>这个命令将展示 hadoop 脚本的使用文档。</p>\n<p>现在可以以支持的三种模式中的一种启动 Hadoop 集群：</p>\n<ul>\n<li>本地单例模式</li>\n<li>伪分布式模式</li>\n<li>完全分布式模式</li>\n</ul>\n<h3 id=\"单例操作\"><a href=\"#单例操作\" class=\"headerlink\" title=\"单例操作\"></a>单例操作</h3><p>默认的，Hadoop 配置为以非分布式模式的单个 Java 进程运行。这对调试非常有帮助。</p>\n<p>下面的示例拷贝 Hadoop 安装目录下的配置文件作为输入，然后找出并展示所有匹配给定正则表达式的内容。输出内容写入指定的输出目录。</p>\n<pre><code>$ mkdir input\n$ cp etc/hadoop/*.xml input\n$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input output &#39;dfs[a-z.]+&#39;\n$ cat output/*\n</code></pre>\n<h3 id=\"伪分布式操作\"><a href=\"#伪分布式操作\" class=\"headerlink\" title=\"伪分布式操作\"></a>伪分布式操作</h3><p>Hadoop 也可以在单个节点上以伪分布式模式运行，每个 Hadoop 守护进程在一个独立的 Java 进程中运行。</p>\n<h4 id=\"配置\"><a href=\"#配置\" class=\"headerlink\" title=\"配置\"></a>配置</h4><p>使用以下配置：</p>\n<p>etc/hadoop/core-site.xml:</p>\n<pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;fs.defaultFS&lt;/name&gt;\n    &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<p>etc/hadoop/hdfs-site.xml:</p>\n<pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.replication&lt;/name&gt;\n    &lt;value&gt;1&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<h4 id=\"设置免密码-ssh\"><a href=\"#设置免密码-ssh\" class=\"headerlink\" title=\"设置免密码 ssh\"></a>设置免密码 ssh</h4><p>检查是否可以无密码 ssh 到本地主机：</p>\n<pre><code>$ ssh localhost\n</code></pre>\n<p>如果不能无密码 ssh 本地主机，则执行以下命令：</p>\n<blockquote>\n<p>官方手册中是用 dsa，但实际操作在 Ubuntu16.04 上没能通过。具体原因有待学习。<br>使用 rsa 通过，下面也以 rsa 为例。</p>\n</blockquote>\n<pre><code>$ ssh-keygen -t rsa -P &#39;&#39; -f ~/.ssh/id_rsa\n$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys\n$ chmod 0600 ~/.ssh/authorized_keys\n</code></pre>\n<h4 id=\"执行\"><a href=\"#执行\" class=\"headerlink\" title=\"执行\"></a>执行</h4><p>下面的操作指南会执行一个本地 MapReduce 任务。如果想在 YARN 上执行一个任务，参见<a href=\"http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html#YARN_on_Single_Node\">单节点上的 YARN</a>。</p>\n<ol>\n<li><p>格式化文件系统：</p>\n<p> $ bin/hdfs namenode -format</p>\n</li>\n<li><p>启动 NameNode 和 DataNode 后台进程：</p>\n<p> $ sbin/start-dfs.sh</p>\n</li>\n</ol>\n<p>Hadoop 后台进程日志写入到 $HADOOP_LOG_DIR 目录（默认是 $HADOOP_HOME/logs）。</p>\n<ol start=\"3\">\n<li>浏览 NameNode 的 Web 界面；默认是：</li>\n</ol>\n<ul>\n<li>NameNode - <a href=\"http://localhost:50070/\">http://localhost:50070/</a></li>\n</ul>\n<ol start=\"4\">\n<li><p>创建执行 MapReduce 任务需要的 HDFS 目录：</p>\n<p> $ bin/hdfs dfs -mkdir /user<br> $ bin/hdfs dfs -mkdir /user/<username></p>\n</li>\n<li><p>拷贝输入文件到分布式文件系统：</p>\n<p> $ bin/hdfs dfs -put etc/hadoop input</p>\n</li>\n<li><p>运行提供的例子：</p>\n<p> $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input output ‘dfs[a-z.]+’</p>\n</li>\n<li><p>检查输出文件：从分布式文件系统拷贝输出文件到本地文件系统并检查：</p>\n<p> $ bin/hdfs dfs -get output output<br> $ cat output/*</p>\n</li>\n</ol>\n<p>或者在分布式文件系统上查看输出文件：</p>\n<pre><code>$ bin/hdfs dfs -cat output/*\n</code></pre>\n<ol start=\"8\">\n<li><p>用下面的命令停止后台进程：</p>\n<p> $ sbin/stop-dfs.sh</p>\n</li>\n</ol>\n<h4 id=\"单个节点上的-YARN\"><a href=\"#单个节点上的-YARN\" class=\"headerlink\" title=\"单个节点上的 YARN\"></a>单个节点上的 YARN</h4><p>设置几个参数就可以在伪分布式模式下在 YARN 上运行 MapReduce 任务，并且运行另外的 ResourceManager 和 NodeManager 后台进程。</p>\n<p>下面的操作指南假设上面的 1.～4. 步已经执行。</p>\n<ol>\n<li>配置参数如下：</li>\n</ol>\n<p>etc/hadoop/mapred-site.xml：</p>\n<pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;\n    &lt;value&gt;yarn&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<p>etc/hadoop/yarn-site.xml：</p>\n<pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<ol start=\"2\">\n<li><p>启动 ResourceManager 和 NodeManager 后台进程：</p>\n<p> $ sbin/start-yarn.sh</p>\n</li>\n<li><p>浏览 ResourceManager 的 Web 界面；默认是：</p>\n</li>\n</ol>\n<ul>\n<li>ResourceManager - <a href=\"http://localhost:8088/\">http://localhost:8088/</a></li>\n</ul>\n<ol start=\"4\">\n<li><p>运行一个 MapReduce 任务。</p>\n</li>\n<li><p>用以下命令停止后台进程：</p>\n<p> $ sbin/stop-yarn.sh</p>\n</li>\n</ol>\n","site":{"data":{}},"excerpt":"<h3 id=\"目的\"><a href=\"#目的\" class=\"headerlink\" title=\"目的\"></a>目的</h3><p>本文档描述如何安装并配置一个单节点 Hadoop，因此可以使用 Hadoop MapReduce 和分布式文件系统（HDFS）快速执行简单的操作。</p>","more":"<h3 id=\"前提\"><a href=\"#前提\" class=\"headerlink\" title=\"前提\"></a>前提</h3><h4 id=\"支持的平台\"><a href=\"#支持的平台\" class=\"headerlink\" title=\"支持的平台\"></a>支持的平台</h4><ul>\n<li>GNU/Linux 作为开发和产品平台都被支持。Hadoop 在 2000 台节点的集群上已经被证明。</li>\n<li>Windows 也是一个被支持的平台，但下面的步骤只针对 Linux。在 Windows 上安装 Hadoop，参见 <a href=\"http://wiki.apache.org/hadoop/Hadoop2OnWindows\">wiki 页</a></li>\n</ul>\n<h4 id=\"必须的软件\"><a href=\"#必须的软件\" class=\"headerlink\" title=\"必须的软件\"></a>必须的软件</h4><p>Linux 必须的软件包括：</p>\n<ol>\n<li>必须安装 Java™。<a href=\"http://wiki.apache.org/hadoop/HadoopJavaVersions\">HadoopJavaVersions</a> 中列出了推荐的 Java 版本。</li>\n<li>必须安装 ssh 并且必须运行 sshd 来使用管理远程 Hadoop 守护进程的 Hadoop 脚本。</li>\n</ol>\n<h4 id=\"安装软件\"><a href=\"#安装软件\" class=\"headerlink\" title=\"安装软件\"></a>安装软件</h4><p>如果集群没有必需的软件则需要安装。在 Ubuntu Linux 上的示例：</p>\n<pre><code>$ sudo apt-get install ssh\n$ sudo apt-get install rsync\n</code></pre>\n<h3 id=\"下载\"><a href=\"#下载\" class=\"headerlink\" title=\"下载\"></a>下载</h3><p>要获取一个 Hadoop 的发行包，从 <a href=\"http://www.apache.org/dyn/closer.cgi/hadoop/common/\">Apache 下载镜像列表</a>中的一个下载当前稳定发布版。</p>\n<h3 id=\"准备启动-Hadoop-集群\"><a href=\"#准备启动-Hadoop-集群\" class=\"headerlink\" title=\"准备启动 Hadoop 集群\"></a>准备启动 Hadoop 集群</h3><p>解包下载的 Hadoop 发行包。在发行包中，编辑文件 etc/hadoop/hadoop-env.sh 定义下面的一些参数：</p>\n<pre><code># set to the root of your Java installation\nexport JAVA_HOME=/usr/java/latest\n</code></pre>\n<p>尝试下面的命令：</p>\n<pre><code>$ bin/hadoop\n</code></pre>\n<p>这个命令将展示 hadoop 脚本的使用文档。</p>\n<p>现在可以以支持的三种模式中的一种启动 Hadoop 集群：</p>\n<ul>\n<li>本地单例模式</li>\n<li>伪分布式模式</li>\n<li>完全分布式模式</li>\n</ul>\n<h3 id=\"单例操作\"><a href=\"#单例操作\" class=\"headerlink\" title=\"单例操作\"></a>单例操作</h3><p>默认的，Hadoop 配置为以非分布式模式的单个 Java 进程运行。这对调试非常有帮助。</p>\n<p>下面的示例拷贝 Hadoop 安装目录下的配置文件作为输入，然后找出并展示所有匹配给定正则表达式的内容。输出内容写入指定的输出目录。</p>\n<pre><code>$ mkdir input\n$ cp etc/hadoop/*.xml input\n$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input output &#39;dfs[a-z.]+&#39;\n$ cat output/*\n</code></pre>\n<h3 id=\"伪分布式操作\"><a href=\"#伪分布式操作\" class=\"headerlink\" title=\"伪分布式操作\"></a>伪分布式操作</h3><p>Hadoop 也可以在单个节点上以伪分布式模式运行，每个 Hadoop 守护进程在一个独立的 Java 进程中运行。</p>\n<h4 id=\"配置\"><a href=\"#配置\" class=\"headerlink\" title=\"配置\"></a>配置</h4><p>使用以下配置：</p>\n<p>etc/hadoop/core-site.xml:</p>\n<pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;fs.defaultFS&lt;/name&gt;\n    &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<p>etc/hadoop/hdfs-site.xml:</p>\n<pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.replication&lt;/name&gt;\n    &lt;value&gt;1&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<h4 id=\"设置免密码-ssh\"><a href=\"#设置免密码-ssh\" class=\"headerlink\" title=\"设置免密码 ssh\"></a>设置免密码 ssh</h4><p>检查是否可以无密码 ssh 到本地主机：</p>\n<pre><code>$ ssh localhost\n</code></pre>\n<p>如果不能无密码 ssh 本地主机，则执行以下命令：</p>\n<blockquote>\n<p>官方手册中是用 dsa，但实际操作在 Ubuntu16.04 上没能通过。具体原因有待学习。<br>使用 rsa 通过，下面也以 rsa 为例。</p>\n</blockquote>\n<pre><code>$ ssh-keygen -t rsa -P &#39;&#39; -f ~/.ssh/id_rsa\n$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys\n$ chmod 0600 ~/.ssh/authorized_keys\n</code></pre>\n<h4 id=\"执行\"><a href=\"#执行\" class=\"headerlink\" title=\"执行\"></a>执行</h4><p>下面的操作指南会执行一个本地 MapReduce 任务。如果想在 YARN 上执行一个任务，参见<a href=\"http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html#YARN_on_Single_Node\">单节点上的 YARN</a>。</p>\n<ol>\n<li><p>格式化文件系统：</p>\n<p> $ bin/hdfs namenode -format</p>\n</li>\n<li><p>启动 NameNode 和 DataNode 后台进程：</p>\n<p> $ sbin/start-dfs.sh</p>\n</li>\n</ol>\n<p>Hadoop 后台进程日志写入到 $HADOOP_LOG_DIR 目录（默认是 $HADOOP_HOME/logs）。</p>\n<ol start=\"3\">\n<li>浏览 NameNode 的 Web 界面；默认是：</li>\n</ol>\n<ul>\n<li>NameNode - <a href=\"http://localhost:50070/\">http://localhost:50070/</a></li>\n</ul>\n<ol start=\"4\">\n<li><p>创建执行 MapReduce 任务需要的 HDFS 目录：</p>\n<p> $ bin/hdfs dfs -mkdir /user<br> $ bin/hdfs dfs -mkdir /user/<username></p>\n</li>\n<li><p>拷贝输入文件到分布式文件系统：</p>\n<p> $ bin/hdfs dfs -put etc/hadoop input</p>\n</li>\n<li><p>运行提供的例子：</p>\n<p> $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input output ‘dfs[a-z.]+’</p>\n</li>\n<li><p>检查输出文件：从分布式文件系统拷贝输出文件到本地文件系统并检查：</p>\n<p> $ bin/hdfs dfs -get output output<br> $ cat output/*</p>\n</li>\n</ol>\n<p>或者在分布式文件系统上查看输出文件：</p>\n<pre><code>$ bin/hdfs dfs -cat output/*\n</code></pre>\n<ol start=\"8\">\n<li><p>用下面的命令停止后台进程：</p>\n<p> $ sbin/stop-dfs.sh</p>\n</li>\n</ol>\n<h4 id=\"单个节点上的-YARN\"><a href=\"#单个节点上的-YARN\" class=\"headerlink\" title=\"单个节点上的 YARN\"></a>单个节点上的 YARN</h4><p>设置几个参数就可以在伪分布式模式下在 YARN 上运行 MapReduce 任务，并且运行另外的 ResourceManager 和 NodeManager 后台进程。</p>\n<p>下面的操作指南假设上面的 1.～4. 步已经执行。</p>\n<ol>\n<li>配置参数如下：</li>\n</ol>\n<p>etc/hadoop/mapred-site.xml：</p>\n<pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;\n    &lt;value&gt;yarn&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<p>etc/hadoop/yarn-site.xml：</p>\n<pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<ol start=\"2\">\n<li><p>启动 ResourceManager 和 NodeManager 后台进程：</p>\n<p> $ sbin/start-yarn.sh</p>\n</li>\n<li><p>浏览 ResourceManager 的 Web 界面；默认是：</p>\n</li>\n</ol>\n<ul>\n<li>ResourceManager - <a href=\"http://localhost:8088/\">http://localhost:8088/</a></li>\n</ul>\n<ol start=\"4\">\n<li><p>运行一个 MapReduce 任务。</p>\n</li>\n<li><p>用以下命令停止后台进程：</p>\n<p> $ sbin/stop-yarn.sh</p>\n</li>\n</ol>"},{"title":"Hexo Markdown 特殊字符转义","date":"2017-03-30T10:53:52.000Z","_content":"\n在使用 Markdown 编写 Hexo 的 Blog 时，对于特殊字符使用“\\”转义有时会不成功，最好的方式是直接使用特殊字符的编码，对应如下：\n\n<!-- more -->\n\n    - &#45; &minus; — 减号\n    ! &#33; — 惊叹号Exclamation mark \n    ” &#34; &quot; 双引号Quotation mark # &#35; — 数字标志Number sign $ &#36; — 美元标志Dollar sign \n    % &#37; — 百分号Percent sign \n    & &#38; &amp; Ampersand \n    ‘ &#39; — 单引号Apostrophe \n    ( &#40; — 小括号左边部分Left parenthesis \n    ) &#41; — 小括号右边部分Right parenthesis \n    * &#42; — 星号Asterisk \n    + &#43; — 加号Plus sign \n    < &#60; &lt; 小于号Less than \n    = &#61; — 等于符号Equals sign \n    > &#62; &gt; 大于号Greater than \n    ? &#63; — 问号Question mark \n    @ &#64; — Commercial at \n    [ &#91; --- 中括号左边部分Left square bracket \n    \\ &#92; --- 反斜杠Reverse solidus (backslash) \n    ] &#93; — 中括号右边部分Right square bracket \n    { &#123; — 大括号左边部分Left curly brace \n    | &#124; — 竖线Vertical bar \n    } &#125; — 大括号右边部分Right curly brace ","source":"_posts/Hexo-Markdown-特殊字符转义.md","raw":"title: Hexo Markdown 特殊字符转义\ntags:\n  - Hexo\n  - Markdown\ncategories:\n  - 语言\n  - Markdown\ndate: 2017-03-30 18:53:52\n---\n\n在使用 Markdown 编写 Hexo 的 Blog 时，对于特殊字符使用“\\”转义有时会不成功，最好的方式是直接使用特殊字符的编码，对应如下：\n\n<!-- more -->\n\n    - &#45; &minus; — 减号\n    ! &#33; — 惊叹号Exclamation mark \n    ” &#34; &quot; 双引号Quotation mark # &#35; — 数字标志Number sign $ &#36; — 美元标志Dollar sign \n    % &#37; — 百分号Percent sign \n    & &#38; &amp; Ampersand \n    ‘ &#39; — 单引号Apostrophe \n    ( &#40; — 小括号左边部分Left parenthesis \n    ) &#41; — 小括号右边部分Right parenthesis \n    * &#42; — 星号Asterisk \n    + &#43; — 加号Plus sign \n    < &#60; &lt; 小于号Less than \n    = &#61; — 等于符号Equals sign \n    > &#62; &gt; 大于号Greater than \n    ? &#63; — 问号Question mark \n    @ &#64; — Commercial at \n    [ &#91; --- 中括号左边部分Left square bracket \n    \\ &#92; --- 反斜杠Reverse solidus (backslash) \n    ] &#93; — 中括号右边部分Right square bracket \n    { &#123; — 大括号左边部分Left curly brace \n    | &#124; — 竖线Vertical bar \n    } &#125; — 大括号右边部分Right curly brace ","slug":"Hexo-Markdown-特殊字符转义","published":1,"updated":"2021-07-19T16:28:00.252Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphmr0037itd30sol64z3","content":"<p>在使用 Markdown 编写 Hexo 的 Blog 时，对于特殊字符使用“\\”转义有时会不成功，最好的方式是直接使用特殊字符的编码，对应如下：</p>\n<span id=\"more\"></span>\n\n<pre><code>- &amp;#45; &amp;minus; — 减号\n! &amp;#33; — 惊叹号Exclamation mark \n” &amp;#34; &amp;quot; 双引号Quotation mark # &amp;#35; — 数字标志Number sign $ &amp;#36; — 美元标志Dollar sign \n% &amp;#37; — 百分号Percent sign \n&amp; &amp;#38; &amp;amp; Ampersand \n‘ &amp;#39; — 单引号Apostrophe \n( &amp;#40; — 小括号左边部分Left parenthesis \n) &amp;#41; — 小括号右边部分Right parenthesis \n* &amp;#42; — 星号Asterisk \n+ &amp;#43; — 加号Plus sign \n&lt; &amp;#60; &amp;lt; 小于号Less than \n= &amp;#61; — 等于符号Equals sign \n&gt; &amp;#62; &amp;gt; 大于号Greater than \n? &amp;#63; — 问号Question mark \n@ &amp;#64; — Commercial at \n[ &amp;#91; --- 中括号左边部分Left square bracket \n\\ &amp;#92; --- 反斜杠Reverse solidus (backslash) \n] &amp;#93; — 中括号右边部分Right square bracket \n&#123; &amp;#123; — 大括号左边部分Left curly brace \n| &amp;#124; — 竖线Vertical bar \n&#125; &amp;#125; — 大括号右边部分Right curly brace \n</code></pre>\n","site":{"data":{}},"excerpt":"<p>在使用 Markdown 编写 Hexo 的 Blog 时，对于特殊字符使用“\\”转义有时会不成功，最好的方式是直接使用特殊字符的编码，对应如下：</p>","more":"<pre><code>- &amp;#45; &amp;minus; — 减号\n! &amp;#33; — 惊叹号Exclamation mark \n” &amp;#34; &amp;quot; 双引号Quotation mark # &amp;#35; — 数字标志Number sign $ &amp;#36; — 美元标志Dollar sign \n% &amp;#37; — 百分号Percent sign \n&amp; &amp;#38; &amp;amp; Ampersand \n‘ &amp;#39; — 单引号Apostrophe \n( &amp;#40; — 小括号左边部分Left parenthesis \n) &amp;#41; — 小括号右边部分Right parenthesis \n* &amp;#42; — 星号Asterisk \n+ &amp;#43; — 加号Plus sign \n&lt; &amp;#60; &amp;lt; 小于号Less than \n= &amp;#61; — 等于符号Equals sign \n&gt; &amp;#62; &amp;gt; 大于号Greater than \n? &amp;#63; — 问号Question mark \n@ &amp;#64; — Commercial at \n[ &amp;#91; --- 中括号左边部分Left square bracket \n\\ &amp;#92; --- 反斜杠Reverse solidus (backslash) \n] &amp;#93; — 中括号右边部分Right square bracket \n&#123; &amp;#123; — 大括号左边部分Left curly brace \n| &amp;#124; — 竖线Vertical bar \n&#125; &amp;#125; — 大括号右边部分Right curly brace \n</code></pre>"},{"title":"Hive 2.1.1 编译安装","date":"2017-06-11T13:12:41.000Z","_content":"\n### 软件要求\n\n- Java 1.7 或更新版本。强烈建议使用 Java 1.8\n- Hadoop 2.x\n\n### 编译打包\n\n    mvn clean package -Pdist -DskipTests\n    \n编译完成后，打包后的 .tar.gz 文件在 packaging/target 下。\n\n<!-- more -->\n\n### 安装 Hive\n\n拷贝编译打包后的 packaging/target/apache-hive-2.1.1-bin.tar.gz 到 /opt/ 目录下，并解压：\n\n    $ cd /opt\n    $ tar xzvf apache-hive-2.1.1-bin.tar.gz\n    $ chown -R hive:hive apache-hive-2.1.1\n    $ ln -s apache-hive-2.1.1 hive\n    $ chown -h hive:hive hive\n\n设置环境变量：\n\n    export HIVE_HOME=/opt/hive\n    export PATH=$HIVE_HOME/bin:$PATH\n\n### 配置 Hive\n\n在 hive 的 home 路径下的 conf 目录下创建 hive-site.xml 文件，并配置以下内容：\n\n    <?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n    <?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n    <configuration>\n      <property>\n        <name>hive.metastore.warehouse.dir</name>\n        <value>/user/hive/warehouse</value>\n      </property> \n      <property>\n        <name>hive.exec.scratchdir</name>\n        <value>/tmp/hive</value>\n      </property>\n      <property>\n        <name>javax.jdo.option.ConnectionURL</name>\n        <value>jdbc:mysql://127.0.0.1/hive?createDatabaseIfNotExist=true</value>\n      </property>\n      <property>\n        <name>javax.jdo.option.ConnectionDriverName</name>\n        <value>com.mysql.jdbc.Driver</value>\n      </property>\n      <property>\n        <name>javax.jdo.option.ConnectionUserName</name>\n        <value>hive</value>\n      </property>\n      <property>\n        <name>javax.jdo.option.ConnectionPassword</name>\n        <value></value>\n      </property>\n      <property>\n        <name>hive.auto.convert.join</name>\n        <value>false</value>\n      </property>\n    </configuration>\n\n下载 MySQL 的 Java 驱动并放到 lib 下。\n\n### 运行 Hive\n\nHive 使用 Hadoop，所以 path 中必须有 Hadoop 或者 export HADOOP_HOME=<hadoop-install-dir>。另外，需要使用下面的命令创建 /tmp 和 /user/hive/warehouse，并且设置 chmod g+w，这样才可以在 hive 中创建表。\n\n    $ hadoop fs -mkdir /tmp\n    $ hadoop fs -mkdir -p /user/hive/warehouse\n    $ hadoop fs -chmod g+w /tmp\n    $ hadoop fs -chmod g+w /user/hive/warehouse\n\n使用以下命令初始化 hive 元数据库：\n\n    $ schematool -initSchema -dbType mysql\n\n使用以下命令进入 hive 命令行模式：\n\n    $ hive\n    hive>\n","source":"_posts/Hive-2-1-1-编译安装.md","raw":"title: Hive 2.1.1 编译安装\ntags:\n  - 大数据\n  - Hive\ncategories:\n  - 大数据\n  - Hive\ndate: 2017-06-11 21:12:41\n---\n\n### 软件要求\n\n- Java 1.7 或更新版本。强烈建议使用 Java 1.8\n- Hadoop 2.x\n\n### 编译打包\n\n    mvn clean package -Pdist -DskipTests\n    \n编译完成后，打包后的 .tar.gz 文件在 packaging/target 下。\n\n<!-- more -->\n\n### 安装 Hive\n\n拷贝编译打包后的 packaging/target/apache-hive-2.1.1-bin.tar.gz 到 /opt/ 目录下，并解压：\n\n    $ cd /opt\n    $ tar xzvf apache-hive-2.1.1-bin.tar.gz\n    $ chown -R hive:hive apache-hive-2.1.1\n    $ ln -s apache-hive-2.1.1 hive\n    $ chown -h hive:hive hive\n\n设置环境变量：\n\n    export HIVE_HOME=/opt/hive\n    export PATH=$HIVE_HOME/bin:$PATH\n\n### 配置 Hive\n\n在 hive 的 home 路径下的 conf 目录下创建 hive-site.xml 文件，并配置以下内容：\n\n    <?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n    <?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n    <configuration>\n      <property>\n        <name>hive.metastore.warehouse.dir</name>\n        <value>/user/hive/warehouse</value>\n      </property> \n      <property>\n        <name>hive.exec.scratchdir</name>\n        <value>/tmp/hive</value>\n      </property>\n      <property>\n        <name>javax.jdo.option.ConnectionURL</name>\n        <value>jdbc:mysql://127.0.0.1/hive?createDatabaseIfNotExist=true</value>\n      </property>\n      <property>\n        <name>javax.jdo.option.ConnectionDriverName</name>\n        <value>com.mysql.jdbc.Driver</value>\n      </property>\n      <property>\n        <name>javax.jdo.option.ConnectionUserName</name>\n        <value>hive</value>\n      </property>\n      <property>\n        <name>javax.jdo.option.ConnectionPassword</name>\n        <value></value>\n      </property>\n      <property>\n        <name>hive.auto.convert.join</name>\n        <value>false</value>\n      </property>\n    </configuration>\n\n下载 MySQL 的 Java 驱动并放到 lib 下。\n\n### 运行 Hive\n\nHive 使用 Hadoop，所以 path 中必须有 Hadoop 或者 export HADOOP_HOME=<hadoop-install-dir>。另外，需要使用下面的命令创建 /tmp 和 /user/hive/warehouse，并且设置 chmod g+w，这样才可以在 hive 中创建表。\n\n    $ hadoop fs -mkdir /tmp\n    $ hadoop fs -mkdir -p /user/hive/warehouse\n    $ hadoop fs -chmod g+w /tmp\n    $ hadoop fs -chmod g+w /user/hive/warehouse\n\n使用以下命令初始化 hive 元数据库：\n\n    $ schematool -initSchema -dbType mysql\n\n使用以下命令进入 hive 命令行模式：\n\n    $ hive\n    hive>\n","slug":"Hive-2-1-1-编译安装","published":1,"updated":"2021-07-19T16:28:00.252Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphms003bitd3fdqv4ufv","content":"<h3 id=\"软件要求\"><a href=\"#软件要求\" class=\"headerlink\" title=\"软件要求\"></a>软件要求</h3><ul>\n<li>Java 1.7 或更新版本。强烈建议使用 Java 1.8</li>\n<li>Hadoop 2.x</li>\n</ul>\n<h3 id=\"编译打包\"><a href=\"#编译打包\" class=\"headerlink\" title=\"编译打包\"></a>编译打包</h3><pre><code>mvn clean package -Pdist -DskipTests\n</code></pre>\n<p>编译完成后，打包后的 .tar.gz 文件在 packaging/target 下。</p>\n<span id=\"more\"></span>\n\n<h3 id=\"安装-Hive\"><a href=\"#安装-Hive\" class=\"headerlink\" title=\"安装 Hive\"></a>安装 Hive</h3><p>拷贝编译打包后的 packaging/target/apache-hive-2.1.1-bin.tar.gz 到 /opt/ 目录下，并解压：</p>\n<pre><code>$ cd /opt\n$ tar xzvf apache-hive-2.1.1-bin.tar.gz\n$ chown -R hive:hive apache-hive-2.1.1\n$ ln -s apache-hive-2.1.1 hive\n$ chown -h hive:hive hive\n</code></pre>\n<p>设置环境变量：</p>\n<pre><code>export HIVE_HOME=/opt/hive\nexport PATH=$HIVE_HOME/bin:$PATH\n</code></pre>\n<h3 id=\"配置-Hive\"><a href=\"#配置-Hive\" class=\"headerlink\" title=\"配置 Hive\"></a>配置 Hive</h3><p>在 hive 的 home 路径下的 conf 目录下创建 hive-site.xml 文件，并配置以下内容：</p>\n<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;\n&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;\n    &lt;value&gt;/user/hive/warehouse&lt;/value&gt;\n  &lt;/property&gt; \n  &lt;property&gt;\n    &lt;name&gt;hive.exec.scratchdir&lt;/name&gt;\n    &lt;value&gt;/tmp/hive&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;\n    &lt;value&gt;jdbc:mysql://127.0.0.1/hive?createDatabaseIfNotExist=true&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;\n    &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;\n    &lt;value&gt;hive&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;\n    &lt;value&gt;&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hive.auto.convert.join&lt;/name&gt;\n    &lt;value&gt;false&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<p>下载 MySQL 的 Java 驱动并放到 lib 下。</p>\n<h3 id=\"运行-Hive\"><a href=\"#运行-Hive\" class=\"headerlink\" title=\"运行 Hive\"></a>运行 Hive</h3><p>Hive 使用 Hadoop，所以 path 中必须有 Hadoop 或者 export HADOOP_HOME=<hadoop-install-dir>。另外，需要使用下面的命令创建 /tmp 和 /user/hive/warehouse，并且设置 chmod g+w，这样才可以在 hive 中创建表。</p>\n<pre><code>$ hadoop fs -mkdir /tmp\n$ hadoop fs -mkdir -p /user/hive/warehouse\n$ hadoop fs -chmod g+w /tmp\n$ hadoop fs -chmod g+w /user/hive/warehouse\n</code></pre>\n<p>使用以下命令初始化 hive 元数据库：</p>\n<pre><code>$ schematool -initSchema -dbType mysql\n</code></pre>\n<p>使用以下命令进入 hive 命令行模式：</p>\n<pre><code>$ hive\nhive&gt;\n</code></pre>\n","site":{"data":{}},"excerpt":"<h3 id=\"软件要求\"><a href=\"#软件要求\" class=\"headerlink\" title=\"软件要求\"></a>软件要求</h3><ul>\n<li>Java 1.7 或更新版本。强烈建议使用 Java 1.8</li>\n<li>Hadoop 2.x</li>\n</ul>\n<h3 id=\"编译打包\"><a href=\"#编译打包\" class=\"headerlink\" title=\"编译打包\"></a>编译打包</h3><pre><code>mvn clean package -Pdist -DskipTests\n</code></pre>\n<p>编译完成后，打包后的 .tar.gz 文件在 packaging/target 下。</p>","more":"<h3 id=\"安装-Hive\"><a href=\"#安装-Hive\" class=\"headerlink\" title=\"安装 Hive\"></a>安装 Hive</h3><p>拷贝编译打包后的 packaging/target/apache-hive-2.1.1-bin.tar.gz 到 /opt/ 目录下，并解压：</p>\n<pre><code>$ cd /opt\n$ tar xzvf apache-hive-2.1.1-bin.tar.gz\n$ chown -R hive:hive apache-hive-2.1.1\n$ ln -s apache-hive-2.1.1 hive\n$ chown -h hive:hive hive\n</code></pre>\n<p>设置环境变量：</p>\n<pre><code>export HIVE_HOME=/opt/hive\nexport PATH=$HIVE_HOME/bin:$PATH\n</code></pre>\n<h3 id=\"配置-Hive\"><a href=\"#配置-Hive\" class=\"headerlink\" title=\"配置 Hive\"></a>配置 Hive</h3><p>在 hive 的 home 路径下的 conf 目录下创建 hive-site.xml 文件，并配置以下内容：</p>\n<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;\n&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;\n    &lt;value&gt;/user/hive/warehouse&lt;/value&gt;\n  &lt;/property&gt; \n  &lt;property&gt;\n    &lt;name&gt;hive.exec.scratchdir&lt;/name&gt;\n    &lt;value&gt;/tmp/hive&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;\n    &lt;value&gt;jdbc:mysql://127.0.0.1/hive?createDatabaseIfNotExist=true&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;\n    &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;\n    &lt;value&gt;hive&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;\n    &lt;value&gt;&lt;/value&gt;\n  &lt;/property&gt;\n  &lt;property&gt;\n    &lt;name&gt;hive.auto.convert.join&lt;/name&gt;\n    &lt;value&gt;false&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<p>下载 MySQL 的 Java 驱动并放到 lib 下。</p>\n<h3 id=\"运行-Hive\"><a href=\"#运行-Hive\" class=\"headerlink\" title=\"运行 Hive\"></a>运行 Hive</h3><p>Hive 使用 Hadoop，所以 path 中必须有 Hadoop 或者 export HADOOP_HOME=<hadoop-install-dir>。另外，需要使用下面的命令创建 /tmp 和 /user/hive/warehouse，并且设置 chmod g+w，这样才可以在 hive 中创建表。</p>\n<pre><code>$ hadoop fs -mkdir /tmp\n$ hadoop fs -mkdir -p /user/hive/warehouse\n$ hadoop fs -chmod g+w /tmp\n$ hadoop fs -chmod g+w /user/hive/warehouse\n</code></pre>\n<p>使用以下命令初始化 hive 元数据库：</p>\n<pre><code>$ schematool -initSchema -dbType mysql\n</code></pre>\n<p>使用以下命令进入 hive 命令行模式：</p>\n<pre><code>$ hive\nhive&gt;\n</code></pre>"},{"title":"Hive MapJoin 的限制","date":"2017-07-08T12:56:22.000Z","_content":"\n- 如果除了一张表之外其他关联的表都很小，关联可以优化为只有 map 的作业。下面的查询不需要 reducer：\n\n    SELECT /*+ MAPJOIN(b) */ a.key, a.value\n    FROM a JOIN b ON a.key = b.key;\n\n对于 A 的每个 mapper，都会完全读取 B。\n\n<!-- more -->\n\n- 下面是不支持的项：\n  - Union 之后跟着 MapJoin\n  - Lateral View 之后跟着 MapJoin\n  - Reduce Sink（Group By/Join/Sort By/Cluster By/Distribute By）之后跟着 MapJoin\n  - MapJoin 之后跟着 Union\n  - MapJoin 之后跟着 Join\n  - MapJoin 之后跟着 MapJoin\n\n- 配置变量 hive.auto.convert.join 如果设置为 true，则如果可能在运行时会自动转化关联为 mapjoin，这用来代替 mapjoin 提示。mapjoin 提示应该只在以下查询中使用。\n  - 如果输入都被分桶或者排序，那么关联应该转化为分桶化的 map 侧关联或者分桶化的合并排序关联。\n\n- 考虑不同键的多个 mapjoin 的可能性：\n\n    select /*+MAPJOIN(smallTableTwo)*/ idOne, idTwo, value FROM\n      ( select /*+MAPJOIN(smallTableOne)*/ idOne, idTwo, value FROM\n        bigTable JOIN smallTableOne on (bigTable.idOne = smallTableOne.idOne)                                                  \n      ) firstjoin                                                            \n      JOIN                                                                 \n      smallTableTwo ON (firstjoin.idTwo = smallTableTwo.idTwo);\n\n上面的查询不支持。去掉 mapjoin 提示，上面的查询会作为 2 个只有 map 的作业执行。如果用户知道输入足够小足以放入内存，下面的可配置参数可以用来确保查询在单个 map-reduce 作业中执行。\n\n  - hive.auto.convert.join.noconditionaltask——是否启用 Hive 的基于输入文件大小的转化普通关联为 mapjoin 的优化。如果启用这个参数，并且 n 个 tables/partitions 关联的 n-1 个 tables/partitions 大小之和比指定的参数小，这个关联会被直接转化为 mapjoin（没有条件任务）。\n  - hive.auto.convert.join.noconditionaltask.size——如果 hive.auto.convert.join.noconditionaltask 是关闭的，这个参数不起作用。然而，如果上面的参数开启，并且 n 个 tables/partitions 关联的 n-1 个 tables/partitions 大小之和比这个配置的参数小，那么这个关联被直接转化为 mapjoin（没有条件任务）。默认为 10MB。\n","source":"_posts/Hive-MapJoin-的限制.md","raw":"title: Hive MapJoin 的限制\ntags:\n  - 大数据\n  - Hive\ncategories:\n  - 大数据\n  - Hive\ndate: 2017-07-08 20:56:22\n---\n\n- 如果除了一张表之外其他关联的表都很小，关联可以优化为只有 map 的作业。下面的查询不需要 reducer：\n\n    SELECT /*+ MAPJOIN(b) */ a.key, a.value\n    FROM a JOIN b ON a.key = b.key;\n\n对于 A 的每个 mapper，都会完全读取 B。\n\n<!-- more -->\n\n- 下面是不支持的项：\n  - Union 之后跟着 MapJoin\n  - Lateral View 之后跟着 MapJoin\n  - Reduce Sink（Group By/Join/Sort By/Cluster By/Distribute By）之后跟着 MapJoin\n  - MapJoin 之后跟着 Union\n  - MapJoin 之后跟着 Join\n  - MapJoin 之后跟着 MapJoin\n\n- 配置变量 hive.auto.convert.join 如果设置为 true，则如果可能在运行时会自动转化关联为 mapjoin，这用来代替 mapjoin 提示。mapjoin 提示应该只在以下查询中使用。\n  - 如果输入都被分桶或者排序，那么关联应该转化为分桶化的 map 侧关联或者分桶化的合并排序关联。\n\n- 考虑不同键的多个 mapjoin 的可能性：\n\n    select /*+MAPJOIN(smallTableTwo)*/ idOne, idTwo, value FROM\n      ( select /*+MAPJOIN(smallTableOne)*/ idOne, idTwo, value FROM\n        bigTable JOIN smallTableOne on (bigTable.idOne = smallTableOne.idOne)                                                  \n      ) firstjoin                                                            \n      JOIN                                                                 \n      smallTableTwo ON (firstjoin.idTwo = smallTableTwo.idTwo);\n\n上面的查询不支持。去掉 mapjoin 提示，上面的查询会作为 2 个只有 map 的作业执行。如果用户知道输入足够小足以放入内存，下面的可配置参数可以用来确保查询在单个 map-reduce 作业中执行。\n\n  - hive.auto.convert.join.noconditionaltask——是否启用 Hive 的基于输入文件大小的转化普通关联为 mapjoin 的优化。如果启用这个参数，并且 n 个 tables/partitions 关联的 n-1 个 tables/partitions 大小之和比指定的参数小，这个关联会被直接转化为 mapjoin（没有条件任务）。\n  - hive.auto.convert.join.noconditionaltask.size——如果 hive.auto.convert.join.noconditionaltask 是关闭的，这个参数不起作用。然而，如果上面的参数开启，并且 n 个 tables/partitions 关联的 n-1 个 tables/partitions 大小之和比这个配置的参数小，那么这个关联被直接转化为 mapjoin（没有条件任务）。默认为 10MB。\n","slug":"Hive-MapJoin-的限制","published":1,"updated":"2021-07-19T16:28:00.252Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphmu003ditd315tm3mml","content":"<ul>\n<li><p>如果除了一张表之外其他关联的表都很小，关联可以优化为只有 map 的作业。下面的查询不需要 reducer：</p>\n<p>  SELECT /*+ MAPJOIN(b) */ a.key, a.value<br>  FROM a JOIN b ON a.key = b.key;</p>\n</li>\n</ul>\n<p>对于 A 的每个 mapper，都会完全读取 B。</p>\n<span id=\"more\"></span>\n\n<ul>\n<li><p>下面是不支持的项：</p>\n<ul>\n<li>Union 之后跟着 MapJoin</li>\n<li>Lateral View 之后跟着 MapJoin</li>\n<li>Reduce Sink（Group By/Join/Sort By/Cluster By/Distribute By）之后跟着 MapJoin</li>\n<li>MapJoin 之后跟着 Union</li>\n<li>MapJoin 之后跟着 Join</li>\n<li>MapJoin 之后跟着 MapJoin</li>\n</ul>\n</li>\n<li><p>配置变量 hive.auto.convert.join 如果设置为 true，则如果可能在运行时会自动转化关联为 mapjoin，这用来代替 mapjoin 提示。mapjoin 提示应该只在以下查询中使用。</p>\n<ul>\n<li>如果输入都被分桶或者排序，那么关联应该转化为分桶化的 map 侧关联或者分桶化的合并排序关联。</li>\n</ul>\n</li>\n<li><p>考虑不同键的多个 mapjoin 的可能性：</p>\n<p>  select /<em>+MAPJOIN(smallTableTwo)</em>/ idOne, idTwo, value FROM</p>\n<pre><code>( select /*+MAPJOIN(smallTableOne)*/ idOne, idTwo, value FROM\n  bigTable JOIN smallTableOne on (bigTable.idOne = smallTableOne.idOne)                                                  \n) firstjoin                                                            \nJOIN                                                                 \nsmallTableTwo ON (firstjoin.idTwo = smallTableTwo.idTwo);\n</code></pre>\n</li>\n</ul>\n<p>上面的查询不支持。去掉 mapjoin 提示，上面的查询会作为 2 个只有 map 的作业执行。如果用户知道输入足够小足以放入内存，下面的可配置参数可以用来确保查询在单个 map-reduce 作业中执行。</p>\n<ul>\n<li>hive.auto.convert.join.noconditionaltask——是否启用 Hive 的基于输入文件大小的转化普通关联为 mapjoin 的优化。如果启用这个参数，并且 n 个 tables/partitions 关联的 n-1 个 tables/partitions 大小之和比指定的参数小，这个关联会被直接转化为 mapjoin（没有条件任务）。</li>\n<li>hive.auto.convert.join.noconditionaltask.size——如果 hive.auto.convert.join.noconditionaltask 是关闭的，这个参数不起作用。然而，如果上面的参数开启，并且 n 个 tables/partitions 关联的 n-1 个 tables/partitions 大小之和比这个配置的参数小，那么这个关联被直接转化为 mapjoin（没有条件任务）。默认为 10MB。</li>\n</ul>\n","site":{"data":{}},"excerpt":"<ul>\n<li><p>如果除了一张表之外其他关联的表都很小，关联可以优化为只有 map 的作业。下面的查询不需要 reducer：</p>\n<p>  SELECT /*+ MAPJOIN(b) */ a.key, a.value<br>  FROM a JOIN b ON a.key = b.key;</p>\n</li>\n</ul>\n<p>对于 A 的每个 mapper，都会完全读取 B。</p>","more":"<ul>\n<li><p>下面是不支持的项：</p>\n<ul>\n<li>Union 之后跟着 MapJoin</li>\n<li>Lateral View 之后跟着 MapJoin</li>\n<li>Reduce Sink（Group By/Join/Sort By/Cluster By/Distribute By）之后跟着 MapJoin</li>\n<li>MapJoin 之后跟着 Union</li>\n<li>MapJoin 之后跟着 Join</li>\n<li>MapJoin 之后跟着 MapJoin</li>\n</ul>\n</li>\n<li><p>配置变量 hive.auto.convert.join 如果设置为 true，则如果可能在运行时会自动转化关联为 mapjoin，这用来代替 mapjoin 提示。mapjoin 提示应该只在以下查询中使用。</p>\n<ul>\n<li>如果输入都被分桶或者排序，那么关联应该转化为分桶化的 map 侧关联或者分桶化的合并排序关联。</li>\n</ul>\n</li>\n<li><p>考虑不同键的多个 mapjoin 的可能性：</p>\n<p>  select /<em>+MAPJOIN(smallTableTwo)</em>/ idOne, idTwo, value FROM</p>\n<pre><code>( select /*+MAPJOIN(smallTableOne)*/ idOne, idTwo, value FROM\n  bigTable JOIN smallTableOne on (bigTable.idOne = smallTableOne.idOne)                                                  \n) firstjoin                                                            \nJOIN                                                                 \nsmallTableTwo ON (firstjoin.idTwo = smallTableTwo.idTwo);\n</code></pre>\n</li>\n</ul>\n<p>上面的查询不支持。去掉 mapjoin 提示，上面的查询会作为 2 个只有 map 的作业执行。如果用户知道输入足够小足以放入内存，下面的可配置参数可以用来确保查询在单个 map-reduce 作业中执行。</p>\n<ul>\n<li>hive.auto.convert.join.noconditionaltask——是否启用 Hive 的基于输入文件大小的转化普通关联为 mapjoin 的优化。如果启用这个参数，并且 n 个 tables/partitions 关联的 n-1 个 tables/partitions 大小之和比指定的参数小，这个关联会被直接转化为 mapjoin（没有条件任务）。</li>\n<li>hive.auto.convert.join.noconditionaltask.size——如果 hive.auto.convert.join.noconditionaltask 是关闭的，这个参数不起作用。然而，如果上面的参数开启，并且 n 个 tables/partitions 关联的 n-1 个 tables/partitions 大小之和比这个配置的参数小，那么这个关联被直接转化为 mapjoin（没有条件任务）。默认为 10MB。</li>\n</ul>"},{"title":"Hive MetaException Invalid partition key & values","date":"2019-03-27T01:27:54.000Z","_content":"在 drop 某个 Table 的时候报了以下异常信息：\n\n    hive> drop table temp.temp_log_day;\n    FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask.     MetaException(message:Invalid partition key & values; keys [dt, device_type, province_alias, hour, ], values [20180312, ])\n\n<!-- more -->\n\n原因是在 Hive 的元数据库中记录该 Table 还有未 drop 掉的 Partition，但是在 HDFS 上已经不存在该 Partition 的路径了。按照提示信息先 drop 对应的 Partition。drop Partition 的时候会提示具体的路径信息：\n\n    hive> ALTER TABLE temp.temp_log_day DROP PARTITION (dt='20180312');\n    FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Failed to delete parent: File does not exist: /user/hive/warehouse/temp.db/temp_log_day/dt=20180312/device_type=box/province_alias=00\n\t    at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getContentSummaryInt(FSDirStatAndListingOp.java:492)\n\t    at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getContentSummary(FSDirStatAndListingOp.java:139)\n\t    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getContentSummary(FSNamesystem.java:3928)\n\t    at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getContentSummary(NameNodeRpcServer.java:1200)\n\t    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getContentSummary(ClientNamenodeProtocolServerSideTranslatorPB.java:877)\n\t    at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\t    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n\t    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\n\t    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\n\t    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)\n\t    at java.security.AccessController.doPrivileged(Native Method)\n\t    at javax.security.auth.Subject.doAs(Subject.java:422)\n\t    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)\n\t    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2045)\n\n先在 HDFS 上创建对应的目录：\n\n    $ hadoop fs -mkdir -p /user/hive/warehouse/temp.db/temp_log_day/dt=20180312/device_type=box/province_alias=00\n\n再次执行 drop 操作：\n\n    hive> drop table temp.temp_log_day;\n    Moved: 'hdfs://ycluster/user/hive/warehouse/temp.db/temp_log_day' to trash at: hdfs://ycluster/user/test/.Trash/Current\n    OK\n","source":"_posts/Hive-MetaException-Invalid-partition-key-values.md","raw":"title: Hive MetaException Invalid partition key & values\ndate: 2019-03-27 09:27:54\ntags:\n- Hive\ncategories:\n- 大数据\n- Hive\n---\n在 drop 某个 Table 的时候报了以下异常信息：\n\n    hive> drop table temp.temp_log_day;\n    FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask.     MetaException(message:Invalid partition key & values; keys [dt, device_type, province_alias, hour, ], values [20180312, ])\n\n<!-- more -->\n\n原因是在 Hive 的元数据库中记录该 Table 还有未 drop 掉的 Partition，但是在 HDFS 上已经不存在该 Partition 的路径了。按照提示信息先 drop 对应的 Partition。drop Partition 的时候会提示具体的路径信息：\n\n    hive> ALTER TABLE temp.temp_log_day DROP PARTITION (dt='20180312');\n    FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Failed to delete parent: File does not exist: /user/hive/warehouse/temp.db/temp_log_day/dt=20180312/device_type=box/province_alias=00\n\t    at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getContentSummaryInt(FSDirStatAndListingOp.java:492)\n\t    at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getContentSummary(FSDirStatAndListingOp.java:139)\n\t    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getContentSummary(FSNamesystem.java:3928)\n\t    at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getContentSummary(NameNodeRpcServer.java:1200)\n\t    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getContentSummary(ClientNamenodeProtocolServerSideTranslatorPB.java:877)\n\t    at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\t    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n\t    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\n\t    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\n\t    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)\n\t    at java.security.AccessController.doPrivileged(Native Method)\n\t    at javax.security.auth.Subject.doAs(Subject.java:422)\n\t    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)\n\t    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2045)\n\n先在 HDFS 上创建对应的目录：\n\n    $ hadoop fs -mkdir -p /user/hive/warehouse/temp.db/temp_log_day/dt=20180312/device_type=box/province_alias=00\n\n再次执行 drop 操作：\n\n    hive> drop table temp.temp_log_day;\n    Moved: 'hdfs://ycluster/user/hive/warehouse/temp.db/temp_log_day' to trash at: hdfs://ycluster/user/test/.Trash/Current\n    OK\n","slug":"Hive-MetaException-Invalid-partition-key-values","published":1,"updated":"2021-07-19T16:28:00.252Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphmv003hitd395tecb6h","content":"<p>在 drop 某个 Table 的时候报了以下异常信息：</p>\n<pre><code>hive&gt; drop table temp.temp_log_day;\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask.     MetaException(message:Invalid partition key &amp; values; keys [dt, device_type, province_alias, hour, ], values [20180312, ])\n</code></pre>\n<span id=\"more\"></span>\n\n<p>原因是在 Hive 的元数据库中记录该 Table 还有未 drop 掉的 Partition，但是在 HDFS 上已经不存在该 Partition 的路径了。按照提示信息先 drop 对应的 Partition。drop Partition 的时候会提示具体的路径信息：</p>\n<pre><code>hive&gt; ALTER TABLE temp.temp_log_day DROP PARTITION (dt=&#39;20180312&#39;);\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Failed to delete parent: File does not exist: /user/hive/warehouse/temp.db/temp_log_day/dt=20180312/device_type=box/province_alias=00\n    at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getContentSummaryInt(FSDirStatAndListingOp.java:492)\n    at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getContentSummary(FSDirStatAndListingOp.java:139)\n    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getContentSummary(FSNamesystem.java:3928)\n    at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getContentSummary(NameNodeRpcServer.java:1200)\n    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getContentSummary(ClientNamenodeProtocolServerSideTranslatorPB.java:877)\n    at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\n    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\n    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:422)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)\n    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2045)\n</code></pre>\n<p>先在 HDFS 上创建对应的目录：</p>\n<pre><code>$ hadoop fs -mkdir -p /user/hive/warehouse/temp.db/temp_log_day/dt=20180312/device_type=box/province_alias=00\n</code></pre>\n<p>再次执行 drop 操作：</p>\n<pre><code>hive&gt; drop table temp.temp_log_day;\nMoved: &#39;hdfs://ycluster/user/hive/warehouse/temp.db/temp_log_day&#39; to trash at: hdfs://ycluster/user/test/.Trash/Current\nOK\n</code></pre>\n","site":{"data":{}},"excerpt":"<p>在 drop 某个 Table 的时候报了以下异常信息：</p>\n<pre><code>hive&gt; drop table temp.temp_log_day;\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask.     MetaException(message:Invalid partition key &amp; values; keys [dt, device_type, province_alias, hour, ], values [20180312, ])\n</code></pre>","more":"<p>原因是在 Hive 的元数据库中记录该 Table 还有未 drop 掉的 Partition，但是在 HDFS 上已经不存在该 Partition 的路径了。按照提示信息先 drop 对应的 Partition。drop Partition 的时候会提示具体的路径信息：</p>\n<pre><code>hive&gt; ALTER TABLE temp.temp_log_day DROP PARTITION (dt=&#39;20180312&#39;);\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Failed to delete parent: File does not exist: /user/hive/warehouse/temp.db/temp_log_day/dt=20180312/device_type=box/province_alias=00\n    at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getContentSummaryInt(FSDirStatAndListingOp.java:492)\n    at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getContentSummary(FSDirStatAndListingOp.java:139)\n    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getContentSummary(FSNamesystem.java:3928)\n    at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getContentSummary(NameNodeRpcServer.java:1200)\n    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getContentSummary(ClientNamenodeProtocolServerSideTranslatorPB.java:877)\n    at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\n    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\n    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:422)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)\n    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2045)\n</code></pre>\n<p>先在 HDFS 上创建对应的目录：</p>\n<pre><code>$ hadoop fs -mkdir -p /user/hive/warehouse/temp.db/temp_log_day/dt=20180312/device_type=box/province_alias=00\n</code></pre>\n<p>再次执行 drop 操作：</p>\n<pre><code>hive&gt; drop table temp.temp_log_day;\nMoved: &#39;hdfs://ycluster/user/hive/warehouse/temp.db/temp_log_day&#39; to trash at: hdfs://ycluster/user/test/.Trash/Current\nOK\n</code></pre>"},{"title":"Hive Metastore: User XX is not allowed to perform this API call","date":"2021-06-30T08:21:53.000Z","_content":"\nHiveServer2启动错误信息如下：\n\n    ```Text\n    2021-06-30T11:47:08,197  WARN [main] server.HiveServer2: Error starting HiveServer2 on attempt 22, will retry in 60000ms\n    java.lang.RuntimeException: Error initializing notification event poll\n      at org.apache.hive.service.server.HiveServer2.init(HiveServer2.java:275) ~[hive-service-3.1.2.jar:3.1.2]\n      at org.apache.hive.service.server.HiveServer2.startHiveServer2(HiveServer2.java:1036) [hive-service-3.1.2.jar:3.1.2]\n      at org.apache.hive.service.server.HiveServer2.access$1600(HiveServer2.java:140) [hive-service-3.1.2.jar:3.1.2]\n      at org.apache.hive.service.server.HiveServer2$StartOptionExecutor.execute(HiveServer2.java:1305) [hive-service-3.1.2.jar:3.1.2]\n      at org.apache.hive.service.server.HiveServer2.main(HiveServer2.java:1149) [hive-service-3.1.2.jar:3.1.2]\n      at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_281]\n      at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_281]\n      at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_281]\n      at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_281]\n      at org.apache.hadoop.util.RunJar.run(RunJar.java:323) [hadoop-common-3.2.2.jar:?]\n      at org.apache.hadoop.util.RunJar.main(RunJar.java:236) [hadoop-common-3.2.2.jar:?]\n    Caused by: java.io.IOException: org.apache.thrift.TApplicationException: Internal error processing get_current_notificationEventId\n      at org.apache.hadoop.hive.metastore.messaging.EventUtils$MSClientNotificationFetcher.getCurrentNotificationEventId(EventUtils.java:75) ~[hive-exec-3.1.2.jar:3.1.2]\n      at org.apache.hadoop.hive.ql.metadata.events.NotificationEventPoll.<init>(NotificationEventPoll.java:103) ~[hive-exec-3.1.2.jar:3.1.2]\n      at org.apache.hadoop.hive.ql.metadata.events.NotificationEventPoll.initialize(NotificationEventPoll.java:59) ~[hive-exec-3.1.2.jar:3.1.2]\n      at org.apache.hive.service.server.HiveServer2.init(HiveServer2.java:273) ~[hive-service-3.1.2.jar:3.1.2]\n      ... 10 more\n    Caused by: org.apache.thrift.TApplicationException: Internal error processing get_current_notificationEventId\n      at org.apache.thrift.TApplicationException.read(TApplicationException.java:111) ~[hive-exec-3.1.2.jar:3.1.2]\n      at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:79) ~[hive-exec-3.1.2.jar:3.1.2]\n      at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_current_notificationEventId(ThriftHiveMetastore.java:5575) ~[hive-exec-3.1.2.jar:3.1.2]\n      at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_current_notificationEventId(ThriftHiveMetastore.java:5563) ~[hive-exec-3.1.2.jar:3.1.2]\n      at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getCurrentNotificationEventId(HiveMetaStoreClient.java:2723) ~[hive-exec-3.1.2.jar:3.1.2]\n      at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source) ~[?:?]\n      at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_281]\n      at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_281]\n      at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:212) ~[hive-exec-3.1.2.jar:3.1.2]\n      at com.sun.proxy.$Proxy27.getCurrentNotificationEventId(Unknown Source) ~[?:?]\n      at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source) ~[?:?]\n      at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_281]\n      at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_281]\n      at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2773) ~[hive-exec-3.1.2.jar:3.1.2]\n      at com.sun.proxy.$Proxy27.getCurrentNotificationEventId(Unknown Source) ~[?:?]\n      at org.apache.hadoop.hive.metastore.messaging.EventUtils$MSClientNotificationFetcher.getCurrentNotificationEventId(EventUtils.java:73) ~[hive-exec-3.1.2.jar:3.1.2]\n      at org.apache.hadoop.hive.ql.metadata.events.NotificationEventPoll.<init>(NotificationEventPoll.java:103) ~[hive-exec-3.1.2.jar:3.1.2]\n      at org.apache.hadoop.hive.ql.metadata.events.NotificationEventPoll.initialize(NotificationEventPoll.java:59) ~[hive-exec-3.1.2.jar:3.1.2]\n      at org.apache.hive.service.server.HiveServer2.init(HiveServer2.java:273) ~[hive-service-3.1.2.jar:3.1.2]\n      ... 10 more\n    ```\n    \n查看Metastore日志错误信息如下：\n\n```Text\n2021-06-30T11:44:45,568 ERROR [pool-6-thread-77] thrift.ProcessFunction: Internal error processing get_current_notificationEventId\norg.apache.thrift.TException: MetaException(message:User root is not allowed to perform this API call)\n  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_current_notificationEventId(HiveMetaStore.java:7481) ~[hive-exec-3.1.2.jar:3.1.2]\n  at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source) ~[?:?]\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_281]\n  at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_281]\n  at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[hive-exec-3.1.2.jar:3.1.2]\n  at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[hive-exec-3.1.2.jar:3.1.2]\n  at com.sun.proxy.$Proxy26.get_current_notificationEventId(Unknown Source) ~[?:?]\n  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_current_notificationEventId.getResult(ThriftHiveMetastore.java:18364) ~[hive-exec-3.1.2.jar:3.1.2]\n  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_current_notificationEventId.getResult(ThriftHiveMetastore.java:18349) ~[hive-exec-3.1.2.jar:3.1.2]\n  at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) [hive-exec-3.1.2.jar:3.1.2]\n  at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:111) [hive-exec-3.1.2.jar:3.1.2]\n  at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:107) [hive-exec-3.1.2.jar:3.1.2]\n  at java.security.AccessController.doPrivileged(Native Method) [?:1.8.0_281]\n  at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_281]\n  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762) [hadoop-common-3.2.2.jar:?]\n  at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:119) [hive-exec-3.1.2.jar:3.1.2]\n  at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) [hive-exec-3.1.2.jar:3.1.2]\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_281]\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_281]\n  at java.lang.Thread.run(Thread.java:748) [?:1.8.0_281]\nCaused by: org.apache.hadoop.hive.metastore.api.MetaException: User root is not allowed to perform this API call\n  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.authorizeProxyPrivilege(HiveMetaStore.java:7517) ~[hive-exec-3.1.2.jar:3.1.2]\n  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_current_notificationEventId(HiveMetaStore.java:7477) ~[hive-exec-3.1.2.jar:3.1.2]\n  ... 19 more\n```\n\n解决方法如下：\n\n- 改用hive用户启动hiveserver2和metastore。\n- 在hadoop配置文件core-site.xml中增加以下配置，并更新集群及客户端。重启metastore及hiveserver2。\n\n```XML\n<property>\n  <name>hadoop.proxyuser.hive.groups</name>\n  <value>*</value>\n</property>\n<property>\n  <name>hadoop.proxyuser.hive.hosts</name>\n  <value>*</value>\n</property>\n```","source":"_posts/Hive-Metastore-User-XX-is-not-allowed-to-perform-this-API-call.md","raw":"title: 'Hive Metastore: User XX is not allowed to perform this API call'\ndate: 2021-06-30 16:21:53\ntags:\n- 大数据\n- Hive\ncategories:\n- 大数据\n- Hive\n---\n\nHiveServer2启动错误信息如下：\n\n    ```Text\n    2021-06-30T11:47:08,197  WARN [main] server.HiveServer2: Error starting HiveServer2 on attempt 22, will retry in 60000ms\n    java.lang.RuntimeException: Error initializing notification event poll\n      at org.apache.hive.service.server.HiveServer2.init(HiveServer2.java:275) ~[hive-service-3.1.2.jar:3.1.2]\n      at org.apache.hive.service.server.HiveServer2.startHiveServer2(HiveServer2.java:1036) [hive-service-3.1.2.jar:3.1.2]\n      at org.apache.hive.service.server.HiveServer2.access$1600(HiveServer2.java:140) [hive-service-3.1.2.jar:3.1.2]\n      at org.apache.hive.service.server.HiveServer2$StartOptionExecutor.execute(HiveServer2.java:1305) [hive-service-3.1.2.jar:3.1.2]\n      at org.apache.hive.service.server.HiveServer2.main(HiveServer2.java:1149) [hive-service-3.1.2.jar:3.1.2]\n      at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_281]\n      at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_281]\n      at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_281]\n      at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_281]\n      at org.apache.hadoop.util.RunJar.run(RunJar.java:323) [hadoop-common-3.2.2.jar:?]\n      at org.apache.hadoop.util.RunJar.main(RunJar.java:236) [hadoop-common-3.2.2.jar:?]\n    Caused by: java.io.IOException: org.apache.thrift.TApplicationException: Internal error processing get_current_notificationEventId\n      at org.apache.hadoop.hive.metastore.messaging.EventUtils$MSClientNotificationFetcher.getCurrentNotificationEventId(EventUtils.java:75) ~[hive-exec-3.1.2.jar:3.1.2]\n      at org.apache.hadoop.hive.ql.metadata.events.NotificationEventPoll.<init>(NotificationEventPoll.java:103) ~[hive-exec-3.1.2.jar:3.1.2]\n      at org.apache.hadoop.hive.ql.metadata.events.NotificationEventPoll.initialize(NotificationEventPoll.java:59) ~[hive-exec-3.1.2.jar:3.1.2]\n      at org.apache.hive.service.server.HiveServer2.init(HiveServer2.java:273) ~[hive-service-3.1.2.jar:3.1.2]\n      ... 10 more\n    Caused by: org.apache.thrift.TApplicationException: Internal error processing get_current_notificationEventId\n      at org.apache.thrift.TApplicationException.read(TApplicationException.java:111) ~[hive-exec-3.1.2.jar:3.1.2]\n      at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:79) ~[hive-exec-3.1.2.jar:3.1.2]\n      at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_current_notificationEventId(ThriftHiveMetastore.java:5575) ~[hive-exec-3.1.2.jar:3.1.2]\n      at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_current_notificationEventId(ThriftHiveMetastore.java:5563) ~[hive-exec-3.1.2.jar:3.1.2]\n      at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getCurrentNotificationEventId(HiveMetaStoreClient.java:2723) ~[hive-exec-3.1.2.jar:3.1.2]\n      at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source) ~[?:?]\n      at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_281]\n      at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_281]\n      at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:212) ~[hive-exec-3.1.2.jar:3.1.2]\n      at com.sun.proxy.$Proxy27.getCurrentNotificationEventId(Unknown Source) ~[?:?]\n      at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source) ~[?:?]\n      at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_281]\n      at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_281]\n      at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2773) ~[hive-exec-3.1.2.jar:3.1.2]\n      at com.sun.proxy.$Proxy27.getCurrentNotificationEventId(Unknown Source) ~[?:?]\n      at org.apache.hadoop.hive.metastore.messaging.EventUtils$MSClientNotificationFetcher.getCurrentNotificationEventId(EventUtils.java:73) ~[hive-exec-3.1.2.jar:3.1.2]\n      at org.apache.hadoop.hive.ql.metadata.events.NotificationEventPoll.<init>(NotificationEventPoll.java:103) ~[hive-exec-3.1.2.jar:3.1.2]\n      at org.apache.hadoop.hive.ql.metadata.events.NotificationEventPoll.initialize(NotificationEventPoll.java:59) ~[hive-exec-3.1.2.jar:3.1.2]\n      at org.apache.hive.service.server.HiveServer2.init(HiveServer2.java:273) ~[hive-service-3.1.2.jar:3.1.2]\n      ... 10 more\n    ```\n    \n查看Metastore日志错误信息如下：\n\n```Text\n2021-06-30T11:44:45,568 ERROR [pool-6-thread-77] thrift.ProcessFunction: Internal error processing get_current_notificationEventId\norg.apache.thrift.TException: MetaException(message:User root is not allowed to perform this API call)\n  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_current_notificationEventId(HiveMetaStore.java:7481) ~[hive-exec-3.1.2.jar:3.1.2]\n  at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source) ~[?:?]\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_281]\n  at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_281]\n  at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[hive-exec-3.1.2.jar:3.1.2]\n  at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[hive-exec-3.1.2.jar:3.1.2]\n  at com.sun.proxy.$Proxy26.get_current_notificationEventId(Unknown Source) ~[?:?]\n  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_current_notificationEventId.getResult(ThriftHiveMetastore.java:18364) ~[hive-exec-3.1.2.jar:3.1.2]\n  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_current_notificationEventId.getResult(ThriftHiveMetastore.java:18349) ~[hive-exec-3.1.2.jar:3.1.2]\n  at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) [hive-exec-3.1.2.jar:3.1.2]\n  at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:111) [hive-exec-3.1.2.jar:3.1.2]\n  at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:107) [hive-exec-3.1.2.jar:3.1.2]\n  at java.security.AccessController.doPrivileged(Native Method) [?:1.8.0_281]\n  at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_281]\n  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762) [hadoop-common-3.2.2.jar:?]\n  at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:119) [hive-exec-3.1.2.jar:3.1.2]\n  at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) [hive-exec-3.1.2.jar:3.1.2]\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_281]\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_281]\n  at java.lang.Thread.run(Thread.java:748) [?:1.8.0_281]\nCaused by: org.apache.hadoop.hive.metastore.api.MetaException: User root is not allowed to perform this API call\n  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.authorizeProxyPrivilege(HiveMetaStore.java:7517) ~[hive-exec-3.1.2.jar:3.1.2]\n  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_current_notificationEventId(HiveMetaStore.java:7477) ~[hive-exec-3.1.2.jar:3.1.2]\n  ... 19 more\n```\n\n解决方法如下：\n\n- 改用hive用户启动hiveserver2和metastore。\n- 在hadoop配置文件core-site.xml中增加以下配置，并更新集群及客户端。重启metastore及hiveserver2。\n\n```XML\n<property>\n  <name>hadoop.proxyuser.hive.groups</name>\n  <value>*</value>\n</property>\n<property>\n  <name>hadoop.proxyuser.hive.hosts</name>\n  <value>*</value>\n</property>\n```","slug":"Hive-Metastore-User-XX-is-not-allowed-to-perform-this-API-call","published":1,"updated":"2021-07-19T16:28:00.044Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphmw003jitd31vi66god","content":"<p>HiveServer2启动错误信息如下：</p>\n<pre><code><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">2021-06-30T11:47:08,197  WARN [main] server.HiveServer2: Error starting HiveServer2 on attempt 22, will retry in 60000ms</span><br><span class=\"line\">java.lang.RuntimeException: Error initializing notification event poll</span><br><span class=\"line\">  at org.apache.hive.service.server.HiveServer2.init(HiveServer2.java:275) ~[hive-service-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.hive.service.server.HiveServer2.startHiveServer2(HiveServer2.java:1036) [hive-service-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.hive.service.server.HiveServer2.access$1600(HiveServer2.java:140) [hive-service-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.hive.service.server.HiveServer2$StartOptionExecutor.execute(HiveServer2.java:1305) [hive-service-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.hive.service.server.HiveServer2.main(HiveServer2.java:1149) [hive-service-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_281]</span><br><span class=\"line\">  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_281]</span><br><span class=\"line\">  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_281]</span><br><span class=\"line\">  at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_281]</span><br><span class=\"line\">  at org.apache.hadoop.util.RunJar.run(RunJar.java:323) [hadoop-common-3.2.2.jar:?]</span><br><span class=\"line\">  at org.apache.hadoop.util.RunJar.main(RunJar.java:236) [hadoop-common-3.2.2.jar:?]</span><br><span class=\"line\">Caused by: java.io.IOException: org.apache.thrift.TApplicationException: Internal error processing get_current_notificationEventId</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.messaging.EventUtils$MSClientNotificationFetcher.getCurrentNotificationEventId(EventUtils.java:75) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.hadoop.hive.ql.metadata.events.NotificationEventPoll.&lt;init&gt;(NotificationEventPoll.java:103) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.hadoop.hive.ql.metadata.events.NotificationEventPoll.initialize(NotificationEventPoll.java:59) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.hive.service.server.HiveServer2.init(HiveServer2.java:273) ~[hive-service-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  ... 10 more</span><br><span class=\"line\">Caused by: org.apache.thrift.TApplicationException: Internal error processing get_current_notificationEventId</span><br><span class=\"line\">  at org.apache.thrift.TApplicationException.read(TApplicationException.java:111) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:79) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_current_notificationEventId(ThriftHiveMetastore.java:5575) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_current_notificationEventId(ThriftHiveMetastore.java:5563) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getCurrentNotificationEventId(HiveMetaStoreClient.java:2723) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source) ~[?:?]</span><br><span class=\"line\">  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_281]</span><br><span class=\"line\">  at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_281]</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:212) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at com.sun.proxy.$Proxy27.getCurrentNotificationEventId(Unknown Source) ~[?:?]</span><br><span class=\"line\">  at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source) ~[?:?]</span><br><span class=\"line\">  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_281]</span><br><span class=\"line\">  at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_281]</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2773) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at com.sun.proxy.$Proxy27.getCurrentNotificationEventId(Unknown Source) ~[?:?]</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.messaging.EventUtils$MSClientNotificationFetcher.getCurrentNotificationEventId(EventUtils.java:73) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.hadoop.hive.ql.metadata.events.NotificationEventPoll.&lt;init&gt;(NotificationEventPoll.java:103) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.hadoop.hive.ql.metadata.events.NotificationEventPoll.initialize(NotificationEventPoll.java:59) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.hive.service.server.HiveServer2.init(HiveServer2.java:273) ~[hive-service-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  ... 10 more</span><br></pre></td></tr></table></figure>\n</code></pre>\n<p>查看Metastore日志错误信息如下：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">2021-06-30T11:44:45,568 ERROR [pool-6-thread-77] thrift.ProcessFunction: Internal error processing get_current_notificationEventId</span><br><span class=\"line\">org.apache.thrift.TException: MetaException(message:User root is not allowed to perform this API call)</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_current_notificationEventId(HiveMetaStore.java:7481) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source) ~[?:?]</span><br><span class=\"line\">  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_281]</span><br><span class=\"line\">  at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_281]</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at com.sun.proxy.$Proxy26.get_current_notificationEventId(Unknown Source) ~[?:?]</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_current_notificationEventId.getResult(ThriftHiveMetastore.java:18364) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_current_notificationEventId.getResult(ThriftHiveMetastore.java:18349) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) [hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:111) [hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:107) [hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at java.security.AccessController.doPrivileged(Native Method) [?:1.8.0_281]</span><br><span class=\"line\">  at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_281]</span><br><span class=\"line\">  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762) [hadoop-common-3.2.2.jar:?]</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:119) [hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) [hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_281]</span><br><span class=\"line\">  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_281]</span><br><span class=\"line\">  at java.lang.Thread.run(Thread.java:748) [?:1.8.0_281]</span><br><span class=\"line\">Caused by: org.apache.hadoop.hive.metastore.api.MetaException: User root is not allowed to perform this API call</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.authorizeProxyPrivilege(HiveMetaStore.java:7517) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_current_notificationEventId(HiveMetaStore.java:7477) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  ... 19 more</span><br></pre></td></tr></table></figure>\n\n<p>解决方法如下：</p>\n<ul>\n<li>改用hive用户启动hiveserver2和metastore。</li>\n<li>在hadoop配置文件core-site.xml中增加以下配置，并更新集群及客户端。重启metastore及hiveserver2。</li>\n</ul>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>hadoop.proxyuser.hive.groups<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>*<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>hadoop.proxyuser.hive.hosts<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>*<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<p>HiveServer2启动错误信息如下：</p>\n<pre><code><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">2021-06-30T11:47:08,197  WARN [main] server.HiveServer2: Error starting HiveServer2 on attempt 22, will retry in 60000ms</span><br><span class=\"line\">java.lang.RuntimeException: Error initializing notification event poll</span><br><span class=\"line\">  at org.apache.hive.service.server.HiveServer2.init(HiveServer2.java:275) ~[hive-service-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.hive.service.server.HiveServer2.startHiveServer2(HiveServer2.java:1036) [hive-service-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.hive.service.server.HiveServer2.access$1600(HiveServer2.java:140) [hive-service-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.hive.service.server.HiveServer2$StartOptionExecutor.execute(HiveServer2.java:1305) [hive-service-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.hive.service.server.HiveServer2.main(HiveServer2.java:1149) [hive-service-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_281]</span><br><span class=\"line\">  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_281]</span><br><span class=\"line\">  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_281]</span><br><span class=\"line\">  at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_281]</span><br><span class=\"line\">  at org.apache.hadoop.util.RunJar.run(RunJar.java:323) [hadoop-common-3.2.2.jar:?]</span><br><span class=\"line\">  at org.apache.hadoop.util.RunJar.main(RunJar.java:236) [hadoop-common-3.2.2.jar:?]</span><br><span class=\"line\">Caused by: java.io.IOException: org.apache.thrift.TApplicationException: Internal error processing get_current_notificationEventId</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.messaging.EventUtils$MSClientNotificationFetcher.getCurrentNotificationEventId(EventUtils.java:75) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.hadoop.hive.ql.metadata.events.NotificationEventPoll.&lt;init&gt;(NotificationEventPoll.java:103) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.hadoop.hive.ql.metadata.events.NotificationEventPoll.initialize(NotificationEventPoll.java:59) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.hive.service.server.HiveServer2.init(HiveServer2.java:273) ~[hive-service-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  ... 10 more</span><br><span class=\"line\">Caused by: org.apache.thrift.TApplicationException: Internal error processing get_current_notificationEventId</span><br><span class=\"line\">  at org.apache.thrift.TApplicationException.read(TApplicationException.java:111) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:79) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_current_notificationEventId(ThriftHiveMetastore.java:5575) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_current_notificationEventId(ThriftHiveMetastore.java:5563) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getCurrentNotificationEventId(HiveMetaStoreClient.java:2723) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source) ~[?:?]</span><br><span class=\"line\">  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_281]</span><br><span class=\"line\">  at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_281]</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:212) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at com.sun.proxy.$Proxy27.getCurrentNotificationEventId(Unknown Source) ~[?:?]</span><br><span class=\"line\">  at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source) ~[?:?]</span><br><span class=\"line\">  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_281]</span><br><span class=\"line\">  at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_281]</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2773) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at com.sun.proxy.$Proxy27.getCurrentNotificationEventId(Unknown Source) ~[?:?]</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.messaging.EventUtils$MSClientNotificationFetcher.getCurrentNotificationEventId(EventUtils.java:73) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.hadoop.hive.ql.metadata.events.NotificationEventPoll.&lt;init&gt;(NotificationEventPoll.java:103) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.hadoop.hive.ql.metadata.events.NotificationEventPoll.initialize(NotificationEventPoll.java:59) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.hive.service.server.HiveServer2.init(HiveServer2.java:273) ~[hive-service-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  ... 10 more</span><br></pre></td></tr></table></figure>\n</code></pre>\n<p>查看Metastore日志错误信息如下：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">2021-06-30T11:44:45,568 ERROR [pool-6-thread-77] thrift.ProcessFunction: Internal error processing get_current_notificationEventId</span><br><span class=\"line\">org.apache.thrift.TException: MetaException(message:User root is not allowed to perform this API call)</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_current_notificationEventId(HiveMetaStore.java:7481) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source) ~[?:?]</span><br><span class=\"line\">  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_281]</span><br><span class=\"line\">  at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_281]</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at com.sun.proxy.$Proxy26.get_current_notificationEventId(Unknown Source) ~[?:?]</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_current_notificationEventId.getResult(ThriftHiveMetastore.java:18364) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_current_notificationEventId.getResult(ThriftHiveMetastore.java:18349) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) [hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:111) [hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:107) [hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at java.security.AccessController.doPrivileged(Native Method) [?:1.8.0_281]</span><br><span class=\"line\">  at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_281]</span><br><span class=\"line\">  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762) [hadoop-common-3.2.2.jar:?]</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:119) [hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) [hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_281]</span><br><span class=\"line\">  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_281]</span><br><span class=\"line\">  at java.lang.Thread.run(Thread.java:748) [?:1.8.0_281]</span><br><span class=\"line\">Caused by: org.apache.hadoop.hive.metastore.api.MetaException: User root is not allowed to perform this API call</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.authorizeProxyPrivilege(HiveMetaStore.java:7517) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_current_notificationEventId(HiveMetaStore.java:7477) ~[hive-exec-3.1.2.jar:3.1.2]</span><br><span class=\"line\">  ... 19 more</span><br></pre></td></tr></table></figure>\n\n<p>解决方法如下：</p>\n<ul>\n<li>改用hive用户启动hiveserver2和metastore。</li>\n<li>在hadoop配置文件core-site.xml中增加以下配置，并更新集群及客户端。重启metastore及hiveserver2。</li>\n</ul>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>hadoop.proxyuser.hive.groups<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>*<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>hadoop.proxyuser.hive.hosts<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>*<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br></pre></td></tr></table></figure>"},{"title":"Hive NoSuchMethodError Preconditions.checkArgument","date":"2021-06-23T09:49:50.000Z","_content":"\n异常信息如下：\n\n```SHELL\nException in thread \"main\" java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V\n  at org.apache.hadoop.conf.Configuration.set(Configuration.java:1357)\n  at org.apache.hadoop.conf.Configuration.set(Configuration.java:1338)\n  at org.apache.hadoop.conf.Configuration.setBoolean(Configuration.java:1679)\n  at org.apache.hadoop.hive.metastore.conf.MetastoreConf.setBoolVar(MetastoreConf.java:1379)\n  at org.apache.hadoop.hive.metastore.conf.MetastoreConf.newMetastoreConf(MetastoreConf.java:1196)\n  at org.apache.hadoop.hive.metastore.HiveMetaStore.main(HiveMetaStore.java:8770)\n  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at org.apache.hadoop.util.RunJar.run(RunJar.java:323)\n  at org.apache.hadoop.util.RunJar.main(RunJar.java:236)\n```\n\n这个异常是Hive和Hadoop依赖的guava版本不一致造成的。解决方法是查看Hadoop安装目录下share/hadoop/common/lib和Hive安装目录下lib中guava的版本，删除版本低的，并拷贝高版本的。","source":"_posts/Hive-NoSuchMethodError-Preconditions-checkArgument.md","raw":"title: Hive NoSuchMethodError Preconditions.checkArgument\ndate: 2021-06-23 17:49:50\ntags:\n- 大数据\n- Hive\n- Hadoop\ncategories:\n- 大数据\n- Hive\n---\n\n异常信息如下：\n\n```SHELL\nException in thread \"main\" java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V\n  at org.apache.hadoop.conf.Configuration.set(Configuration.java:1357)\n  at org.apache.hadoop.conf.Configuration.set(Configuration.java:1338)\n  at org.apache.hadoop.conf.Configuration.setBoolean(Configuration.java:1679)\n  at org.apache.hadoop.hive.metastore.conf.MetastoreConf.setBoolVar(MetastoreConf.java:1379)\n  at org.apache.hadoop.hive.metastore.conf.MetastoreConf.newMetastoreConf(MetastoreConf.java:1196)\n  at org.apache.hadoop.hive.metastore.HiveMetaStore.main(HiveMetaStore.java:8770)\n  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at org.apache.hadoop.util.RunJar.run(RunJar.java:323)\n  at org.apache.hadoop.util.RunJar.main(RunJar.java:236)\n```\n\n这个异常是Hive和Hadoop依赖的guava版本不一致造成的。解决方法是查看Hadoop安装目录下share/hadoop/common/lib和Hive安装目录下lib中guava的版本，删除版本低的，并拷贝高版本的。","slug":"Hive-NoSuchMethodError-Preconditions-checkArgument","published":1,"updated":"2021-07-19T16:28:00.196Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphmy003mitd3b1yx4ylr","content":"<p>异常信息如下：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Exception in thread &quot;main&quot; java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V</span><br><span class=\"line\">  at org.apache.hadoop.conf.Configuration.set(Configuration.java:1357)</span><br><span class=\"line\">  at org.apache.hadoop.conf.Configuration.set(Configuration.java:1338)</span><br><span class=\"line\">  at org.apache.hadoop.conf.Configuration.setBoolean(Configuration.java:1679)</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.conf.MetastoreConf.setBoolVar(MetastoreConf.java:1379)</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.conf.MetastoreConf.newMetastoreConf(MetastoreConf.java:1196)</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.HiveMetaStore.main(HiveMetaStore.java:8770)</span><br><span class=\"line\">  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class=\"line\">  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class=\"line\">  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class=\"line\">  at java.lang.reflect.Method.invoke(Method.java:498)</span><br><span class=\"line\">  at org.apache.hadoop.util.RunJar.run(RunJar.java:323)</span><br><span class=\"line\">  at org.apache.hadoop.util.RunJar.main(RunJar.java:236)</span><br></pre></td></tr></table></figure>\n\n<p>这个异常是Hive和Hadoop依赖的guava版本不一致造成的。解决方法是查看Hadoop安装目录下share/hadoop/common/lib和Hive安装目录下lib中guava的版本，删除版本低的，并拷贝高版本的。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>异常信息如下：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Exception in thread &quot;main&quot; java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V</span><br><span class=\"line\">  at org.apache.hadoop.conf.Configuration.set(Configuration.java:1357)</span><br><span class=\"line\">  at org.apache.hadoop.conf.Configuration.set(Configuration.java:1338)</span><br><span class=\"line\">  at org.apache.hadoop.conf.Configuration.setBoolean(Configuration.java:1679)</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.conf.MetastoreConf.setBoolVar(MetastoreConf.java:1379)</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.conf.MetastoreConf.newMetastoreConf(MetastoreConf.java:1196)</span><br><span class=\"line\">  at org.apache.hadoop.hive.metastore.HiveMetaStore.main(HiveMetaStore.java:8770)</span><br><span class=\"line\">  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class=\"line\">  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class=\"line\">  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class=\"line\">  at java.lang.reflect.Method.invoke(Method.java:498)</span><br><span class=\"line\">  at org.apache.hadoop.util.RunJar.run(RunJar.java:323)</span><br><span class=\"line\">  at org.apache.hadoop.util.RunJar.main(RunJar.java:236)</span><br></pre></td></tr></table></figure>\n\n<p>这个异常是Hive和Hadoop依赖的guava版本不一致造成的。解决方法是查看Hadoop安装目录下share/hadoop/common/lib和Hive安装目录下lib中guava的版本，删除版本低的，并拷贝高版本的。</p>\n"},{"title":"Hive join 要点","date":"2017-07-08T09:59:59.000Z","_content":"\n在写 Hive join 查询时需要关注以下要点：\n\n- 允许复杂关联表达式：\n\n    SELECT a.* FROM a JOIN b ON (a.id = b.id);\n    SELECT a.* FROM a JOIN b ON (a.id = b.id AND a.department = b.department);\n    SELECT a.* FROM a LEFT OUTER JOIN b ON (a.id <> b.id);\n\n以上都是有效的关联。\n\n<!-- more -->\n\n- 在一个查询中可以关联 2 张以上的表：\n\n    SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2);\n\n是有效的关联。\n\n- 如果在关联从句中所有表都使用同一列进行关联，Hive 会转化多表关联为一个 map / reduce 任务：\n\n    SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1);\n\n上面的语句会被转化为一个 map / reduce 任务，因为只有 b 的 key1 列被用作关联。换句话说，\n\n    SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2);\n    \n上面的语句会被转化为两个 map / reduce 任务，因为第一个关联条件使用 b 中的 key1，b 中的 key2 用在第二个关联条件中。第一个 map / reduce 任务关联 a 和 b，然后结果在第二个 map / reduce 任务中与 c 关联。\n\n- 在关联的每个 map / reduce 阶段，序列中最后的表通过 reducer 被分流，其他表被缓存在 reducer 中。因此，通过组织表，是最大的表出现在序列的最后，缓存关联键特定值的行在 reducer 中，有助于降低 reducer 内存的需求。例如，在下面的语句中：\n\n    SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1);\n\n三张表在一个 map / reduce 任务中关联，表 a 和 表 b 的关联键的特定值被缓存在 reducer 的内存中。然后对从 c 获取的每一行用缓存的每一行来计算关联。类似的：\n\n    SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2);\n    \n计算这个关联需要调起两个 map / reduce 任务。第一个任务关联 a 和 b，缓存 a 的值并在 reducer 中分流 b 的值。第二个任务缓存第一个任务的结果，且通过 reducer 分流 c 的值。\n\n- 在关联的每个 map / reduce 阶段，可以通过提示指定被分流的表。例如，在下面的语句中：\n\n    SELECT /*+ STREAMTABLE(a) */ a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1);\n\n三张表在一个 map / reduce 任务中关联，表 b 和 表 c 的关联键的特定值被缓存在 reducer 的内存中。然后，对从 a 获取的每一行用缓存的每一行来计算关联。如果 STREAMTABLE 提示被省略，Hive 分流关联中最右面的a表。\n\n- LEFT、RIGHT 和 FULL OUTER 关联的存在是为了对 ON 字句不能匹配的记录提供更多的控制。例如：\n\n    SELECT a.val, b.val FROM a LEFT OUTER JOIN b ON (a.key=b.key);\n\n这个查询将返回 a 中所有的行。当 b.key 等于 a.key 时输出为“a.val,b.val”；当没有相应的 b.key 时输出为“a.val,NULL”。b 中没有对应 a.key 的行被丢弃。为了理解“FROM a LEFT OUTER JOIN b”语法是如何工作的必须写在一行——在这个查询中 a 是 b 的 LEFT，因此所有 a 中的行被保留；a RIGHT OUTER JOIN 会保留 b 中所有的行；a FULL OUTER JOIN 将会保留 a 中的所有行及 b 中所有的行。OUTER JOIN 应该符合标准的 SQL 规范。\n\n- 关联发生在 WHERE 从句之前。因此，如果要限制关联的输出，条件应该在 WHERE 从句中，否则它应该在 JOIN 从句中。这个问题的一大困惑是分区表：\n\n    SELECT a.val, b.val FROM a LEFT OUTER JOIN b ON (a.key=b.key)\n    WHERE a.ds='2009-07-07' AND b.ds='2009-07-07';\n\n这个查询会关联 a 和 b，生成一个 a.val 和  b.val 的列表。然而，WHERE 从句可以引用关联输出中 a 和 b 的其他列，然后过滤它们之后输出。然而无论何时，JOIN 中的一行发现 a 的一个 key 且没有 b 的 key，所有 b 的列将会是 NULL，包括 ds 列。这就是说，你将过滤出没有有效 b.key 的关联的所有行，因此你对 LEFT OUTER 有更明智的要求。换种说法，关联的 LEFT OUTER 部分跟 WHERE 从句中引用 b 中的任意列是不相关的。反而，当外关联时，使用这个语法：\n\n    SELECT a.val, b.val FROM a LEFT OUTER JOIN b\n    ON (a.key=b.key AND b.ds='2009-07-07' AND a.ds='2009-07-07');\n\n结果是关联的输出被提前过滤，对于有效 a.key 但没有匹配的 b.key 的行避免了后过滤。同样的逻辑也适用于 RIGHT 和 FULL 关联。\n\n**测试两种写法的输出是否有区别：**\n\n数据准备：\n\n    create table a(key string,ds string);\n    create table b(key string,ds string);\n    insert into a values('k1','2009-07-07'),('k2','2009-07-07'),('k3','2009-07-08'),('k4','2009-07-07'),('k6','2009-07-07');\n    insert into b values('k1','2009-07-07'),('k2','2009-07-07'),('k3','2009-07-07'),('k4','2009-07-08'),('k5','2009-07-07');\n\n第一种方法：\n\n    SELECT a.key, b.key FROM a LEFT OUTER JOIN b ON (a.key=b.key) WHERE a.ds='2009-07-07' AND b.ds='2009-07-07';\n    k1\tk1\n    k2\tk2\n\n第二种方法：\n\n    SELECT a.key, b.key FROM a LEFT OUTER JOIN b ON (a.key=b.key AND b.ds='2009-07-07' AND a.ds='2009-07-07');\n    k1\tk1\n    k2\tk2\n    k3\tNULL\n    k4\tNULL\n    k6\tNULL\n\n- 关联是不能交换的！关联是左结合的，不管是 LEFT 还是 RIGHT 关联。\n\n    SELECT a.val1, a.val2, b.val, c.val\n    FROM a\n    JOIN b ON (a.key = b.key)\n    LEFT OUTER JOIN c ON (a.key = c.key);\n\n首先关联 a 和 b，丢弃掉 a 和 b 所有不能关联的行。之后的表与 c 关联。这为如果在 a 和 c 中都存在的 key 但是在 b 中不存在的情况提供了直观的结果：整行（包括 a.val1、a.val2 和 a.key）都会在“a JOIN b”环节丢弃掉，因为它不在 b 中。结果中没有 a.key，因此当与 c 进行 LEFT OUTER JOIN 时，c.val 也不在，因为没有 c.key 匹配 a.key（因此 a 中的行被移除）。同样的，如果是 RIGHT OUTER JOIN（而不是 LEFT），我们最终会得到意向不到的效果：NULL, NULL, NULL, c.val，因为即使我们指定了 a.key=c.key 作为关联键，也会在第一个关联中丢弃不匹配的 a 中的行。\n\n为了得到更直观的效果，应该替换为 FROM c LEFT OUTER JOIN a ON (c.key = a.key) LEFT OUTER JOIN b ON (c.key = b.key)。\n\n- LEFT SEMI JOIN 以一种有效的方式实现了无关联的 IN/EXISTS 子查询表达式。从 Hive 0.13 起 IN/NOT IN/EXISTS/NOT EXISTS 操作符通过使用子查询被支持，所以大多数这些关联不需要再手动执行。使用 LEFT SEMI JOIN 的限制是，右手边的表只能在关联条件（ON 字句）中被引用，不能在 WHERE 或者 SELECT 中使用。\n\n    SELECT a.key, a.value\n    FROM a\n    WHERE a.key in\n     (SELECT b.key\n      FROM B);\n\n可以被写作：\n\n    SELECT a.key, a.val\n    FROM a LEFT SEMI JOIN b ON (a.key = b.key);\n    \n- 如果除了一张表外其他表都很小，关联可以被优化为一个只有 map 的任务。\n\n    SELECT /*+ MAPJOIN(b) */ a.key, a.value\n    FROM a JOIN b ON a.key = b.key;\n\n这个查询不需要 reducer。每一个 A 的 mapper，B 都被完全读入。限制是 a FULL/RIGHT OUTER JOIN b 不能被优化。\n\n- 如果关联的表以关联的列被分桶化，并且一张表的桶数量是另外一张表桶数量的很多倍，那么每个桶可以互相关联。如果表 A 有 4 个桶，表 B 也有 4 个桶，下面的关联可以只在 mapper 中完成。\n\n    SELECT /*+ MAPJOIN(b) */ a.key, a.value\n    FROM a JOIN b ON a.key = b.key;\n\n只获取需要的桶，而不是针对 A 的每个 mapper 都获取 B 的全部。对于上面的查询，处理 A 的桶 1 的 mapper 只获取 B 的桶 1。这不是默认行为，由以下参数控制：\n\n    set hive.optimize.bucketmapjoin = true;\n    \n- 如果关联的表以关联的列排序且分桶化，并且它们有相同的桶数量，可以进行合并排序关联。在 mapper 中相关的桶彼此关联。如果 A 和 B 都有 4 个桶，下面的关联可以只在 mapper 中完成。\n\n    SELECT /*+ MAPJOIN(b) */ a.key, a.value\n    FROM A a JOIN B b ON a.key = b.key;\n\nA 的桶的 mapper 将通过相关的 B 的桶处理。这不是默认行为，需要设置下面的参数：\n\n    set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;\n    set hive.optimize.bucketmapjoin = true;\n    set hive.optimize.bucketmapjoin.sortedmerge = true;\n","source":"_posts/Hive-join-要点.md","raw":"title: Hive join 要点\ntags:\n  - Hive\ncategories:\n  - 大数据\n  - Hive\ndate: 2017-07-08 17:59:59\n---\n\n在写 Hive join 查询时需要关注以下要点：\n\n- 允许复杂关联表达式：\n\n    SELECT a.* FROM a JOIN b ON (a.id = b.id);\n    SELECT a.* FROM a JOIN b ON (a.id = b.id AND a.department = b.department);\n    SELECT a.* FROM a LEFT OUTER JOIN b ON (a.id <> b.id);\n\n以上都是有效的关联。\n\n<!-- more -->\n\n- 在一个查询中可以关联 2 张以上的表：\n\n    SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2);\n\n是有效的关联。\n\n- 如果在关联从句中所有表都使用同一列进行关联，Hive 会转化多表关联为一个 map / reduce 任务：\n\n    SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1);\n\n上面的语句会被转化为一个 map / reduce 任务，因为只有 b 的 key1 列被用作关联。换句话说，\n\n    SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2);\n    \n上面的语句会被转化为两个 map / reduce 任务，因为第一个关联条件使用 b 中的 key1，b 中的 key2 用在第二个关联条件中。第一个 map / reduce 任务关联 a 和 b，然后结果在第二个 map / reduce 任务中与 c 关联。\n\n- 在关联的每个 map / reduce 阶段，序列中最后的表通过 reducer 被分流，其他表被缓存在 reducer 中。因此，通过组织表，是最大的表出现在序列的最后，缓存关联键特定值的行在 reducer 中，有助于降低 reducer 内存的需求。例如，在下面的语句中：\n\n    SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1);\n\n三张表在一个 map / reduce 任务中关联，表 a 和 表 b 的关联键的特定值被缓存在 reducer 的内存中。然后对从 c 获取的每一行用缓存的每一行来计算关联。类似的：\n\n    SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2);\n    \n计算这个关联需要调起两个 map / reduce 任务。第一个任务关联 a 和 b，缓存 a 的值并在 reducer 中分流 b 的值。第二个任务缓存第一个任务的结果，且通过 reducer 分流 c 的值。\n\n- 在关联的每个 map / reduce 阶段，可以通过提示指定被分流的表。例如，在下面的语句中：\n\n    SELECT /*+ STREAMTABLE(a) */ a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1);\n\n三张表在一个 map / reduce 任务中关联，表 b 和 表 c 的关联键的特定值被缓存在 reducer 的内存中。然后，对从 a 获取的每一行用缓存的每一行来计算关联。如果 STREAMTABLE 提示被省略，Hive 分流关联中最右面的a表。\n\n- LEFT、RIGHT 和 FULL OUTER 关联的存在是为了对 ON 字句不能匹配的记录提供更多的控制。例如：\n\n    SELECT a.val, b.val FROM a LEFT OUTER JOIN b ON (a.key=b.key);\n\n这个查询将返回 a 中所有的行。当 b.key 等于 a.key 时输出为“a.val,b.val”；当没有相应的 b.key 时输出为“a.val,NULL”。b 中没有对应 a.key 的行被丢弃。为了理解“FROM a LEFT OUTER JOIN b”语法是如何工作的必须写在一行——在这个查询中 a 是 b 的 LEFT，因此所有 a 中的行被保留；a RIGHT OUTER JOIN 会保留 b 中所有的行；a FULL OUTER JOIN 将会保留 a 中的所有行及 b 中所有的行。OUTER JOIN 应该符合标准的 SQL 规范。\n\n- 关联发生在 WHERE 从句之前。因此，如果要限制关联的输出，条件应该在 WHERE 从句中，否则它应该在 JOIN 从句中。这个问题的一大困惑是分区表：\n\n    SELECT a.val, b.val FROM a LEFT OUTER JOIN b ON (a.key=b.key)\n    WHERE a.ds='2009-07-07' AND b.ds='2009-07-07';\n\n这个查询会关联 a 和 b，生成一个 a.val 和  b.val 的列表。然而，WHERE 从句可以引用关联输出中 a 和 b 的其他列，然后过滤它们之后输出。然而无论何时，JOIN 中的一行发现 a 的一个 key 且没有 b 的 key，所有 b 的列将会是 NULL，包括 ds 列。这就是说，你将过滤出没有有效 b.key 的关联的所有行，因此你对 LEFT OUTER 有更明智的要求。换种说法，关联的 LEFT OUTER 部分跟 WHERE 从句中引用 b 中的任意列是不相关的。反而，当外关联时，使用这个语法：\n\n    SELECT a.val, b.val FROM a LEFT OUTER JOIN b\n    ON (a.key=b.key AND b.ds='2009-07-07' AND a.ds='2009-07-07');\n\n结果是关联的输出被提前过滤，对于有效 a.key 但没有匹配的 b.key 的行避免了后过滤。同样的逻辑也适用于 RIGHT 和 FULL 关联。\n\n**测试两种写法的输出是否有区别：**\n\n数据准备：\n\n    create table a(key string,ds string);\n    create table b(key string,ds string);\n    insert into a values('k1','2009-07-07'),('k2','2009-07-07'),('k3','2009-07-08'),('k4','2009-07-07'),('k6','2009-07-07');\n    insert into b values('k1','2009-07-07'),('k2','2009-07-07'),('k3','2009-07-07'),('k4','2009-07-08'),('k5','2009-07-07');\n\n第一种方法：\n\n    SELECT a.key, b.key FROM a LEFT OUTER JOIN b ON (a.key=b.key) WHERE a.ds='2009-07-07' AND b.ds='2009-07-07';\n    k1\tk1\n    k2\tk2\n\n第二种方法：\n\n    SELECT a.key, b.key FROM a LEFT OUTER JOIN b ON (a.key=b.key AND b.ds='2009-07-07' AND a.ds='2009-07-07');\n    k1\tk1\n    k2\tk2\n    k3\tNULL\n    k4\tNULL\n    k6\tNULL\n\n- 关联是不能交换的！关联是左结合的，不管是 LEFT 还是 RIGHT 关联。\n\n    SELECT a.val1, a.val2, b.val, c.val\n    FROM a\n    JOIN b ON (a.key = b.key)\n    LEFT OUTER JOIN c ON (a.key = c.key);\n\n首先关联 a 和 b，丢弃掉 a 和 b 所有不能关联的行。之后的表与 c 关联。这为如果在 a 和 c 中都存在的 key 但是在 b 中不存在的情况提供了直观的结果：整行（包括 a.val1、a.val2 和 a.key）都会在“a JOIN b”环节丢弃掉，因为它不在 b 中。结果中没有 a.key，因此当与 c 进行 LEFT OUTER JOIN 时，c.val 也不在，因为没有 c.key 匹配 a.key（因此 a 中的行被移除）。同样的，如果是 RIGHT OUTER JOIN（而不是 LEFT），我们最终会得到意向不到的效果：NULL, NULL, NULL, c.val，因为即使我们指定了 a.key=c.key 作为关联键，也会在第一个关联中丢弃不匹配的 a 中的行。\n\n为了得到更直观的效果，应该替换为 FROM c LEFT OUTER JOIN a ON (c.key = a.key) LEFT OUTER JOIN b ON (c.key = b.key)。\n\n- LEFT SEMI JOIN 以一种有效的方式实现了无关联的 IN/EXISTS 子查询表达式。从 Hive 0.13 起 IN/NOT IN/EXISTS/NOT EXISTS 操作符通过使用子查询被支持，所以大多数这些关联不需要再手动执行。使用 LEFT SEMI JOIN 的限制是，右手边的表只能在关联条件（ON 字句）中被引用，不能在 WHERE 或者 SELECT 中使用。\n\n    SELECT a.key, a.value\n    FROM a\n    WHERE a.key in\n     (SELECT b.key\n      FROM B);\n\n可以被写作：\n\n    SELECT a.key, a.val\n    FROM a LEFT SEMI JOIN b ON (a.key = b.key);\n    \n- 如果除了一张表外其他表都很小，关联可以被优化为一个只有 map 的任务。\n\n    SELECT /*+ MAPJOIN(b) */ a.key, a.value\n    FROM a JOIN b ON a.key = b.key;\n\n这个查询不需要 reducer。每一个 A 的 mapper，B 都被完全读入。限制是 a FULL/RIGHT OUTER JOIN b 不能被优化。\n\n- 如果关联的表以关联的列被分桶化，并且一张表的桶数量是另外一张表桶数量的很多倍，那么每个桶可以互相关联。如果表 A 有 4 个桶，表 B 也有 4 个桶，下面的关联可以只在 mapper 中完成。\n\n    SELECT /*+ MAPJOIN(b) */ a.key, a.value\n    FROM a JOIN b ON a.key = b.key;\n\n只获取需要的桶，而不是针对 A 的每个 mapper 都获取 B 的全部。对于上面的查询，处理 A 的桶 1 的 mapper 只获取 B 的桶 1。这不是默认行为，由以下参数控制：\n\n    set hive.optimize.bucketmapjoin = true;\n    \n- 如果关联的表以关联的列排序且分桶化，并且它们有相同的桶数量，可以进行合并排序关联。在 mapper 中相关的桶彼此关联。如果 A 和 B 都有 4 个桶，下面的关联可以只在 mapper 中完成。\n\n    SELECT /*+ MAPJOIN(b) */ a.key, a.value\n    FROM A a JOIN B b ON a.key = b.key;\n\nA 的桶的 mapper 将通过相关的 B 的桶处理。这不是默认行为，需要设置下面的参数：\n\n    set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;\n    set hive.optimize.bucketmapjoin = true;\n    set hive.optimize.bucketmapjoin.sortedmerge = true;\n","slug":"Hive-join-要点","published":1,"updated":"2021-07-19T16:28:00.256Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphmz003pitd3gz803y3s","content":"<p>在写 Hive join 查询时需要关注以下要点：</p>\n<ul>\n<li><p>允许复杂关联表达式：</p>\n<p>  SELECT a.* FROM a JOIN b ON (a.id = b.id);<br>  SELECT a.* FROM a JOIN b ON (a.id = b.id AND a.department = b.department);<br>  SELECT a.* FROM a LEFT OUTER JOIN b ON (a.id &lt;&gt; b.id);</p>\n</li>\n</ul>\n<p>以上都是有效的关联。</p>\n<span id=\"more\"></span>\n\n<ul>\n<li><p>在一个查询中可以关联 2 张以上的表：</p>\n<p>  SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2);</p>\n</li>\n</ul>\n<p>是有效的关联。</p>\n<ul>\n<li><p>如果在关联从句中所有表都使用同一列进行关联，Hive 会转化多表关联为一个 map / reduce 任务：</p>\n<p>  SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1);</p>\n</li>\n</ul>\n<p>上面的语句会被转化为一个 map / reduce 任务，因为只有 b 的 key1 列被用作关联。换句话说，</p>\n<pre><code>SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2);\n</code></pre>\n<p>上面的语句会被转化为两个 map / reduce 任务，因为第一个关联条件使用 b 中的 key1，b 中的 key2 用在第二个关联条件中。第一个 map / reduce 任务关联 a 和 b，然后结果在第二个 map / reduce 任务中与 c 关联。</p>\n<ul>\n<li><p>在关联的每个 map / reduce 阶段，序列中最后的表通过 reducer 被分流，其他表被缓存在 reducer 中。因此，通过组织表，是最大的表出现在序列的最后，缓存关联键特定值的行在 reducer 中，有助于降低 reducer 内存的需求。例如，在下面的语句中：</p>\n<p>  SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1);</p>\n</li>\n</ul>\n<p>三张表在一个 map / reduce 任务中关联，表 a 和 表 b 的关联键的特定值被缓存在 reducer 的内存中。然后对从 c 获取的每一行用缓存的每一行来计算关联。类似的：</p>\n<pre><code>SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2);\n</code></pre>\n<p>计算这个关联需要调起两个 map / reduce 任务。第一个任务关联 a 和 b，缓存 a 的值并在 reducer 中分流 b 的值。第二个任务缓存第一个任务的结果，且通过 reducer 分流 c 的值。</p>\n<ul>\n<li><p>在关联的每个 map / reduce 阶段，可以通过提示指定被分流的表。例如，在下面的语句中：</p>\n<p>  SELECT /*+ STREAMTABLE(a) */ a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1);</p>\n</li>\n</ul>\n<p>三张表在一个 map / reduce 任务中关联，表 b 和 表 c 的关联键的特定值被缓存在 reducer 的内存中。然后，对从 a 获取的每一行用缓存的每一行来计算关联。如果 STREAMTABLE 提示被省略，Hive 分流关联中最右面的a表。</p>\n<ul>\n<li><p>LEFT、RIGHT 和 FULL OUTER 关联的存在是为了对 ON 字句不能匹配的记录提供更多的控制。例如：</p>\n<p>  SELECT a.val, b.val FROM a LEFT OUTER JOIN b ON (a.key=b.key);</p>\n</li>\n</ul>\n<p>这个查询将返回 a 中所有的行。当 b.key 等于 a.key 时输出为“a.val,b.val”；当没有相应的 b.key 时输出为“a.val,NULL”。b 中没有对应 a.key 的行被丢弃。为了理解“FROM a LEFT OUTER JOIN b”语法是如何工作的必须写在一行——在这个查询中 a 是 b 的 LEFT，因此所有 a 中的行被保留；a RIGHT OUTER JOIN 会保留 b 中所有的行；a FULL OUTER JOIN 将会保留 a 中的所有行及 b 中所有的行。OUTER JOIN 应该符合标准的 SQL 规范。</p>\n<ul>\n<li><p>关联发生在 WHERE 从句之前。因此，如果要限制关联的输出，条件应该在 WHERE 从句中，否则它应该在 JOIN 从句中。这个问题的一大困惑是分区表：</p>\n<p>  SELECT a.val, b.val FROM a LEFT OUTER JOIN b ON (a.key=b.key)<br>  WHERE a.ds=’2009-07-07’ AND b.ds=’2009-07-07’;</p>\n</li>\n</ul>\n<p>这个查询会关联 a 和 b，生成一个 a.val 和  b.val 的列表。然而，WHERE 从句可以引用关联输出中 a 和 b 的其他列，然后过滤它们之后输出。然而无论何时，JOIN 中的一行发现 a 的一个 key 且没有 b 的 key，所有 b 的列将会是 NULL，包括 ds 列。这就是说，你将过滤出没有有效 b.key 的关联的所有行，因此你对 LEFT OUTER 有更明智的要求。换种说法，关联的 LEFT OUTER 部分跟 WHERE 从句中引用 b 中的任意列是不相关的。反而，当外关联时，使用这个语法：</p>\n<pre><code>SELECT a.val, b.val FROM a LEFT OUTER JOIN b\nON (a.key=b.key AND b.ds=&#39;2009-07-07&#39; AND a.ds=&#39;2009-07-07&#39;);\n</code></pre>\n<p>结果是关联的输出被提前过滤，对于有效 a.key 但没有匹配的 b.key 的行避免了后过滤。同样的逻辑也适用于 RIGHT 和 FULL 关联。</p>\n<p><strong>测试两种写法的输出是否有区别：</strong></p>\n<p>数据准备：</p>\n<pre><code>create table a(key string,ds string);\ncreate table b(key string,ds string);\ninsert into a values(&#39;k1&#39;,&#39;2009-07-07&#39;),(&#39;k2&#39;,&#39;2009-07-07&#39;),(&#39;k3&#39;,&#39;2009-07-08&#39;),(&#39;k4&#39;,&#39;2009-07-07&#39;),(&#39;k6&#39;,&#39;2009-07-07&#39;);\ninsert into b values(&#39;k1&#39;,&#39;2009-07-07&#39;),(&#39;k2&#39;,&#39;2009-07-07&#39;),(&#39;k3&#39;,&#39;2009-07-07&#39;),(&#39;k4&#39;,&#39;2009-07-08&#39;),(&#39;k5&#39;,&#39;2009-07-07&#39;);\n</code></pre>\n<p>第一种方法：</p>\n<pre><code>SELECT a.key, b.key FROM a LEFT OUTER JOIN b ON (a.key=b.key) WHERE a.ds=&#39;2009-07-07&#39; AND b.ds=&#39;2009-07-07&#39;;\nk1    k1\nk2    k2\n</code></pre>\n<p>第二种方法：</p>\n<pre><code>SELECT a.key, b.key FROM a LEFT OUTER JOIN b ON (a.key=b.key AND b.ds=&#39;2009-07-07&#39; AND a.ds=&#39;2009-07-07&#39;);\nk1    k1\nk2    k2\nk3    NULL\nk4    NULL\nk6    NULL\n</code></pre>\n<ul>\n<li><p>关联是不能交换的！关联是左结合的，不管是 LEFT 还是 RIGHT 关联。</p>\n<p>  SELECT a.val1, a.val2, b.val, c.val<br>  FROM a<br>  JOIN b ON (a.key = b.key)<br>  LEFT OUTER JOIN c ON (a.key = c.key);</p>\n</li>\n</ul>\n<p>首先关联 a 和 b，丢弃掉 a 和 b 所有不能关联的行。之后的表与 c 关联。这为如果在 a 和 c 中都存在的 key 但是在 b 中不存在的情况提供了直观的结果：整行（包括 a.val1、a.val2 和 a.key）都会在“a JOIN b”环节丢弃掉，因为它不在 b 中。结果中没有 a.key，因此当与 c 进行 LEFT OUTER JOIN 时，c.val 也不在，因为没有 c.key 匹配 a.key（因此 a 中的行被移除）。同样的，如果是 RIGHT OUTER JOIN（而不是 LEFT），我们最终会得到意向不到的效果：NULL, NULL, NULL, c.val，因为即使我们指定了 a.key=c.key 作为关联键，也会在第一个关联中丢弃不匹配的 a 中的行。</p>\n<p>为了得到更直观的效果，应该替换为 FROM c LEFT OUTER JOIN a ON (c.key = a.key) LEFT OUTER JOIN b ON (c.key = b.key)。</p>\n<ul>\n<li><p>LEFT SEMI JOIN 以一种有效的方式实现了无关联的 IN/EXISTS 子查询表达式。从 Hive 0.13 起 IN/NOT IN/EXISTS/NOT EXISTS 操作符通过使用子查询被支持，所以大多数这些关联不需要再手动执行。使用 LEFT SEMI JOIN 的限制是，右手边的表只能在关联条件（ON 字句）中被引用，不能在 WHERE 或者 SELECT 中使用。</p>\n<p>  SELECT a.key, a.value<br>  FROM a<br>  WHERE a.key in<br>   (SELECT b.key</p>\n<pre><code>FROM B);\n</code></pre>\n</li>\n</ul>\n<p>可以被写作：</p>\n<pre><code>SELECT a.key, a.val\nFROM a LEFT SEMI JOIN b ON (a.key = b.key);\n</code></pre>\n<ul>\n<li><p>如果除了一张表外其他表都很小，关联可以被优化为一个只有 map 的任务。</p>\n<p>  SELECT /*+ MAPJOIN(b) */ a.key, a.value<br>  FROM a JOIN b ON a.key = b.key;</p>\n</li>\n</ul>\n<p>这个查询不需要 reducer。每一个 A 的 mapper，B 都被完全读入。限制是 a FULL/RIGHT OUTER JOIN b 不能被优化。</p>\n<ul>\n<li><p>如果关联的表以关联的列被分桶化，并且一张表的桶数量是另外一张表桶数量的很多倍，那么每个桶可以互相关联。如果表 A 有 4 个桶，表 B 也有 4 个桶，下面的关联可以只在 mapper 中完成。</p>\n<p>  SELECT /*+ MAPJOIN(b) */ a.key, a.value<br>  FROM a JOIN b ON a.key = b.key;</p>\n</li>\n</ul>\n<p>只获取需要的桶，而不是针对 A 的每个 mapper 都获取 B 的全部。对于上面的查询，处理 A 的桶 1 的 mapper 只获取 B 的桶 1。这不是默认行为，由以下参数控制：</p>\n<pre><code>set hive.optimize.bucketmapjoin = true;\n</code></pre>\n<ul>\n<li><p>如果关联的表以关联的列排序且分桶化，并且它们有相同的桶数量，可以进行合并排序关联。在 mapper 中相关的桶彼此关联。如果 A 和 B 都有 4 个桶，下面的关联可以只在 mapper 中完成。</p>\n<p>  SELECT /*+ MAPJOIN(b) */ a.key, a.value<br>  FROM A a JOIN B b ON a.key = b.key;</p>\n</li>\n</ul>\n<p>A 的桶的 mapper 将通过相关的 B 的桶处理。这不是默认行为，需要设置下面的参数：</p>\n<pre><code>set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;\nset hive.optimize.bucketmapjoin = true;\nset hive.optimize.bucketmapjoin.sortedmerge = true;\n</code></pre>\n","site":{"data":{}},"excerpt":"<p>在写 Hive join 查询时需要关注以下要点：</p>\n<ul>\n<li><p>允许复杂关联表达式：</p>\n<p>  SELECT a.* FROM a JOIN b ON (a.id = b.id);<br>  SELECT a.* FROM a JOIN b ON (a.id = b.id AND a.department = b.department);<br>  SELECT a.* FROM a LEFT OUTER JOIN b ON (a.id &lt;&gt; b.id);</p>\n</li>\n</ul>\n<p>以上都是有效的关联。</p>","more":"<ul>\n<li><p>在一个查询中可以关联 2 张以上的表：</p>\n<p>  SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2);</p>\n</li>\n</ul>\n<p>是有效的关联。</p>\n<ul>\n<li><p>如果在关联从句中所有表都使用同一列进行关联，Hive 会转化多表关联为一个 map / reduce 任务：</p>\n<p>  SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1);</p>\n</li>\n</ul>\n<p>上面的语句会被转化为一个 map / reduce 任务，因为只有 b 的 key1 列被用作关联。换句话说，</p>\n<pre><code>SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2);\n</code></pre>\n<p>上面的语句会被转化为两个 map / reduce 任务，因为第一个关联条件使用 b 中的 key1，b 中的 key2 用在第二个关联条件中。第一个 map / reduce 任务关联 a 和 b，然后结果在第二个 map / reduce 任务中与 c 关联。</p>\n<ul>\n<li><p>在关联的每个 map / reduce 阶段，序列中最后的表通过 reducer 被分流，其他表被缓存在 reducer 中。因此，通过组织表，是最大的表出现在序列的最后，缓存关联键特定值的行在 reducer 中，有助于降低 reducer 内存的需求。例如，在下面的语句中：</p>\n<p>  SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1);</p>\n</li>\n</ul>\n<p>三张表在一个 map / reduce 任务中关联，表 a 和 表 b 的关联键的特定值被缓存在 reducer 的内存中。然后对从 c 获取的每一行用缓存的每一行来计算关联。类似的：</p>\n<pre><code>SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2);\n</code></pre>\n<p>计算这个关联需要调起两个 map / reduce 任务。第一个任务关联 a 和 b，缓存 a 的值并在 reducer 中分流 b 的值。第二个任务缓存第一个任务的结果，且通过 reducer 分流 c 的值。</p>\n<ul>\n<li><p>在关联的每个 map / reduce 阶段，可以通过提示指定被分流的表。例如，在下面的语句中：</p>\n<p>  SELECT /*+ STREAMTABLE(a) */ a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1);</p>\n</li>\n</ul>\n<p>三张表在一个 map / reduce 任务中关联，表 b 和 表 c 的关联键的特定值被缓存在 reducer 的内存中。然后，对从 a 获取的每一行用缓存的每一行来计算关联。如果 STREAMTABLE 提示被省略，Hive 分流关联中最右面的a表。</p>\n<ul>\n<li><p>LEFT、RIGHT 和 FULL OUTER 关联的存在是为了对 ON 字句不能匹配的记录提供更多的控制。例如：</p>\n<p>  SELECT a.val, b.val FROM a LEFT OUTER JOIN b ON (a.key=b.key);</p>\n</li>\n</ul>\n<p>这个查询将返回 a 中所有的行。当 b.key 等于 a.key 时输出为“a.val,b.val”；当没有相应的 b.key 时输出为“a.val,NULL”。b 中没有对应 a.key 的行被丢弃。为了理解“FROM a LEFT OUTER JOIN b”语法是如何工作的必须写在一行——在这个查询中 a 是 b 的 LEFT，因此所有 a 中的行被保留；a RIGHT OUTER JOIN 会保留 b 中所有的行；a FULL OUTER JOIN 将会保留 a 中的所有行及 b 中所有的行。OUTER JOIN 应该符合标准的 SQL 规范。</p>\n<ul>\n<li><p>关联发生在 WHERE 从句之前。因此，如果要限制关联的输出，条件应该在 WHERE 从句中，否则它应该在 JOIN 从句中。这个问题的一大困惑是分区表：</p>\n<p>  SELECT a.val, b.val FROM a LEFT OUTER JOIN b ON (a.key=b.key)<br>  WHERE a.ds=’2009-07-07’ AND b.ds=’2009-07-07’;</p>\n</li>\n</ul>\n<p>这个查询会关联 a 和 b，生成一个 a.val 和  b.val 的列表。然而，WHERE 从句可以引用关联输出中 a 和 b 的其他列，然后过滤它们之后输出。然而无论何时，JOIN 中的一行发现 a 的一个 key 且没有 b 的 key，所有 b 的列将会是 NULL，包括 ds 列。这就是说，你将过滤出没有有效 b.key 的关联的所有行，因此你对 LEFT OUTER 有更明智的要求。换种说法，关联的 LEFT OUTER 部分跟 WHERE 从句中引用 b 中的任意列是不相关的。反而，当外关联时，使用这个语法：</p>\n<pre><code>SELECT a.val, b.val FROM a LEFT OUTER JOIN b\nON (a.key=b.key AND b.ds=&#39;2009-07-07&#39; AND a.ds=&#39;2009-07-07&#39;);\n</code></pre>\n<p>结果是关联的输出被提前过滤，对于有效 a.key 但没有匹配的 b.key 的行避免了后过滤。同样的逻辑也适用于 RIGHT 和 FULL 关联。</p>\n<p><strong>测试两种写法的输出是否有区别：</strong></p>\n<p>数据准备：</p>\n<pre><code>create table a(key string,ds string);\ncreate table b(key string,ds string);\ninsert into a values(&#39;k1&#39;,&#39;2009-07-07&#39;),(&#39;k2&#39;,&#39;2009-07-07&#39;),(&#39;k3&#39;,&#39;2009-07-08&#39;),(&#39;k4&#39;,&#39;2009-07-07&#39;),(&#39;k6&#39;,&#39;2009-07-07&#39;);\ninsert into b values(&#39;k1&#39;,&#39;2009-07-07&#39;),(&#39;k2&#39;,&#39;2009-07-07&#39;),(&#39;k3&#39;,&#39;2009-07-07&#39;),(&#39;k4&#39;,&#39;2009-07-08&#39;),(&#39;k5&#39;,&#39;2009-07-07&#39;);\n</code></pre>\n<p>第一种方法：</p>\n<pre><code>SELECT a.key, b.key FROM a LEFT OUTER JOIN b ON (a.key=b.key) WHERE a.ds=&#39;2009-07-07&#39; AND b.ds=&#39;2009-07-07&#39;;\nk1    k1\nk2    k2\n</code></pre>\n<p>第二种方法：</p>\n<pre><code>SELECT a.key, b.key FROM a LEFT OUTER JOIN b ON (a.key=b.key AND b.ds=&#39;2009-07-07&#39; AND a.ds=&#39;2009-07-07&#39;);\nk1    k1\nk2    k2\nk3    NULL\nk4    NULL\nk6    NULL\n</code></pre>\n<ul>\n<li><p>关联是不能交换的！关联是左结合的，不管是 LEFT 还是 RIGHT 关联。</p>\n<p>  SELECT a.val1, a.val2, b.val, c.val<br>  FROM a<br>  JOIN b ON (a.key = b.key)<br>  LEFT OUTER JOIN c ON (a.key = c.key);</p>\n</li>\n</ul>\n<p>首先关联 a 和 b，丢弃掉 a 和 b 所有不能关联的行。之后的表与 c 关联。这为如果在 a 和 c 中都存在的 key 但是在 b 中不存在的情况提供了直观的结果：整行（包括 a.val1、a.val2 和 a.key）都会在“a JOIN b”环节丢弃掉，因为它不在 b 中。结果中没有 a.key，因此当与 c 进行 LEFT OUTER JOIN 时，c.val 也不在，因为没有 c.key 匹配 a.key（因此 a 中的行被移除）。同样的，如果是 RIGHT OUTER JOIN（而不是 LEFT），我们最终会得到意向不到的效果：NULL, NULL, NULL, c.val，因为即使我们指定了 a.key=c.key 作为关联键，也会在第一个关联中丢弃不匹配的 a 中的行。</p>\n<p>为了得到更直观的效果，应该替换为 FROM c LEFT OUTER JOIN a ON (c.key = a.key) LEFT OUTER JOIN b ON (c.key = b.key)。</p>\n<ul>\n<li><p>LEFT SEMI JOIN 以一种有效的方式实现了无关联的 IN/EXISTS 子查询表达式。从 Hive 0.13 起 IN/NOT IN/EXISTS/NOT EXISTS 操作符通过使用子查询被支持，所以大多数这些关联不需要再手动执行。使用 LEFT SEMI JOIN 的限制是，右手边的表只能在关联条件（ON 字句）中被引用，不能在 WHERE 或者 SELECT 中使用。</p>\n<p>  SELECT a.key, a.value<br>  FROM a<br>  WHERE a.key in<br>   (SELECT b.key</p>\n<pre><code>FROM B);\n</code></pre>\n</li>\n</ul>\n<p>可以被写作：</p>\n<pre><code>SELECT a.key, a.val\nFROM a LEFT SEMI JOIN b ON (a.key = b.key);\n</code></pre>\n<ul>\n<li><p>如果除了一张表外其他表都很小，关联可以被优化为一个只有 map 的任务。</p>\n<p>  SELECT /*+ MAPJOIN(b) */ a.key, a.value<br>  FROM a JOIN b ON a.key = b.key;</p>\n</li>\n</ul>\n<p>这个查询不需要 reducer。每一个 A 的 mapper，B 都被完全读入。限制是 a FULL/RIGHT OUTER JOIN b 不能被优化。</p>\n<ul>\n<li><p>如果关联的表以关联的列被分桶化，并且一张表的桶数量是另外一张表桶数量的很多倍，那么每个桶可以互相关联。如果表 A 有 4 个桶，表 B 也有 4 个桶，下面的关联可以只在 mapper 中完成。</p>\n<p>  SELECT /*+ MAPJOIN(b) */ a.key, a.value<br>  FROM a JOIN b ON a.key = b.key;</p>\n</li>\n</ul>\n<p>只获取需要的桶，而不是针对 A 的每个 mapper 都获取 B 的全部。对于上面的查询，处理 A 的桶 1 的 mapper 只获取 B 的桶 1。这不是默认行为，由以下参数控制：</p>\n<pre><code>set hive.optimize.bucketmapjoin = true;\n</code></pre>\n<ul>\n<li><p>如果关联的表以关联的列排序且分桶化，并且它们有相同的桶数量，可以进行合并排序关联。在 mapper 中相关的桶彼此关联。如果 A 和 B 都有 4 个桶，下面的关联可以只在 mapper 中完成。</p>\n<p>  SELECT /*+ MAPJOIN(b) */ a.key, a.value<br>  FROM A a JOIN B b ON a.key = b.key;</p>\n</li>\n</ul>\n<p>A 的桶的 mapper 将通过相关的 B 的桶处理。这不是默认行为，需要设置下面的参数：</p>\n<pre><code>set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;\nset hive.optimize.bucketmapjoin = true;\nset hive.optimize.bucketmapjoin.sortedmerge = true;\n</code></pre>"},{"title":"Hive limit 操作假死问题","date":"2017-09-29T07:04:22.000Z","_content":"\n\n昨天同事反映 hive 在做 limit 查询时很慢，HQL 如下：\n\n    select * from dwd.dwd_flow_box_playqos_day where curdate = '20170926' and final_play_time is null limit 10;\n    \n执行测试发现执行该 HQL 时，hive 客户端一直处于假死状态，没有任何信息输出，等待很久之后才有结果输出。\n\n<!-- more -->\n\n首先检查是否 MR 执行过程有问题。在 ResourceManager 上发现并没有对应的 MR Job 信息。难道这个 HQL 没有生成 MR？使用 explain 查看该 HQL 的执行计划，如下：\n\n    hive> explain select * from dwd.dwd_flow_box_playqos_day where curdate = '20170926' and final_play_time is null limit 10;\n    OK\n    STAGE DEPENDENCIES:\n      Stage-0 is a root stage\n    \n    STAGE PLANS:\n      Stage: Stage-0\n        Fetch Operator\n          limit: 10\n          Processor Tree:\n            TableScan\n              alias: dwd_flow_box_playqos_day\n              Statistics: Num rows: 296929317 Data size: 1022092601961 Basic stats: COMPLETE Column stats: NONE\n              Filter Operator\n                predicate: final_play_time is null (type: boolean)\n                Statistics: Num rows: 148464658 Data size: 511046299259 Basic stats: COMPLETE Column stats: NONE\n                Select Operator\n                  expressions: device_id (type: string), province_id (type: string), province_name (type: string), city_id (type: string), city_name (type: string), sp_type (type: string), terminal_id (type: string), state (type: string), activate_date (type: string), play_type (type: string), version_id (type: string), version_type (type: string), platform_id (type: string), mac (type: string), ip (type: string), req_time (type: string), cur_time (type: string), client_time (type: string), server_time (type: string), sort_id (type: string), url (type: string), opentimecost (type: string), buffcount (type: string), buffavertimecost (type: string), seekcount (type: string), seektimeavertimecost (type: string), playtotaltimecost (type: string), program_serise_id (type: string), program_series_name (type: string), program_series_type (type: string), uuid (type: string), playpoint (type: string), starttime (type: string), endtime (type: string), pausecount (type: string), pausetotaltime (type: string), null (type: string), is_short_video (type: string), '20170926' (type: string), terminal_type (type: string)\n                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17, _col18, _col19, _col20, _col21, _col22, _col23, _col24, _col25, _col26, _col27, _col28, _col29, _col30, _col31, _col32, _col33, _col34, _col35, _col36, _col37, _col38, _col39\n                  Statistics: Num rows: 148464658 Data size: 511046299259 Basic stats: COMPLETE Column stats: NONE\n                  Limit\n                    Number of rows: 10\n                    Statistics: Num rows: 10 Data size: 34420 Basic stats: COMPLETE Column stats: NONE\n                    ListSink\n    \n    Time taken: 2.003 seconds, Fetched: 23 row(s)\n    \n通过执行计划可以看出，该 HQL 确实没有 MR 过程，只有一个 Fetch Operator 过程。查看 Hive 文档发现，Hive 会将一些 HQL 转换单个 FETCH 任务，以降低延迟。是否允许转换通过 hive.fetch.task.conversion 进行配置：\n\n    **hive.fetch.task.conversion**\n\n    - Default Value: minimal in Hive 0.10.0 through 0.13.1, more in Hive 0.14.0 and later\n    - Added In: Hive 0.10.0 with HIVE-2925; default changed in Hive 0.14.0 with HIVE-7397\n\n    Some select queries can be converted to a single FETCH task, minimizing latency. Currently the query should be single sourced not having any subquery and should not have any aggregations or distincts (which incur RS – ReduceSinkOperator, requiring a MapReduce task), lateral views and joins.\n\n    Supported values are none, minimal and more.\n\n    - none:  Disable hive.fetch.task.conversion (value added in Hive 0.14.0 with HIVE-8389)\n    - minimal:  SELECT *, FILTER on partition columns (WHERE and HAVING clauses), LIMIT only\n    - more:  SELECT, FILTER, LIMIT only (including TABLESAMPLE, virtual columns)\n             \"more\" can take any kind of expressions in the SELECT clause, including UDFs.\n             (UDTFs and lateral views are not yet supported – see HIVE-5718.)\n\n关闭该功能确实可以解决上面 HQL 出现的问题。但是，其他可以快速返回的 limit 查询就会被转为 MR 执行，返回速度会变慢。\n\nHive 配置中有一个 hive.fetch.task.conversion.threshold，该参数的作用是根据查询输入数据量判断是否将查询转换为单个 FETCH 任务。该参数默认配置为 1G。<span style=\"color:red;\">但是，经过测试，发现在我的 Hive 2.1.1 上不起作用。这个问题需要继续定位。</span>\n\nhive.fetch.task.conversion.threshold 参数说明如下：\n\n    - Default Value: -1 in Hive 0.13.0 and 0.13.1, 1073741824 (1 GB) in Hive 0.14.0 and later \n    - Added In: Hive 0.13.0 with HIVE-3990; default changed in Hive 0.14.0 with HIVE-7397\n\n    Input threshold (in bytes) for applying hive.fetch.task.conversion. If target table is native, input length is calculated by summation of file lengths. If it's not native, the storage handler for the table can optionally implement the org.apache.hadoop.hive.ql.metadata.InputEstimator interface. A negative threshold means hive.fetch.task.conversion is applied without any input length threshold.\n","source":"_posts/Hive-limit-操作假死问题.md","raw":"title: Hive limit 操作假死问题\ntags:\n  - Hive\ncategories:\n  - 大数据\n  - Hive\ndate: 2017-09-29 15:04:22\n---\n\n\n昨天同事反映 hive 在做 limit 查询时很慢，HQL 如下：\n\n    select * from dwd.dwd_flow_box_playqos_day where curdate = '20170926' and final_play_time is null limit 10;\n    \n执行测试发现执行该 HQL 时，hive 客户端一直处于假死状态，没有任何信息输出，等待很久之后才有结果输出。\n\n<!-- more -->\n\n首先检查是否 MR 执行过程有问题。在 ResourceManager 上发现并没有对应的 MR Job 信息。难道这个 HQL 没有生成 MR？使用 explain 查看该 HQL 的执行计划，如下：\n\n    hive> explain select * from dwd.dwd_flow_box_playqos_day where curdate = '20170926' and final_play_time is null limit 10;\n    OK\n    STAGE DEPENDENCIES:\n      Stage-0 is a root stage\n    \n    STAGE PLANS:\n      Stage: Stage-0\n        Fetch Operator\n          limit: 10\n          Processor Tree:\n            TableScan\n              alias: dwd_flow_box_playqos_day\n              Statistics: Num rows: 296929317 Data size: 1022092601961 Basic stats: COMPLETE Column stats: NONE\n              Filter Operator\n                predicate: final_play_time is null (type: boolean)\n                Statistics: Num rows: 148464658 Data size: 511046299259 Basic stats: COMPLETE Column stats: NONE\n                Select Operator\n                  expressions: device_id (type: string), province_id (type: string), province_name (type: string), city_id (type: string), city_name (type: string), sp_type (type: string), terminal_id (type: string), state (type: string), activate_date (type: string), play_type (type: string), version_id (type: string), version_type (type: string), platform_id (type: string), mac (type: string), ip (type: string), req_time (type: string), cur_time (type: string), client_time (type: string), server_time (type: string), sort_id (type: string), url (type: string), opentimecost (type: string), buffcount (type: string), buffavertimecost (type: string), seekcount (type: string), seektimeavertimecost (type: string), playtotaltimecost (type: string), program_serise_id (type: string), program_series_name (type: string), program_series_type (type: string), uuid (type: string), playpoint (type: string), starttime (type: string), endtime (type: string), pausecount (type: string), pausetotaltime (type: string), null (type: string), is_short_video (type: string), '20170926' (type: string), terminal_type (type: string)\n                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17, _col18, _col19, _col20, _col21, _col22, _col23, _col24, _col25, _col26, _col27, _col28, _col29, _col30, _col31, _col32, _col33, _col34, _col35, _col36, _col37, _col38, _col39\n                  Statistics: Num rows: 148464658 Data size: 511046299259 Basic stats: COMPLETE Column stats: NONE\n                  Limit\n                    Number of rows: 10\n                    Statistics: Num rows: 10 Data size: 34420 Basic stats: COMPLETE Column stats: NONE\n                    ListSink\n    \n    Time taken: 2.003 seconds, Fetched: 23 row(s)\n    \n通过执行计划可以看出，该 HQL 确实没有 MR 过程，只有一个 Fetch Operator 过程。查看 Hive 文档发现，Hive 会将一些 HQL 转换单个 FETCH 任务，以降低延迟。是否允许转换通过 hive.fetch.task.conversion 进行配置：\n\n    **hive.fetch.task.conversion**\n\n    - Default Value: minimal in Hive 0.10.0 through 0.13.1, more in Hive 0.14.0 and later\n    - Added In: Hive 0.10.0 with HIVE-2925; default changed in Hive 0.14.0 with HIVE-7397\n\n    Some select queries can be converted to a single FETCH task, minimizing latency. Currently the query should be single sourced not having any subquery and should not have any aggregations or distincts (which incur RS – ReduceSinkOperator, requiring a MapReduce task), lateral views and joins.\n\n    Supported values are none, minimal and more.\n\n    - none:  Disable hive.fetch.task.conversion (value added in Hive 0.14.0 with HIVE-8389)\n    - minimal:  SELECT *, FILTER on partition columns (WHERE and HAVING clauses), LIMIT only\n    - more:  SELECT, FILTER, LIMIT only (including TABLESAMPLE, virtual columns)\n             \"more\" can take any kind of expressions in the SELECT clause, including UDFs.\n             (UDTFs and lateral views are not yet supported – see HIVE-5718.)\n\n关闭该功能确实可以解决上面 HQL 出现的问题。但是，其他可以快速返回的 limit 查询就会被转为 MR 执行，返回速度会变慢。\n\nHive 配置中有一个 hive.fetch.task.conversion.threshold，该参数的作用是根据查询输入数据量判断是否将查询转换为单个 FETCH 任务。该参数默认配置为 1G。<span style=\"color:red;\">但是，经过测试，发现在我的 Hive 2.1.1 上不起作用。这个问题需要继续定位。</span>\n\nhive.fetch.task.conversion.threshold 参数说明如下：\n\n    - Default Value: -1 in Hive 0.13.0 and 0.13.1, 1073741824 (1 GB) in Hive 0.14.0 and later \n    - Added In: Hive 0.13.0 with HIVE-3990; default changed in Hive 0.14.0 with HIVE-7397\n\n    Input threshold (in bytes) for applying hive.fetch.task.conversion. If target table is native, input length is calculated by summation of file lengths. If it's not native, the storage handler for the table can optionally implement the org.apache.hadoop.hive.ql.metadata.InputEstimator interface. A negative threshold means hive.fetch.task.conversion is applied without any input length threshold.\n","slug":"Hive-limit-操作假死问题","published":1,"updated":"2021-07-19T16:28:00.256Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphn1003ritd3bpc37519","content":"<p>昨天同事反映 hive 在做 limit 查询时很慢，HQL 如下：</p>\n<pre><code>select * from dwd.dwd_flow_box_playqos_day where curdate = &#39;20170926&#39; and final_play_time is null limit 10;\n</code></pre>\n<p>执行测试发现执行该 HQL 时，hive 客户端一直处于假死状态，没有任何信息输出，等待很久之后才有结果输出。</p>\n<span id=\"more\"></span>\n\n<p>首先检查是否 MR 执行过程有问题。在 ResourceManager 上发现并没有对应的 MR Job 信息。难道这个 HQL 没有生成 MR？使用 explain 查看该 HQL 的执行计划，如下：</p>\n<pre><code>hive&gt; explain select * from dwd.dwd_flow_box_playqos_day where curdate = &#39;20170926&#39; and final_play_time is null limit 10;\nOK\nSTAGE DEPENDENCIES:\n  Stage-0 is a root stage\n\nSTAGE PLANS:\n  Stage: Stage-0\n    Fetch Operator\n      limit: 10\n      Processor Tree:\n        TableScan\n          alias: dwd_flow_box_playqos_day\n          Statistics: Num rows: 296929317 Data size: 1022092601961 Basic stats: COMPLETE Column stats: NONE\n          Filter Operator\n            predicate: final_play_time is null (type: boolean)\n            Statistics: Num rows: 148464658 Data size: 511046299259 Basic stats: COMPLETE Column stats: NONE\n            Select Operator\n              expressions: device_id (type: string), province_id (type: string), province_name (type: string), city_id (type: string), city_name (type: string), sp_type (type: string), terminal_id (type: string), state (type: string), activate_date (type: string), play_type (type: string), version_id (type: string), version_type (type: string), platform_id (type: string), mac (type: string), ip (type: string), req_time (type: string), cur_time (type: string), client_time (type: string), server_time (type: string), sort_id (type: string), url (type: string), opentimecost (type: string), buffcount (type: string), buffavertimecost (type: string), seekcount (type: string), seektimeavertimecost (type: string), playtotaltimecost (type: string), program_serise_id (type: string), program_series_name (type: string), program_series_type (type: string), uuid (type: string), playpoint (type: string), starttime (type: string), endtime (type: string), pausecount (type: string), pausetotaltime (type: string), null (type: string), is_short_video (type: string), &#39;20170926&#39; (type: string), terminal_type (type: string)\n              outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17, _col18, _col19, _col20, _col21, _col22, _col23, _col24, _col25, _col26, _col27, _col28, _col29, _col30, _col31, _col32, _col33, _col34, _col35, _col36, _col37, _col38, _col39\n              Statistics: Num rows: 148464658 Data size: 511046299259 Basic stats: COMPLETE Column stats: NONE\n              Limit\n                Number of rows: 10\n                Statistics: Num rows: 10 Data size: 34420 Basic stats: COMPLETE Column stats: NONE\n                ListSink\n\nTime taken: 2.003 seconds, Fetched: 23 row(s)\n</code></pre>\n<p>通过执行计划可以看出，该 HQL 确实没有 MR 过程，只有一个 Fetch Operator 过程。查看 Hive 文档发现，Hive 会将一些 HQL 转换单个 FETCH 任务，以降低延迟。是否允许转换通过 hive.fetch.task.conversion 进行配置：</p>\n<pre><code>**hive.fetch.task.conversion**\n\n- Default Value: minimal in Hive 0.10.0 through 0.13.1, more in Hive 0.14.0 and later\n- Added In: Hive 0.10.0 with HIVE-2925; default changed in Hive 0.14.0 with HIVE-7397\n\nSome select queries can be converted to a single FETCH task, minimizing latency. Currently the query should be single sourced not having any subquery and should not have any aggregations or distincts (which incur RS – ReduceSinkOperator, requiring a MapReduce task), lateral views and joins.\n\nSupported values are none, minimal and more.\n\n- none:  Disable hive.fetch.task.conversion (value added in Hive 0.14.0 with HIVE-8389)\n- minimal:  SELECT *, FILTER on partition columns (WHERE and HAVING clauses), LIMIT only\n- more:  SELECT, FILTER, LIMIT only (including TABLESAMPLE, virtual columns)\n         &quot;more&quot; can take any kind of expressions in the SELECT clause, including UDFs.\n         (UDTFs and lateral views are not yet supported – see HIVE-5718.)\n</code></pre>\n<p>关闭该功能确实可以解决上面 HQL 出现的问题。但是，其他可以快速返回的 limit 查询就会被转为 MR 执行，返回速度会变慢。</p>\n<p>Hive 配置中有一个 hive.fetch.task.conversion.threshold，该参数的作用是根据查询输入数据量判断是否将查询转换为单个 FETCH 任务。该参数默认配置为 1G。<span style=\"color:red;\">但是，经过测试，发现在我的 Hive 2.1.1 上不起作用。这个问题需要继续定位。</span></p>\n<p>hive.fetch.task.conversion.threshold 参数说明如下：</p>\n<pre><code>- Default Value: -1 in Hive 0.13.0 and 0.13.1, 1073741824 (1 GB) in Hive 0.14.0 and later \n- Added In: Hive 0.13.0 with HIVE-3990; default changed in Hive 0.14.0 with HIVE-7397\n\nInput threshold (in bytes) for applying hive.fetch.task.conversion. If target table is native, input length is calculated by summation of file lengths. If it&#39;s not native, the storage handler for the table can optionally implement the org.apache.hadoop.hive.ql.metadata.InputEstimator interface. A negative threshold means hive.fetch.task.conversion is applied without any input length threshold.\n</code></pre>\n","site":{"data":{}},"excerpt":"<p>昨天同事反映 hive 在做 limit 查询时很慢，HQL 如下：</p>\n<pre><code>select * from dwd.dwd_flow_box_playqos_day where curdate = &#39;20170926&#39; and final_play_time is null limit 10;\n</code></pre>\n<p>执行测试发现执行该 HQL 时，hive 客户端一直处于假死状态，没有任何信息输出，等待很久之后才有结果输出。</p>","more":"<p>首先检查是否 MR 执行过程有问题。在 ResourceManager 上发现并没有对应的 MR Job 信息。难道这个 HQL 没有生成 MR？使用 explain 查看该 HQL 的执行计划，如下：</p>\n<pre><code>hive&gt; explain select * from dwd.dwd_flow_box_playqos_day where curdate = &#39;20170926&#39; and final_play_time is null limit 10;\nOK\nSTAGE DEPENDENCIES:\n  Stage-0 is a root stage\n\nSTAGE PLANS:\n  Stage: Stage-0\n    Fetch Operator\n      limit: 10\n      Processor Tree:\n        TableScan\n          alias: dwd_flow_box_playqos_day\n          Statistics: Num rows: 296929317 Data size: 1022092601961 Basic stats: COMPLETE Column stats: NONE\n          Filter Operator\n            predicate: final_play_time is null (type: boolean)\n            Statistics: Num rows: 148464658 Data size: 511046299259 Basic stats: COMPLETE Column stats: NONE\n            Select Operator\n              expressions: device_id (type: string), province_id (type: string), province_name (type: string), city_id (type: string), city_name (type: string), sp_type (type: string), terminal_id (type: string), state (type: string), activate_date (type: string), play_type (type: string), version_id (type: string), version_type (type: string), platform_id (type: string), mac (type: string), ip (type: string), req_time (type: string), cur_time (type: string), client_time (type: string), server_time (type: string), sort_id (type: string), url (type: string), opentimecost (type: string), buffcount (type: string), buffavertimecost (type: string), seekcount (type: string), seektimeavertimecost (type: string), playtotaltimecost (type: string), program_serise_id (type: string), program_series_name (type: string), program_series_type (type: string), uuid (type: string), playpoint (type: string), starttime (type: string), endtime (type: string), pausecount (type: string), pausetotaltime (type: string), null (type: string), is_short_video (type: string), &#39;20170926&#39; (type: string), terminal_type (type: string)\n              outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9, _col10, _col11, _col12, _col13, _col14, _col15, _col16, _col17, _col18, _col19, _col20, _col21, _col22, _col23, _col24, _col25, _col26, _col27, _col28, _col29, _col30, _col31, _col32, _col33, _col34, _col35, _col36, _col37, _col38, _col39\n              Statistics: Num rows: 148464658 Data size: 511046299259 Basic stats: COMPLETE Column stats: NONE\n              Limit\n                Number of rows: 10\n                Statistics: Num rows: 10 Data size: 34420 Basic stats: COMPLETE Column stats: NONE\n                ListSink\n\nTime taken: 2.003 seconds, Fetched: 23 row(s)\n</code></pre>\n<p>通过执行计划可以看出，该 HQL 确实没有 MR 过程，只有一个 Fetch Operator 过程。查看 Hive 文档发现，Hive 会将一些 HQL 转换单个 FETCH 任务，以降低延迟。是否允许转换通过 hive.fetch.task.conversion 进行配置：</p>\n<pre><code>**hive.fetch.task.conversion**\n\n- Default Value: minimal in Hive 0.10.0 through 0.13.1, more in Hive 0.14.0 and later\n- Added In: Hive 0.10.0 with HIVE-2925; default changed in Hive 0.14.0 with HIVE-7397\n\nSome select queries can be converted to a single FETCH task, minimizing latency. Currently the query should be single sourced not having any subquery and should not have any aggregations or distincts (which incur RS – ReduceSinkOperator, requiring a MapReduce task), lateral views and joins.\n\nSupported values are none, minimal and more.\n\n- none:  Disable hive.fetch.task.conversion (value added in Hive 0.14.0 with HIVE-8389)\n- minimal:  SELECT *, FILTER on partition columns (WHERE and HAVING clauses), LIMIT only\n- more:  SELECT, FILTER, LIMIT only (including TABLESAMPLE, virtual columns)\n         &quot;more&quot; can take any kind of expressions in the SELECT clause, including UDFs.\n         (UDTFs and lateral views are not yet supported – see HIVE-5718.)\n</code></pre>\n<p>关闭该功能确实可以解决上面 HQL 出现的问题。但是，其他可以快速返回的 limit 查询就会被转为 MR 执行，返回速度会变慢。</p>\n<p>Hive 配置中有一个 hive.fetch.task.conversion.threshold，该参数的作用是根据查询输入数据量判断是否将查询转换为单个 FETCH 任务。该参数默认配置为 1G。<span style=\"color:red;\">但是，经过测试，发现在我的 Hive 2.1.1 上不起作用。这个问题需要继续定位。</span></p>\n<p>hive.fetch.task.conversion.threshold 参数说明如下：</p>\n<pre><code>- Default Value: -1 in Hive 0.13.0 and 0.13.1, 1073741824 (1 GB) in Hive 0.14.0 and later \n- Added In: Hive 0.13.0 with HIVE-3990; default changed in Hive 0.14.0 with HIVE-7397\n\nInput threshold (in bytes) for applying hive.fetch.task.conversion. If target table is native, input length is calculated by summation of file lengths. If it&#39;s not native, the storage handler for the table can optionally implement the org.apache.hadoop.hive.ql.metadata.InputEstimator interface. A negative threshold means hive.fetch.task.conversion is applied without any input length threshold.\n</code></pre>"},{"title":"Hive 加载文件数据到表","date":"2016-05-15T05:19:21.000Z","_content":"\n#### Loading files into tables\n\nHive 在加载数据进表的时候不会做任何转换。Load 操作只是纯粹将数据文件复制/移动到 Hive 表关联的位置。\n\n> 注意：Hive 表字段分隔符必须与文件中数据字段分隔符一致。\n\n<!-- more -->\n\n##### 语法\n\n    LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]\n\n##### 简要说明\n\n- *filepath* 可以是：\n  - 一个相对路径，如：project/data1\n  - 一个绝对路径，如：/user/hive/project/data1\n  - 带有方案和授权（可选）的完整 URI，如：hdfs://namenode:9000/user/hive/project/data1\n- 加载的目标可以是表或者分区。如果表是分区的，则必须指定左右分区列的值来指定这个表特定的分区。\n- *filepath* 可以指向一个文件（在这种场景下 Hive 移动这个文件到表中）；或者它是一个目录（在这种场景下 Hive 移动目录下所有的文件到表中）。在两种场景下，*filepath* 指向一个文件集合。\n- 如果指定了 LOCAL 关键字：\n  - load 命令将在本地文件系统查找 *filepath*。如果指定了一个相对地址，将解释为用户当前工作目录的相对路径。用户也可以为本地文件制定全 URI－如：file:///user/hive/project/data1\n  - load 命令会尝试拷贝指定到 *filepath* 下的所有文件到目标文件系统。通过查看表的位置属性来推断目标文件系统。复制的数据文件将移动到这个表中。\n- 如果不指定 LOCAL 关键字，Hive 将使用 *filepath* 的全 URI（如果指定了一个），或者应用以下规则：\n  - 如果方案和授权没有指定，Hive 将使用指定 Namenode URI 的 hadoop 配置参数 fs.default.name 中的方案和授权。\n  - 如果不是绝对路径，Hive 将相对于 /user/<username&gt; 推断路径。\n  - Hive 将移动 *filepath* 指向的文件到表（或分区）中。\n- 如果使用了 OVERWRITE 关键字，则目标表（或分区）中的内容将被 *filepath* 指向的文件删除和替换；否则 *filepath* 指向的文件将追加到表中。\n  - 注意，如果目标表（分区）已经有一个与 *filepath* 下包含的名字冲突的文件，那么已经存在的文件将被新文件替换。\n\n##### 注意\n\n- *filepath* 不能包含子目录。\n- 如果不提供 LOCAL 关键字，*filepath* 必须跟指向表（或分区）位置相同的文件系统。\n- Hive 只做简单的检查来确保加载的文件跟表匹配。一般检查如果表是以 sequencefile 格式存储的，加载的文件也要是 sequencefile 文件，反过来亦然。\n","source":"_posts/Hive-加载文件数据到表中.md","raw":"title: Hive 加载文件数据到表\ntags:\n  - Hive\ncategories:\n  - 大数据\n  - Hive\ndate: 2016-05-15 13:19:21\n---\n\n#### Loading files into tables\n\nHive 在加载数据进表的时候不会做任何转换。Load 操作只是纯粹将数据文件复制/移动到 Hive 表关联的位置。\n\n> 注意：Hive 表字段分隔符必须与文件中数据字段分隔符一致。\n\n<!-- more -->\n\n##### 语法\n\n    LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]\n\n##### 简要说明\n\n- *filepath* 可以是：\n  - 一个相对路径，如：project/data1\n  - 一个绝对路径，如：/user/hive/project/data1\n  - 带有方案和授权（可选）的完整 URI，如：hdfs://namenode:9000/user/hive/project/data1\n- 加载的目标可以是表或者分区。如果表是分区的，则必须指定左右分区列的值来指定这个表特定的分区。\n- *filepath* 可以指向一个文件（在这种场景下 Hive 移动这个文件到表中）；或者它是一个目录（在这种场景下 Hive 移动目录下所有的文件到表中）。在两种场景下，*filepath* 指向一个文件集合。\n- 如果指定了 LOCAL 关键字：\n  - load 命令将在本地文件系统查找 *filepath*。如果指定了一个相对地址，将解释为用户当前工作目录的相对路径。用户也可以为本地文件制定全 URI－如：file:///user/hive/project/data1\n  - load 命令会尝试拷贝指定到 *filepath* 下的所有文件到目标文件系统。通过查看表的位置属性来推断目标文件系统。复制的数据文件将移动到这个表中。\n- 如果不指定 LOCAL 关键字，Hive 将使用 *filepath* 的全 URI（如果指定了一个），或者应用以下规则：\n  - 如果方案和授权没有指定，Hive 将使用指定 Namenode URI 的 hadoop 配置参数 fs.default.name 中的方案和授权。\n  - 如果不是绝对路径，Hive 将相对于 /user/<username&gt; 推断路径。\n  - Hive 将移动 *filepath* 指向的文件到表（或分区）中。\n- 如果使用了 OVERWRITE 关键字，则目标表（或分区）中的内容将被 *filepath* 指向的文件删除和替换；否则 *filepath* 指向的文件将追加到表中。\n  - 注意，如果目标表（分区）已经有一个与 *filepath* 下包含的名字冲突的文件，那么已经存在的文件将被新文件替换。\n\n##### 注意\n\n- *filepath* 不能包含子目录。\n- 如果不提供 LOCAL 关键字，*filepath* 必须跟指向表（或分区）位置相同的文件系统。\n- Hive 只做简单的检查来确保加载的文件跟表匹配。一般检查如果表是以 sequencefile 格式存储的，加载的文件也要是 sequencefile 文件，反过来亦然。\n","slug":"Hive-加载文件数据到表中","published":1,"updated":"2021-07-19T16:28:00.256Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphn2003vitd39gtffy6a","content":"<h4 id=\"Loading-files-into-tables\"><a href=\"#Loading-files-into-tables\" class=\"headerlink\" title=\"Loading files into tables\"></a>Loading files into tables</h4><p>Hive 在加载数据进表的时候不会做任何转换。Load 操作只是纯粹将数据文件复制/移动到 Hive 表关联的位置。</p>\n<blockquote>\n<p>注意：Hive 表字段分隔符必须与文件中数据字段分隔符一致。</p>\n</blockquote>\n<span id=\"more\"></span>\n\n<h5 id=\"语法\"><a href=\"#语法\" class=\"headerlink\" title=\"语法\"></a>语法</h5><pre><code>LOAD DATA [LOCAL] INPATH &#39;filepath&#39; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]\n</code></pre>\n<h5 id=\"简要说明\"><a href=\"#简要说明\" class=\"headerlink\" title=\"简要说明\"></a>简要说明</h5><ul>\n<li><em>filepath</em> 可以是：<ul>\n<li>一个相对路径，如：project/data1</li>\n<li>一个绝对路径，如：/user/hive/project/data1</li>\n<li>带有方案和授权（可选）的完整 URI，如：hdfs://namenode:9000/user/hive/project/data1</li>\n</ul>\n</li>\n<li>加载的目标可以是表或者分区。如果表是分区的，则必须指定左右分区列的值来指定这个表特定的分区。</li>\n<li><em>filepath</em> 可以指向一个文件（在这种场景下 Hive 移动这个文件到表中）；或者它是一个目录（在这种场景下 Hive 移动目录下所有的文件到表中）。在两种场景下，<em>filepath</em> 指向一个文件集合。</li>\n<li>如果指定了 LOCAL 关键字：<ul>\n<li>load 命令将在本地文件系统查找 <em>filepath</em>。如果指定了一个相对地址，将解释为用户当前工作目录的相对路径。用户也可以为本地文件制定全 URI－如：file:///user/hive/project/data1</li>\n<li>load 命令会尝试拷贝指定到 <em>filepath</em> 下的所有文件到目标文件系统。通过查看表的位置属性来推断目标文件系统。复制的数据文件将移动到这个表中。</li>\n</ul>\n</li>\n<li>如果不指定 LOCAL 关键字，Hive 将使用 <em>filepath</em> 的全 URI（如果指定了一个），或者应用以下规则：<ul>\n<li>如果方案和授权没有指定，Hive 将使用指定 Namenode URI 的 hadoop 配置参数 fs.default.name 中的方案和授权。</li>\n<li>如果不是绝对路径，Hive 将相对于 /user/&lt;username&gt; 推断路径。</li>\n<li>Hive 将移动 <em>filepath</em> 指向的文件到表（或分区）中。</li>\n</ul>\n</li>\n<li>如果使用了 OVERWRITE 关键字，则目标表（或分区）中的内容将被 <em>filepath</em> 指向的文件删除和替换；否则 <em>filepath</em> 指向的文件将追加到表中。<ul>\n<li>注意，如果目标表（分区）已经有一个与 <em>filepath</em> 下包含的名字冲突的文件，那么已经存在的文件将被新文件替换。</li>\n</ul>\n</li>\n</ul>\n<h5 id=\"注意\"><a href=\"#注意\" class=\"headerlink\" title=\"注意\"></a>注意</h5><ul>\n<li><em>filepath</em> 不能包含子目录。</li>\n<li>如果不提供 LOCAL 关键字，<em>filepath</em> 必须跟指向表（或分区）位置相同的文件系统。</li>\n<li>Hive 只做简单的检查来确保加载的文件跟表匹配。一般检查如果表是以 sequencefile 格式存储的，加载的文件也要是 sequencefile 文件，反过来亦然。</li>\n</ul>\n","site":{"data":{}},"excerpt":"<h4 id=\"Loading-files-into-tables\"><a href=\"#Loading-files-into-tables\" class=\"headerlink\" title=\"Loading files into tables\"></a>Loading files into tables</h4><p>Hive 在加载数据进表的时候不会做任何转换。Load 操作只是纯粹将数据文件复制/移动到 Hive 表关联的位置。</p>\n<blockquote>\n<p>注意：Hive 表字段分隔符必须与文件中数据字段分隔符一致。</p>\n</blockquote>","more":"<h5 id=\"语法\"><a href=\"#语法\" class=\"headerlink\" title=\"语法\"></a>语法</h5><pre><code>LOAD DATA [LOCAL] INPATH &#39;filepath&#39; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]\n</code></pre>\n<h5 id=\"简要说明\"><a href=\"#简要说明\" class=\"headerlink\" title=\"简要说明\"></a>简要说明</h5><ul>\n<li><em>filepath</em> 可以是：<ul>\n<li>一个相对路径，如：project/data1</li>\n<li>一个绝对路径，如：/user/hive/project/data1</li>\n<li>带有方案和授权（可选）的完整 URI，如：hdfs://namenode:9000/user/hive/project/data1</li>\n</ul>\n</li>\n<li>加载的目标可以是表或者分区。如果表是分区的，则必须指定左右分区列的值来指定这个表特定的分区。</li>\n<li><em>filepath</em> 可以指向一个文件（在这种场景下 Hive 移动这个文件到表中）；或者它是一个目录（在这种场景下 Hive 移动目录下所有的文件到表中）。在两种场景下，<em>filepath</em> 指向一个文件集合。</li>\n<li>如果指定了 LOCAL 关键字：<ul>\n<li>load 命令将在本地文件系统查找 <em>filepath</em>。如果指定了一个相对地址，将解释为用户当前工作目录的相对路径。用户也可以为本地文件制定全 URI－如：file:///user/hive/project/data1</li>\n<li>load 命令会尝试拷贝指定到 <em>filepath</em> 下的所有文件到目标文件系统。通过查看表的位置属性来推断目标文件系统。复制的数据文件将移动到这个表中。</li>\n</ul>\n</li>\n<li>如果不指定 LOCAL 关键字，Hive 将使用 <em>filepath</em> 的全 URI（如果指定了一个），或者应用以下规则：<ul>\n<li>如果方案和授权没有指定，Hive 将使用指定 Namenode URI 的 hadoop 配置参数 fs.default.name 中的方案和授权。</li>\n<li>如果不是绝对路径，Hive 将相对于 /user/&lt;username&gt; 推断路径。</li>\n<li>Hive 将移动 <em>filepath</em> 指向的文件到表（或分区）中。</li>\n</ul>\n</li>\n<li>如果使用了 OVERWRITE 关键字，则目标表（或分区）中的内容将被 <em>filepath</em> 指向的文件删除和替换；否则 <em>filepath</em> 指向的文件将追加到表中。<ul>\n<li>注意，如果目标表（分区）已经有一个与 <em>filepath</em> 下包含的名字冲突的文件，那么已经存在的文件将被新文件替换。</li>\n</ul>\n</li>\n</ul>\n<h5 id=\"注意\"><a href=\"#注意\" class=\"headerlink\" title=\"注意\"></a>注意</h5><ul>\n<li><em>filepath</em> 不能包含子目录。</li>\n<li>如果不提供 LOCAL 关键字，<em>filepath</em> 必须跟指向表（或分区）位置相同的文件系统。</li>\n<li>Hive 只做简单的检查来确保加载的文件跟表匹配。一般检查如果表是以 sequencefile 格式存储的，加载的文件也要是 sequencefile 文件，反过来亦然。</li>\n</ul>"},{"title":"HiveServer2 Load Data To Table AccessControlException","date":"2021-03-05T07:54:29.000Z","_content":"\n在用HiveServer2执行语句”load data inpath '/user/test/test.csv' into table dev.load_test_tbl;“时报错信息如下：\n\n\tFailed with exception org.apache.hadoop.security.AccessControlException: User does not belong to data-dev\n\t\tat org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.setOwner(FSDirAttrOp.java:86)\n\t\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setOwner(FSNamesystem.java:1676)\n\t\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setOwner(NameNodeRpcServer.java:703)\n\t\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.setOwner(ClientNamenodeProtocolServerSideTranslatorPB.java:464)\n\t\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\t\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n\t\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\n\t\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\n\t\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)\n\t\tat java.security.AccessController.doPrivileged(Native Method)\n\t\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\t\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)\n\t\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2045)\n\t\n\tFAILED: Operation cancelled\n\t\n原因是Hive默认在Load数据时会修改HDFS上目的地下的数据文件与目的地目录的属主及组等权限信息一致，而提交任务的用户信息与目标目录的信息不一致，所以抛出如上异常。\n\n解决方法：在hiveserver2-site.xml中增加以下配置：\n\n    <property>\n        <name>hive.warehouse.subdir.inherit.perms</name>\n        <value>false</value>\n    </property>\n","source":"_posts/HiveServer2-Load-Data-To-Table-AccessControlException.md","raw":"title: HiveServer2 Load Data To Table AccessControlException\ndate: 2021-03-05 15:54:29\ntags:\n- Hive\n- Hadoop\n- 大数据\ncategories:\n- 大数据\n- Hive\n---\n\n在用HiveServer2执行语句”load data inpath '/user/test/test.csv' into table dev.load_test_tbl;“时报错信息如下：\n\n\tFailed with exception org.apache.hadoop.security.AccessControlException: User does not belong to data-dev\n\t\tat org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.setOwner(FSDirAttrOp.java:86)\n\t\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setOwner(FSNamesystem.java:1676)\n\t\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setOwner(NameNodeRpcServer.java:703)\n\t\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.setOwner(ClientNamenodeProtocolServerSideTranslatorPB.java:464)\n\t\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\t\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n\t\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\n\t\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\n\t\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)\n\t\tat java.security.AccessController.doPrivileged(Native Method)\n\t\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\t\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)\n\t\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2045)\n\t\n\tFAILED: Operation cancelled\n\t\n原因是Hive默认在Load数据时会修改HDFS上目的地下的数据文件与目的地目录的属主及组等权限信息一致，而提交任务的用户信息与目标目录的信息不一致，所以抛出如上异常。\n\n解决方法：在hiveserver2-site.xml中增加以下配置：\n\n    <property>\n        <name>hive.warehouse.subdir.inherit.perms</name>\n        <value>false</value>\n    </property>\n","slug":"HiveServer2-Load-Data-To-Table-AccessControlException","published":1,"updated":"2021-07-19T16:27:59.916Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphn4003yitd3fttq7xig","content":"<p>在用HiveServer2执行语句”load data inpath ‘/user/test/test.csv’ into table dev.load_test_tbl;“时报错信息如下：</p>\n<pre><code>Failed with exception org.apache.hadoop.security.AccessControlException: User does not belong to data-dev\n    at org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.setOwner(FSDirAttrOp.java:86)\n    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setOwner(FSNamesystem.java:1676)\n    at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setOwner(NameNodeRpcServer.java:703)\n    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.setOwner(ClientNamenodeProtocolServerSideTranslatorPB.java:464)\n    at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\n    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\n    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:422)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)\n    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2045)\n\nFAILED: Operation cancelled\n</code></pre>\n<p>原因是Hive默认在Load数据时会修改HDFS上目的地下的数据文件与目的地目录的属主及组等权限信息一致，而提交任务的用户信息与目标目录的信息不一致，所以抛出如上异常。</p>\n<p>解决方法：在hiveserver2-site.xml中增加以下配置：</p>\n<pre><code>&lt;property&gt;\n    &lt;name&gt;hive.warehouse.subdir.inherit.perms&lt;/name&gt;\n    &lt;value&gt;false&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>在用HiveServer2执行语句”load data inpath ‘/user/test/test.csv’ into table dev.load_test_tbl;“时报错信息如下：</p>\n<pre><code>Failed with exception org.apache.hadoop.security.AccessControlException: User does not belong to data-dev\n    at org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.setOwner(FSDirAttrOp.java:86)\n    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setOwner(FSNamesystem.java:1676)\n    at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setOwner(NameNodeRpcServer.java:703)\n    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.setOwner(ClientNamenodeProtocolServerSideTranslatorPB.java:464)\n    at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\n    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\n    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:422)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)\n    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2045)\n\nFAILED: Operation cancelled\n</code></pre>\n<p>原因是Hive默认在Load数据时会修改HDFS上目的地下的数据文件与目的地目录的属主及组等权限信息一致，而提交任务的用户信息与目标目录的信息不一致，所以抛出如上异常。</p>\n<p>解决方法：在hiveserver2-site.xml中增加以下配置：</p>\n<pre><code>&lt;property&gt;\n    &lt;name&gt;hive.warehouse.subdir.inherit.perms&lt;/name&gt;\n    &lt;value&gt;false&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n"},{"title":"HiverServer2 启用 Ranger 行过滤规则后表 Insert Values 操作权限问题","date":"2021-03-05T09:46:11.000Z","_content":"\nHiverServer2 启用 Ranger 行过滤规则后表 Insert Values 操作提示以下权限问题：\n\n\tError: Error while compiling statement: FAILED: HiveAccessControlException Permission denied: user [test] does not have [UPDATE] privilege on [dev/test_insert_table_values] (state=42000,code=40000)\n\t\n默认的 Ranger 会拒绝启用行过滤规则表的 UPDATE 权限。Ranger 源代码如下（Ranger版本1.2.0）：类org.apache.ranger.authorization.hive.authorizer.RangerHiveAuthorizer中L1789到L1813\n\n\tprivate boolean isBlockAccessIfRowfilterColumnMaskSpecified(\n\t\t\tHiveOperationType hiveOpType, RangerHiveAccessRequest request) {\n\t\tboolean ret = false;\n\t\tRangerHiveResource resource = (RangerHiveResource) request.getResource();\n\t\tHiveObjectType objType = resource.getObjectType();\n\n\t\tif (objType == HiveObjectType.TABLE || objType == HiveObjectType.VIEW\n\t\t\t\t|| objType == HiveObjectType.COLUMN) {\n\t\t\tret = hiveOpType == HiveOperationType.EXPORT;\n\n\t\t\tif (!ret) {\n\t\t\t\tif (request.getHiveAccessType() == HiveAccessType.UPDATE\n\t\t\t\t\t\t&& hivePlugin.BlockUpdateIfRowfilterColumnMaskSpecified) {\n\t\t\t\t\tret = true;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif (LOG.isDebugEnabled()) {\n\t\t\tLOG.debug(\"isBlockAccessIfRowfilterColumnMaskSpecified(\" + hiveOpType\n\t\t\t\t\t+ \", \" + request + \"): \" + ret);\n\t\t}\n\n\t\treturn ret;\n\t}\n\n通过源代码跟踪找到解决方案：在ranger-hive-security.xml配置文件中设置xasecure.hive.block.update.if.rowfilter.columnmask.specified的值为false即可。如下：\n\n\t<property>\n\t\t<name>xasecure.hive.block.update.if.rowfilter.columnmask.specified</name>\n\t\t<value>false</value>\n\t</property>\n\n\n","source":"_posts/HiverServer2-启用-Ranger-行过滤规则后表-Insert-Values操作权限问题.md","raw":"title: HiverServer2 启用 Ranger 行过滤规则后表 Insert Values 操作权限问题\ndate: 2021-03-05 17:46:11\ntags:\n- Hive\n- Ranger\n- 大数据\ncategories:\n- 大数据\n- Ranger\n---\n\nHiverServer2 启用 Ranger 行过滤规则后表 Insert Values 操作提示以下权限问题：\n\n\tError: Error while compiling statement: FAILED: HiveAccessControlException Permission denied: user [test] does not have [UPDATE] privilege on [dev/test_insert_table_values] (state=42000,code=40000)\n\t\n默认的 Ranger 会拒绝启用行过滤规则表的 UPDATE 权限。Ranger 源代码如下（Ranger版本1.2.0）：类org.apache.ranger.authorization.hive.authorizer.RangerHiveAuthorizer中L1789到L1813\n\n\tprivate boolean isBlockAccessIfRowfilterColumnMaskSpecified(\n\t\t\tHiveOperationType hiveOpType, RangerHiveAccessRequest request) {\n\t\tboolean ret = false;\n\t\tRangerHiveResource resource = (RangerHiveResource) request.getResource();\n\t\tHiveObjectType objType = resource.getObjectType();\n\n\t\tif (objType == HiveObjectType.TABLE || objType == HiveObjectType.VIEW\n\t\t\t\t|| objType == HiveObjectType.COLUMN) {\n\t\t\tret = hiveOpType == HiveOperationType.EXPORT;\n\n\t\t\tif (!ret) {\n\t\t\t\tif (request.getHiveAccessType() == HiveAccessType.UPDATE\n\t\t\t\t\t\t&& hivePlugin.BlockUpdateIfRowfilterColumnMaskSpecified) {\n\t\t\t\t\tret = true;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif (LOG.isDebugEnabled()) {\n\t\t\tLOG.debug(\"isBlockAccessIfRowfilterColumnMaskSpecified(\" + hiveOpType\n\t\t\t\t\t+ \", \" + request + \"): \" + ret);\n\t\t}\n\n\t\treturn ret;\n\t}\n\n通过源代码跟踪找到解决方案：在ranger-hive-security.xml配置文件中设置xasecure.hive.block.update.if.rowfilter.columnmask.specified的值为false即可。如下：\n\n\t<property>\n\t\t<name>xasecure.hive.block.update.if.rowfilter.columnmask.specified</name>\n\t\t<value>false</value>\n\t</property>\n\n\n","slug":"HiverServer2-启用-Ranger-行过滤规则后表-Insert-Values操作权限问题","published":1,"updated":"2021-07-19T16:28:00.132Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphn50042itd3h3nuake6","content":"<p>HiverServer2 启用 Ranger 行过滤规则后表 Insert Values 操作提示以下权限问题：</p>\n<pre><code>Error: Error while compiling statement: FAILED: HiveAccessControlException Permission denied: user [test] does not have [UPDATE] privilege on [dev/test_insert_table_values] (state=42000,code=40000)\n</code></pre>\n<p>默认的 Ranger 会拒绝启用行过滤规则表的 UPDATE 权限。Ranger 源代码如下（Ranger版本1.2.0）：类org.apache.ranger.authorization.hive.authorizer.RangerHiveAuthorizer中L1789到L1813</p>\n<pre><code>private boolean isBlockAccessIfRowfilterColumnMaskSpecified(\n        HiveOperationType hiveOpType, RangerHiveAccessRequest request) &#123;\n    boolean ret = false;\n    RangerHiveResource resource = (RangerHiveResource) request.getResource();\n    HiveObjectType objType = resource.getObjectType();\n\n    if (objType == HiveObjectType.TABLE || objType == HiveObjectType.VIEW\n            || objType == HiveObjectType.COLUMN) &#123;\n        ret = hiveOpType == HiveOperationType.EXPORT;\n\n        if (!ret) &#123;\n            if (request.getHiveAccessType() == HiveAccessType.UPDATE\n                    &amp;&amp; hivePlugin.BlockUpdateIfRowfilterColumnMaskSpecified) &#123;\n                ret = true;\n            &#125;\n        &#125;\n    &#125;\n\n    if (LOG.isDebugEnabled()) &#123;\n        LOG.debug(&quot;isBlockAccessIfRowfilterColumnMaskSpecified(&quot; + hiveOpType\n                + &quot;, &quot; + request + &quot;): &quot; + ret);\n    &#125;\n\n    return ret;\n&#125;\n</code></pre>\n<p>通过源代码跟踪找到解决方案：在ranger-hive-security.xml配置文件中设置xasecure.hive.block.update.if.rowfilter.columnmask.specified的值为false即可。如下：</p>\n<pre><code>&lt;property&gt;\n    &lt;name&gt;xasecure.hive.block.update.if.rowfilter.columnmask.specified&lt;/name&gt;\n    &lt;value&gt;false&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>HiverServer2 启用 Ranger 行过滤规则后表 Insert Values 操作提示以下权限问题：</p>\n<pre><code>Error: Error while compiling statement: FAILED: HiveAccessControlException Permission denied: user [test] does not have [UPDATE] privilege on [dev/test_insert_table_values] (state=42000,code=40000)\n</code></pre>\n<p>默认的 Ranger 会拒绝启用行过滤规则表的 UPDATE 权限。Ranger 源代码如下（Ranger版本1.2.0）：类org.apache.ranger.authorization.hive.authorizer.RangerHiveAuthorizer中L1789到L1813</p>\n<pre><code>private boolean isBlockAccessIfRowfilterColumnMaskSpecified(\n        HiveOperationType hiveOpType, RangerHiveAccessRequest request) &#123;\n    boolean ret = false;\n    RangerHiveResource resource = (RangerHiveResource) request.getResource();\n    HiveObjectType objType = resource.getObjectType();\n\n    if (objType == HiveObjectType.TABLE || objType == HiveObjectType.VIEW\n            || objType == HiveObjectType.COLUMN) &#123;\n        ret = hiveOpType == HiveOperationType.EXPORT;\n\n        if (!ret) &#123;\n            if (request.getHiveAccessType() == HiveAccessType.UPDATE\n                    &amp;&amp; hivePlugin.BlockUpdateIfRowfilterColumnMaskSpecified) &#123;\n                ret = true;\n            &#125;\n        &#125;\n    &#125;\n\n    if (LOG.isDebugEnabled()) &#123;\n        LOG.debug(&quot;isBlockAccessIfRowfilterColumnMaskSpecified(&quot; + hiveOpType\n                + &quot;, &quot; + request + &quot;): &quot; + ret);\n    &#125;\n\n    return ret;\n&#125;\n</code></pre>\n<p>通过源代码跟踪找到解决方案：在ranger-hive-security.xml配置文件中设置xasecure.hive.block.update.if.rowfilter.columnmask.specified的值为false即可。如下：</p>\n<pre><code>&lt;property&gt;\n    &lt;name&gt;xasecure.hive.block.update.if.rowfilter.columnmask.specified&lt;/name&gt;\n    &lt;value&gt;false&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n"},{"title":"Hiveserver2 User: ** is not allowed to impersonate **","date":"2020-09-09T03:00:06.000Z","_content":"\n确认已经配置了Hadoop的超级代理用户，并且也重启了NameNode服务。后来发现是因为ResourceManager没有重启，任务提交到Yarn时出现权限不够导致的。\n\n使用以下刷新命令，不必重启：\n\n    $ hdfs dfsadmin -refreshSuperUserGroupsConfiguration\n    $ yarn rmadmin -refreshSuperUserGroupsConfiguration\n\n执行完成后需要重启HiveServer2。然后问题解决。\n","source":"_posts/Hiveserver2-User-is-not-allowed-to-impersonate.md","raw":"title: 'Hiveserver2 User: ** is not allowed to impersonate **'\ndate: 2020-09-09 11:00:06\ntags:\n- Hadoop\n- Hive\n- HDFS\ncategories:\n- 大数据\n- Hive\n---\n\n确认已经配置了Hadoop的超级代理用户，并且也重启了NameNode服务。后来发现是因为ResourceManager没有重启，任务提交到Yarn时出现权限不够导致的。\n\n使用以下刷新命令，不必重启：\n\n    $ hdfs dfsadmin -refreshSuperUserGroupsConfiguration\n    $ yarn rmadmin -refreshSuperUserGroupsConfiguration\n\n执行完成后需要重启HiveServer2。然后问题解决。\n","slug":"Hiveserver2-User-is-not-allowed-to-impersonate","published":1,"updated":"2021-07-19T16:28:00.360Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphn60046itd3bdke7eu1","content":"<p>确认已经配置了Hadoop的超级代理用户，并且也重启了NameNode服务。后来发现是因为ResourceManager没有重启，任务提交到Yarn时出现权限不够导致的。</p>\n<p>使用以下刷新命令，不必重启：</p>\n<pre><code>$ hdfs dfsadmin -refreshSuperUserGroupsConfiguration\n$ yarn rmadmin -refreshSuperUserGroupsConfiguration\n</code></pre>\n<p>执行完成后需要重启HiveServer2。然后问题解决。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>确认已经配置了Hadoop的超级代理用户，并且也重启了NameNode服务。后来发现是因为ResourceManager没有重启，任务提交到Yarn时出现权限不够导致的。</p>\n<p>使用以下刷新命令，不必重启：</p>\n<pre><code>$ hdfs dfsadmin -refreshSuperUserGroupsConfiguration\n$ yarn rmadmin -refreshSuperUserGroupsConfiguration\n</code></pre>\n<p>执行完成后需要重启HiveServer2。然后问题解决。</p>\n"},{"title":"Hive启用RowLevelFilter后Create Table AS异常","date":"2021-04-19T03:54:54.000Z","_content":"\n### 版本信息\n\n- Hive 2.1.1\n- Ranger 1.2.0\n- Hadoop 2.7.3\n\n### 问题现象\n\nRanger中启用行过滤规则后出现异常信息：Error: Error running query: java.lang.AssertionError: Unexpected type UNEXPECTED (state=,code=0)。如下图：\n![异常信息](/uploads/20210419/ctaException.png)\n\n### 问题调查\n\nHive社区Jira参考信息：<https://issues.apache.org/jira/browse/HIVE-22029>\n\n- org.apache.hadoop.hive.ql.parse.CalcitePlanner：\n\n  - L292-L298：\n\n    ```Java\n    // 2. Regen OP plan from optimized AST\n    init(false);\n    if (cboCtx.type == PreCboCtx.Type.CTAS) {\n      // Redo create-table analysis, because it's not part of doPhase1.\n      setAST(newAST);\n      newAST = reAnalyzeCtasAfterCbo(newAST);\n    }\n    ```\n    \n  - L647-L657：\n\n    ```Java\n    ASTNode reAnalyzeCtasAfterCbo(ASTNode newAst) throws SemanticException {\n      // analyzeCreateTable uses this.ast, but doPhase1 doesn't, so only reset it\n      // here.\n      newAst = analyzeCreateTable(newAst, getQB(), null);\n      if (newAst == null) {\n        LOG.error(\"analyzeCreateTable failed to initialize CTAS after CBO;\" + \" new ast is \"\n            + getAST().dump());\n        throw new SemanticException(\"analyzeCreateTable failed to initialize CTAS after CBO\");\n      }\n      return newAst;\n    }\n    ```\n    \n  - L590-L604：\n\n    ```Java\n    private void set(Type type, ASTNode ast) {\n      if (this.type != Type.NONE) {\n        STATIC_LOG.warn(\"Setting \" + type + \" when already \" + this.type + \"; node \" + ast.dump()\n            + \" vs old node \" + nodeOfInterest.dump());\n        this.type = Type.UNEXPECTED;\n        return;\n      }\n      this.type = type;\n      this.nodeOfInterest = ast;\n    }\n\n    @Override\n    void setCTASToken(ASTNode child) {\n      set(PreCboCtx.Type.CTAS, child);\n    }\n    ```\n\n- org.apache.hadoop.hive.ql.parse.SemanticAnalyzer\n\n  - L11573-L11597:\n  \n    ```Java\n    case HiveParser.TOK_QUERY: // CTAS\n      if (command_type == CTLT) {\n        throw new SemanticException(ErrorMsg.CTAS_CTLT_COEXISTENCE.getMsg());\n      }\n      if (cols.size() != 0) {\n        throw new SemanticException(ErrorMsg.CTAS_COLLST_COEXISTENCE.getMsg());\n      }\n      if (partCols.size() != 0 || bucketCols.size() != 0) {\n        boolean dynPart = HiveConf.getBoolVar(conf, HiveConf.ConfVars.DYNAMICPARTITIONING);\n        if (dynPart == false) {\n          throw new SemanticException(ErrorMsg.CTAS_PARCOL_COEXISTENCE.getMsg());\n        } else {\n          // TODO: support dynamic partition for CTAS\n          throw new SemanticException(ErrorMsg.CTAS_PARCOL_COEXISTENCE.getMsg());\n        }\n      }\n      if (isExt) {\n        throw new SemanticException(ErrorMsg.CTAS_EXTTBL_COEXISTENCE.getMsg());\n      }\n      command_type = CTAS;\n      if (plannerCtx != null) {\n        plannerCtx.setCTASToken(child);\n      }\n      selectStmt = child;\n      break;\n      ```\n      \n### 解决方法\n\n禁用CBO可以解决问题。\n\n    ```XML\n    <property>\n        <name>hive.cbo.enable</name>\n        <value>false</value>\n    </property>\n    ```","source":"_posts/Hive启用RowLevelFilter后Create-Table-AS异常.md","raw":"title: Hive启用RowLevelFilter后Create Table AS异常\ndate: 2021-04-19 11:54:54\ntags:\n- 大数据\n- Hive\n- Ranger\ncategories:\n- 大数据\n- Hive\n---\n\n### 版本信息\n\n- Hive 2.1.1\n- Ranger 1.2.0\n- Hadoop 2.7.3\n\n### 问题现象\n\nRanger中启用行过滤规则后出现异常信息：Error: Error running query: java.lang.AssertionError: Unexpected type UNEXPECTED (state=,code=0)。如下图：\n![异常信息](/uploads/20210419/ctaException.png)\n\n### 问题调查\n\nHive社区Jira参考信息：<https://issues.apache.org/jira/browse/HIVE-22029>\n\n- org.apache.hadoop.hive.ql.parse.CalcitePlanner：\n\n  - L292-L298：\n\n    ```Java\n    // 2. Regen OP plan from optimized AST\n    init(false);\n    if (cboCtx.type == PreCboCtx.Type.CTAS) {\n      // Redo create-table analysis, because it's not part of doPhase1.\n      setAST(newAST);\n      newAST = reAnalyzeCtasAfterCbo(newAST);\n    }\n    ```\n    \n  - L647-L657：\n\n    ```Java\n    ASTNode reAnalyzeCtasAfterCbo(ASTNode newAst) throws SemanticException {\n      // analyzeCreateTable uses this.ast, but doPhase1 doesn't, so only reset it\n      // here.\n      newAst = analyzeCreateTable(newAst, getQB(), null);\n      if (newAst == null) {\n        LOG.error(\"analyzeCreateTable failed to initialize CTAS after CBO;\" + \" new ast is \"\n            + getAST().dump());\n        throw new SemanticException(\"analyzeCreateTable failed to initialize CTAS after CBO\");\n      }\n      return newAst;\n    }\n    ```\n    \n  - L590-L604：\n\n    ```Java\n    private void set(Type type, ASTNode ast) {\n      if (this.type != Type.NONE) {\n        STATIC_LOG.warn(\"Setting \" + type + \" when already \" + this.type + \"; node \" + ast.dump()\n            + \" vs old node \" + nodeOfInterest.dump());\n        this.type = Type.UNEXPECTED;\n        return;\n      }\n      this.type = type;\n      this.nodeOfInterest = ast;\n    }\n\n    @Override\n    void setCTASToken(ASTNode child) {\n      set(PreCboCtx.Type.CTAS, child);\n    }\n    ```\n\n- org.apache.hadoop.hive.ql.parse.SemanticAnalyzer\n\n  - L11573-L11597:\n  \n    ```Java\n    case HiveParser.TOK_QUERY: // CTAS\n      if (command_type == CTLT) {\n        throw new SemanticException(ErrorMsg.CTAS_CTLT_COEXISTENCE.getMsg());\n      }\n      if (cols.size() != 0) {\n        throw new SemanticException(ErrorMsg.CTAS_COLLST_COEXISTENCE.getMsg());\n      }\n      if (partCols.size() != 0 || bucketCols.size() != 0) {\n        boolean dynPart = HiveConf.getBoolVar(conf, HiveConf.ConfVars.DYNAMICPARTITIONING);\n        if (dynPart == false) {\n          throw new SemanticException(ErrorMsg.CTAS_PARCOL_COEXISTENCE.getMsg());\n        } else {\n          // TODO: support dynamic partition for CTAS\n          throw new SemanticException(ErrorMsg.CTAS_PARCOL_COEXISTENCE.getMsg());\n        }\n      }\n      if (isExt) {\n        throw new SemanticException(ErrorMsg.CTAS_EXTTBL_COEXISTENCE.getMsg());\n      }\n      command_type = CTAS;\n      if (plannerCtx != null) {\n        plannerCtx.setCTASToken(child);\n      }\n      selectStmt = child;\n      break;\n      ```\n      \n### 解决方法\n\n禁用CBO可以解决问题。\n\n    ```XML\n    <property>\n        <name>hive.cbo.enable</name>\n        <value>false</value>\n    </property>\n    ```","slug":"Hive启用RowLevelFilter后Create-Table-AS异常","published":1,"updated":"2021-07-19T16:28:00.144Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphn8004aitd3fr7lcyia","content":"<h3 id=\"版本信息\"><a href=\"#版本信息\" class=\"headerlink\" title=\"版本信息\"></a>版本信息</h3><ul>\n<li>Hive 2.1.1</li>\n<li>Ranger 1.2.0</li>\n<li>Hadoop 2.7.3</li>\n</ul>\n<h3 id=\"问题现象\"><a href=\"#问题现象\" class=\"headerlink\" title=\"问题现象\"></a>问题现象</h3><p>Ranger中启用行过滤规则后出现异常信息：Error: Error running query: java.lang.AssertionError: Unexpected type UNEXPECTED (state=,code=0)。如下图：<br><img src=\"/uploads/20210419/ctaException.png\" alt=\"异常信息\"></p>\n<h3 id=\"问题调查\"><a href=\"#问题调查\" class=\"headerlink\" title=\"问题调查\"></a>问题调查</h3><p>Hive社区Jira参考信息：<a href=\"https://issues.apache.org/jira/browse/HIVE-22029\">https://issues.apache.org/jira/browse/HIVE-22029</a></p>\n<ul>\n<li><p>org.apache.hadoop.hive.ql.parse.CalcitePlanner：</p>\n<ul>\n<li><p>L292-L298：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 2. Regen OP plan from optimized AST</span></span><br><span class=\"line\">init(<span class=\"keyword\">false</span>);</span><br><span class=\"line\"><span class=\"keyword\">if</span> (cboCtx.type == PreCboCtx.Type.CTAS) &#123;</span><br><span class=\"line\">  <span class=\"comment\">// Redo create-table analysis, because it&#x27;s not part of doPhase1.</span></span><br><span class=\"line\">  setAST(newAST);</span><br><span class=\"line\">  newAST = reAnalyzeCtasAfterCbo(newAST);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n<li><p>L647-L657：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">ASTNode <span class=\"title\">reAnalyzeCtasAfterCbo</span><span class=\"params\">(ASTNode newAst)</span> <span class=\"keyword\">throws</span> SemanticException </span>&#123;</span><br><span class=\"line\">  <span class=\"comment\">// analyzeCreateTable uses this.ast, but doPhase1 doesn&#x27;t, so only reset it</span></span><br><span class=\"line\">  <span class=\"comment\">// here.</span></span><br><span class=\"line\">  newAst = analyzeCreateTable(newAst, getQB(), <span class=\"keyword\">null</span>);</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (newAst == <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">    LOG.error(<span class=\"string\">&quot;analyzeCreateTable failed to initialize CTAS after CBO;&quot;</span> + <span class=\"string\">&quot; new ast is &quot;</span></span><br><span class=\"line\">        + getAST().dump());</span><br><span class=\"line\">    <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> SemanticException(<span class=\"string\">&quot;analyzeCreateTable failed to initialize CTAS after CBO&quot;</span>);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">return</span> newAst;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n<li><p>L590-L604：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title\">set</span><span class=\"params\">(Type type, ASTNode ast)</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (<span class=\"keyword\">this</span>.type != Type.NONE) &#123;</span><br><span class=\"line\">    STATIC_LOG.warn(<span class=\"string\">&quot;Setting &quot;</span> + type + <span class=\"string\">&quot; when already &quot;</span> + <span class=\"keyword\">this</span>.type + <span class=\"string\">&quot;; node &quot;</span> + ast.dump()</span><br><span class=\"line\">        + <span class=\"string\">&quot; vs old node &quot;</span> + nodeOfInterest.dump());</span><br><span class=\"line\">    <span class=\"keyword\">this</span>.type = Type.UNEXPECTED;</span><br><span class=\"line\">    <span class=\"keyword\">return</span>;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">this</span>.type = type;</span><br><span class=\"line\">  <span class=\"keyword\">this</span>.nodeOfInterest = ast;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@Override</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">setCTASToken</span><span class=\"params\">(ASTNode child)</span> </span>&#123;</span><br><span class=\"line\">  set(PreCboCtx.Type.CTAS, child);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n</ul>\n</li>\n<li><p>org.apache.hadoop.hive.ql.parse.SemanticAnalyzer</p>\n<ul>\n<li><p>L11573-L11597:</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">case</span> HiveParser.TOK_QUERY: <span class=\"comment\">// CTAS</span></span><br><span class=\"line\">  <span class=\"keyword\">if</span> (command_type == CTLT) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> SemanticException(ErrorMsg.CTAS_CTLT_COEXISTENCE.getMsg());</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (cols.size() != <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> SemanticException(ErrorMsg.CTAS_COLLST_COEXISTENCE.getMsg());</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (partCols.size() != <span class=\"number\">0</span> || bucketCols.size() != <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">boolean</span> dynPart = HiveConf.getBoolVar(conf, HiveConf.ConfVars.DYNAMICPARTITIONING);</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (dynPart == <span class=\"keyword\">false</span>) &#123;</span><br><span class=\"line\">      <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> SemanticException(ErrorMsg.CTAS_PARCOL_COEXISTENCE.getMsg());</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">      <span class=\"comment\">// <span class=\"doctag\">TODO:</span> support dynamic partition for CTAS</span></span><br><span class=\"line\">      <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> SemanticException(ErrorMsg.CTAS_PARCOL_COEXISTENCE.getMsg());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (isExt) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> SemanticException(ErrorMsg.CTAS_EXTTBL_COEXISTENCE.getMsg());</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  command_type = CTAS;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (plannerCtx != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">    plannerCtx.setCTASToken(child);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  selectStmt = child;</span><br><span class=\"line\">  <span class=\"keyword\">break</span>;</span><br></pre></td></tr></table></figure></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"解决方法\"><a href=\"#解决方法\" class=\"headerlink\" title=\"解决方法\"></a>解决方法</h3><p>禁用CBO可以解决问题。</p>\n<pre><code><figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>hive.cbo.enable<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>false<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br></pre></td></tr></table></figure>\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"版本信息\"><a href=\"#版本信息\" class=\"headerlink\" title=\"版本信息\"></a>版本信息</h3><ul>\n<li>Hive 2.1.1</li>\n<li>Ranger 1.2.0</li>\n<li>Hadoop 2.7.3</li>\n</ul>\n<h3 id=\"问题现象\"><a href=\"#问题现象\" class=\"headerlink\" title=\"问题现象\"></a>问题现象</h3><p>Ranger中启用行过滤规则后出现异常信息：Error: Error running query: java.lang.AssertionError: Unexpected type UNEXPECTED (state=,code=0)。如下图：<br><img src=\"/uploads/20210419/ctaException.png\" alt=\"异常信息\"></p>\n<h3 id=\"问题调查\"><a href=\"#问题调查\" class=\"headerlink\" title=\"问题调查\"></a>问题调查</h3><p>Hive社区Jira参考信息：<a href=\"https://issues.apache.org/jira/browse/HIVE-22029\">https://issues.apache.org/jira/browse/HIVE-22029</a></p>\n<ul>\n<li><p>org.apache.hadoop.hive.ql.parse.CalcitePlanner：</p>\n<ul>\n<li><p>L292-L298：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 2. Regen OP plan from optimized AST</span></span><br><span class=\"line\">init(<span class=\"keyword\">false</span>);</span><br><span class=\"line\"><span class=\"keyword\">if</span> (cboCtx.type == PreCboCtx.Type.CTAS) &#123;</span><br><span class=\"line\">  <span class=\"comment\">// Redo create-table analysis, because it&#x27;s not part of doPhase1.</span></span><br><span class=\"line\">  setAST(newAST);</span><br><span class=\"line\">  newAST = reAnalyzeCtasAfterCbo(newAST);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n<li><p>L647-L657：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">ASTNode <span class=\"title\">reAnalyzeCtasAfterCbo</span><span class=\"params\">(ASTNode newAst)</span> <span class=\"keyword\">throws</span> SemanticException </span>&#123;</span><br><span class=\"line\">  <span class=\"comment\">// analyzeCreateTable uses this.ast, but doPhase1 doesn&#x27;t, so only reset it</span></span><br><span class=\"line\">  <span class=\"comment\">// here.</span></span><br><span class=\"line\">  newAst = analyzeCreateTable(newAst, getQB(), <span class=\"keyword\">null</span>);</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (newAst == <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">    LOG.error(<span class=\"string\">&quot;analyzeCreateTable failed to initialize CTAS after CBO;&quot;</span> + <span class=\"string\">&quot; new ast is &quot;</span></span><br><span class=\"line\">        + getAST().dump());</span><br><span class=\"line\">    <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> SemanticException(<span class=\"string\">&quot;analyzeCreateTable failed to initialize CTAS after CBO&quot;</span>);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">return</span> newAst;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n<li><p>L590-L604：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title\">set</span><span class=\"params\">(Type type, ASTNode ast)</span> </span>&#123;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (<span class=\"keyword\">this</span>.type != Type.NONE) &#123;</span><br><span class=\"line\">    STATIC_LOG.warn(<span class=\"string\">&quot;Setting &quot;</span> + type + <span class=\"string\">&quot; when already &quot;</span> + <span class=\"keyword\">this</span>.type + <span class=\"string\">&quot;; node &quot;</span> + ast.dump()</span><br><span class=\"line\">        + <span class=\"string\">&quot; vs old node &quot;</span> + nodeOfInterest.dump());</span><br><span class=\"line\">    <span class=\"keyword\">this</span>.type = Type.UNEXPECTED;</span><br><span class=\"line\">    <span class=\"keyword\">return</span>;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">this</span>.type = type;</span><br><span class=\"line\">  <span class=\"keyword\">this</span>.nodeOfInterest = ast;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">@Override</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">setCTASToken</span><span class=\"params\">(ASTNode child)</span> </span>&#123;</span><br><span class=\"line\">  set(PreCboCtx.Type.CTAS, child);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></li>\n</ul>\n</li>\n<li><p>org.apache.hadoop.hive.ql.parse.SemanticAnalyzer</p>\n<ul>\n<li><p>L11573-L11597:</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">case</span> HiveParser.TOK_QUERY: <span class=\"comment\">// CTAS</span></span><br><span class=\"line\">  <span class=\"keyword\">if</span> (command_type == CTLT) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> SemanticException(ErrorMsg.CTAS_CTLT_COEXISTENCE.getMsg());</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (cols.size() != <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> SemanticException(ErrorMsg.CTAS_COLLST_COEXISTENCE.getMsg());</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (partCols.size() != <span class=\"number\">0</span> || bucketCols.size() != <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">boolean</span> dynPart = HiveConf.getBoolVar(conf, HiveConf.ConfVars.DYNAMICPARTITIONING);</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (dynPart == <span class=\"keyword\">false</span>) &#123;</span><br><span class=\"line\">      <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> SemanticException(ErrorMsg.CTAS_PARCOL_COEXISTENCE.getMsg());</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">      <span class=\"comment\">// <span class=\"doctag\">TODO:</span> support dynamic partition for CTAS</span></span><br><span class=\"line\">      <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> SemanticException(ErrorMsg.CTAS_PARCOL_COEXISTENCE.getMsg());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (isExt) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> SemanticException(ErrorMsg.CTAS_EXTTBL_COEXISTENCE.getMsg());</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  command_type = CTAS;</span><br><span class=\"line\">  <span class=\"keyword\">if</span> (plannerCtx != <span class=\"keyword\">null</span>) &#123;</span><br><span class=\"line\">    plannerCtx.setCTASToken(child);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">  selectStmt = child;</span><br><span class=\"line\">  <span class=\"keyword\">break</span>;</span><br></pre></td></tr></table></figure></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"解决方法\"><a href=\"#解决方法\" class=\"headerlink\" title=\"解决方法\"></a>解决方法</h3><p>禁用CBO可以解决问题。</p>\n<pre><code><figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>hive.cbo.enable<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></span><br><span class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>false<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></span><br><span class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></span><br></pre></td></tr></table></figure>\n</code></pre>\n"},{"title":"Hue Load Balance配置","date":"2020-11-18T09:11:29.000Z","_content":"\n直接上配置：\n\n\tserver {\n\t  server_name 192.168.72.31;\n\t  listen 8001;\n\t  \n\t  charset utf-8;\n\t  \n\t  proxy_connect_timeout 600s;\n\t  proxy_read_timeout 600s;\n\t  proxy_send_timeout 600s;\n\t  \n\t  location / {\n\t    proxy_set_header Host $http_x_forwarded_for;\n\t    proxy_set_header X-Forwarded-For $http_x_forwarded_for;\n\t    \n\t    proxy_pass http://hue;\n\t  }\n\t}\n\t\n\tupstream hue {\n\t  hash $cookie_sessionid;\n\t  \n\t  server 192.168.72.22:8888 max_fails=3;\n\t  server 192.168.72.31:8888 max_fails=3;\n\t}\n\n重点是调整 upstream hash 的策略。因为 Hue 是需要保持 session 的，同一个 session 的请求需要发送到同一台后端服务器上。简单的可以采用 ip_hash 策略，这个策略存在两个重要的问题：\n\n- 对隐藏在局域网后的用户不起作用。\n- ip_hash 只取 IPV4 的前三段值做 Hash，在非公网大并发场景下会出现负载非常不均衡的情况。参见：http://nginx.org/en/docs/http/ngx_http_upstream_module.html#ip_hash\n\n基于以上两点修改了 Nginx 的配置采用 $cookie_sessionid 做为 hash 值，达到 session 级别的负载均衡。\n\n主要的问题其实是这种方式未做到按照后端服务器实际负载再按照 session 级别来分配请求。后续打算研究 HAProxy 是否可以做到。","source":"_posts/Hue-Load-Balance配置.md","raw":"title: Hue Load Balance配置\ndate: 2020-11-18 17:11:29\ntags:\n- Hue\n- Nginx\ncategories:\n- 大数据\n- Hue\n---\n\n直接上配置：\n\n\tserver {\n\t  server_name 192.168.72.31;\n\t  listen 8001;\n\t  \n\t  charset utf-8;\n\t  \n\t  proxy_connect_timeout 600s;\n\t  proxy_read_timeout 600s;\n\t  proxy_send_timeout 600s;\n\t  \n\t  location / {\n\t    proxy_set_header Host $http_x_forwarded_for;\n\t    proxy_set_header X-Forwarded-For $http_x_forwarded_for;\n\t    \n\t    proxy_pass http://hue;\n\t  }\n\t}\n\t\n\tupstream hue {\n\t  hash $cookie_sessionid;\n\t  \n\t  server 192.168.72.22:8888 max_fails=3;\n\t  server 192.168.72.31:8888 max_fails=3;\n\t}\n\n重点是调整 upstream hash 的策略。因为 Hue 是需要保持 session 的，同一个 session 的请求需要发送到同一台后端服务器上。简单的可以采用 ip_hash 策略，这个策略存在两个重要的问题：\n\n- 对隐藏在局域网后的用户不起作用。\n- ip_hash 只取 IPV4 的前三段值做 Hash，在非公网大并发场景下会出现负载非常不均衡的情况。参见：http://nginx.org/en/docs/http/ngx_http_upstream_module.html#ip_hash\n\n基于以上两点修改了 Nginx 的配置采用 $cookie_sessionid 做为 hash 值，达到 session 级别的负载均衡。\n\n主要的问题其实是这种方式未做到按照后端服务器实际负载再按照 session 级别来分配请求。后续打算研究 HAProxy 是否可以做到。","slug":"Hue-Load-Balance配置","published":1,"updated":"2021-07-19T16:28:00.004Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphn9004ditd3fiar1jtk","content":"<p>直接上配置：</p>\n<pre><code>server &#123;\n  server_name 192.168.72.31;\n  listen 8001;\n  \n  charset utf-8;\n  \n  proxy_connect_timeout 600s;\n  proxy_read_timeout 600s;\n  proxy_send_timeout 600s;\n  \n  location / &#123;\n    proxy_set_header Host $http_x_forwarded_for;\n    proxy_set_header X-Forwarded-For $http_x_forwarded_for;\n    \n    proxy_pass http://hue;\n  &#125;\n&#125;\n\nupstream hue &#123;\n  hash $cookie_sessionid;\n  \n  server 192.168.72.22:8888 max_fails=3;\n  server 192.168.72.31:8888 max_fails=3;\n&#125;\n</code></pre>\n<p>重点是调整 upstream hash 的策略。因为 Hue 是需要保持 session 的，同一个 session 的请求需要发送到同一台后端服务器上。简单的可以采用 ip_hash 策略，这个策略存在两个重要的问题：</p>\n<ul>\n<li>对隐藏在局域网后的用户不起作用。</li>\n<li>ip_hash 只取 IPV4 的前三段值做 Hash，在非公网大并发场景下会出现负载非常不均衡的情况。参见：<a href=\"http://nginx.org/en/docs/http/ngx_http_upstream_module.html#ip_hash\">http://nginx.org/en/docs/http/ngx_http_upstream_module.html#ip_hash</a></li>\n</ul>\n<p>基于以上两点修改了 Nginx 的配置采用 $cookie_sessionid 做为 hash 值，达到 session 级别的负载均衡。</p>\n<p>主要的问题其实是这种方式未做到按照后端服务器实际负载再按照 session 级别来分配请求。后续打算研究 HAProxy 是否可以做到。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>直接上配置：</p>\n<pre><code>server &#123;\n  server_name 192.168.72.31;\n  listen 8001;\n  \n  charset utf-8;\n  \n  proxy_connect_timeout 600s;\n  proxy_read_timeout 600s;\n  proxy_send_timeout 600s;\n  \n  location / &#123;\n    proxy_set_header Host $http_x_forwarded_for;\n    proxy_set_header X-Forwarded-For $http_x_forwarded_for;\n    \n    proxy_pass http://hue;\n  &#125;\n&#125;\n\nupstream hue &#123;\n  hash $cookie_sessionid;\n  \n  server 192.168.72.22:8888 max_fails=3;\n  server 192.168.72.31:8888 max_fails=3;\n&#125;\n</code></pre>\n<p>重点是调整 upstream hash 的策略。因为 Hue 是需要保持 session 的，同一个 session 的请求需要发送到同一台后端服务器上。简单的可以采用 ip_hash 策略，这个策略存在两个重要的问题：</p>\n<ul>\n<li>对隐藏在局域网后的用户不起作用。</li>\n<li>ip_hash 只取 IPV4 的前三段值做 Hash，在非公网大并发场景下会出现负载非常不均衡的情况。参见：<a href=\"http://nginx.org/en/docs/http/ngx_http_upstream_module.html#ip_hash\">http://nginx.org/en/docs/http/ngx_http_upstream_module.html#ip_hash</a></li>\n</ul>\n<p>基于以上两点修改了 Nginx 的配置采用 $cookie_sessionid 做为 hash 值，达到 session 级别的负载均衡。</p>\n<p>主要的问题其实是这种方式未做到按照后端服务器实际负载再按照 session 级别来分配请求。后续打算研究 HAProxy 是否可以做到。</p>\n"},{"title":"Hue UI 展示中文","date":"2020-09-17T09:14:19.000Z","_content":"\n如果使用开发分支代码（如master分支）编译安装，需要自己编译语言文件。例如Hue安装目录为“/opt/hue”，则安装后执行以下命令：\n\n    $ cd /opt/hue\n    $ make locales\n    \n如果需要则重启Hue即可。是否需要重启我没有验证：）\n","source":"_posts/Hue-UI-展示中文.md","raw":"title: Hue UI 展示中文\ndate: 2020-09-17 17:14:19\ntags:\n- Hue\n- 大数据\ncategories:\n- 大数据\n- Hue\n---\n\n如果使用开发分支代码（如master分支）编译安装，需要自己编译语言文件。例如Hue安装目录为“/opt/hue”，则安装后执行以下命令：\n\n    $ cd /opt/hue\n    $ make locales\n    \n如果需要则重启Hue即可。是否需要重启我没有验证：）\n","slug":"Hue-UI-展示中文","published":1,"updated":"2021-07-19T16:27:59.940Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphna004hitd3bedr4mqf","content":"<p>如果使用开发分支代码（如master分支）编译安装，需要自己编译语言文件。例如Hue安装目录为“/opt/hue”，则安装后执行以下命令：</p>\n<pre><code>$ cd /opt/hue\n$ make locales\n</code></pre>\n<p>如果需要则重启Hue即可。是否需要重启我没有验证：）</p>\n","site":{"data":{}},"excerpt":"","more":"<p>如果使用开发分支代码（如master分支）编译安装，需要自己编译语言文件。例如Hue安装目录为“/opt/hue”，则安装后执行以下命令：</p>\n<pre><code>$ cd /opt/hue\n$ make locales\n</code></pre>\n<p>如果需要则重启Hue即可。是否需要重启我没有验证：）</p>\n"},{"title":"Hue 编译异常：ImportError: cannot import name 'six' from 'urllib3.packages'","date":"2020-06-23T09:14:36.000Z","_content":"\n在编译Hue的时候出现错误信息如下：\n\n    Running '/home/zhangjc/ysten/git/ysten-hue/build/env/bin/hue makemigrations --noinput' with None\n    Traceback (most recent call last):\n      File \"/home/zhangjc/ysten/git/ysten-hue/build/env/bin/hue\", line 33, in <module>\n        sys.exit(load_entry_point('desktop', 'console_scripts', 'hue')())\n      File \"/home/zhangjc/ysten/git/ysten-hue/desktop/core/src/desktop/manage_entry.py\", line 225, in entry\n        execute_from_command_line(sys.argv)\n      File \"/home/zhangjc/ysten/git/ysten-hue/build/env/lib/python3.7/site-packages/django/core/management/__init__.py\", line 364, in execute_from_command_line\n        utility.execute()\n      File \"/home/zhangjc/ysten/git/ysten-hue/build/env/lib/python3.7/site-packages/django/core/management/__init__.py\", line 338, in execute\n        django.setup()\n      File \"/home/zhangjc/ysten/git/ysten-hue/build/env/lib/python3.7/site-packages/django/__init__.py\", line 27, in setup\n        apps.populate(settings.INSTALLED_APPS)\n      File \"/home/zhangjc/ysten/git/ysten-hue/build/env/lib/python3.7/site-packages/django/apps/registry.py\", line 108, in populate\n        app_config.import_models()\n      File \"/home/zhangjc/ysten/git/ysten-hue/build/env/lib/python3.7/site-packages/django/apps/config.py\", line 202, in import_models\n        self.models_module = import_module(models_module_name)\n      File \"/home/zhangjc/ysten/git/ysten-hue/build/env/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n        return _bootstrap._gcd_import(name[level:], package, level)\n      File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n      File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n      File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n      File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n      File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n      File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n      File \"/home/zhangjc/ysten/git/ysten-hue/desktop/core/src/desktop/models.py\", line 47, in <module>\n        from useradmin.models import User, Group, get_organization\n      File \"/home/zhangjc/ysten/git/ysten-hue/apps/useradmin/src/useradmin/models.py\", line 56, in <module>\n        from desktop.lib.connectors.models import _get_installed_connectors, Connector\n      File \"/home/zhangjc/ysten/git/ysten-hue/desktop/core/src/desktop/lib/connectors/models.py\", line 29, in <module>\n        from desktop.lib.connectors.types import get_connectors_types\n      File \"/home/zhangjc/ysten/git/ysten-hue/desktop/core/src/desktop/lib/connectors/types.py\", line 24, in <module>\n        from desktop.lib.exceptions_renderable import PopupException\n      File \"/home/zhangjc/ysten/git/ysten-hue/desktop/core/src/desktop/lib/exceptions_renderable.py\", line 31, in <module>\n        import desktop.lib.django_util\n      File \"/home/zhangjc/ysten/git/ysten-hue/desktop/core/src/desktop/lib/django_util.py\", line 42, in <module>\n        import desktop.lib.thrift_util\n      File \"/home/zhangjc/ysten/git/ysten-hue/desktop/core/src/desktop/lib/thrift_util.py\", line 49, in <module>\n        from desktop.lib.thrift_.http_client import THttpClient\n      File \"/home/zhangjc/ysten/git/ysten-hue/desktop/core/src/desktop/lib/thrift_/http_client.py\", line 26, in <module>\n        from desktop.lib.rest.http_client import HttpClient\n      File \"/home/zhangjc/ysten/git/ysten-hue/desktop/core/src/desktop/lib/rest/http_client.py\", line 32, in <module>\n        from urllib3.contrib import pyopenssl\n      File \"/usr/lib/python3/dist-packages/urllib3/contrib/pyopenssl.py\", line 62, in <module>\n        from ..packages import six\n    ImportError: cannot import name 'six' from 'urllib3.packages' (/usr/lib/python3/dist-packages/urllib3/packages/__init__.py)\n\n错误原因：根据错误信息可以看到是“/usr/lib/python3/dist-packages/urllib3”这个包的问题。正确的应该是引用“build/env/lib/python3.7/site-packages/”下的包。查看\"/usr/lib/python3/dist-packages/urllib3\"下的包信息，发现包版本比较低。\n\n    $ ls -l /usr/lib/python3/dist-packages/\n    drwxr-xr-x  6 root root   4096 1月  13 17:07 urllib3\n    drwxr-xr-x  2 root root   4096 1月  13 17:06 urllib3-1.22.egg-info\n\n解决方法：删除有问题的包后再试成功。\n\n    $ sudo rm -rf /usr/lib/python3/dist-packages/urllib3*\n\n查看“build/env/lib/python3.7/site-packages/”下的urllib3包信息：\n\n    $ ls -l build/env/lib/python3.7/site-packages/|grep urllib3\n    drwxr-xr-x  6 zhangjc zhangjc   4096 6月  23 17:06 urllib3\n    drwxr-xr-x  2 zhangjc zhangjc   4096 6月  23 17:06 urllib3-1.25.9.dist-info","source":"_posts/Hue-编译异常：ImportError-cannot-import-name-six-from-urllib3-packages.md","raw":"title: 'Hue 编译异常：ImportError: cannot import name ''six'' from ''urllib3.packages'''\ndate: 2020-06-23 17:14:36\ntags:\n- Hue\n- Python3\ncategories:\n- 大数据\n- Hue\n---\n\n在编译Hue的时候出现错误信息如下：\n\n    Running '/home/zhangjc/ysten/git/ysten-hue/build/env/bin/hue makemigrations --noinput' with None\n    Traceback (most recent call last):\n      File \"/home/zhangjc/ysten/git/ysten-hue/build/env/bin/hue\", line 33, in <module>\n        sys.exit(load_entry_point('desktop', 'console_scripts', 'hue')())\n      File \"/home/zhangjc/ysten/git/ysten-hue/desktop/core/src/desktop/manage_entry.py\", line 225, in entry\n        execute_from_command_line(sys.argv)\n      File \"/home/zhangjc/ysten/git/ysten-hue/build/env/lib/python3.7/site-packages/django/core/management/__init__.py\", line 364, in execute_from_command_line\n        utility.execute()\n      File \"/home/zhangjc/ysten/git/ysten-hue/build/env/lib/python3.7/site-packages/django/core/management/__init__.py\", line 338, in execute\n        django.setup()\n      File \"/home/zhangjc/ysten/git/ysten-hue/build/env/lib/python3.7/site-packages/django/__init__.py\", line 27, in setup\n        apps.populate(settings.INSTALLED_APPS)\n      File \"/home/zhangjc/ysten/git/ysten-hue/build/env/lib/python3.7/site-packages/django/apps/registry.py\", line 108, in populate\n        app_config.import_models()\n      File \"/home/zhangjc/ysten/git/ysten-hue/build/env/lib/python3.7/site-packages/django/apps/config.py\", line 202, in import_models\n        self.models_module = import_module(models_module_name)\n      File \"/home/zhangjc/ysten/git/ysten-hue/build/env/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n        return _bootstrap._gcd_import(name[level:], package, level)\n      File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n      File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n      File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n      File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n      File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n      File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n      File \"/home/zhangjc/ysten/git/ysten-hue/desktop/core/src/desktop/models.py\", line 47, in <module>\n        from useradmin.models import User, Group, get_organization\n      File \"/home/zhangjc/ysten/git/ysten-hue/apps/useradmin/src/useradmin/models.py\", line 56, in <module>\n        from desktop.lib.connectors.models import _get_installed_connectors, Connector\n      File \"/home/zhangjc/ysten/git/ysten-hue/desktop/core/src/desktop/lib/connectors/models.py\", line 29, in <module>\n        from desktop.lib.connectors.types import get_connectors_types\n      File \"/home/zhangjc/ysten/git/ysten-hue/desktop/core/src/desktop/lib/connectors/types.py\", line 24, in <module>\n        from desktop.lib.exceptions_renderable import PopupException\n      File \"/home/zhangjc/ysten/git/ysten-hue/desktop/core/src/desktop/lib/exceptions_renderable.py\", line 31, in <module>\n        import desktop.lib.django_util\n      File \"/home/zhangjc/ysten/git/ysten-hue/desktop/core/src/desktop/lib/django_util.py\", line 42, in <module>\n        import desktop.lib.thrift_util\n      File \"/home/zhangjc/ysten/git/ysten-hue/desktop/core/src/desktop/lib/thrift_util.py\", line 49, in <module>\n        from desktop.lib.thrift_.http_client import THttpClient\n      File \"/home/zhangjc/ysten/git/ysten-hue/desktop/core/src/desktop/lib/thrift_/http_client.py\", line 26, in <module>\n        from desktop.lib.rest.http_client import HttpClient\n      File \"/home/zhangjc/ysten/git/ysten-hue/desktop/core/src/desktop/lib/rest/http_client.py\", line 32, in <module>\n        from urllib3.contrib import pyopenssl\n      File \"/usr/lib/python3/dist-packages/urllib3/contrib/pyopenssl.py\", line 62, in <module>\n        from ..packages import six\n    ImportError: cannot import name 'six' from 'urllib3.packages' (/usr/lib/python3/dist-packages/urllib3/packages/__init__.py)\n\n错误原因：根据错误信息可以看到是“/usr/lib/python3/dist-packages/urllib3”这个包的问题。正确的应该是引用“build/env/lib/python3.7/site-packages/”下的包。查看\"/usr/lib/python3/dist-packages/urllib3\"下的包信息，发现包版本比较低。\n\n    $ ls -l /usr/lib/python3/dist-packages/\n    drwxr-xr-x  6 root root   4096 1月  13 17:07 urllib3\n    drwxr-xr-x  2 root root   4096 1月  13 17:06 urllib3-1.22.egg-info\n\n解决方法：删除有问题的包后再试成功。\n\n    $ sudo rm -rf /usr/lib/python3/dist-packages/urllib3*\n\n查看“build/env/lib/python3.7/site-packages/”下的urllib3包信息：\n\n    $ ls -l build/env/lib/python3.7/site-packages/|grep urllib3\n    drwxr-xr-x  6 zhangjc zhangjc   4096 6月  23 17:06 urllib3\n    drwxr-xr-x  2 zhangjc zhangjc   4096 6月  23 17:06 urllib3-1.25.9.dist-info","slug":"Hue-编译异常：ImportError-cannot-import-name-six-from-urllib3-packages","published":1,"updated":"2021-07-19T16:28:00.208Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphnc004litd32qq03ujs","content":"<p>在编译Hue的时候出现错误信息如下：</p>\n<pre><code>Running &#39;/home/zhangjc/ysten/git/ysten-hue/build/env/bin/hue makemigrations --noinput&#39; with None\nTraceback (most recent call last):\n  File &quot;/home/zhangjc/ysten/git/ysten-hue/build/env/bin/hue&quot;, line 33, in &lt;module&gt;\n    sys.exit(load_entry_point(&#39;desktop&#39;, &#39;console_scripts&#39;, &#39;hue&#39;)())\n  File &quot;/home/zhangjc/ysten/git/ysten-hue/desktop/core/src/desktop/manage_entry.py&quot;, line 225, in entry\n    execute_from_command_line(sys.argv)\n  File &quot;/home/zhangjc/ysten/git/ysten-hue/build/env/lib/python3.7/site-packages/django/core/management/__init__.py&quot;, line 364, in execute_from_command_line\n    utility.execute()\n  File &quot;/home/zhangjc/ysten/git/ysten-hue/build/env/lib/python3.7/site-packages/django/core/management/__init__.py&quot;, line 338, in execute\n    django.setup()\n  File &quot;/home/zhangjc/ysten/git/ysten-hue/build/env/lib/python3.7/site-packages/django/__init__.py&quot;, line 27, in setup\n    apps.populate(settings.INSTALLED_APPS)\n  File &quot;/home/zhangjc/ysten/git/ysten-hue/build/env/lib/python3.7/site-packages/django/apps/registry.py&quot;, line 108, in populate\n    app_config.import_models()\n  File &quot;/home/zhangjc/ysten/git/ysten-hue/build/env/lib/python3.7/site-packages/django/apps/config.py&quot;, line 202, in import_models\n    self.models_module = import_module(models_module_name)\n  File &quot;/home/zhangjc/ysten/git/ysten-hue/build/env/lib/python3.7/importlib/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;/home/zhangjc/ysten/git/ysten-hue/desktop/core/src/desktop/models.py&quot;, line 47, in &lt;module&gt;\n    from useradmin.models import User, Group, get_organization\n  File &quot;/home/zhangjc/ysten/git/ysten-hue/apps/useradmin/src/useradmin/models.py&quot;, line 56, in &lt;module&gt;\n    from desktop.lib.connectors.models import _get_installed_connectors, Connector\n  File &quot;/home/zhangjc/ysten/git/ysten-hue/desktop/core/src/desktop/lib/connectors/models.py&quot;, line 29, in &lt;module&gt;\n    from desktop.lib.connectors.types import get_connectors_types\n  File &quot;/home/zhangjc/ysten/git/ysten-hue/desktop/core/src/desktop/lib/connectors/types.py&quot;, line 24, in &lt;module&gt;\n    from desktop.lib.exceptions_renderable import PopupException\n  File &quot;/home/zhangjc/ysten/git/ysten-hue/desktop/core/src/desktop/lib/exceptions_renderable.py&quot;, line 31, in &lt;module&gt;\n    import desktop.lib.django_util\n  File &quot;/home/zhangjc/ysten/git/ysten-hue/desktop/core/src/desktop/lib/django_util.py&quot;, line 42, in &lt;module&gt;\n    import desktop.lib.thrift_util\n  File &quot;/home/zhangjc/ysten/git/ysten-hue/desktop/core/src/desktop/lib/thrift_util.py&quot;, line 49, in &lt;module&gt;\n    from desktop.lib.thrift_.http_client import THttpClient\n  File &quot;/home/zhangjc/ysten/git/ysten-hue/desktop/core/src/desktop/lib/thrift_/http_client.py&quot;, line 26, in &lt;module&gt;\n    from desktop.lib.rest.http_client import HttpClient\n  File &quot;/home/zhangjc/ysten/git/ysten-hue/desktop/core/src/desktop/lib/rest/http_client.py&quot;, line 32, in &lt;module&gt;\n    from urllib3.contrib import pyopenssl\n  File &quot;/usr/lib/python3/dist-packages/urllib3/contrib/pyopenssl.py&quot;, line 62, in &lt;module&gt;\n    from ..packages import six\nImportError: cannot import name &#39;six&#39; from &#39;urllib3.packages&#39; (/usr/lib/python3/dist-packages/urllib3/packages/__init__.py)\n</code></pre>\n<p>错误原因：根据错误信息可以看到是“/usr/lib/python3/dist-packages/urllib3”这个包的问题。正确的应该是引用“build/env/lib/python3.7/site-packages/”下的包。查看”/usr/lib/python3/dist-packages/urllib3”下的包信息，发现包版本比较低。</p>\n<pre><code>$ ls -l /usr/lib/python3/dist-packages/\ndrwxr-xr-x  6 root root   4096 1月  13 17:07 urllib3\ndrwxr-xr-x  2 root root   4096 1月  13 17:06 urllib3-1.22.egg-info\n</code></pre>\n<p>解决方法：删除有问题的包后再试成功。</p>\n<pre><code>$ sudo rm -rf /usr/lib/python3/dist-packages/urllib3*\n</code></pre>\n<p>查看“build/env/lib/python3.7/site-packages/”下的urllib3包信息：</p>\n<pre><code>$ ls -l build/env/lib/python3.7/site-packages/|grep urllib3\ndrwxr-xr-x  6 zhangjc zhangjc   4096 6月  23 17:06 urllib3\ndrwxr-xr-x  2 zhangjc zhangjc   4096 6月  23 17:06 urllib3-1.25.9.dist-info\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>在编译Hue的时候出现错误信息如下：</p>\n<pre><code>Running &#39;/home/zhangjc/ysten/git/ysten-hue/build/env/bin/hue makemigrations --noinput&#39; with None\nTraceback (most recent call last):\n  File &quot;/home/zhangjc/ysten/git/ysten-hue/build/env/bin/hue&quot;, line 33, in &lt;module&gt;\n    sys.exit(load_entry_point(&#39;desktop&#39;, &#39;console_scripts&#39;, &#39;hue&#39;)())\n  File &quot;/home/zhangjc/ysten/git/ysten-hue/desktop/core/src/desktop/manage_entry.py&quot;, line 225, in entry\n    execute_from_command_line(sys.argv)\n  File &quot;/home/zhangjc/ysten/git/ysten-hue/build/env/lib/python3.7/site-packages/django/core/management/__init__.py&quot;, line 364, in execute_from_command_line\n    utility.execute()\n  File &quot;/home/zhangjc/ysten/git/ysten-hue/build/env/lib/python3.7/site-packages/django/core/management/__init__.py&quot;, line 338, in execute\n    django.setup()\n  File &quot;/home/zhangjc/ysten/git/ysten-hue/build/env/lib/python3.7/site-packages/django/__init__.py&quot;, line 27, in setup\n    apps.populate(settings.INSTALLED_APPS)\n  File &quot;/home/zhangjc/ysten/git/ysten-hue/build/env/lib/python3.7/site-packages/django/apps/registry.py&quot;, line 108, in populate\n    app_config.import_models()\n  File &quot;/home/zhangjc/ysten/git/ysten-hue/build/env/lib/python3.7/site-packages/django/apps/config.py&quot;, line 202, in import_models\n    self.models_module = import_module(models_module_name)\n  File &quot;/home/zhangjc/ysten/git/ysten-hue/build/env/lib/python3.7/importlib/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;/home/zhangjc/ysten/git/ysten-hue/desktop/core/src/desktop/models.py&quot;, line 47, in &lt;module&gt;\n    from useradmin.models import User, Group, get_organization\n  File &quot;/home/zhangjc/ysten/git/ysten-hue/apps/useradmin/src/useradmin/models.py&quot;, line 56, in &lt;module&gt;\n    from desktop.lib.connectors.models import _get_installed_connectors, Connector\n  File &quot;/home/zhangjc/ysten/git/ysten-hue/desktop/core/src/desktop/lib/connectors/models.py&quot;, line 29, in &lt;module&gt;\n    from desktop.lib.connectors.types import get_connectors_types\n  File &quot;/home/zhangjc/ysten/git/ysten-hue/desktop/core/src/desktop/lib/connectors/types.py&quot;, line 24, in &lt;module&gt;\n    from desktop.lib.exceptions_renderable import PopupException\n  File &quot;/home/zhangjc/ysten/git/ysten-hue/desktop/core/src/desktop/lib/exceptions_renderable.py&quot;, line 31, in &lt;module&gt;\n    import desktop.lib.django_util\n  File &quot;/home/zhangjc/ysten/git/ysten-hue/desktop/core/src/desktop/lib/django_util.py&quot;, line 42, in &lt;module&gt;\n    import desktop.lib.thrift_util\n  File &quot;/home/zhangjc/ysten/git/ysten-hue/desktop/core/src/desktop/lib/thrift_util.py&quot;, line 49, in &lt;module&gt;\n    from desktop.lib.thrift_.http_client import THttpClient\n  File &quot;/home/zhangjc/ysten/git/ysten-hue/desktop/core/src/desktop/lib/thrift_/http_client.py&quot;, line 26, in &lt;module&gt;\n    from desktop.lib.rest.http_client import HttpClient\n  File &quot;/home/zhangjc/ysten/git/ysten-hue/desktop/core/src/desktop/lib/rest/http_client.py&quot;, line 32, in &lt;module&gt;\n    from urllib3.contrib import pyopenssl\n  File &quot;/usr/lib/python3/dist-packages/urllib3/contrib/pyopenssl.py&quot;, line 62, in &lt;module&gt;\n    from ..packages import six\nImportError: cannot import name &#39;six&#39; from &#39;urllib3.packages&#39; (/usr/lib/python3/dist-packages/urllib3/packages/__init__.py)\n</code></pre>\n<p>错误原因：根据错误信息可以看到是“/usr/lib/python3/dist-packages/urllib3”这个包的问题。正确的应该是引用“build/env/lib/python3.7/site-packages/”下的包。查看”/usr/lib/python3/dist-packages/urllib3”下的包信息，发现包版本比较低。</p>\n<pre><code>$ ls -l /usr/lib/python3/dist-packages/\ndrwxr-xr-x  6 root root   4096 1月  13 17:07 urllib3\ndrwxr-xr-x  2 root root   4096 1月  13 17:06 urllib3-1.22.egg-info\n</code></pre>\n<p>解决方法：删除有问题的包后再试成功。</p>\n<pre><code>$ sudo rm -rf /usr/lib/python3/dist-packages/urllib3*\n</code></pre>\n<p>查看“build/env/lib/python3.7/site-packages/”下的urllib3包信息：</p>\n<pre><code>$ ls -l build/env/lib/python3.7/site-packages/|grep urllib3\ndrwxr-xr-x  6 zhangjc zhangjc   4096 6月  23 17:06 urllib3\ndrwxr-xr-x  2 zhangjc zhangjc   4096 6月  23 17:06 urllib3-1.25.9.dist-info\n</code></pre>\n"},{"title":"Hue安装ImportError错误一例","date":"2021-01-13T03:50:09.000Z","_content":"\n错误信息如下：\n\n\tTraceback (most recent call last):\n\t  File \"/opt/hue-4.8.0/hue/build/env/bin/hue\", line 11, in <module>\n\t    load_entry_point('desktop', 'console_scripts', 'hue')()\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/manage_entry.py\", line 225, in entry\n\t    execute_from_command_line(sys.argv)\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/core/management/__init__.py\", line 364, in execute_from_command_line\n\t    utility.execute()\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/core/management/__init__.py\", line 338, in execute\n\t    django.setup()\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/__init__.py\", line 27, in setup\n\t    apps.populate(settings.INSTALLED_APPS)\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/apps/registry.py\", line 108, in populate\n\t    app_config.import_models()\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/apps/config.py\", line 202, in import_models\n\t    self.models_module = import_module(models_module_name)\n\t  File \"/usr/lib64/python2.7/importlib/__init__.py\", line 37, in import_module\n\t    __import__(name)\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/models.py\", line 47, in <module>\n\t    from useradmin.models import User, Group, get_organization\n\t  File \"/opt/hue-4.8.0/hue/apps/useradmin/src/useradmin/models.py\", line 56, in <module>\n\t    from desktop.lib.connectors.models import _get_installed_connectors, Connector\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/connectors/models.py\", line 29, in <module>\n\t    from desktop.lib.connectors.types import get_connectors_types\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/connectors/types.py\", line 24, in <module>\n\t    from desktop.lib.exceptions_renderable import PopupException\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/exceptions_renderable.py\", line 31, in <module>\n\t    import desktop.lib.django_util\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/django_util.py\", line 42, in <module>\n\t    import desktop.lib.thrift_util\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/thrift_util.py\", line 49, in <module>\n\t    from desktop.lib.thrift_.http_client import THttpClient\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/thrift_/http_client.py\", line 26, in <module>\n\t    from desktop.lib.rest.http_client import HttpClient\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/rest/http_client.py\", line 32, in <module>\n\t    from urllib3.contrib import pyopenssl\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/urllib3-1.25.8-py2.7.egg/urllib3/contrib/pyopenssl.py\", line 46, in <module>\n\t    import OpenSSL.SSL\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/pyOpenSSL-17.5.0-py2.7.egg/OpenSSL/__init__.py\", line 8, in <module>\n\t    from OpenSSL import crypto, SSL\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/pyOpenSSL-17.5.0-py2.7.egg/OpenSSL/crypto.py\", line 16, in <module>\n\t    from OpenSSL._util import (\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/pyOpenSSL-17.5.0-py2.7.egg/OpenSSL/_util.py\", line 6, in <module>\n\t    from cryptography.hazmat.bindings.openssl.binding import Binding\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/cryptography-2.9-py2.7-linux-x86_64.egg/cryptography/hazmat/bindings/openssl/binding.py\", line 16, in <module>\n\t    from cryptography.hazmat.bindings._openssl import ffi, lib\n\tImportError: /opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/cryptography-2.9-py2.7-linux-x86_64.egg/cryptography/hazmat/bindings/_openssl.so: undefined symbol: RSA_set0_factors\n\tRunning '/opt/hue-4.8.0/hue/build/env/bin/hue migrate --fake-initial' with None\n\tTraceback (most recent call last):\n\t  File \"/opt/hue-4.8.0/hue/build/env/bin/hue\", line 11, in <module>\n\t    load_entry_point('desktop', 'console_scripts', 'hue')()\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/manage_entry.py\", line 225, in entry\n\t    execute_from_command_line(sys.argv)\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/core/management/__init__.py\", line 364, in execute_from_command_line\n\t    utility.execute()\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/core/management/__init__.py\", line 338, in execute\n\t    django.setup()\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/__init__.py\", line 27, in setup\n\t    apps.populate(settings.INSTALLED_APPS)\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/apps/registry.py\", line 108, in populate\n\t    app_config.import_models()\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/apps/config.py\", line 202, in import_models\n\t    self.models_module = import_module(models_module_name)\n\t  File \"/usr/lib64/python2.7/importlib/__init__.py\", line 37, in import_module\n\t    __import__(name)\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/models.py\", line 47, in <module>\n\t    from useradmin.models import User, Group, get_organization\n\t  File \"/opt/hue-4.8.0/hue/apps/useradmin/src/useradmin/models.py\", line 56, in <module>\n\t    from desktop.lib.connectors.models import _get_installed_connectors, Connector\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/connectors/models.py\", line 29, in <module>\n\t    from desktop.lib.connectors.types import get_connectors_types\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/connectors/types.py\", line 24, in <module>\n\t    from desktop.lib.exceptions_renderable import PopupException\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/exceptions_renderable.py\", line 31, in <module>\n\t    import desktop.lib.django_util\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/django_util.py\", line 42, in <module>\n\t    import desktop.lib.thrift_util\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/thrift_util.py\", line 49, in <module>\n\t    from desktop.lib.thrift_.http_client import THttpClient\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/thrift_/http_client.py\", line 26, in <module>\n\t    from desktop.lib.rest.http_client import HttpClient\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/rest/http_client.py\", line 32, in <module>\n\t    from urllib3.contrib import pyopenssl\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/urllib3-1.25.8-py2.7.egg/urllib3/contrib/pyopenssl.py\", line 46, in <module>\n\t    import OpenSSL.SSL\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/pyOpenSSL-17.5.0-py2.7.egg/OpenSSL/__init__.py\", line 8, in <module>\n\t    from OpenSSL import crypto, SSL\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/pyOpenSSL-17.5.0-py2.7.egg/OpenSSL/crypto.py\", line 16, in <module>\n\t    from OpenSSL._util import (\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/pyOpenSSL-17.5.0-py2.7.egg/OpenSSL/_util.py\", line 6, in <module>\n\t    from cryptography.hazmat.bindings.openssl.binding import Binding\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/cryptography-2.9-py2.7-linux-x86_64.egg/cryptography/hazmat/bindings/openssl/binding.py\", line 16, in <module>\n\t    from cryptography.hazmat.bindings._openssl import ffi, lib\n\tImportError: /opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/cryptography-2.9-py2.7-linux-x86_64.egg/cryptography/hazmat/bindings/_openssl.so: undefined symbol: RSA_set0_factors\n\tRunning '/opt/hue-4.8.0/hue/build/env/bin/hue collectstatic --noinput' with None\n\tTraceback (most recent call last):\n\t  File \"/opt/hue-4.8.0/hue/build/env/bin/hue\", line 11, in <module>\n\t    load_entry_point('desktop', 'console_scripts', 'hue')()\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/manage_entry.py\", line 225, in entry\n\t    execute_from_command_line(sys.argv)\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/core/management/__init__.py\", line 364, in execute_from_command_line\n\t    utility.execute()\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/core/management/__init__.py\", line 338, in execute\n\t    django.setup()\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/__init__.py\", line 27, in setup\n\t    apps.populate(settings.INSTALLED_APPS)\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/apps/registry.py\", line 108, in populate\n\t    app_config.import_models()\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/apps/config.py\", line 202, in import_models\n\t    self.models_module = import_module(models_module_name)\n\t  File \"/usr/lib64/python2.7/importlib/__init__.py\", line 37, in import_module\n\t    __import__(name)\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/models.py\", line 47, in <module>\n\t    from useradmin.models import User, Group, get_organization\n\t  File \"/opt/hue-4.8.0/hue/apps/useradmin/src/useradmin/models.py\", line 56, in <module>\n\t    from desktop.lib.connectors.models import _get_installed_connectors, Connector\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/connectors/models.py\", line 29, in <module>\n\t    from desktop.lib.connectors.types import get_connectors_types\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/connectors/types.py\", line 24, in <module>\n\t    from desktop.lib.exceptions_renderable import PopupException\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/exceptions_renderable.py\", line 31, in <module>\n\t    import desktop.lib.django_util\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/django_util.py\", line 42, in <module>\n\t    import desktop.lib.thrift_util\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/thrift_util.py\", line 49, in <module>\n\t    from desktop.lib.thrift_.http_client import THttpClient\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/thrift_/http_client.py\", line 26, in <module>\n\t    from desktop.lib.rest.http_client import HttpClient\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/rest/http_client.py\", line 32, in <module>\n\t    from urllib3.contrib import pyopenssl\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/urllib3-1.25.8-py2.7.egg/urllib3/contrib/pyopenssl.py\", line 46, in <module>\n\t    import OpenSSL.SSL\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/pyOpenSSL-17.5.0-py2.7.egg/OpenSSL/__init__.py\", line 8, in <module>\n\t    from OpenSSL import crypto, SSL\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/pyOpenSSL-17.5.0-py2.7.egg/OpenSSL/crypto.py\", line 16, in <module>\n\t    from OpenSSL._util import (\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/pyOpenSSL-17.5.0-py2.7.egg/OpenSSL/_util.py\", line 6, in <module>\n\t    from cryptography.hazmat.bindings.openssl.binding import Binding\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/cryptography-2.9-py2.7-linux-x86_64.egg/cryptography/hazmat/bindings/openssl/binding.py\", line 16, in <module>\n\t    from cryptography.hazmat.bindings._openssl import ffi, lib\n\tImportError: /opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/cryptography-2.9-py2.7-linux-x86_64.egg/cryptography/hazmat/bindings/_openssl.so: undefined symbol: RSA_set0_factors\n\t\n错误原因是系统中有多个OpenSSL的版本，因为自己编译安装的版本未创建对应的软连接，导致编译时查找的版本不正确。通过以下方式解决：\n\n\t# ln -s /usr/local/openssl/lib/libssl.so.1.1 /usr/lib64/libssl.so.1.1\n\t# ln -s /usr/local/openssl/lib/libcrypto.so.1.1 /usr/lib64/libcrypto.so.1.1\n\t# export LDFLAGS=\"-L/usr/local/openssl/lib\"\n\t# export CPPFLAGS=\"-I/usr/local/openssl/include\"\n\t# export PKG_CONFIG_PATH=\"/usr/local/openssl/lib/pkgconfig\"\n\t\n完成后再次编译安装成功。对于OpenSSL的处理可以参见我的另外一篇博文：[CentOS 6.8 安装 Python3 Could not build the ssl module](http://zhang-jc.github.io/2018/11/27/CentOS-6-8-%E5%AE%89%E8%A3%85-Python3-Could-not-build-the-ssl-module/)","source":"_posts/Hue安装ImportError错误一例.md","raw":"title: Hue安装ImportError错误一例\ndate: 2021-01-13 11:50:09\ntags:\n- Hue\n- Linux\ncategories:\n- 大数据\n- Hue\n---\n\n错误信息如下：\n\n\tTraceback (most recent call last):\n\t  File \"/opt/hue-4.8.0/hue/build/env/bin/hue\", line 11, in <module>\n\t    load_entry_point('desktop', 'console_scripts', 'hue')()\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/manage_entry.py\", line 225, in entry\n\t    execute_from_command_line(sys.argv)\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/core/management/__init__.py\", line 364, in execute_from_command_line\n\t    utility.execute()\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/core/management/__init__.py\", line 338, in execute\n\t    django.setup()\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/__init__.py\", line 27, in setup\n\t    apps.populate(settings.INSTALLED_APPS)\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/apps/registry.py\", line 108, in populate\n\t    app_config.import_models()\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/apps/config.py\", line 202, in import_models\n\t    self.models_module = import_module(models_module_name)\n\t  File \"/usr/lib64/python2.7/importlib/__init__.py\", line 37, in import_module\n\t    __import__(name)\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/models.py\", line 47, in <module>\n\t    from useradmin.models import User, Group, get_organization\n\t  File \"/opt/hue-4.8.0/hue/apps/useradmin/src/useradmin/models.py\", line 56, in <module>\n\t    from desktop.lib.connectors.models import _get_installed_connectors, Connector\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/connectors/models.py\", line 29, in <module>\n\t    from desktop.lib.connectors.types import get_connectors_types\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/connectors/types.py\", line 24, in <module>\n\t    from desktop.lib.exceptions_renderable import PopupException\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/exceptions_renderable.py\", line 31, in <module>\n\t    import desktop.lib.django_util\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/django_util.py\", line 42, in <module>\n\t    import desktop.lib.thrift_util\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/thrift_util.py\", line 49, in <module>\n\t    from desktop.lib.thrift_.http_client import THttpClient\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/thrift_/http_client.py\", line 26, in <module>\n\t    from desktop.lib.rest.http_client import HttpClient\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/rest/http_client.py\", line 32, in <module>\n\t    from urllib3.contrib import pyopenssl\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/urllib3-1.25.8-py2.7.egg/urllib3/contrib/pyopenssl.py\", line 46, in <module>\n\t    import OpenSSL.SSL\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/pyOpenSSL-17.5.0-py2.7.egg/OpenSSL/__init__.py\", line 8, in <module>\n\t    from OpenSSL import crypto, SSL\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/pyOpenSSL-17.5.0-py2.7.egg/OpenSSL/crypto.py\", line 16, in <module>\n\t    from OpenSSL._util import (\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/pyOpenSSL-17.5.0-py2.7.egg/OpenSSL/_util.py\", line 6, in <module>\n\t    from cryptography.hazmat.bindings.openssl.binding import Binding\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/cryptography-2.9-py2.7-linux-x86_64.egg/cryptography/hazmat/bindings/openssl/binding.py\", line 16, in <module>\n\t    from cryptography.hazmat.bindings._openssl import ffi, lib\n\tImportError: /opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/cryptography-2.9-py2.7-linux-x86_64.egg/cryptography/hazmat/bindings/_openssl.so: undefined symbol: RSA_set0_factors\n\tRunning '/opt/hue-4.8.0/hue/build/env/bin/hue migrate --fake-initial' with None\n\tTraceback (most recent call last):\n\t  File \"/opt/hue-4.8.0/hue/build/env/bin/hue\", line 11, in <module>\n\t    load_entry_point('desktop', 'console_scripts', 'hue')()\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/manage_entry.py\", line 225, in entry\n\t    execute_from_command_line(sys.argv)\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/core/management/__init__.py\", line 364, in execute_from_command_line\n\t    utility.execute()\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/core/management/__init__.py\", line 338, in execute\n\t    django.setup()\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/__init__.py\", line 27, in setup\n\t    apps.populate(settings.INSTALLED_APPS)\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/apps/registry.py\", line 108, in populate\n\t    app_config.import_models()\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/apps/config.py\", line 202, in import_models\n\t    self.models_module = import_module(models_module_name)\n\t  File \"/usr/lib64/python2.7/importlib/__init__.py\", line 37, in import_module\n\t    __import__(name)\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/models.py\", line 47, in <module>\n\t    from useradmin.models import User, Group, get_organization\n\t  File \"/opt/hue-4.8.0/hue/apps/useradmin/src/useradmin/models.py\", line 56, in <module>\n\t    from desktop.lib.connectors.models import _get_installed_connectors, Connector\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/connectors/models.py\", line 29, in <module>\n\t    from desktop.lib.connectors.types import get_connectors_types\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/connectors/types.py\", line 24, in <module>\n\t    from desktop.lib.exceptions_renderable import PopupException\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/exceptions_renderable.py\", line 31, in <module>\n\t    import desktop.lib.django_util\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/django_util.py\", line 42, in <module>\n\t    import desktop.lib.thrift_util\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/thrift_util.py\", line 49, in <module>\n\t    from desktop.lib.thrift_.http_client import THttpClient\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/thrift_/http_client.py\", line 26, in <module>\n\t    from desktop.lib.rest.http_client import HttpClient\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/rest/http_client.py\", line 32, in <module>\n\t    from urllib3.contrib import pyopenssl\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/urllib3-1.25.8-py2.7.egg/urllib3/contrib/pyopenssl.py\", line 46, in <module>\n\t    import OpenSSL.SSL\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/pyOpenSSL-17.5.0-py2.7.egg/OpenSSL/__init__.py\", line 8, in <module>\n\t    from OpenSSL import crypto, SSL\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/pyOpenSSL-17.5.0-py2.7.egg/OpenSSL/crypto.py\", line 16, in <module>\n\t    from OpenSSL._util import (\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/pyOpenSSL-17.5.0-py2.7.egg/OpenSSL/_util.py\", line 6, in <module>\n\t    from cryptography.hazmat.bindings.openssl.binding import Binding\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/cryptography-2.9-py2.7-linux-x86_64.egg/cryptography/hazmat/bindings/openssl/binding.py\", line 16, in <module>\n\t    from cryptography.hazmat.bindings._openssl import ffi, lib\n\tImportError: /opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/cryptography-2.9-py2.7-linux-x86_64.egg/cryptography/hazmat/bindings/_openssl.so: undefined symbol: RSA_set0_factors\n\tRunning '/opt/hue-4.8.0/hue/build/env/bin/hue collectstatic --noinput' with None\n\tTraceback (most recent call last):\n\t  File \"/opt/hue-4.8.0/hue/build/env/bin/hue\", line 11, in <module>\n\t    load_entry_point('desktop', 'console_scripts', 'hue')()\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/manage_entry.py\", line 225, in entry\n\t    execute_from_command_line(sys.argv)\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/core/management/__init__.py\", line 364, in execute_from_command_line\n\t    utility.execute()\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/core/management/__init__.py\", line 338, in execute\n\t    django.setup()\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/__init__.py\", line 27, in setup\n\t    apps.populate(settings.INSTALLED_APPS)\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/apps/registry.py\", line 108, in populate\n\t    app_config.import_models()\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/apps/config.py\", line 202, in import_models\n\t    self.models_module = import_module(models_module_name)\n\t  File \"/usr/lib64/python2.7/importlib/__init__.py\", line 37, in import_module\n\t    __import__(name)\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/models.py\", line 47, in <module>\n\t    from useradmin.models import User, Group, get_organization\n\t  File \"/opt/hue-4.8.0/hue/apps/useradmin/src/useradmin/models.py\", line 56, in <module>\n\t    from desktop.lib.connectors.models import _get_installed_connectors, Connector\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/connectors/models.py\", line 29, in <module>\n\t    from desktop.lib.connectors.types import get_connectors_types\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/connectors/types.py\", line 24, in <module>\n\t    from desktop.lib.exceptions_renderable import PopupException\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/exceptions_renderable.py\", line 31, in <module>\n\t    import desktop.lib.django_util\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/django_util.py\", line 42, in <module>\n\t    import desktop.lib.thrift_util\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/thrift_util.py\", line 49, in <module>\n\t    from desktop.lib.thrift_.http_client import THttpClient\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/thrift_/http_client.py\", line 26, in <module>\n\t    from desktop.lib.rest.http_client import HttpClient\n\t  File \"/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/rest/http_client.py\", line 32, in <module>\n\t    from urllib3.contrib import pyopenssl\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/urllib3-1.25.8-py2.7.egg/urllib3/contrib/pyopenssl.py\", line 46, in <module>\n\t    import OpenSSL.SSL\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/pyOpenSSL-17.5.0-py2.7.egg/OpenSSL/__init__.py\", line 8, in <module>\n\t    from OpenSSL import crypto, SSL\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/pyOpenSSL-17.5.0-py2.7.egg/OpenSSL/crypto.py\", line 16, in <module>\n\t    from OpenSSL._util import (\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/pyOpenSSL-17.5.0-py2.7.egg/OpenSSL/_util.py\", line 6, in <module>\n\t    from cryptography.hazmat.bindings.openssl.binding import Binding\n\t  File \"/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/cryptography-2.9-py2.7-linux-x86_64.egg/cryptography/hazmat/bindings/openssl/binding.py\", line 16, in <module>\n\t    from cryptography.hazmat.bindings._openssl import ffi, lib\n\tImportError: /opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/cryptography-2.9-py2.7-linux-x86_64.egg/cryptography/hazmat/bindings/_openssl.so: undefined symbol: RSA_set0_factors\n\t\n错误原因是系统中有多个OpenSSL的版本，因为自己编译安装的版本未创建对应的软连接，导致编译时查找的版本不正确。通过以下方式解决：\n\n\t# ln -s /usr/local/openssl/lib/libssl.so.1.1 /usr/lib64/libssl.so.1.1\n\t# ln -s /usr/local/openssl/lib/libcrypto.so.1.1 /usr/lib64/libcrypto.so.1.1\n\t# export LDFLAGS=\"-L/usr/local/openssl/lib\"\n\t# export CPPFLAGS=\"-I/usr/local/openssl/include\"\n\t# export PKG_CONFIG_PATH=\"/usr/local/openssl/lib/pkgconfig\"\n\t\n完成后再次编译安装成功。对于OpenSSL的处理可以参见我的另外一篇博文：[CentOS 6.8 安装 Python3 Could not build the ssl module](http://zhang-jc.github.io/2018/11/27/CentOS-6-8-%E5%AE%89%E8%A3%85-Python3-Could-not-build-the-ssl-module/)","slug":"Hue安装ImportError错误一例","published":1,"updated":"2021-07-19T16:28:00.100Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphnd004pitd393aq1oyr","content":"<p>错误信息如下：</p>\n<pre><code>Traceback (most recent call last):\n  File &quot;/opt/hue-4.8.0/hue/build/env/bin/hue&quot;, line 11, in &lt;module&gt;\n    load_entry_point(&#39;desktop&#39;, &#39;console_scripts&#39;, &#39;hue&#39;)()\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/manage_entry.py&quot;, line 225, in entry\n    execute_from_command_line(sys.argv)\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/core/management/__init__.py&quot;, line 364, in execute_from_command_line\n    utility.execute()\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/core/management/__init__.py&quot;, line 338, in execute\n    django.setup()\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/__init__.py&quot;, line 27, in setup\n    apps.populate(settings.INSTALLED_APPS)\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/apps/registry.py&quot;, line 108, in populate\n    app_config.import_models()\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/apps/config.py&quot;, line 202, in import_models\n    self.models_module = import_module(models_module_name)\n  File &quot;/usr/lib64/python2.7/importlib/__init__.py&quot;, line 37, in import_module\n    __import__(name)\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/models.py&quot;, line 47, in &lt;module&gt;\n    from useradmin.models import User, Group, get_organization\n  File &quot;/opt/hue-4.8.0/hue/apps/useradmin/src/useradmin/models.py&quot;, line 56, in &lt;module&gt;\n    from desktop.lib.connectors.models import _get_installed_connectors, Connector\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/connectors/models.py&quot;, line 29, in &lt;module&gt;\n    from desktop.lib.connectors.types import get_connectors_types\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/connectors/types.py&quot;, line 24, in &lt;module&gt;\n    from desktop.lib.exceptions_renderable import PopupException\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/exceptions_renderable.py&quot;, line 31, in &lt;module&gt;\n    import desktop.lib.django_util\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/django_util.py&quot;, line 42, in &lt;module&gt;\n    import desktop.lib.thrift_util\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/thrift_util.py&quot;, line 49, in &lt;module&gt;\n    from desktop.lib.thrift_.http_client import THttpClient\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/thrift_/http_client.py&quot;, line 26, in &lt;module&gt;\n    from desktop.lib.rest.http_client import HttpClient\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/rest/http_client.py&quot;, line 32, in &lt;module&gt;\n    from urllib3.contrib import pyopenssl\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/urllib3-1.25.8-py2.7.egg/urllib3/contrib/pyopenssl.py&quot;, line 46, in &lt;module&gt;\n    import OpenSSL.SSL\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/pyOpenSSL-17.5.0-py2.7.egg/OpenSSL/__init__.py&quot;, line 8, in &lt;module&gt;\n    from OpenSSL import crypto, SSL\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/pyOpenSSL-17.5.0-py2.7.egg/OpenSSL/crypto.py&quot;, line 16, in &lt;module&gt;\n    from OpenSSL._util import (\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/pyOpenSSL-17.5.0-py2.7.egg/OpenSSL/_util.py&quot;, line 6, in &lt;module&gt;\n    from cryptography.hazmat.bindings.openssl.binding import Binding\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/cryptography-2.9-py2.7-linux-x86_64.egg/cryptography/hazmat/bindings/openssl/binding.py&quot;, line 16, in &lt;module&gt;\n    from cryptography.hazmat.bindings._openssl import ffi, lib\nImportError: /opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/cryptography-2.9-py2.7-linux-x86_64.egg/cryptography/hazmat/bindings/_openssl.so: undefined symbol: RSA_set0_factors\nRunning &#39;/opt/hue-4.8.0/hue/build/env/bin/hue migrate --fake-initial&#39; with None\nTraceback (most recent call last):\n  File &quot;/opt/hue-4.8.0/hue/build/env/bin/hue&quot;, line 11, in &lt;module&gt;\n    load_entry_point(&#39;desktop&#39;, &#39;console_scripts&#39;, &#39;hue&#39;)()\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/manage_entry.py&quot;, line 225, in entry\n    execute_from_command_line(sys.argv)\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/core/management/__init__.py&quot;, line 364, in execute_from_command_line\n    utility.execute()\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/core/management/__init__.py&quot;, line 338, in execute\n    django.setup()\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/__init__.py&quot;, line 27, in setup\n    apps.populate(settings.INSTALLED_APPS)\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/apps/registry.py&quot;, line 108, in populate\n    app_config.import_models()\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/apps/config.py&quot;, line 202, in import_models\n    self.models_module = import_module(models_module_name)\n  File &quot;/usr/lib64/python2.7/importlib/__init__.py&quot;, line 37, in import_module\n    __import__(name)\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/models.py&quot;, line 47, in &lt;module&gt;\n    from useradmin.models import User, Group, get_organization\n  File &quot;/opt/hue-4.8.0/hue/apps/useradmin/src/useradmin/models.py&quot;, line 56, in &lt;module&gt;\n    from desktop.lib.connectors.models import _get_installed_connectors, Connector\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/connectors/models.py&quot;, line 29, in &lt;module&gt;\n    from desktop.lib.connectors.types import get_connectors_types\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/connectors/types.py&quot;, line 24, in &lt;module&gt;\n    from desktop.lib.exceptions_renderable import PopupException\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/exceptions_renderable.py&quot;, line 31, in &lt;module&gt;\n    import desktop.lib.django_util\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/django_util.py&quot;, line 42, in &lt;module&gt;\n    import desktop.lib.thrift_util\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/thrift_util.py&quot;, line 49, in &lt;module&gt;\n    from desktop.lib.thrift_.http_client import THttpClient\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/thrift_/http_client.py&quot;, line 26, in &lt;module&gt;\n    from desktop.lib.rest.http_client import HttpClient\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/rest/http_client.py&quot;, line 32, in &lt;module&gt;\n    from urllib3.contrib import pyopenssl\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/urllib3-1.25.8-py2.7.egg/urllib3/contrib/pyopenssl.py&quot;, line 46, in &lt;module&gt;\n    import OpenSSL.SSL\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/pyOpenSSL-17.5.0-py2.7.egg/OpenSSL/__init__.py&quot;, line 8, in &lt;module&gt;\n    from OpenSSL import crypto, SSL\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/pyOpenSSL-17.5.0-py2.7.egg/OpenSSL/crypto.py&quot;, line 16, in &lt;module&gt;\n    from OpenSSL._util import (\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/pyOpenSSL-17.5.0-py2.7.egg/OpenSSL/_util.py&quot;, line 6, in &lt;module&gt;\n    from cryptography.hazmat.bindings.openssl.binding import Binding\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/cryptography-2.9-py2.7-linux-x86_64.egg/cryptography/hazmat/bindings/openssl/binding.py&quot;, line 16, in &lt;module&gt;\n    from cryptography.hazmat.bindings._openssl import ffi, lib\nImportError: /opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/cryptography-2.9-py2.7-linux-x86_64.egg/cryptography/hazmat/bindings/_openssl.so: undefined symbol: RSA_set0_factors\nRunning &#39;/opt/hue-4.8.0/hue/build/env/bin/hue collectstatic --noinput&#39; with None\nTraceback (most recent call last):\n  File &quot;/opt/hue-4.8.0/hue/build/env/bin/hue&quot;, line 11, in &lt;module&gt;\n    load_entry_point(&#39;desktop&#39;, &#39;console_scripts&#39;, &#39;hue&#39;)()\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/manage_entry.py&quot;, line 225, in entry\n    execute_from_command_line(sys.argv)\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/core/management/__init__.py&quot;, line 364, in execute_from_command_line\n    utility.execute()\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/core/management/__init__.py&quot;, line 338, in execute\n    django.setup()\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/__init__.py&quot;, line 27, in setup\n    apps.populate(settings.INSTALLED_APPS)\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/apps/registry.py&quot;, line 108, in populate\n    app_config.import_models()\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/apps/config.py&quot;, line 202, in import_models\n    self.models_module = import_module(models_module_name)\n  File &quot;/usr/lib64/python2.7/importlib/__init__.py&quot;, line 37, in import_module\n    __import__(name)\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/models.py&quot;, line 47, in &lt;module&gt;\n    from useradmin.models import User, Group, get_organization\n  File &quot;/opt/hue-4.8.0/hue/apps/useradmin/src/useradmin/models.py&quot;, line 56, in &lt;module&gt;\n    from desktop.lib.connectors.models import _get_installed_connectors, Connector\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/connectors/models.py&quot;, line 29, in &lt;module&gt;\n    from desktop.lib.connectors.types import get_connectors_types\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/connectors/types.py&quot;, line 24, in &lt;module&gt;\n    from desktop.lib.exceptions_renderable import PopupException\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/exceptions_renderable.py&quot;, line 31, in &lt;module&gt;\n    import desktop.lib.django_util\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/django_util.py&quot;, line 42, in &lt;module&gt;\n    import desktop.lib.thrift_util\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/thrift_util.py&quot;, line 49, in &lt;module&gt;\n    from desktop.lib.thrift_.http_client import THttpClient\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/thrift_/http_client.py&quot;, line 26, in &lt;module&gt;\n    from desktop.lib.rest.http_client import HttpClient\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/rest/http_client.py&quot;, line 32, in &lt;module&gt;\n    from urllib3.contrib import pyopenssl\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/urllib3-1.25.8-py2.7.egg/urllib3/contrib/pyopenssl.py&quot;, line 46, in &lt;module&gt;\n    import OpenSSL.SSL\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/pyOpenSSL-17.5.0-py2.7.egg/OpenSSL/__init__.py&quot;, line 8, in &lt;module&gt;\n    from OpenSSL import crypto, SSL\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/pyOpenSSL-17.5.0-py2.7.egg/OpenSSL/crypto.py&quot;, line 16, in &lt;module&gt;\n    from OpenSSL._util import (\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/pyOpenSSL-17.5.0-py2.7.egg/OpenSSL/_util.py&quot;, line 6, in &lt;module&gt;\n    from cryptography.hazmat.bindings.openssl.binding import Binding\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/cryptography-2.9-py2.7-linux-x86_64.egg/cryptography/hazmat/bindings/openssl/binding.py&quot;, line 16, in &lt;module&gt;\n    from cryptography.hazmat.bindings._openssl import ffi, lib\nImportError: /opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/cryptography-2.9-py2.7-linux-x86_64.egg/cryptography/hazmat/bindings/_openssl.so: undefined symbol: RSA_set0_factors\n</code></pre>\n<p>错误原因是系统中有多个OpenSSL的版本，因为自己编译安装的版本未创建对应的软连接，导致编译时查找的版本不正确。通过以下方式解决：</p>\n<pre><code># ln -s /usr/local/openssl/lib/libssl.so.1.1 /usr/lib64/libssl.so.1.1\n# ln -s /usr/local/openssl/lib/libcrypto.so.1.1 /usr/lib64/libcrypto.so.1.1\n# export LDFLAGS=&quot;-L/usr/local/openssl/lib&quot;\n# export CPPFLAGS=&quot;-I/usr/local/openssl/include&quot;\n# export PKG_CONFIG_PATH=&quot;/usr/local/openssl/lib/pkgconfig&quot;\n</code></pre>\n<p>完成后再次编译安装成功。对于OpenSSL的处理可以参见我的另外一篇博文：<a href=\"http://zhang-jc.github.io/2018/11/27/CentOS-6-8-%E5%AE%89%E8%A3%85-Python3-Could-not-build-the-ssl-module/\">CentOS 6.8 安装 Python3 Could not build the ssl module</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>错误信息如下：</p>\n<pre><code>Traceback (most recent call last):\n  File &quot;/opt/hue-4.8.0/hue/build/env/bin/hue&quot;, line 11, in &lt;module&gt;\n    load_entry_point(&#39;desktop&#39;, &#39;console_scripts&#39;, &#39;hue&#39;)()\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/manage_entry.py&quot;, line 225, in entry\n    execute_from_command_line(sys.argv)\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/core/management/__init__.py&quot;, line 364, in execute_from_command_line\n    utility.execute()\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/core/management/__init__.py&quot;, line 338, in execute\n    django.setup()\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/__init__.py&quot;, line 27, in setup\n    apps.populate(settings.INSTALLED_APPS)\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/apps/registry.py&quot;, line 108, in populate\n    app_config.import_models()\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/apps/config.py&quot;, line 202, in import_models\n    self.models_module = import_module(models_module_name)\n  File &quot;/usr/lib64/python2.7/importlib/__init__.py&quot;, line 37, in import_module\n    __import__(name)\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/models.py&quot;, line 47, in &lt;module&gt;\n    from useradmin.models import User, Group, get_organization\n  File &quot;/opt/hue-4.8.0/hue/apps/useradmin/src/useradmin/models.py&quot;, line 56, in &lt;module&gt;\n    from desktop.lib.connectors.models import _get_installed_connectors, Connector\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/connectors/models.py&quot;, line 29, in &lt;module&gt;\n    from desktop.lib.connectors.types import get_connectors_types\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/connectors/types.py&quot;, line 24, in &lt;module&gt;\n    from desktop.lib.exceptions_renderable import PopupException\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/exceptions_renderable.py&quot;, line 31, in &lt;module&gt;\n    import desktop.lib.django_util\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/django_util.py&quot;, line 42, in &lt;module&gt;\n    import desktop.lib.thrift_util\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/thrift_util.py&quot;, line 49, in &lt;module&gt;\n    from desktop.lib.thrift_.http_client import THttpClient\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/thrift_/http_client.py&quot;, line 26, in &lt;module&gt;\n    from desktop.lib.rest.http_client import HttpClient\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/rest/http_client.py&quot;, line 32, in &lt;module&gt;\n    from urllib3.contrib import pyopenssl\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/urllib3-1.25.8-py2.7.egg/urllib3/contrib/pyopenssl.py&quot;, line 46, in &lt;module&gt;\n    import OpenSSL.SSL\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/pyOpenSSL-17.5.0-py2.7.egg/OpenSSL/__init__.py&quot;, line 8, in &lt;module&gt;\n    from OpenSSL import crypto, SSL\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/pyOpenSSL-17.5.0-py2.7.egg/OpenSSL/crypto.py&quot;, line 16, in &lt;module&gt;\n    from OpenSSL._util import (\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/pyOpenSSL-17.5.0-py2.7.egg/OpenSSL/_util.py&quot;, line 6, in &lt;module&gt;\n    from cryptography.hazmat.bindings.openssl.binding import Binding\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/cryptography-2.9-py2.7-linux-x86_64.egg/cryptography/hazmat/bindings/openssl/binding.py&quot;, line 16, in &lt;module&gt;\n    from cryptography.hazmat.bindings._openssl import ffi, lib\nImportError: /opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/cryptography-2.9-py2.7-linux-x86_64.egg/cryptography/hazmat/bindings/_openssl.so: undefined symbol: RSA_set0_factors\nRunning &#39;/opt/hue-4.8.0/hue/build/env/bin/hue migrate --fake-initial&#39; with None\nTraceback (most recent call last):\n  File &quot;/opt/hue-4.8.0/hue/build/env/bin/hue&quot;, line 11, in &lt;module&gt;\n    load_entry_point(&#39;desktop&#39;, &#39;console_scripts&#39;, &#39;hue&#39;)()\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/manage_entry.py&quot;, line 225, in entry\n    execute_from_command_line(sys.argv)\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/core/management/__init__.py&quot;, line 364, in execute_from_command_line\n    utility.execute()\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/core/management/__init__.py&quot;, line 338, in execute\n    django.setup()\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/__init__.py&quot;, line 27, in setup\n    apps.populate(settings.INSTALLED_APPS)\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/apps/registry.py&quot;, line 108, in populate\n    app_config.import_models()\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/apps/config.py&quot;, line 202, in import_models\n    self.models_module = import_module(models_module_name)\n  File &quot;/usr/lib64/python2.7/importlib/__init__.py&quot;, line 37, in import_module\n    __import__(name)\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/models.py&quot;, line 47, in &lt;module&gt;\n    from useradmin.models import User, Group, get_organization\n  File &quot;/opt/hue-4.8.0/hue/apps/useradmin/src/useradmin/models.py&quot;, line 56, in &lt;module&gt;\n    from desktop.lib.connectors.models import _get_installed_connectors, Connector\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/connectors/models.py&quot;, line 29, in &lt;module&gt;\n    from desktop.lib.connectors.types import get_connectors_types\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/connectors/types.py&quot;, line 24, in &lt;module&gt;\n    from desktop.lib.exceptions_renderable import PopupException\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/exceptions_renderable.py&quot;, line 31, in &lt;module&gt;\n    import desktop.lib.django_util\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/django_util.py&quot;, line 42, in &lt;module&gt;\n    import desktop.lib.thrift_util\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/thrift_util.py&quot;, line 49, in &lt;module&gt;\n    from desktop.lib.thrift_.http_client import THttpClient\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/thrift_/http_client.py&quot;, line 26, in &lt;module&gt;\n    from desktop.lib.rest.http_client import HttpClient\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/rest/http_client.py&quot;, line 32, in &lt;module&gt;\n    from urllib3.contrib import pyopenssl\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/urllib3-1.25.8-py2.7.egg/urllib3/contrib/pyopenssl.py&quot;, line 46, in &lt;module&gt;\n    import OpenSSL.SSL\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/pyOpenSSL-17.5.0-py2.7.egg/OpenSSL/__init__.py&quot;, line 8, in &lt;module&gt;\n    from OpenSSL import crypto, SSL\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/pyOpenSSL-17.5.0-py2.7.egg/OpenSSL/crypto.py&quot;, line 16, in &lt;module&gt;\n    from OpenSSL._util import (\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/pyOpenSSL-17.5.0-py2.7.egg/OpenSSL/_util.py&quot;, line 6, in &lt;module&gt;\n    from cryptography.hazmat.bindings.openssl.binding import Binding\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/cryptography-2.9-py2.7-linux-x86_64.egg/cryptography/hazmat/bindings/openssl/binding.py&quot;, line 16, in &lt;module&gt;\n    from cryptography.hazmat.bindings._openssl import ffi, lib\nImportError: /opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/cryptography-2.9-py2.7-linux-x86_64.egg/cryptography/hazmat/bindings/_openssl.so: undefined symbol: RSA_set0_factors\nRunning &#39;/opt/hue-4.8.0/hue/build/env/bin/hue collectstatic --noinput&#39; with None\nTraceback (most recent call last):\n  File &quot;/opt/hue-4.8.0/hue/build/env/bin/hue&quot;, line 11, in &lt;module&gt;\n    load_entry_point(&#39;desktop&#39;, &#39;console_scripts&#39;, &#39;hue&#39;)()\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/manage_entry.py&quot;, line 225, in entry\n    execute_from_command_line(sys.argv)\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/core/management/__init__.py&quot;, line 364, in execute_from_command_line\n    utility.execute()\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/core/management/__init__.py&quot;, line 338, in execute\n    django.setup()\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/__init__.py&quot;, line 27, in setup\n    apps.populate(settings.INSTALLED_APPS)\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/apps/registry.py&quot;, line 108, in populate\n    app_config.import_models()\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/Django-1.11.29-py2.7.egg/django/apps/config.py&quot;, line 202, in import_models\n    self.models_module = import_module(models_module_name)\n  File &quot;/usr/lib64/python2.7/importlib/__init__.py&quot;, line 37, in import_module\n    __import__(name)\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/models.py&quot;, line 47, in &lt;module&gt;\n    from useradmin.models import User, Group, get_organization\n  File &quot;/opt/hue-4.8.0/hue/apps/useradmin/src/useradmin/models.py&quot;, line 56, in &lt;module&gt;\n    from desktop.lib.connectors.models import _get_installed_connectors, Connector\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/connectors/models.py&quot;, line 29, in &lt;module&gt;\n    from desktop.lib.connectors.types import get_connectors_types\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/connectors/types.py&quot;, line 24, in &lt;module&gt;\n    from desktop.lib.exceptions_renderable import PopupException\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/exceptions_renderable.py&quot;, line 31, in &lt;module&gt;\n    import desktop.lib.django_util\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/django_util.py&quot;, line 42, in &lt;module&gt;\n    import desktop.lib.thrift_util\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/thrift_util.py&quot;, line 49, in &lt;module&gt;\n    from desktop.lib.thrift_.http_client import THttpClient\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/thrift_/http_client.py&quot;, line 26, in &lt;module&gt;\n    from desktop.lib.rest.http_client import HttpClient\n  File &quot;/opt/hue-4.8.0/hue/desktop/core/src/desktop/lib/rest/http_client.py&quot;, line 32, in &lt;module&gt;\n    from urllib3.contrib import pyopenssl\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/urllib3-1.25.8-py2.7.egg/urllib3/contrib/pyopenssl.py&quot;, line 46, in &lt;module&gt;\n    import OpenSSL.SSL\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/pyOpenSSL-17.5.0-py2.7.egg/OpenSSL/__init__.py&quot;, line 8, in &lt;module&gt;\n    from OpenSSL import crypto, SSL\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/pyOpenSSL-17.5.0-py2.7.egg/OpenSSL/crypto.py&quot;, line 16, in &lt;module&gt;\n    from OpenSSL._util import (\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/pyOpenSSL-17.5.0-py2.7.egg/OpenSSL/_util.py&quot;, line 6, in &lt;module&gt;\n    from cryptography.hazmat.bindings.openssl.binding import Binding\n  File &quot;/opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/cryptography-2.9-py2.7-linux-x86_64.egg/cryptography/hazmat/bindings/openssl/binding.py&quot;, line 16, in &lt;module&gt;\n    from cryptography.hazmat.bindings._openssl import ffi, lib\nImportError: /opt/hue-4.8.0/hue/build/env/lib/python2.7/site-packages/cryptography-2.9-py2.7-linux-x86_64.egg/cryptography/hazmat/bindings/_openssl.so: undefined symbol: RSA_set0_factors\n</code></pre>\n<p>错误原因是系统中有多个OpenSSL的版本，因为自己编译安装的版本未创建对应的软连接，导致编译时查找的版本不正确。通过以下方式解决：</p>\n<pre><code># ln -s /usr/local/openssl/lib/libssl.so.1.1 /usr/lib64/libssl.so.1.1\n# ln -s /usr/local/openssl/lib/libcrypto.so.1.1 /usr/lib64/libcrypto.so.1.1\n# export LDFLAGS=&quot;-L/usr/local/openssl/lib&quot;\n# export CPPFLAGS=&quot;-I/usr/local/openssl/include&quot;\n# export PKG_CONFIG_PATH=&quot;/usr/local/openssl/lib/pkgconfig&quot;\n</code></pre>\n<p>完成后再次编译安装成功。对于OpenSSL的处理可以参见我的另外一篇博文：<a href=\"http://zhang-jc.github.io/2018/11/27/CentOS-6-8-%E5%AE%89%E8%A3%85-Python3-Could-not-build-the-ssl-module/\">CentOS 6.8 安装 Python3 Could not build the ssl module</a></p>\n"},{"title":"ImportError: libffi.so.5: cannot open shared object file: No such file or directory","date":"2019-10-10T05:41:47.000Z","_content":"在 CentOS 7.4 中通过源码的方式安装 Requests 时，报错信息如下：\n\n    # pip3 install .\n    Processing /data/setup/psf-requests-fab1fd1\n        ERROR: Command errored out with exit status 1:\n         command: /opt/Python-3.7.2/bin/python3.7 -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-nuwyypgj/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-nuwyypgj/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base pip-egg-info\n             cwd: /tmp/pip-req-build-nuwyypgj/\n        Complete output (11 lines):\n        Traceback (most recent call last):\n          File \"<string>\", line 1, in <module>\n          File \"/opt/Python-3.7.2/lib/python3.7/site-packages/setuptools/__init__.py\", line 18, in <module>\n            from setuptools.dist import Distribution, Feature\n          File \"/opt/Python-3.7.2/lib/python3.7/site-packages/setuptools/dist.py\", line 31, in <module>\n            from setuptools import windows_support\n          File \"/opt/Python-3.7.2/lib/python3.7/site-packages/setuptools/windows_support.py\", line 2, in <module>\n            import ctypes\n          File \"/opt/Python-3.7.2/lib/python3.7/ctypes/__init__.py\", line 7, in <module>\n            from _ctypes import Union, Structure, Array\n        ImportError: libffi.so.5: cannot open shared object file: No such file or directory\n        ----------------------------------------\n    ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n\n其实在 CentOS 7.4 下已经存在 libffi.so.6，解决方法是创建软连接，如下：\n\n    # cd /usr/lib64\n    # ln -s libffi.so.6 libffi.so.5\n    # ln -s libffi.so.6.0.1 libffi.so.5.0.1","source":"_posts/ImportError-libffi-so-5-cannot-open-shared-object-file-No-such-file-or-directory.md","raw":"title: >-\n  ImportError: libffi.so.5: cannot open shared object file: No such file or\n  directory\ndate: 2019-10-10 13:41:47\ntags:\n- Python3\n- Requests\n- CentOS\n- Linux\ncategories:\n- 工具箱\n- Requests\n---\n在 CentOS 7.4 中通过源码的方式安装 Requests 时，报错信息如下：\n\n    # pip3 install .\n    Processing /data/setup/psf-requests-fab1fd1\n        ERROR: Command errored out with exit status 1:\n         command: /opt/Python-3.7.2/bin/python3.7 -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-nuwyypgj/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-nuwyypgj/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base pip-egg-info\n             cwd: /tmp/pip-req-build-nuwyypgj/\n        Complete output (11 lines):\n        Traceback (most recent call last):\n          File \"<string>\", line 1, in <module>\n          File \"/opt/Python-3.7.2/lib/python3.7/site-packages/setuptools/__init__.py\", line 18, in <module>\n            from setuptools.dist import Distribution, Feature\n          File \"/opt/Python-3.7.2/lib/python3.7/site-packages/setuptools/dist.py\", line 31, in <module>\n            from setuptools import windows_support\n          File \"/opt/Python-3.7.2/lib/python3.7/site-packages/setuptools/windows_support.py\", line 2, in <module>\n            import ctypes\n          File \"/opt/Python-3.7.2/lib/python3.7/ctypes/__init__.py\", line 7, in <module>\n            from _ctypes import Union, Structure, Array\n        ImportError: libffi.so.5: cannot open shared object file: No such file or directory\n        ----------------------------------------\n    ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n\n其实在 CentOS 7.4 下已经存在 libffi.so.6，解决方法是创建软连接，如下：\n\n    # cd /usr/lib64\n    # ln -s libffi.so.6 libffi.so.5\n    # ln -s libffi.so.6.0.1 libffi.so.5.0.1","slug":"ImportError-libffi-so-5-cannot-open-shared-object-file-No-such-file-or-directory","published":1,"updated":"2021-07-19T16:28:00.260Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphnf004titd32cqj89lz","content":"<p>在 CentOS 7.4 中通过源码的方式安装 Requests 时，报错信息如下：</p>\n<pre><code># pip3 install .\nProcessing /data/setup/psf-requests-fab1fd1\n    ERROR: Command errored out with exit status 1:\n     command: /opt/Python-3.7.2/bin/python3.7 -c &#39;import sys, setuptools, tokenize; sys.argv[0] = &#39;&quot;&#39;&quot;&#39;/tmp/pip-req-build-nuwyypgj/setup.py&#39;&quot;&#39;&quot;&#39;; __file__=&#39;&quot;&#39;&quot;&#39;/tmp/pip-req-build-nuwyypgj/setup.py&#39;&quot;&#39;&quot;&#39;;f=getattr(tokenize, &#39;&quot;&#39;&quot;&#39;open&#39;&quot;&#39;&quot;&#39;, open)(__file__);code=f.read().replace(&#39;&quot;&#39;&quot;&#39;\\r\\n&#39;&quot;&#39;&quot;&#39;, &#39;&quot;&#39;&quot;&#39;\\n&#39;&quot;&#39;&quot;&#39;);f.close();exec(compile(code, __file__, &#39;&quot;&#39;&quot;&#39;exec&#39;&quot;&#39;&quot;&#39;))&#39; egg_info --egg-base pip-egg-info\n         cwd: /tmp/pip-req-build-nuwyypgj/\n    Complete output (11 lines):\n    Traceback (most recent call last):\n      File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;\n      File &quot;/opt/Python-3.7.2/lib/python3.7/site-packages/setuptools/__init__.py&quot;, line 18, in &lt;module&gt;\n        from setuptools.dist import Distribution, Feature\n      File &quot;/opt/Python-3.7.2/lib/python3.7/site-packages/setuptools/dist.py&quot;, line 31, in &lt;module&gt;\n        from setuptools import windows_support\n      File &quot;/opt/Python-3.7.2/lib/python3.7/site-packages/setuptools/windows_support.py&quot;, line 2, in &lt;module&gt;\n        import ctypes\n      File &quot;/opt/Python-3.7.2/lib/python3.7/ctypes/__init__.py&quot;, line 7, in &lt;module&gt;\n        from _ctypes import Union, Structure, Array\n    ImportError: libffi.so.5: cannot open shared object file: No such file or directory\n    ----------------------------------------\nERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n</code></pre>\n<p>其实在 CentOS 7.4 下已经存在 libffi.so.6，解决方法是创建软连接，如下：</p>\n<pre><code># cd /usr/lib64\n# ln -s libffi.so.6 libffi.so.5\n# ln -s libffi.so.6.0.1 libffi.so.5.0.1\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>在 CentOS 7.4 中通过源码的方式安装 Requests 时，报错信息如下：</p>\n<pre><code># pip3 install .\nProcessing /data/setup/psf-requests-fab1fd1\n    ERROR: Command errored out with exit status 1:\n     command: /opt/Python-3.7.2/bin/python3.7 -c &#39;import sys, setuptools, tokenize; sys.argv[0] = &#39;&quot;&#39;&quot;&#39;/tmp/pip-req-build-nuwyypgj/setup.py&#39;&quot;&#39;&quot;&#39;; __file__=&#39;&quot;&#39;&quot;&#39;/tmp/pip-req-build-nuwyypgj/setup.py&#39;&quot;&#39;&quot;&#39;;f=getattr(tokenize, &#39;&quot;&#39;&quot;&#39;open&#39;&quot;&#39;&quot;&#39;, open)(__file__);code=f.read().replace(&#39;&quot;&#39;&quot;&#39;\\r\\n&#39;&quot;&#39;&quot;&#39;, &#39;&quot;&#39;&quot;&#39;\\n&#39;&quot;&#39;&quot;&#39;);f.close();exec(compile(code, __file__, &#39;&quot;&#39;&quot;&#39;exec&#39;&quot;&#39;&quot;&#39;))&#39; egg_info --egg-base pip-egg-info\n         cwd: /tmp/pip-req-build-nuwyypgj/\n    Complete output (11 lines):\n    Traceback (most recent call last):\n      File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;\n      File &quot;/opt/Python-3.7.2/lib/python3.7/site-packages/setuptools/__init__.py&quot;, line 18, in &lt;module&gt;\n        from setuptools.dist import Distribution, Feature\n      File &quot;/opt/Python-3.7.2/lib/python3.7/site-packages/setuptools/dist.py&quot;, line 31, in &lt;module&gt;\n        from setuptools import windows_support\n      File &quot;/opt/Python-3.7.2/lib/python3.7/site-packages/setuptools/windows_support.py&quot;, line 2, in &lt;module&gt;\n        import ctypes\n      File &quot;/opt/Python-3.7.2/lib/python3.7/ctypes/__init__.py&quot;, line 7, in &lt;module&gt;\n        from _ctypes import Union, Structure, Array\n    ImportError: libffi.so.5: cannot open shared object file: No such file or directory\n    ----------------------------------------\nERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\n</code></pre>\n<p>其实在 CentOS 7.4 下已经存在 libffi.so.6，解决方法是创建软连接，如下：</p>\n<pre><code># cd /usr/lib64\n# ln -s libffi.so.6 libffi.so.5\n# ln -s libffi.so.6.0.1 libffi.so.5.0.1\n</code></pre>\n"},{"title":"JMeter 测试 MongoDB","date":"2016-04-17T14:37:11.000Z","_content":"\n\n### 简介\n\n**Apache JMeter** 是一个 100% 纯 Java 应用程序，设计来做“客户端/服务器”软件（例如 [Web 应用程序](http://jmeter.apache.org/usermanual/build-web-test-plan.html)）。它可以用来测试静态和动态资源的性能，例如静态文件、Java Servlets、ASP.NET、PHP、CGI 脚本、Java 对象、[数据库](http://jmeter.apache.org/usermanual/build-db-test-plan.html)、[FTP 服务器](http://jmeter.apache.org/usermanual/build-ftp-test-plan.html)，还有更多。JMeter 可以给服务器、网络或对象模拟重负载来测试它的强度或者分析不同负载类型下的综合性能。\n\n<!-- more -->\n\n此外，通过创建[断言](http://jmeter.apache.org/usermanual/test_plan.html#assertions)脚本验证应用程序是否返回期望的结果，JMeter可以帮助回归测试你的应用程序。为了最大的灵活性，JMeter 允许用正则表达式创建断言。\n\n但是请注意，JMeter 不是一个浏览器，它在协议层运行。\n\n### 安装\n\n下载网址：<http://jmeter.apache.org/download_jmeter.cgi>。首先正确安装 JRE/JDK，并且设置了 JAVA_HOME 变量。解压下载的文件包到安装目录。\n\n### 运行 JMeter\n\n运行 bin 目录下的 jmeter 启动 JMeter，很快 JMeter 的图形界面就会出现。\n\n### 创建数据库测试计划\n\n本文创建 50 个用户向 MongoDB 服务器发送同一个 Collection 的 Insert Document 请求各 1000 次。因此，总共会像 MongoDB 发送 50 × 1000 = 50000 次 Insert 请求。构建本次测试，需要用到以下组件：[Thread Group](http://jmeter.apache.org/usermanual/test_plan.html#thread_group)、[JDBC Request](http://jmeter.apache.org/usermanual/component_reference.html#JDBC_Request)、[Summary Report](http://jmeter.apache.org/usermanual/component_reference.html#Summary_Report)。\n\n#### 添加用户\n\n用 JMeter 做的所有测试计划的第一步就是添加 [Thread Group](http://jmeter.apache.org/usermanual/test_plan.html#thread_group) 组件。Thread Group 组件告诉 JMeter 模拟多少个用户、多久发送一个请求，以及发送多少次请求。\n\n选中左侧窗格中的测试计划，点击鼠标右键。在弹出的菜单选择菜单项：添加 -> Threads(Users) -> 线程组。添加之后，在右边的窗格就可以看到 Thread Group 的控制面板。配置参数如下：\n\n- 名称：MongoDB Thread Group\n- 线程数：50\n- Ramp-Up Period (in seconds)：10。该参数告诉 JMeter 延迟多长时间启动所有用户，此处设为 10，JMeter 在 10 秒内完成所有用户的启动，也就是每隔 200 毫秒启动一个用户（10s / 50 = 0.2）。如果设为 0，则 JMeter 会立即启动所有用户。\n- 循环次数：1000。这个参数告诉 JMeter 重复多少次测试。要永久重复则选中“永远”选择框。\n\n配置后的参数如图：![Thread Group](/uploads/20160417/mongodb-thread-group.png)\n\n#### 添加 MongoDB Source Config\n\n选中刚才添加的 MongoDB Thread Group，点右键，在右键菜单选择菜单项：添加 -> 配置元件 -> MongoDB Source Config。配置面板同样在右侧窗格展示。参数配置如下：\n\n- 名称：MongoDB Source Config\n- Server Address List：127.0.0.1\n- MongoDB Source：jmeterMongoDBSource\n\n其他参数保持默认，如图：![MongoDB Source Config](/uploads/20160417/mongodb-source-config.png)\n\n#### 添加　MongoDB Script\n\n同样选中 MongoDB Thread Group，点右键，在右键菜单选择菜单项：添加 -> Sampler -> MongoDB Script。右侧窗格展示配置面板，参数配置如下：\n\n- MongoDB Source：jmeterMongoDBSource，与　MongoDB Source Config　中配置的一致。\n- Database Name：test\n- Script：db.jmeter.insert({ \"p1\" : \"0\", \"p2\" : \"00\", \"bd\" : \"HUAWEI\", \"p3\" : \"001\", \"lo\" : \"110.802298\", \"pcode\" : \"010110468\", \"nt\" : \"wifi\", \"imsi\" : \"-\", \"uuid\" : \"49d66ae4a91b679a78d08ea4d61b7861_1460615318980\" })\n\n其他参数保持默认，如图：![MongoDB Script](/uploads/20160417/mongodb-script.png)\n\n#### 添加察看结果树和聚合报告\n\n还是选中 MongoDB Thread Group，点右键，在右键菜单选择菜单项：添加 -> 监听器 -> 察看结果树 和 聚合报告。\n\n点击“启动”按钮开始测试。结果如图：![测试结果](/uploads/20160417/mongodb-test-result.png)\n","source":"_posts/JMeter-测试-MongoDB.md","raw":"title: JMeter 测试 MongoDB\ntags:\n  - JMeter\n  - MongoDB\ncategories:\n  - 工具箱\n  - JMeter\ndate: 2016-04-17 22:37:11\n---\n\n\n### 简介\n\n**Apache JMeter** 是一个 100% 纯 Java 应用程序，设计来做“客户端/服务器”软件（例如 [Web 应用程序](http://jmeter.apache.org/usermanual/build-web-test-plan.html)）。它可以用来测试静态和动态资源的性能，例如静态文件、Java Servlets、ASP.NET、PHP、CGI 脚本、Java 对象、[数据库](http://jmeter.apache.org/usermanual/build-db-test-plan.html)、[FTP 服务器](http://jmeter.apache.org/usermanual/build-ftp-test-plan.html)，还有更多。JMeter 可以给服务器、网络或对象模拟重负载来测试它的强度或者分析不同负载类型下的综合性能。\n\n<!-- more -->\n\n此外，通过创建[断言](http://jmeter.apache.org/usermanual/test_plan.html#assertions)脚本验证应用程序是否返回期望的结果，JMeter可以帮助回归测试你的应用程序。为了最大的灵活性，JMeter 允许用正则表达式创建断言。\n\n但是请注意，JMeter 不是一个浏览器，它在协议层运行。\n\n### 安装\n\n下载网址：<http://jmeter.apache.org/download_jmeter.cgi>。首先正确安装 JRE/JDK，并且设置了 JAVA_HOME 变量。解压下载的文件包到安装目录。\n\n### 运行 JMeter\n\n运行 bin 目录下的 jmeter 启动 JMeter，很快 JMeter 的图形界面就会出现。\n\n### 创建数据库测试计划\n\n本文创建 50 个用户向 MongoDB 服务器发送同一个 Collection 的 Insert Document 请求各 1000 次。因此，总共会像 MongoDB 发送 50 × 1000 = 50000 次 Insert 请求。构建本次测试，需要用到以下组件：[Thread Group](http://jmeter.apache.org/usermanual/test_plan.html#thread_group)、[JDBC Request](http://jmeter.apache.org/usermanual/component_reference.html#JDBC_Request)、[Summary Report](http://jmeter.apache.org/usermanual/component_reference.html#Summary_Report)。\n\n#### 添加用户\n\n用 JMeter 做的所有测试计划的第一步就是添加 [Thread Group](http://jmeter.apache.org/usermanual/test_plan.html#thread_group) 组件。Thread Group 组件告诉 JMeter 模拟多少个用户、多久发送一个请求，以及发送多少次请求。\n\n选中左侧窗格中的测试计划，点击鼠标右键。在弹出的菜单选择菜单项：添加 -> Threads(Users) -> 线程组。添加之后，在右边的窗格就可以看到 Thread Group 的控制面板。配置参数如下：\n\n- 名称：MongoDB Thread Group\n- 线程数：50\n- Ramp-Up Period (in seconds)：10。该参数告诉 JMeter 延迟多长时间启动所有用户，此处设为 10，JMeter 在 10 秒内完成所有用户的启动，也就是每隔 200 毫秒启动一个用户（10s / 50 = 0.2）。如果设为 0，则 JMeter 会立即启动所有用户。\n- 循环次数：1000。这个参数告诉 JMeter 重复多少次测试。要永久重复则选中“永远”选择框。\n\n配置后的参数如图：![Thread Group](/uploads/20160417/mongodb-thread-group.png)\n\n#### 添加 MongoDB Source Config\n\n选中刚才添加的 MongoDB Thread Group，点右键，在右键菜单选择菜单项：添加 -> 配置元件 -> MongoDB Source Config。配置面板同样在右侧窗格展示。参数配置如下：\n\n- 名称：MongoDB Source Config\n- Server Address List：127.0.0.1\n- MongoDB Source：jmeterMongoDBSource\n\n其他参数保持默认，如图：![MongoDB Source Config](/uploads/20160417/mongodb-source-config.png)\n\n#### 添加　MongoDB Script\n\n同样选中 MongoDB Thread Group，点右键，在右键菜单选择菜单项：添加 -> Sampler -> MongoDB Script。右侧窗格展示配置面板，参数配置如下：\n\n- MongoDB Source：jmeterMongoDBSource，与　MongoDB Source Config　中配置的一致。\n- Database Name：test\n- Script：db.jmeter.insert({ \"p1\" : \"0\", \"p2\" : \"00\", \"bd\" : \"HUAWEI\", \"p3\" : \"001\", \"lo\" : \"110.802298\", \"pcode\" : \"010110468\", \"nt\" : \"wifi\", \"imsi\" : \"-\", \"uuid\" : \"49d66ae4a91b679a78d08ea4d61b7861_1460615318980\" })\n\n其他参数保持默认，如图：![MongoDB Script](/uploads/20160417/mongodb-script.png)\n\n#### 添加察看结果树和聚合报告\n\n还是选中 MongoDB Thread Group，点右键，在右键菜单选择菜单项：添加 -> 监听器 -> 察看结果树 和 聚合报告。\n\n点击“启动”按钮开始测试。结果如图：![测试结果](/uploads/20160417/mongodb-test-result.png)\n","slug":"JMeter-测试-MongoDB","published":1,"updated":"2021-07-19T16:28:00.264Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphng004xitd36bg4c948","content":"<h3 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h3><p><strong>Apache JMeter</strong> 是一个 100% 纯 Java 应用程序，设计来做“客户端/服务器”软件（例如 <a href=\"http://jmeter.apache.org/usermanual/build-web-test-plan.html\">Web 应用程序</a>）。它可以用来测试静态和动态资源的性能，例如静态文件、Java Servlets、ASP.NET、PHP、CGI 脚本、Java 对象、<a href=\"http://jmeter.apache.org/usermanual/build-db-test-plan.html\">数据库</a>、<a href=\"http://jmeter.apache.org/usermanual/build-ftp-test-plan.html\">FTP 服务器</a>，还有更多。JMeter 可以给服务器、网络或对象模拟重负载来测试它的强度或者分析不同负载类型下的综合性能。</p>\n<span id=\"more\"></span>\n\n<p>此外，通过创建<a href=\"http://jmeter.apache.org/usermanual/test_plan.html#assertions\">断言</a>脚本验证应用程序是否返回期望的结果，JMeter可以帮助回归测试你的应用程序。为了最大的灵活性，JMeter 允许用正则表达式创建断言。</p>\n<p>但是请注意，JMeter 不是一个浏览器，它在协议层运行。</p>\n<h3 id=\"安装\"><a href=\"#安装\" class=\"headerlink\" title=\"安装\"></a>安装</h3><p>下载网址：<a href=\"http://jmeter.apache.org/download_jmeter.cgi\">http://jmeter.apache.org/download_jmeter.cgi</a>。首先正确安装 JRE/JDK，并且设置了 JAVA_HOME 变量。解压下载的文件包到安装目录。</p>\n<h3 id=\"运行-JMeter\"><a href=\"#运行-JMeter\" class=\"headerlink\" title=\"运行 JMeter\"></a>运行 JMeter</h3><p>运行 bin 目录下的 jmeter 启动 JMeter，很快 JMeter 的图形界面就会出现。</p>\n<h3 id=\"创建数据库测试计划\"><a href=\"#创建数据库测试计划\" class=\"headerlink\" title=\"创建数据库测试计划\"></a>创建数据库测试计划</h3><p>本文创建 50 个用户向 MongoDB 服务器发送同一个 Collection 的 Insert Document 请求各 1000 次。因此，总共会像 MongoDB 发送 50 × 1000 = 50000 次 Insert 请求。构建本次测试，需要用到以下组件：<a href=\"http://jmeter.apache.org/usermanual/test_plan.html#thread_group\">Thread Group</a>、<a href=\"http://jmeter.apache.org/usermanual/component_reference.html#JDBC_Request\">JDBC Request</a>、<a href=\"http://jmeter.apache.org/usermanual/component_reference.html#Summary_Report\">Summary Report</a>。</p>\n<h4 id=\"添加用户\"><a href=\"#添加用户\" class=\"headerlink\" title=\"添加用户\"></a>添加用户</h4><p>用 JMeter 做的所有测试计划的第一步就是添加 <a href=\"http://jmeter.apache.org/usermanual/test_plan.html#thread_group\">Thread Group</a> 组件。Thread Group 组件告诉 JMeter 模拟多少个用户、多久发送一个请求，以及发送多少次请求。</p>\n<p>选中左侧窗格中的测试计划，点击鼠标右键。在弹出的菜单选择菜单项：添加 -&gt; Threads(Users) -&gt; 线程组。添加之后，在右边的窗格就可以看到 Thread Group 的控制面板。配置参数如下：</p>\n<ul>\n<li>名称：MongoDB Thread Group</li>\n<li>线程数：50</li>\n<li>Ramp-Up Period (in seconds)：10。该参数告诉 JMeter 延迟多长时间启动所有用户，此处设为 10，JMeter 在 10 秒内完成所有用户的启动，也就是每隔 200 毫秒启动一个用户（10s / 50 = 0.2）。如果设为 0，则 JMeter 会立即启动所有用户。</li>\n<li>循环次数：1000。这个参数告诉 JMeter 重复多少次测试。要永久重复则选中“永远”选择框。</li>\n</ul>\n<p>配置后的参数如图：<img src=\"/uploads/20160417/mongodb-thread-group.png\" alt=\"Thread Group\"></p>\n<h4 id=\"添加-MongoDB-Source-Config\"><a href=\"#添加-MongoDB-Source-Config\" class=\"headerlink\" title=\"添加 MongoDB Source Config\"></a>添加 MongoDB Source Config</h4><p>选中刚才添加的 MongoDB Thread Group，点右键，在右键菜单选择菜单项：添加 -&gt; 配置元件 -&gt; MongoDB Source Config。配置面板同样在右侧窗格展示。参数配置如下：</p>\n<ul>\n<li>名称：MongoDB Source Config</li>\n<li>Server Address List：127.0.0.1</li>\n<li>MongoDB Source：jmeterMongoDBSource</li>\n</ul>\n<p>其他参数保持默认，如图：<img src=\"/uploads/20160417/mongodb-source-config.png\" alt=\"MongoDB Source Config\"></p>\n<h4 id=\"添加-MongoDB-Script\"><a href=\"#添加-MongoDB-Script\" class=\"headerlink\" title=\"添加　MongoDB Script\"></a>添加　MongoDB Script</h4><p>同样选中 MongoDB Thread Group，点右键，在右键菜单选择菜单项：添加 -&gt; Sampler -&gt; MongoDB Script。右侧窗格展示配置面板，参数配置如下：</p>\n<ul>\n<li>MongoDB Source：jmeterMongoDBSource，与　MongoDB Source Config　中配置的一致。</li>\n<li>Database Name：test</li>\n<li>Script：db.jmeter.insert({ “p1” : “0”, “p2” : “00”, “bd” : “HUAWEI”, “p3” : “001”, “lo” : “110.802298”, “pcode” : “010110468”, “nt” : “wifi”, “imsi” : “-“, “uuid” : “49d66ae4a91b679a78d08ea4d61b7861_1460615318980” })</li>\n</ul>\n<p>其他参数保持默认，如图：<img src=\"/uploads/20160417/mongodb-script.png\" alt=\"MongoDB Script\"></p>\n<h4 id=\"添加察看结果树和聚合报告\"><a href=\"#添加察看结果树和聚合报告\" class=\"headerlink\" title=\"添加察看结果树和聚合报告\"></a>添加察看结果树和聚合报告</h4><p>还是选中 MongoDB Thread Group，点右键，在右键菜单选择菜单项：添加 -&gt; 监听器 -&gt; 察看结果树 和 聚合报告。</p>\n<p>点击“启动”按钮开始测试。结果如图：<img src=\"/uploads/20160417/mongodb-test-result.png\" alt=\"测试结果\"></p>\n","site":{"data":{}},"excerpt":"<h3 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h3><p><strong>Apache JMeter</strong> 是一个 100% 纯 Java 应用程序，设计来做“客户端/服务器”软件（例如 <a href=\"http://jmeter.apache.org/usermanual/build-web-test-plan.html\">Web 应用程序</a>）。它可以用来测试静态和动态资源的性能，例如静态文件、Java Servlets、ASP.NET、PHP、CGI 脚本、Java 对象、<a href=\"http://jmeter.apache.org/usermanual/build-db-test-plan.html\">数据库</a>、<a href=\"http://jmeter.apache.org/usermanual/build-ftp-test-plan.html\">FTP 服务器</a>，还有更多。JMeter 可以给服务器、网络或对象模拟重负载来测试它的强度或者分析不同负载类型下的综合性能。</p>","more":"<p>此外，通过创建<a href=\"http://jmeter.apache.org/usermanual/test_plan.html#assertions\">断言</a>脚本验证应用程序是否返回期望的结果，JMeter可以帮助回归测试你的应用程序。为了最大的灵活性，JMeter 允许用正则表达式创建断言。</p>\n<p>但是请注意，JMeter 不是一个浏览器，它在协议层运行。</p>\n<h3 id=\"安装\"><a href=\"#安装\" class=\"headerlink\" title=\"安装\"></a>安装</h3><p>下载网址：<a href=\"http://jmeter.apache.org/download_jmeter.cgi\">http://jmeter.apache.org/download_jmeter.cgi</a>。首先正确安装 JRE/JDK，并且设置了 JAVA_HOME 变量。解压下载的文件包到安装目录。</p>\n<h3 id=\"运行-JMeter\"><a href=\"#运行-JMeter\" class=\"headerlink\" title=\"运行 JMeter\"></a>运行 JMeter</h3><p>运行 bin 目录下的 jmeter 启动 JMeter，很快 JMeter 的图形界面就会出现。</p>\n<h3 id=\"创建数据库测试计划\"><a href=\"#创建数据库测试计划\" class=\"headerlink\" title=\"创建数据库测试计划\"></a>创建数据库测试计划</h3><p>本文创建 50 个用户向 MongoDB 服务器发送同一个 Collection 的 Insert Document 请求各 1000 次。因此，总共会像 MongoDB 发送 50 × 1000 = 50000 次 Insert 请求。构建本次测试，需要用到以下组件：<a href=\"http://jmeter.apache.org/usermanual/test_plan.html#thread_group\">Thread Group</a>、<a href=\"http://jmeter.apache.org/usermanual/component_reference.html#JDBC_Request\">JDBC Request</a>、<a href=\"http://jmeter.apache.org/usermanual/component_reference.html#Summary_Report\">Summary Report</a>。</p>\n<h4 id=\"添加用户\"><a href=\"#添加用户\" class=\"headerlink\" title=\"添加用户\"></a>添加用户</h4><p>用 JMeter 做的所有测试计划的第一步就是添加 <a href=\"http://jmeter.apache.org/usermanual/test_plan.html#thread_group\">Thread Group</a> 组件。Thread Group 组件告诉 JMeter 模拟多少个用户、多久发送一个请求，以及发送多少次请求。</p>\n<p>选中左侧窗格中的测试计划，点击鼠标右键。在弹出的菜单选择菜单项：添加 -&gt; Threads(Users) -&gt; 线程组。添加之后，在右边的窗格就可以看到 Thread Group 的控制面板。配置参数如下：</p>\n<ul>\n<li>名称：MongoDB Thread Group</li>\n<li>线程数：50</li>\n<li>Ramp-Up Period (in seconds)：10。该参数告诉 JMeter 延迟多长时间启动所有用户，此处设为 10，JMeter 在 10 秒内完成所有用户的启动，也就是每隔 200 毫秒启动一个用户（10s / 50 = 0.2）。如果设为 0，则 JMeter 会立即启动所有用户。</li>\n<li>循环次数：1000。这个参数告诉 JMeter 重复多少次测试。要永久重复则选中“永远”选择框。</li>\n</ul>\n<p>配置后的参数如图：<img src=\"/uploads/20160417/mongodb-thread-group.png\" alt=\"Thread Group\"></p>\n<h4 id=\"添加-MongoDB-Source-Config\"><a href=\"#添加-MongoDB-Source-Config\" class=\"headerlink\" title=\"添加 MongoDB Source Config\"></a>添加 MongoDB Source Config</h4><p>选中刚才添加的 MongoDB Thread Group，点右键，在右键菜单选择菜单项：添加 -&gt; 配置元件 -&gt; MongoDB Source Config。配置面板同样在右侧窗格展示。参数配置如下：</p>\n<ul>\n<li>名称：MongoDB Source Config</li>\n<li>Server Address List：127.0.0.1</li>\n<li>MongoDB Source：jmeterMongoDBSource</li>\n</ul>\n<p>其他参数保持默认，如图：<img src=\"/uploads/20160417/mongodb-source-config.png\" alt=\"MongoDB Source Config\"></p>\n<h4 id=\"添加-MongoDB-Script\"><a href=\"#添加-MongoDB-Script\" class=\"headerlink\" title=\"添加　MongoDB Script\"></a>添加　MongoDB Script</h4><p>同样选中 MongoDB Thread Group，点右键，在右键菜单选择菜单项：添加 -&gt; Sampler -&gt; MongoDB Script。右侧窗格展示配置面板，参数配置如下：</p>\n<ul>\n<li>MongoDB Source：jmeterMongoDBSource，与　MongoDB Source Config　中配置的一致。</li>\n<li>Database Name：test</li>\n<li>Script：db.jmeter.insert({ “p1” : “0”, “p2” : “00”, “bd” : “HUAWEI”, “p3” : “001”, “lo” : “110.802298”, “pcode” : “010110468”, “nt” : “wifi”, “imsi” : “-“, “uuid” : “49d66ae4a91b679a78d08ea4d61b7861_1460615318980” })</li>\n</ul>\n<p>其他参数保持默认，如图：<img src=\"/uploads/20160417/mongodb-script.png\" alt=\"MongoDB Script\"></p>\n<h4 id=\"添加察看结果树和聚合报告\"><a href=\"#添加察看结果树和聚合报告\" class=\"headerlink\" title=\"添加察看结果树和聚合报告\"></a>添加察看结果树和聚合报告</h4><p>还是选中 MongoDB Thread Group，点右键，在右键菜单选择菜单项：添加 -&gt; 监听器 -&gt; 察看结果树 和 聚合报告。</p>\n<p>点击“启动”按钮开始测试。结果如图：<img src=\"/uploads/20160417/mongodb-test-result.png\" alt=\"测试结果\"></p>"},{"title":"Java Socket 多线程实例","date":"2016-06-02T15:10:22.000Z","_content":"\n### 概述\nJava 网络编程的客户端和服务端编写都非常简单。客户端使用 Socket 对象与服务端交互；服务端创建 ServerSocket 对象，然后使用accept()方法进行监听，并返回一个Socket的对象。accept 方法是阻塞调用，并将一直阻塞，直到接收到下一个客户端请求为止。非多线程情况下，服务器同时只能服务一个客户端。这通常是不能满足现实需求的。\n\n<!-- more -->\n\n对于现实场景是通过在服务端创建多个线程来服务多个客户端，如下图：\n![Java Socket 多线程示意图](/uploads/20160602/Socket.png)\n\n### 服务端\n\n    import java.io.IOException;\n    import java.io.InputStream;\n    import java.io.OutputStream;\n    import java.net.ServerSocket;\n    import java.net.Socket;\n    import java.net.SocketException;\n    import java.util.concurrent.ExecutorService;\n    import java.util.concurrent.Executors;\n\n    public class EchoMultiServer {\n\t    private static ServerSocket server = null;\n\t    private static ExecutorService threadPool;\n\n\t    public static void main(String[] args) {\n\t\t    threadPool = Executors.newCachedThreadPool();\n\t\t    threadPool.submit(new Monitor());\n\n\t\t    try {\n\t\t\t    server = new ServerSocket(10000);\n\n\t\t\t    System.out.println(\"Server listening on port 10000 ....\");\n\t\t\t    System.out.println(\"Hit Enter to stop the server\");\n\n\t\t\t    while (true) {\n\t\t\t\t    Socket socket = server.accept();\n\t\t\t\t    System.out.println(\"Thread created\");\n\t\t\t\t    threadPool.submit(new EchoThread(socket));\n\t\t\t    }\n\t\t    } catch (SocketException e) {\n\t\t\t    System.out.println(\"Server is down\");\n\t\t    } catch (IOException e) {\n\t\t\t    e.printStackTrace();\n\t\t    }\n\t    }\n\n\t    private static void shutdownServer() {\n\t\t    try {\n\t\t\t    server.close();\n\t\t    } catch (IOException e) {\n\t\t\t    e.printStackTrace();\n\t\t    }\n\n\t\t    threadPool.shutdown();\n\t\t    System.exit(0);\n\t    }\n\n\t    private static class Monitor implements Runnable {\n\n\t\t    @Override\n\t\t    public void run() {\n\t\t\t    try {\n\t\t\t\t    while (System.in.read() != '\\n') {}\n\t\t\t    } catch (IOException e) {\n\t\t\t    }\n\n\t\t\t    shutdownServer();\n\t\t    }\n\t    }\n    }\n\n    class EchoThread implements Runnable {\n\t    private Socket socket = null;\n\t    private byte[] buffer = new byte[512];\n\n\t    public EchoThread(Socket socket) {\n\t\t    this.socket = socket;\n\t    }\n\n\t    @Override\n\t    public void run() {\n\t\t    try {\n\t\t\t    InputStream is = socket.getInputStream();\n\t\t\t    is.read(buffer);\n\t\t\t    OutputStream os = socket.getOutputStream();\n\t\t\t    os.write(buffer);\n\t\t    } catch (IOException e) {\n\t\t\t    e.printStackTrace();\n\t\t    } finally {\n\t\t\t    try {\n\t\t\t\t    socket.close();\n\t\t\t    } catch (IOException e) {\n\t\t\t\t    e.printStackTrace();\n\t\t\t    }\n\t\t    }\n\t    }\n    }\n\n### 客户端\n\n    import java.io.InputStream;\n    import java.io.OutputStream;\n    import java.net.InetSocketAddress;\n    import java.net.Socket;\n    import java.util.Scanner;\n\n    public class EchoMultiClient {\n\t    private static int counter;\n\n\t    public static void main(String[] args) {\n\t\t    for (int i = 0; i < 100; i++) {\n\t\t\t    counter++;\n\t\t\t    new Thread(new Client(counter)).start();\n\t\t    }\n\t    }\n\n\t    private static class Client implements Runnable {\n\t\t    private int counter;\n\n\t\t    public Client(int counter) {\n\t\t\t    this.counter = counter;\n\t\t    }\n\n\t\t    @Override\n\t\t    public void run() {\n\t\t\t    try {\n\t\t\t\t    Socket socket = new Socket();\n\t\t\t\t    socket.connect(new InetSocketAddress(\"localhost\", 10000), 1000);\n\n\t\t\t\t    try {\n\t\t\t\t\t    OutputStream os = socket.getOutputStream();\n\t\t\t\t\t    String str = \"Hello from Client \" + counter;\n\t\t\t\t\t    os.write(str.getBytes());\n\t\t\t\t\t    InputStream is = socket.getInputStream();\n\t\t\t\t\t    Scanner scanner = new Scanner(is);\n\t\t\t\t\t    while (scanner.hasNextLine()) {\n\t\t\t\t\t\t    System.out.println(scanner.nextLine());\n\t\t\t\t\t    }\n\n\t\t\t\t\t    Thread.sleep(10);\n\t\t\t\t\t    scanner.close();\n\t\t\t\t    } finally {\n\t\t\t\t\t    socket.close();\n\t\t\t\t    }\n\t\t\t    } catch (Exception e) {\n\t\t\t\t    e.printStackTrace();\n\t\t\t    }\n\t\t    }\n\t    }\n    }\n","source":"_posts/Java-Socket-多线程实例.md","raw":"title: Java Socket 多线程实例\ntags:\n  - Java\ncategories:\n  - 语言\n  - Java\ndate: 2016-06-02 23:10:22\n---\n\n### 概述\nJava 网络编程的客户端和服务端编写都非常简单。客户端使用 Socket 对象与服务端交互；服务端创建 ServerSocket 对象，然后使用accept()方法进行监听，并返回一个Socket的对象。accept 方法是阻塞调用，并将一直阻塞，直到接收到下一个客户端请求为止。非多线程情况下，服务器同时只能服务一个客户端。这通常是不能满足现实需求的。\n\n<!-- more -->\n\n对于现实场景是通过在服务端创建多个线程来服务多个客户端，如下图：\n![Java Socket 多线程示意图](/uploads/20160602/Socket.png)\n\n### 服务端\n\n    import java.io.IOException;\n    import java.io.InputStream;\n    import java.io.OutputStream;\n    import java.net.ServerSocket;\n    import java.net.Socket;\n    import java.net.SocketException;\n    import java.util.concurrent.ExecutorService;\n    import java.util.concurrent.Executors;\n\n    public class EchoMultiServer {\n\t    private static ServerSocket server = null;\n\t    private static ExecutorService threadPool;\n\n\t    public static void main(String[] args) {\n\t\t    threadPool = Executors.newCachedThreadPool();\n\t\t    threadPool.submit(new Monitor());\n\n\t\t    try {\n\t\t\t    server = new ServerSocket(10000);\n\n\t\t\t    System.out.println(\"Server listening on port 10000 ....\");\n\t\t\t    System.out.println(\"Hit Enter to stop the server\");\n\n\t\t\t    while (true) {\n\t\t\t\t    Socket socket = server.accept();\n\t\t\t\t    System.out.println(\"Thread created\");\n\t\t\t\t    threadPool.submit(new EchoThread(socket));\n\t\t\t    }\n\t\t    } catch (SocketException e) {\n\t\t\t    System.out.println(\"Server is down\");\n\t\t    } catch (IOException e) {\n\t\t\t    e.printStackTrace();\n\t\t    }\n\t    }\n\n\t    private static void shutdownServer() {\n\t\t    try {\n\t\t\t    server.close();\n\t\t    } catch (IOException e) {\n\t\t\t    e.printStackTrace();\n\t\t    }\n\n\t\t    threadPool.shutdown();\n\t\t    System.exit(0);\n\t    }\n\n\t    private static class Monitor implements Runnable {\n\n\t\t    @Override\n\t\t    public void run() {\n\t\t\t    try {\n\t\t\t\t    while (System.in.read() != '\\n') {}\n\t\t\t    } catch (IOException e) {\n\t\t\t    }\n\n\t\t\t    shutdownServer();\n\t\t    }\n\t    }\n    }\n\n    class EchoThread implements Runnable {\n\t    private Socket socket = null;\n\t    private byte[] buffer = new byte[512];\n\n\t    public EchoThread(Socket socket) {\n\t\t    this.socket = socket;\n\t    }\n\n\t    @Override\n\t    public void run() {\n\t\t    try {\n\t\t\t    InputStream is = socket.getInputStream();\n\t\t\t    is.read(buffer);\n\t\t\t    OutputStream os = socket.getOutputStream();\n\t\t\t    os.write(buffer);\n\t\t    } catch (IOException e) {\n\t\t\t    e.printStackTrace();\n\t\t    } finally {\n\t\t\t    try {\n\t\t\t\t    socket.close();\n\t\t\t    } catch (IOException e) {\n\t\t\t\t    e.printStackTrace();\n\t\t\t    }\n\t\t    }\n\t    }\n    }\n\n### 客户端\n\n    import java.io.InputStream;\n    import java.io.OutputStream;\n    import java.net.InetSocketAddress;\n    import java.net.Socket;\n    import java.util.Scanner;\n\n    public class EchoMultiClient {\n\t    private static int counter;\n\n\t    public static void main(String[] args) {\n\t\t    for (int i = 0; i < 100; i++) {\n\t\t\t    counter++;\n\t\t\t    new Thread(new Client(counter)).start();\n\t\t    }\n\t    }\n\n\t    private static class Client implements Runnable {\n\t\t    private int counter;\n\n\t\t    public Client(int counter) {\n\t\t\t    this.counter = counter;\n\t\t    }\n\n\t\t    @Override\n\t\t    public void run() {\n\t\t\t    try {\n\t\t\t\t    Socket socket = new Socket();\n\t\t\t\t    socket.connect(new InetSocketAddress(\"localhost\", 10000), 1000);\n\n\t\t\t\t    try {\n\t\t\t\t\t    OutputStream os = socket.getOutputStream();\n\t\t\t\t\t    String str = \"Hello from Client \" + counter;\n\t\t\t\t\t    os.write(str.getBytes());\n\t\t\t\t\t    InputStream is = socket.getInputStream();\n\t\t\t\t\t    Scanner scanner = new Scanner(is);\n\t\t\t\t\t    while (scanner.hasNextLine()) {\n\t\t\t\t\t\t    System.out.println(scanner.nextLine());\n\t\t\t\t\t    }\n\n\t\t\t\t\t    Thread.sleep(10);\n\t\t\t\t\t    scanner.close();\n\t\t\t\t    } finally {\n\t\t\t\t\t    socket.close();\n\t\t\t\t    }\n\t\t\t    } catch (Exception e) {\n\t\t\t\t    e.printStackTrace();\n\t\t\t    }\n\t\t    }\n\t    }\n    }\n","slug":"Java-Socket-多线程实例","published":1,"updated":"2021-07-19T16:28:00.264Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphnj0051itd31d2m4d7p","content":"<h3 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h3><p>Java 网络编程的客户端和服务端编写都非常简单。客户端使用 Socket 对象与服务端交互；服务端创建 ServerSocket 对象，然后使用accept()方法进行监听，并返回一个Socket的对象。accept 方法是阻塞调用，并将一直阻塞，直到接收到下一个客户端请求为止。非多线程情况下，服务器同时只能服务一个客户端。这通常是不能满足现实需求的。</p>\n<span id=\"more\"></span>\n\n<p>对于现实场景是通过在服务端创建多个线程来服务多个客户端，如下图：<br><img src=\"/uploads/20160602/Socket.png\" alt=\"Java Socket 多线程示意图\"></p>\n<h3 id=\"服务端\"><a href=\"#服务端\" class=\"headerlink\" title=\"服务端\"></a>服务端</h3><pre><code>import java.io.IOException;\nimport java.io.InputStream;\nimport java.io.OutputStream;\nimport java.net.ServerSocket;\nimport java.net.Socket;\nimport java.net.SocketException;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Executors;\n\npublic class EchoMultiServer &#123;\n    private static ServerSocket server = null;\n    private static ExecutorService threadPool;\n\n    public static void main(String[] args) &#123;\n        threadPool = Executors.newCachedThreadPool();\n        threadPool.submit(new Monitor());\n\n        try &#123;\n            server = new ServerSocket(10000);\n\n            System.out.println(&quot;Server listening on port 10000 ....&quot;);\n            System.out.println(&quot;Hit Enter to stop the server&quot;);\n\n            while (true) &#123;\n                Socket socket = server.accept();\n                System.out.println(&quot;Thread created&quot;);\n                threadPool.submit(new EchoThread(socket));\n            &#125;\n        &#125; catch (SocketException e) &#123;\n            System.out.println(&quot;Server is down&quot;);\n        &#125; catch (IOException e) &#123;\n            e.printStackTrace();\n        &#125;\n    &#125;\n\n    private static void shutdownServer() &#123;\n        try &#123;\n            server.close();\n        &#125; catch (IOException e) &#123;\n            e.printStackTrace();\n        &#125;\n\n        threadPool.shutdown();\n        System.exit(0);\n    &#125;\n\n    private static class Monitor implements Runnable &#123;\n\n        @Override\n        public void run() &#123;\n            try &#123;\n                while (System.in.read() != &#39;\\n&#39;) &#123;&#125;\n            &#125; catch (IOException e) &#123;\n            &#125;\n\n            shutdownServer();\n        &#125;\n    &#125;\n&#125;\n\nclass EchoThread implements Runnable &#123;\n    private Socket socket = null;\n    private byte[] buffer = new byte[512];\n\n    public EchoThread(Socket socket) &#123;\n        this.socket = socket;\n    &#125;\n\n    @Override\n    public void run() &#123;\n        try &#123;\n            InputStream is = socket.getInputStream();\n            is.read(buffer);\n            OutputStream os = socket.getOutputStream();\n            os.write(buffer);\n        &#125; catch (IOException e) &#123;\n            e.printStackTrace();\n        &#125; finally &#123;\n            try &#123;\n                socket.close();\n            &#125; catch (IOException e) &#123;\n                e.printStackTrace();\n            &#125;\n        &#125;\n    &#125;\n&#125;\n</code></pre>\n<h3 id=\"客户端\"><a href=\"#客户端\" class=\"headerlink\" title=\"客户端\"></a>客户端</h3><pre><code>import java.io.InputStream;\nimport java.io.OutputStream;\nimport java.net.InetSocketAddress;\nimport java.net.Socket;\nimport java.util.Scanner;\n\npublic class EchoMultiClient &#123;\n    private static int counter;\n\n    public static void main(String[] args) &#123;\n        for (int i = 0; i &lt; 100; i++) &#123;\n            counter++;\n            new Thread(new Client(counter)).start();\n        &#125;\n    &#125;\n\n    private static class Client implements Runnable &#123;\n        private int counter;\n\n        public Client(int counter) &#123;\n            this.counter = counter;\n        &#125;\n\n        @Override\n        public void run() &#123;\n            try &#123;\n                Socket socket = new Socket();\n                socket.connect(new InetSocketAddress(&quot;localhost&quot;, 10000), 1000);\n\n                try &#123;\n                    OutputStream os = socket.getOutputStream();\n                    String str = &quot;Hello from Client &quot; + counter;\n                    os.write(str.getBytes());\n                    InputStream is = socket.getInputStream();\n                    Scanner scanner = new Scanner(is);\n                    while (scanner.hasNextLine()) &#123;\n                        System.out.println(scanner.nextLine());\n                    &#125;\n\n                    Thread.sleep(10);\n                    scanner.close();\n                &#125; finally &#123;\n                    socket.close();\n                &#125;\n            &#125; catch (Exception e) &#123;\n                e.printStackTrace();\n            &#125;\n        &#125;\n    &#125;\n&#125;\n</code></pre>\n","site":{"data":{}},"excerpt":"<h3 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h3><p>Java 网络编程的客户端和服务端编写都非常简单。客户端使用 Socket 对象与服务端交互；服务端创建 ServerSocket 对象，然后使用accept()方法进行监听，并返回一个Socket的对象。accept 方法是阻塞调用，并将一直阻塞，直到接收到下一个客户端请求为止。非多线程情况下，服务器同时只能服务一个客户端。这通常是不能满足现实需求的。</p>","more":"<p>对于现实场景是通过在服务端创建多个线程来服务多个客户端，如下图：<br><img src=\"/uploads/20160602/Socket.png\" alt=\"Java Socket 多线程示意图\"></p>\n<h3 id=\"服务端\"><a href=\"#服务端\" class=\"headerlink\" title=\"服务端\"></a>服务端</h3><pre><code>import java.io.IOException;\nimport java.io.InputStream;\nimport java.io.OutputStream;\nimport java.net.ServerSocket;\nimport java.net.Socket;\nimport java.net.SocketException;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Executors;\n\npublic class EchoMultiServer &#123;\n    private static ServerSocket server = null;\n    private static ExecutorService threadPool;\n\n    public static void main(String[] args) &#123;\n        threadPool = Executors.newCachedThreadPool();\n        threadPool.submit(new Monitor());\n\n        try &#123;\n            server = new ServerSocket(10000);\n\n            System.out.println(&quot;Server listening on port 10000 ....&quot;);\n            System.out.println(&quot;Hit Enter to stop the server&quot;);\n\n            while (true) &#123;\n                Socket socket = server.accept();\n                System.out.println(&quot;Thread created&quot;);\n                threadPool.submit(new EchoThread(socket));\n            &#125;\n        &#125; catch (SocketException e) &#123;\n            System.out.println(&quot;Server is down&quot;);\n        &#125; catch (IOException e) &#123;\n            e.printStackTrace();\n        &#125;\n    &#125;\n\n    private static void shutdownServer() &#123;\n        try &#123;\n            server.close();\n        &#125; catch (IOException e) &#123;\n            e.printStackTrace();\n        &#125;\n\n        threadPool.shutdown();\n        System.exit(0);\n    &#125;\n\n    private static class Monitor implements Runnable &#123;\n\n        @Override\n        public void run() &#123;\n            try &#123;\n                while (System.in.read() != &#39;\\n&#39;) &#123;&#125;\n            &#125; catch (IOException e) &#123;\n            &#125;\n\n            shutdownServer();\n        &#125;\n    &#125;\n&#125;\n\nclass EchoThread implements Runnable &#123;\n    private Socket socket = null;\n    private byte[] buffer = new byte[512];\n\n    public EchoThread(Socket socket) &#123;\n        this.socket = socket;\n    &#125;\n\n    @Override\n    public void run() &#123;\n        try &#123;\n            InputStream is = socket.getInputStream();\n            is.read(buffer);\n            OutputStream os = socket.getOutputStream();\n            os.write(buffer);\n        &#125; catch (IOException e) &#123;\n            e.printStackTrace();\n        &#125; finally &#123;\n            try &#123;\n                socket.close();\n            &#125; catch (IOException e) &#123;\n                e.printStackTrace();\n            &#125;\n        &#125;\n    &#125;\n&#125;\n</code></pre>\n<h3 id=\"客户端\"><a href=\"#客户端\" class=\"headerlink\" title=\"客户端\"></a>客户端</h3><pre><code>import java.io.InputStream;\nimport java.io.OutputStream;\nimport java.net.InetSocketAddress;\nimport java.net.Socket;\nimport java.util.Scanner;\n\npublic class EchoMultiClient &#123;\n    private static int counter;\n\n    public static void main(String[] args) &#123;\n        for (int i = 0; i &lt; 100; i++) &#123;\n            counter++;\n            new Thread(new Client(counter)).start();\n        &#125;\n    &#125;\n\n    private static class Client implements Runnable &#123;\n        private int counter;\n\n        public Client(int counter) &#123;\n            this.counter = counter;\n        &#125;\n\n        @Override\n        public void run() &#123;\n            try &#123;\n                Socket socket = new Socket();\n                socket.connect(new InetSocketAddress(&quot;localhost&quot;, 10000), 1000);\n\n                try &#123;\n                    OutputStream os = socket.getOutputStream();\n                    String str = &quot;Hello from Client &quot; + counter;\n                    os.write(str.getBytes());\n                    InputStream is = socket.getInputStream();\n                    Scanner scanner = new Scanner(is);\n                    while (scanner.hasNextLine()) &#123;\n                        System.out.println(scanner.nextLine());\n                    &#125;\n\n                    Thread.sleep(10);\n                    scanner.close();\n                &#125; finally &#123;\n                    socket.close();\n                &#125;\n            &#125; catch (Exception e) &#123;\n                e.printStackTrace();\n            &#125;\n        &#125;\n    &#125;\n&#125;\n</code></pre>"},{"title":"Java 单例模式","date":"2016-05-12T20:56:38.000Z","_content":"\n单例模式是 Java 程序设计最常用的设计模式之一。在 Java 语言中，单例模式有几种不同的实现方式。从实例创建时机分为两大类：\n\n- 懒汉式：在需要时才创建类唯一的实例\n- 饿汉式：在类文件加载时创建类唯一的实例\n\n<!-- more -->\n\n一、懒汉式\n\n根据有锁、无锁及锁级别的不同可以有三种实现方式。\n\n1、最简单的实现\n\n    public class Singleton {\n      private Singleton() {}\n      private static Singleton single=null;\n\n      public static Singleton getInstance() {\n        if (single == null) {\n          single = new Singleton();\n        }\n        return single;\n      }\n    }\n\n该方法通过私有化构造方法，并且提供生成类对象的静态 getInstance 方法来保证该类只生成一个实例。\n\n但是，这种方式只能用在非多线程并发的场景。\n\n2、方法锁的实现\n\n为了解决实现 1 不能在多线程场景下使用的问题，给 getInstance 方法做线程同步是一种简单的实现方式。\n\n    public class Singleton {\n      private Singleton() {}\n      private static Singleton single=null;\n\n      public static synchronized Singleton getInstance() {\n        if (single == null) {\n          single = new Singleton();\n        }\n        return single;\n      }\n    }\n\n该方法解决了多线程同步问题，保证在多线程场景下也只生成一个实例。但每次调用 getInstance 方法都需要做线程间同步会非常浪费资源。只需要在实例还未生成时才需要做线程同步，实例生成以后直接使用即可，所以就有了方法 3 。\n\n3、双重检查锁定\n\n    public class Singleton {\n      private Singleton() {}\n      private static Singleton single=null;\n\n      public static Singleton getInstance() {\n        if (singleton == null) {\n          synchronized (Singleton.class) {\n            if (singleton == null) {\n              singleton = new Singleton();\n            }\n          }\n        }\n        return singleton;\n      }\n    }\n\n该实现方式避免了多线程场景下每次都做线程同步的问题，但在实例生成之前线程间仍然是需要做线程同步的。但该方法还是需要线程之间的同步，如果实例初始化比较复杂或者需要的时间比较长会阻塞很多线程等待。方法 4 完全不需要线程间同步。\n\n4、静态内部类\n\n    public class Singleton {\n      private static class LazyHolder {\n        private static final Singleton INSTANCE = new Singleton();\n      }\n      private Singleton (){}\n      public static final Singleton getInstance() {\n        return LazyHolder.INSTANCE;\n      }\n    }\n\n该方法通过类加载器在加载 LazyHolder 类并初始化时生成 Singleton 类的实例来保障单例。\n\n> 猜想：该方法并非完全不做线程同步。只不过在多线程场景下，多线程同步是由类加载器或者 JVM 来完成的。只是不需要业务程序代码做线程同步的工作。猜想是否正确还需要做更深的研究。\n\n二、饿汉式\n\n    public class Singleton {\n      private Singleton() {}\n      private static final Singleton single = new Singleton();\n\n      public static Singleton1 getInstance() {\n        return single;\n      }\n    }\n\n该方法跟懒汉式方法 4 很相似，只是类实例生成时间不同。懒汉式方法 4 类的唯一实例是在调用 getInstance 方法时才会生成；而该方法是在类加载时就会生成类的唯一实例。两种方法都是通过类加载器保障保证单例。\n\n###总结：###\n\n不同的应用场景可以灵活选择实现方法：\n\n- 非多线程场景可以采用懒汉式方法 1 就够了，简单。\n- 多线程场景下，如果实例初始化不需要其他参数的时候（生成实例时调用无参构造方法），使用懒汉式方法 4。\n- 多线程场景下，如果实例初始化时需要其他参数（生成实例时调用带参数的构造方法），使用懒汉式方法 3。\n- 饿汉式的优势是类加载时唯一实例就生成了，多线程使用时可以减少延迟。但即使不使用该实例，类加载时也会生成一个实例在内存中。基本属于空间换时间的情况。内存不敏感的场景比较适合使用。\n","source":"_posts/Java-单例模式.md","raw":"title: Java 单例模式\ntags:\n  - 设计模式\n  - Java\ncategories:\n  - 语言\n  - Java\ndate: 2016-05-13 04:56:38\n---\n\n单例模式是 Java 程序设计最常用的设计模式之一。在 Java 语言中，单例模式有几种不同的实现方式。从实例创建时机分为两大类：\n\n- 懒汉式：在需要时才创建类唯一的实例\n- 饿汉式：在类文件加载时创建类唯一的实例\n\n<!-- more -->\n\n一、懒汉式\n\n根据有锁、无锁及锁级别的不同可以有三种实现方式。\n\n1、最简单的实现\n\n    public class Singleton {\n      private Singleton() {}\n      private static Singleton single=null;\n\n      public static Singleton getInstance() {\n        if (single == null) {\n          single = new Singleton();\n        }\n        return single;\n      }\n    }\n\n该方法通过私有化构造方法，并且提供生成类对象的静态 getInstance 方法来保证该类只生成一个实例。\n\n但是，这种方式只能用在非多线程并发的场景。\n\n2、方法锁的实现\n\n为了解决实现 1 不能在多线程场景下使用的问题，给 getInstance 方法做线程同步是一种简单的实现方式。\n\n    public class Singleton {\n      private Singleton() {}\n      private static Singleton single=null;\n\n      public static synchronized Singleton getInstance() {\n        if (single == null) {\n          single = new Singleton();\n        }\n        return single;\n      }\n    }\n\n该方法解决了多线程同步问题，保证在多线程场景下也只生成一个实例。但每次调用 getInstance 方法都需要做线程间同步会非常浪费资源。只需要在实例还未生成时才需要做线程同步，实例生成以后直接使用即可，所以就有了方法 3 。\n\n3、双重检查锁定\n\n    public class Singleton {\n      private Singleton() {}\n      private static Singleton single=null;\n\n      public static Singleton getInstance() {\n        if (singleton == null) {\n          synchronized (Singleton.class) {\n            if (singleton == null) {\n              singleton = new Singleton();\n            }\n          }\n        }\n        return singleton;\n      }\n    }\n\n该实现方式避免了多线程场景下每次都做线程同步的问题，但在实例生成之前线程间仍然是需要做线程同步的。但该方法还是需要线程之间的同步，如果实例初始化比较复杂或者需要的时间比较长会阻塞很多线程等待。方法 4 完全不需要线程间同步。\n\n4、静态内部类\n\n    public class Singleton {\n      private static class LazyHolder {\n        private static final Singleton INSTANCE = new Singleton();\n      }\n      private Singleton (){}\n      public static final Singleton getInstance() {\n        return LazyHolder.INSTANCE;\n      }\n    }\n\n该方法通过类加载器在加载 LazyHolder 类并初始化时生成 Singleton 类的实例来保障单例。\n\n> 猜想：该方法并非完全不做线程同步。只不过在多线程场景下，多线程同步是由类加载器或者 JVM 来完成的。只是不需要业务程序代码做线程同步的工作。猜想是否正确还需要做更深的研究。\n\n二、饿汉式\n\n    public class Singleton {\n      private Singleton() {}\n      private static final Singleton single = new Singleton();\n\n      public static Singleton1 getInstance() {\n        return single;\n      }\n    }\n\n该方法跟懒汉式方法 4 很相似，只是类实例生成时间不同。懒汉式方法 4 类的唯一实例是在调用 getInstance 方法时才会生成；而该方法是在类加载时就会生成类的唯一实例。两种方法都是通过类加载器保障保证单例。\n\n###总结：###\n\n不同的应用场景可以灵活选择实现方法：\n\n- 非多线程场景可以采用懒汉式方法 1 就够了，简单。\n- 多线程场景下，如果实例初始化不需要其他参数的时候（生成实例时调用无参构造方法），使用懒汉式方法 4。\n- 多线程场景下，如果实例初始化时需要其他参数（生成实例时调用带参数的构造方法），使用懒汉式方法 3。\n- 饿汉式的优势是类加载时唯一实例就生成了，多线程使用时可以减少延迟。但即使不使用该实例，类加载时也会生成一个实例在内存中。基本属于空间换时间的情况。内存不敏感的场景比较适合使用。\n","slug":"Java-单例模式","published":1,"updated":"2021-07-19T16:28:00.264Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphnk0055itd35hc38xe1","content":"<p>单例模式是 Java 程序设计最常用的设计模式之一。在 Java 语言中，单例模式有几种不同的实现方式。从实例创建时机分为两大类：</p>\n<ul>\n<li>懒汉式：在需要时才创建类唯一的实例</li>\n<li>饿汉式：在类文件加载时创建类唯一的实例</li>\n</ul>\n<span id=\"more\"></span>\n\n<p>一、懒汉式</p>\n<p>根据有锁、无锁及锁级别的不同可以有三种实现方式。</p>\n<p>1、最简单的实现</p>\n<pre><code>public class Singleton &#123;\n  private Singleton() &#123;&#125;\n  private static Singleton single=null;\n\n  public static Singleton getInstance() &#123;\n    if (single == null) &#123;\n      single = new Singleton();\n    &#125;\n    return single;\n  &#125;\n&#125;\n</code></pre>\n<p>该方法通过私有化构造方法，并且提供生成类对象的静态 getInstance 方法来保证该类只生成一个实例。</p>\n<p>但是，这种方式只能用在非多线程并发的场景。</p>\n<p>2、方法锁的实现</p>\n<p>为了解决实现 1 不能在多线程场景下使用的问题，给 getInstance 方法做线程同步是一种简单的实现方式。</p>\n<pre><code>public class Singleton &#123;\n  private Singleton() &#123;&#125;\n  private static Singleton single=null;\n\n  public static synchronized Singleton getInstance() &#123;\n    if (single == null) &#123;\n      single = new Singleton();\n    &#125;\n    return single;\n  &#125;\n&#125;\n</code></pre>\n<p>该方法解决了多线程同步问题，保证在多线程场景下也只生成一个实例。但每次调用 getInstance 方法都需要做线程间同步会非常浪费资源。只需要在实例还未生成时才需要做线程同步，实例生成以后直接使用即可，所以就有了方法 3 。</p>\n<p>3、双重检查锁定</p>\n<pre><code>public class Singleton &#123;\n  private Singleton() &#123;&#125;\n  private static Singleton single=null;\n\n  public static Singleton getInstance() &#123;\n    if (singleton == null) &#123;\n      synchronized (Singleton.class) &#123;\n        if (singleton == null) &#123;\n          singleton = new Singleton();\n        &#125;\n      &#125;\n    &#125;\n    return singleton;\n  &#125;\n&#125;\n</code></pre>\n<p>该实现方式避免了多线程场景下每次都做线程同步的问题，但在实例生成之前线程间仍然是需要做线程同步的。但该方法还是需要线程之间的同步，如果实例初始化比较复杂或者需要的时间比较长会阻塞很多线程等待。方法 4 完全不需要线程间同步。</p>\n<p>4、静态内部类</p>\n<pre><code>public class Singleton &#123;\n  private static class LazyHolder &#123;\n    private static final Singleton INSTANCE = new Singleton();\n  &#125;\n  private Singleton ()&#123;&#125;\n  public static final Singleton getInstance() &#123;\n    return LazyHolder.INSTANCE;\n  &#125;\n&#125;\n</code></pre>\n<p>该方法通过类加载器在加载 LazyHolder 类并初始化时生成 Singleton 类的实例来保障单例。</p>\n<blockquote>\n<p>猜想：该方法并非完全不做线程同步。只不过在多线程场景下，多线程同步是由类加载器或者 JVM 来完成的。只是不需要业务程序代码做线程同步的工作。猜想是否正确还需要做更深的研究。</p>\n</blockquote>\n<p>二、饿汉式</p>\n<pre><code>public class Singleton &#123;\n  private Singleton() &#123;&#125;\n  private static final Singleton single = new Singleton();\n\n  public static Singleton1 getInstance() &#123;\n    return single;\n  &#125;\n&#125;\n</code></pre>\n<p>该方法跟懒汉式方法 4 很相似，只是类实例生成时间不同。懒汉式方法 4 类的唯一实例是在调用 getInstance 方法时才会生成；而该方法是在类加载时就会生成类的唯一实例。两种方法都是通过类加载器保障保证单例。</p>\n<p>###总结：###</p>\n<p>不同的应用场景可以灵活选择实现方法：</p>\n<ul>\n<li>非多线程场景可以采用懒汉式方法 1 就够了，简单。</li>\n<li>多线程场景下，如果实例初始化不需要其他参数的时候（生成实例时调用无参构造方法），使用懒汉式方法 4。</li>\n<li>多线程场景下，如果实例初始化时需要其他参数（生成实例时调用带参数的构造方法），使用懒汉式方法 3。</li>\n<li>饿汉式的优势是类加载时唯一实例就生成了，多线程使用时可以减少延迟。但即使不使用该实例，类加载时也会生成一个实例在内存中。基本属于空间换时间的情况。内存不敏感的场景比较适合使用。</li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>单例模式是 Java 程序设计最常用的设计模式之一。在 Java 语言中，单例模式有几种不同的实现方式。从实例创建时机分为两大类：</p>\n<ul>\n<li>懒汉式：在需要时才创建类唯一的实例</li>\n<li>饿汉式：在类文件加载时创建类唯一的实例</li>\n</ul>","more":"<p>一、懒汉式</p>\n<p>根据有锁、无锁及锁级别的不同可以有三种实现方式。</p>\n<p>1、最简单的实现</p>\n<pre><code>public class Singleton &#123;\n  private Singleton() &#123;&#125;\n  private static Singleton single=null;\n\n  public static Singleton getInstance() &#123;\n    if (single == null) &#123;\n      single = new Singleton();\n    &#125;\n    return single;\n  &#125;\n&#125;\n</code></pre>\n<p>该方法通过私有化构造方法，并且提供生成类对象的静态 getInstance 方法来保证该类只生成一个实例。</p>\n<p>但是，这种方式只能用在非多线程并发的场景。</p>\n<p>2、方法锁的实现</p>\n<p>为了解决实现 1 不能在多线程场景下使用的问题，给 getInstance 方法做线程同步是一种简单的实现方式。</p>\n<pre><code>public class Singleton &#123;\n  private Singleton() &#123;&#125;\n  private static Singleton single=null;\n\n  public static synchronized Singleton getInstance() &#123;\n    if (single == null) &#123;\n      single = new Singleton();\n    &#125;\n    return single;\n  &#125;\n&#125;\n</code></pre>\n<p>该方法解决了多线程同步问题，保证在多线程场景下也只生成一个实例。但每次调用 getInstance 方法都需要做线程间同步会非常浪费资源。只需要在实例还未生成时才需要做线程同步，实例生成以后直接使用即可，所以就有了方法 3 。</p>\n<p>3、双重检查锁定</p>\n<pre><code>public class Singleton &#123;\n  private Singleton() &#123;&#125;\n  private static Singleton single=null;\n\n  public static Singleton getInstance() &#123;\n    if (singleton == null) &#123;\n      synchronized (Singleton.class) &#123;\n        if (singleton == null) &#123;\n          singleton = new Singleton();\n        &#125;\n      &#125;\n    &#125;\n    return singleton;\n  &#125;\n&#125;\n</code></pre>\n<p>该实现方式避免了多线程场景下每次都做线程同步的问题，但在实例生成之前线程间仍然是需要做线程同步的。但该方法还是需要线程之间的同步，如果实例初始化比较复杂或者需要的时间比较长会阻塞很多线程等待。方法 4 完全不需要线程间同步。</p>\n<p>4、静态内部类</p>\n<pre><code>public class Singleton &#123;\n  private static class LazyHolder &#123;\n    private static final Singleton INSTANCE = new Singleton();\n  &#125;\n  private Singleton ()&#123;&#125;\n  public static final Singleton getInstance() &#123;\n    return LazyHolder.INSTANCE;\n  &#125;\n&#125;\n</code></pre>\n<p>该方法通过类加载器在加载 LazyHolder 类并初始化时生成 Singleton 类的实例来保障单例。</p>\n<blockquote>\n<p>猜想：该方法并非完全不做线程同步。只不过在多线程场景下，多线程同步是由类加载器或者 JVM 来完成的。只是不需要业务程序代码做线程同步的工作。猜想是否正确还需要做更深的研究。</p>\n</blockquote>\n<p>二、饿汉式</p>\n<pre><code>public class Singleton &#123;\n  private Singleton() &#123;&#125;\n  private static final Singleton single = new Singleton();\n\n  public static Singleton1 getInstance() &#123;\n    return single;\n  &#125;\n&#125;\n</code></pre>\n<p>该方法跟懒汉式方法 4 很相似，只是类实例生成时间不同。懒汉式方法 4 类的唯一实例是在调用 getInstance 方法时才会生成；而该方法是在类加载时就会生成类的唯一实例。两种方法都是通过类加载器保障保证单例。</p>\n<p>###总结：###</p>\n<p>不同的应用场景可以灵活选择实现方法：</p>\n<ul>\n<li>非多线程场景可以采用懒汉式方法 1 就够了，简单。</li>\n<li>多线程场景下，如果实例初始化不需要其他参数的时候（生成实例时调用无参构造方法），使用懒汉式方法 4。</li>\n<li>多线程场景下，如果实例初始化时需要其他参数（生成实例时调用带参数的构造方法），使用懒汉式方法 3。</li>\n<li>饿汉式的优势是类加载时唯一实例就生成了，多线程使用时可以减少延迟。但即使不使用该实例，类加载时也会生成一个实例在内存中。基本属于空间换时间的情况。内存不敏感的场景比较适合使用。</li>\n</ul>"},{"title":"Java 读取 Properties 文件的方法","date":"2017-09-28T02:00:20.000Z","_content":"\n\nJava 读取 Properties 文件有两种简单方法，就是使用 ClassLoader 中的资源读取方法。\n\n- public InputStream getResourceAsStream(String name)\n    该方法是非静态方法，所以不能在静态代码中使用。\n- public static InputStream getSystemResourceAsStream(String name)\n    该方法是静态方法，可以在静态代码中使用。 \n","source":"_posts/Java-读取-Properties-文件的方法.md","raw":"title: Java 读取 Properties 文件的方法\ntags:\n  - Java\ncategories:\n  - 语言\n  - Java\ndate: 2017-09-28 10:00:20\n---\n\n\nJava 读取 Properties 文件有两种简单方法，就是使用 ClassLoader 中的资源读取方法。\n\n- public InputStream getResourceAsStream(String name)\n    该方法是非静态方法，所以不能在静态代码中使用。\n- public static InputStream getSystemResourceAsStream(String name)\n    该方法是静态方法，可以在静态代码中使用。 \n","slug":"Java-读取-Properties-文件的方法","published":1,"updated":"2021-07-19T16:28:00.264Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphnl0059itd31vybd5w8","content":"<p>Java 读取 Properties 文件有两种简单方法，就是使用 ClassLoader 中的资源读取方法。</p>\n<ul>\n<li>public InputStream getResourceAsStream(String name)<br>  该方法是非静态方法，所以不能在静态代码中使用。</li>\n<li>public static InputStream getSystemResourceAsStream(String name)<br>  该方法是静态方法，可以在静态代码中使用。 </li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>Java 读取 Properties 文件有两种简单方法，就是使用 ClassLoader 中的资源读取方法。</p>\n<ul>\n<li>public InputStream getResourceAsStream(String name)<br>  该方法是非静态方法，所以不能在静态代码中使用。</li>\n<li>public static InputStream getSystemResourceAsStream(String name)<br>  该方法是静态方法，可以在静态代码中使用。 </li>\n</ul>\n"},{"title":"Jenkins REST API","date":"2015-11-24T05:12:59.000Z","_content":"\n#### buildWithParameters（参数化构建项目）\n\n\turl=http://${ip}:${port}/job/${projectName}/buildWithParameters?args=${argsValue}\n\tcurl -X POST ${url} --user ${userName}:${password}\n\n<!-- more -->\n\nurl  --  完整的 REST API 请求地址，包含请求参数  \nip  --  Jenkins Master 主机的ip地址（如果有域名可以使用域名）  \nport  --  Jenkins 服务监听端口  \nprojectName  --  Jenkins 通过参数构建的项目的名称  \nargs --  要构建项目配置的构建时需要输入参数的名称，在构建任务内部可以通过 ${args} 获取输入的值（以 shell 为例）  \nargsValue  --  构建时需要输入的参数值  \ncurl -X POST  --  以 POST 方法请求  \nuserName  --  具有构建项目权限的用户名  \npassword  --  具有构建项目权限的用户的口令\n\n注意：\n\n- 如果参数中包含 url 保留字符或者中文需要做 url 编码，如：\n\n\turl=http://${ip}:${port}/job/${projectName}/buildWithParameters?args=a b\n\nurl 编码后变为：\n\n\turl=http://${ip}:${port}/job/${projectName}/buildWithParameters?args=a%20b\n\n- 如果 Jenkins 设置匿名用户可以触发构建，则 --user 参数可以不用。安全考虑应禁用匿名用户触发构建。\n\n#### Disable/Enable（禁用/启用项目）\n\n\thttp://${ip}:${port}/job/${projectName}/enable\n\thttp://${ip}:${port}/job/${projectName}/disable\n\n项目禁用后，该项目在构建队列中的任务不会再构建。\n","source":"_posts/Jenkins-REST-API.md","raw":"title: Jenkins REST API\ntags:\n  - Jenkins\ncategories:\n  - 开发工具\n  - Jenkins\ndate: 2015-11-24 13:12:59\n---\n\n#### buildWithParameters（参数化构建项目）\n\n\turl=http://${ip}:${port}/job/${projectName}/buildWithParameters?args=${argsValue}\n\tcurl -X POST ${url} --user ${userName}:${password}\n\n<!-- more -->\n\nurl  --  完整的 REST API 请求地址，包含请求参数  \nip  --  Jenkins Master 主机的ip地址（如果有域名可以使用域名）  \nport  --  Jenkins 服务监听端口  \nprojectName  --  Jenkins 通过参数构建的项目的名称  \nargs --  要构建项目配置的构建时需要输入参数的名称，在构建任务内部可以通过 ${args} 获取输入的值（以 shell 为例）  \nargsValue  --  构建时需要输入的参数值  \ncurl -X POST  --  以 POST 方法请求  \nuserName  --  具有构建项目权限的用户名  \npassword  --  具有构建项目权限的用户的口令\n\n注意：\n\n- 如果参数中包含 url 保留字符或者中文需要做 url 编码，如：\n\n\turl=http://${ip}:${port}/job/${projectName}/buildWithParameters?args=a b\n\nurl 编码后变为：\n\n\turl=http://${ip}:${port}/job/${projectName}/buildWithParameters?args=a%20b\n\n- 如果 Jenkins 设置匿名用户可以触发构建，则 --user 参数可以不用。安全考虑应禁用匿名用户触发构建。\n\n#### Disable/Enable（禁用/启用项目）\n\n\thttp://${ip}:${port}/job/${projectName}/enable\n\thttp://${ip}:${port}/job/${projectName}/disable\n\n项目禁用后，该项目在构建队列中的任务不会再构建。\n","slug":"Jenkins-REST-API","published":1,"updated":"2021-07-19T16:28:00.264Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphnn005ditd3afsc8vcx","content":"<h4 id=\"buildWithParameters（参数化构建项目）\"><a href=\"#buildWithParameters（参数化构建项目）\" class=\"headerlink\" title=\"buildWithParameters（参数化构建项目）\"></a>buildWithParameters（参数化构建项目）</h4><pre><code>url=http://$&#123;ip&#125;:$&#123;port&#125;/job/$&#123;projectName&#125;/buildWithParameters?args=$&#123;argsValue&#125;\ncurl -X POST $&#123;url&#125; --user $&#123;userName&#125;:$&#123;password&#125;\n</code></pre>\n<span id=\"more\"></span>\n\n<p>url  –  完整的 REST API 请求地址，包含请求参数<br>ip  –  Jenkins Master 主机的ip地址（如果有域名可以使用域名）<br>port  –  Jenkins 服务监听端口<br>projectName  –  Jenkins 通过参数构建的项目的名称<br>args –  要构建项目配置的构建时需要输入参数的名称，在构建任务内部可以通过 ${args} 获取输入的值（以 shell 为例）<br>argsValue  –  构建时需要输入的参数值<br>curl -X POST  –  以 POST 方法请求<br>userName  –  具有构建项目权限的用户名<br>password  –  具有构建项目权限的用户的口令</p>\n<p>注意：</p>\n<ul>\n<li><p>如果参数中包含 url 保留字符或者中文需要做 url 编码，如：</p>\n<p>  url=http://${ip}:${port}/job/${projectName}/buildWithParameters?args=a b</p>\n</li>\n</ul>\n<p>url 编码后变为：</p>\n<pre><code>url=http://$&#123;ip&#125;:$&#123;port&#125;/job/$&#123;projectName&#125;/buildWithParameters?args=a%20b\n</code></pre>\n<ul>\n<li>如果 Jenkins 设置匿名用户可以触发构建，则 –user 参数可以不用。安全考虑应禁用匿名用户触发构建。</li>\n</ul>\n<h4 id=\"Disable-Enable（禁用-启用项目）\"><a href=\"#Disable-Enable（禁用-启用项目）\" class=\"headerlink\" title=\"Disable/Enable（禁用/启用项目）\"></a>Disable/Enable（禁用/启用项目）</h4><pre><code>http://$&#123;ip&#125;:$&#123;port&#125;/job/$&#123;projectName&#125;/enable\nhttp://$&#123;ip&#125;:$&#123;port&#125;/job/$&#123;projectName&#125;/disable\n</code></pre>\n<p>项目禁用后，该项目在构建队列中的任务不会再构建。</p>\n","site":{"data":{}},"excerpt":"<h4 id=\"buildWithParameters（参数化构建项目）\"><a href=\"#buildWithParameters（参数化构建项目）\" class=\"headerlink\" title=\"buildWithParameters（参数化构建项目）\"></a>buildWithParameters（参数化构建项目）</h4><pre><code>url=http://$&#123;ip&#125;:$&#123;port&#125;/job/$&#123;projectName&#125;/buildWithParameters?args=$&#123;argsValue&#125;\ncurl -X POST $&#123;url&#125; --user $&#123;userName&#125;:$&#123;password&#125;\n</code></pre>","more":"<p>url  –  完整的 REST API 请求地址，包含请求参数<br>ip  –  Jenkins Master 主机的ip地址（如果有域名可以使用域名）<br>port  –  Jenkins 服务监听端口<br>projectName  –  Jenkins 通过参数构建的项目的名称<br>args –  要构建项目配置的构建时需要输入参数的名称，在构建任务内部可以通过 ${args} 获取输入的值（以 shell 为例）<br>argsValue  –  构建时需要输入的参数值<br>curl -X POST  –  以 POST 方法请求<br>userName  –  具有构建项目权限的用户名<br>password  –  具有构建项目权限的用户的口令</p>\n<p>注意：</p>\n<ul>\n<li><p>如果参数中包含 url 保留字符或者中文需要做 url 编码，如：</p>\n<p>  url=http://${ip}:${port}/job/${projectName}/buildWithParameters?args=a b</p>\n</li>\n</ul>\n<p>url 编码后变为：</p>\n<pre><code>url=http://$&#123;ip&#125;:$&#123;port&#125;/job/$&#123;projectName&#125;/buildWithParameters?args=a%20b\n</code></pre>\n<ul>\n<li>如果 Jenkins 设置匿名用户可以触发构建，则 –user 参数可以不用。安全考虑应禁用匿名用户触发构建。</li>\n</ul>\n<h4 id=\"Disable-Enable（禁用-启用项目）\"><a href=\"#Disable-Enable（禁用-启用项目）\" class=\"headerlink\" title=\"Disable/Enable（禁用/启用项目）\"></a>Disable/Enable（禁用/启用项目）</h4><pre><code>http://$&#123;ip&#125;:$&#123;port&#125;/job/$&#123;projectName&#125;/enable\nhttp://$&#123;ip&#125;:$&#123;port&#125;/job/$&#123;projectName&#125;/disable\n</code></pre>\n<p>项目禁用后，该项目在构建队列中的任务不会再构建。</p>"},{"title":"Jenkins protoc: command not found","date":"2019-03-20T04:07:39.000Z","_content":"在使用 Jenkins 编译 Hadoop3.1.2 时报错信息如下：\n\n    [INFO] --- hadoop-maven-plugins:3.1.2:protoc (compile-protoc) @ hadoop-common ---\n    [WARNING] [protoc, --version] failed: java.io.IOException: Cannot run program \"protoc\": error=13, 权限不够\n    [ERROR] stdout: []\n    [INFO] ------------------------------------------------------------------------\n\n<!-- more -->\n\n原因是 Jenkins 中找不到 protoc 命令。解决方法是在 Jenkins 中配置环境变量 PATH 指定 protoc 路径。\n![jenkins path](/uploads/20190320/jenkins-path.png)\n\n再次执行发现以下错误：\n\n    protoc: error while loading shared libraries: libprotobuf.so.8: cannot open shared object file: No such file or directory\n\n解决方法跟 PATH 配置一样道理，配置一个新的环境变量 LD_LIBRARY_PATH：\n![jenkins LD_LIBRARY_PATH](/uploads/20190320/jenkins-LD_LIBRARY_PATH.png)\n","source":"_posts/Jenkins-protoc-command-not-found.md","raw":"title: 'Jenkins protoc: command not found'\ndate: 2019-03-20 12:07:39\ntags:\n- Jenkins\ncategories:\n- 开发工具\n- Jenkins\n---\n在使用 Jenkins 编译 Hadoop3.1.2 时报错信息如下：\n\n    [INFO] --- hadoop-maven-plugins:3.1.2:protoc (compile-protoc) @ hadoop-common ---\n    [WARNING] [protoc, --version] failed: java.io.IOException: Cannot run program \"protoc\": error=13, 权限不够\n    [ERROR] stdout: []\n    [INFO] ------------------------------------------------------------------------\n\n<!-- more -->\n\n原因是 Jenkins 中找不到 protoc 命令。解决方法是在 Jenkins 中配置环境变量 PATH 指定 protoc 路径。\n![jenkins path](/uploads/20190320/jenkins-path.png)\n\n再次执行发现以下错误：\n\n    protoc: error while loading shared libraries: libprotobuf.so.8: cannot open shared object file: No such file or directory\n\n解决方法跟 PATH 配置一样道理，配置一个新的环境变量 LD_LIBRARY_PATH：\n![jenkins LD_LIBRARY_PATH](/uploads/20190320/jenkins-LD_LIBRARY_PATH.png)\n","slug":"Jenkins-protoc-command-not-found","published":1,"updated":"2021-07-19T16:28:00.264Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphnp005gitd3g1qa5ale","content":"<p>在使用 Jenkins 编译 Hadoop3.1.2 时报错信息如下：</p>\n<pre><code>[INFO] --- hadoop-maven-plugins:3.1.2:protoc (compile-protoc) @ hadoop-common ---\n[WARNING] [protoc, --version] failed: java.io.IOException: Cannot run program &quot;protoc&quot;: error=13, 权限不够\n[ERROR] stdout: []\n[INFO] ------------------------------------------------------------------------\n</code></pre>\n<span id=\"more\"></span>\n\n<p>原因是 Jenkins 中找不到 protoc 命令。解决方法是在 Jenkins 中配置环境变量 PATH 指定 protoc 路径。<br><img src=\"/uploads/20190320/jenkins-path.png\" alt=\"jenkins path\"></p>\n<p>再次执行发现以下错误：</p>\n<pre><code>protoc: error while loading shared libraries: libprotobuf.so.8: cannot open shared object file: No such file or directory\n</code></pre>\n<p>解决方法跟 PATH 配置一样道理，配置一个新的环境变量 LD_LIBRARY_PATH：<br><img src=\"/uploads/20190320/jenkins-LD_LIBRARY_PATH.png\" alt=\"jenkins LD_LIBRARY_PATH\"></p>\n","site":{"data":{}},"excerpt":"<p>在使用 Jenkins 编译 Hadoop3.1.2 时报错信息如下：</p>\n<pre><code>[INFO] --- hadoop-maven-plugins:3.1.2:protoc (compile-protoc) @ hadoop-common ---\n[WARNING] [protoc, --version] failed: java.io.IOException: Cannot run program &quot;protoc&quot;: error=13, 权限不够\n[ERROR] stdout: []\n[INFO] ------------------------------------------------------------------------\n</code></pre>","more":"<p>原因是 Jenkins 中找不到 protoc 命令。解决方法是在 Jenkins 中配置环境变量 PATH 指定 protoc 路径。<br><img src=\"/uploads/20190320/jenkins-path.png\" alt=\"jenkins path\"></p>\n<p>再次执行发现以下错误：</p>\n<pre><code>protoc: error while loading shared libraries: libprotobuf.so.8: cannot open shared object file: No such file or directory\n</code></pre>\n<p>解决方法跟 PATH 配置一样道理，配置一个新的环境变量 LD_LIBRARY_PATH：<br><img src=\"/uploads/20190320/jenkins-LD_LIBRARY_PATH.png\" alt=\"jenkins LD_LIBRARY_PATH\"></p>"},{"title":"Jenkins 安装插件网络错误问题解决","date":"2019-03-01T06:24:20.000Z","_content":"\nJenkins 安装插件时报网络不通的错误，根据错误信息看是连接 https://www.google.com 网络不通。解决这个问题的方法是修改“Update Site”的URL地址，将默认地址由 https 改为 http。\n“Update Site”配置项位置在 “插件管理->Advanced”中，修改后如下图：\n![Jenkins Update Site](/uploads/20190301/jenkinsUpdateSite.png)\n","source":"_posts/Jenkins-安装插件网络错误问题解决.md","raw":"title: Jenkins 安装插件网络错误问题解决\ndate: 2019-03-01 14:24:20\ntags:\n- Jenkins\ncategories:\n- 开发工具\n- Jenkins\n---\n\nJenkins 安装插件时报网络不通的错误，根据错误信息看是连接 https://www.google.com 网络不通。解决这个问题的方法是修改“Update Site”的URL地址，将默认地址由 https 改为 http。\n“Update Site”配置项位置在 “插件管理->Advanced”中，修改后如下图：\n![Jenkins Update Site](/uploads/20190301/jenkinsUpdateSite.png)\n","slug":"Jenkins-安装插件网络错误问题解决","published":1,"updated":"2021-07-19T16:28:00.264Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphnq005litd3795l8sef","content":"<p>Jenkins 安装插件时报网络不通的错误，根据错误信息看是连接 <a href=\"https://www.google.com/\">https://www.google.com</a> 网络不通。解决这个问题的方法是修改“Update Site”的URL地址，将默认地址由 https 改为 http。<br>“Update Site”配置项位置在 “插件管理-&gt;Advanced”中，修改后如下图：<br><img src=\"/uploads/20190301/jenkinsUpdateSite.png\" alt=\"Jenkins Update Site\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p>Jenkins 安装插件时报网络不通的错误，根据错误信息看是连接 <a href=\"https://www.google.com/\">https://www.google.com</a> 网络不通。解决这个问题的方法是修改“Update Site”的URL地址，将默认地址由 https 改为 http。<br>“Update Site”配置项位置在 “插件管理-&gt;Advanced”中，修改后如下图：<br><img src=\"/uploads/20190301/jenkinsUpdateSite.png\" alt=\"Jenkins Update Site\"></p>\n"},{"title":"Jenkins 重启后 Maven 的 Project 加载失败","date":"2019-03-19T07:23:05.000Z","_content":"Jenkins 重启后发现 Maven 的项目都没有正常加载。检查 Jenkins 的启动日志发现以下错误信息：\n\n    java.io.IOException: Unable to read /home/jenkins/.jenkins/jobs/test-maven/config.xml\n    \tat hudson.XmlFile.read(XmlFile.java:149)\n    \tat hudson.model.Items.load(Items.java:371)\n    \tat jenkins.model.Jenkins$14.run(Jenkins.java:3128)\n    \tat org.jvnet.hudson.reactor.TaskGraphBuilder$TaskImpl.run(TaskGraphBuilder.java:169)\n    \tat org.jvnet.hudson.reactor.Reactor.runTask(Reactor.java:296)\n    \tat jenkins.model.Jenkins$5.runTask(Jenkins.java:1069)\n    \tat org.jvnet.hudson.reactor.Reactor$2.run(Reactor.java:214)\n    \tat org.jvnet.hudson.reactor.Reactor$Node.run(Reactor.java:117)\n    \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    \tat java.lang.Thread.run(Thread.java:748)\n    Caused by: com.thoughtworks.xstream.mapper.CannotResolveClassException: maven2-moduleset\n    \tat com.thoughtworks.xstream.mapper.DefaultMapper.realClass(DefaultMapper.java:79)\n    \tat com.thoughtworks.xstream.mapper.MapperWrapper.realClass(MapperWrapper.java:30)\n    \tat com.thoughtworks.xstream.mapper.DynamicProxyMapper.realClass(DynamicProxyMapper.java:55)\n    \tat com.thoughtworks.xstream.mapper.MapperWrapper.realClass(MapperWrapper.java:30)\n    \tat com.thoughtworks.xstream.mapper.PackageAliasingMapper.realClass(PackageAliasingMapper.java:88)\n    \tat com.thoughtworks.xstream.mapper.MapperWrapper.realClass(MapperWrapper.java:30)\n\n将 Maven Integration plugin 重装并重启 Jenkins 后恢复正常。\n","source":"_posts/Jenkins-重启后-Maven-的-Project-加载失败.md","raw":"title: Jenkins 重启后 Maven 的 Project 加载失败\ndate: 2019-03-19 15:23:05\ntags:\n- Jenkins\ncategories:\n- 开发工具\n- Jenkins\n---\nJenkins 重启后发现 Maven 的项目都没有正常加载。检查 Jenkins 的启动日志发现以下错误信息：\n\n    java.io.IOException: Unable to read /home/jenkins/.jenkins/jobs/test-maven/config.xml\n    \tat hudson.XmlFile.read(XmlFile.java:149)\n    \tat hudson.model.Items.load(Items.java:371)\n    \tat jenkins.model.Jenkins$14.run(Jenkins.java:3128)\n    \tat org.jvnet.hudson.reactor.TaskGraphBuilder$TaskImpl.run(TaskGraphBuilder.java:169)\n    \tat org.jvnet.hudson.reactor.Reactor.runTask(Reactor.java:296)\n    \tat jenkins.model.Jenkins$5.runTask(Jenkins.java:1069)\n    \tat org.jvnet.hudson.reactor.Reactor$2.run(Reactor.java:214)\n    \tat org.jvnet.hudson.reactor.Reactor$Node.run(Reactor.java:117)\n    \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    \tat java.lang.Thread.run(Thread.java:748)\n    Caused by: com.thoughtworks.xstream.mapper.CannotResolveClassException: maven2-moduleset\n    \tat com.thoughtworks.xstream.mapper.DefaultMapper.realClass(DefaultMapper.java:79)\n    \tat com.thoughtworks.xstream.mapper.MapperWrapper.realClass(MapperWrapper.java:30)\n    \tat com.thoughtworks.xstream.mapper.DynamicProxyMapper.realClass(DynamicProxyMapper.java:55)\n    \tat com.thoughtworks.xstream.mapper.MapperWrapper.realClass(MapperWrapper.java:30)\n    \tat com.thoughtworks.xstream.mapper.PackageAliasingMapper.realClass(PackageAliasingMapper.java:88)\n    \tat com.thoughtworks.xstream.mapper.MapperWrapper.realClass(MapperWrapper.java:30)\n\n将 Maven Integration plugin 重装并重启 Jenkins 后恢复正常。\n","slug":"Jenkins-重启后-Maven-的-Project-加载失败","published":1,"updated":"2021-07-19T16:28:00.264Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphns005oitd37byaezoh","content":"<p>Jenkins 重启后发现 Maven 的项目都没有正常加载。检查 Jenkins 的启动日志发现以下错误信息：</p>\n<pre><code>java.io.IOException: Unable to read /home/jenkins/.jenkins/jobs/test-maven/config.xml\n    at hudson.XmlFile.read(XmlFile.java:149)\n    at hudson.model.Items.load(Items.java:371)\n    at jenkins.model.Jenkins$14.run(Jenkins.java:3128)\n    at org.jvnet.hudson.reactor.TaskGraphBuilder$TaskImpl.run(TaskGraphBuilder.java:169)\n    at org.jvnet.hudson.reactor.Reactor.runTask(Reactor.java:296)\n    at jenkins.model.Jenkins$5.runTask(Jenkins.java:1069)\n    at org.jvnet.hudson.reactor.Reactor$2.run(Reactor.java:214)\n    at org.jvnet.hudson.reactor.Reactor$Node.run(Reactor.java:117)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run(Thread.java:748)\nCaused by: com.thoughtworks.xstream.mapper.CannotResolveClassException: maven2-moduleset\n    at com.thoughtworks.xstream.mapper.DefaultMapper.realClass(DefaultMapper.java:79)\n    at com.thoughtworks.xstream.mapper.MapperWrapper.realClass(MapperWrapper.java:30)\n    at com.thoughtworks.xstream.mapper.DynamicProxyMapper.realClass(DynamicProxyMapper.java:55)\n    at com.thoughtworks.xstream.mapper.MapperWrapper.realClass(MapperWrapper.java:30)\n    at com.thoughtworks.xstream.mapper.PackageAliasingMapper.realClass(PackageAliasingMapper.java:88)\n    at com.thoughtworks.xstream.mapper.MapperWrapper.realClass(MapperWrapper.java:30)\n</code></pre>\n<p>将 Maven Integration plugin 重装并重启 Jenkins 后恢复正常。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>Jenkins 重启后发现 Maven 的项目都没有正常加载。检查 Jenkins 的启动日志发现以下错误信息：</p>\n<pre><code>java.io.IOException: Unable to read /home/jenkins/.jenkins/jobs/test-maven/config.xml\n    at hudson.XmlFile.read(XmlFile.java:149)\n    at hudson.model.Items.load(Items.java:371)\n    at jenkins.model.Jenkins$14.run(Jenkins.java:3128)\n    at org.jvnet.hudson.reactor.TaskGraphBuilder$TaskImpl.run(TaskGraphBuilder.java:169)\n    at org.jvnet.hudson.reactor.Reactor.runTask(Reactor.java:296)\n    at jenkins.model.Jenkins$5.runTask(Jenkins.java:1069)\n    at org.jvnet.hudson.reactor.Reactor$2.run(Reactor.java:214)\n    at org.jvnet.hudson.reactor.Reactor$Node.run(Reactor.java:117)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run(Thread.java:748)\nCaused by: com.thoughtworks.xstream.mapper.CannotResolveClassException: maven2-moduleset\n    at com.thoughtworks.xstream.mapper.DefaultMapper.realClass(DefaultMapper.java:79)\n    at com.thoughtworks.xstream.mapper.MapperWrapper.realClass(MapperWrapper.java:30)\n    at com.thoughtworks.xstream.mapper.DynamicProxyMapper.realClass(DynamicProxyMapper.java:55)\n    at com.thoughtworks.xstream.mapper.MapperWrapper.realClass(MapperWrapper.java:30)\n    at com.thoughtworks.xstream.mapper.PackageAliasingMapper.realClass(PackageAliasingMapper.java:88)\n    at com.thoughtworks.xstream.mapper.MapperWrapper.realClass(MapperWrapper.java:30)\n</code></pre>\n<p>将 Maven Integration plugin 重装并重启 Jenkins 后恢复正常。</p>\n"},{"title":"Jetty 嵌入式 HelloWorld","date":"2016-04-10T10:08:48.000Z","_content":"\n\n### 简介\n\nJetty 是一个开源的项目，提供 HTTP 服务端、HTTP 客户端和 javax.servlet 容器。  \n下面看如何利用 Jetty 的 API 快速开发嵌入式代码。\n\n<!-- more -->\n\n### 下载 Jar 包\n\nJetty 被分解很多 jar 包和依赖包，通过选择 jar 包的最小集合达到占用最小空间的目的。通常，最好使用像 Maven 一样的工具管理 jar 包，但本例中使用包含 Jetty 所有类的聚合 jar 包。可以用 [curl](https://curl.haxx.se/) 命令或者浏览器手动下载 [jetty-all.jar](http://central.maven.org/maven2/org/eclipse/jetty/aggregate/jetty-all/9.3.7.v20160115/jetty-all-9.3.7.v20160115-uber.jar)。\n\n像下面这样使用 curl 命令：\n\n\tmkdir Demo\n\tcd Demo\n\tcurl -o jetty-all-uber.jar http://central.maven.org/maven2/org/eclipse/jetty/aggregate/jetty-all/9.3.7.v20160115/jetty-all-9.3.7.v20160115-uber.jar\n\n### HelloWorld\n\nJetty 官方文档中的 [嵌入 Jetty](http://www.eclipse.org/jetty/documentation/current/advanced-embedding.html)一章有很多通过 Jetty API 编写的实例。本教程用只包含一个 Main 方法的简单 HelloWorld 处理器运行服务器。[HelloWorld.java](https://raw.githubusercontent.com/eclipse/jetty.project/master/examples/embedded/src/main/java/org/eclipse/jetty/embedded/HelloWorld.java) 文件代码如下：\n\n\tpackage org.eclipse.jetty.embedded;\n\n\timport java.io.IOException;\n\timport javax.servlet.ServletException;\n\timport javax.servlet.http.HttpServletRequest;\n\timport javax.servlet.http.HttpServletResponse;\n\timport org.eclipse.jetty.server.Request;\n\timport org.eclipse.jetty.server.Server;\n\timport org.eclipse.jetty.server.handler.AbstractHandler;\n\n\tpublic class HelloWorld extends AbstractHandler {\n\n\t    @Override\n\t    public void handle(String target, Request baseRequest, HttpServletRequest request, HttpServletResponse response) throws IOException, ServletException {\n\t        response.setContentType(\"text/html; charset=utf-8\");\n\t        response.setStatus(HttpServletResponse.SC_OK);\n\t        response.getWriter().println(\"<h1>Hello World</h1>\");\n\t        baseRequest.setHandled(true);\n\t    }\n\n\t    public static void main(String[] args) throws Exception {\n\t        Server server = new Server(8080);\n\t        server.setHandler(new HelloWorld());\n\t        server.start();\n\t        server.join();\n\t    }\n\t}\n\n### 编译 HelloWorld\n\n下面的命令编译 HelloWorld 类：\n\n\tmkdir classes\n\tjavac -d classes -cp jetty-all-uber.jar HelloWorld.java\n\n### 运行处理器和服务器\n\n下面的命令运行 HelloWorld 实例：\n\n\tjava -cp classes:jetty-all-uber.jar org.eclipse.jetty.embedded.HelloWorld\n\n在浏览器地址栏输入：[http://localhost:8080](http://localhost:8080) 查看 HelloWorld 页面。\n","source":"_posts/Jetty-嵌入式-HelloWorld.md","raw":"title: Jetty 嵌入式 HelloWorld\ntags:\n  - Jetty\ncategories:\n  - 开发工具\n  - Jetty\ndate: 2016-04-10 18:08:48\n---\n\n\n### 简介\n\nJetty 是一个开源的项目，提供 HTTP 服务端、HTTP 客户端和 javax.servlet 容器。  \n下面看如何利用 Jetty 的 API 快速开发嵌入式代码。\n\n<!-- more -->\n\n### 下载 Jar 包\n\nJetty 被分解很多 jar 包和依赖包，通过选择 jar 包的最小集合达到占用最小空间的目的。通常，最好使用像 Maven 一样的工具管理 jar 包，但本例中使用包含 Jetty 所有类的聚合 jar 包。可以用 [curl](https://curl.haxx.se/) 命令或者浏览器手动下载 [jetty-all.jar](http://central.maven.org/maven2/org/eclipse/jetty/aggregate/jetty-all/9.3.7.v20160115/jetty-all-9.3.7.v20160115-uber.jar)。\n\n像下面这样使用 curl 命令：\n\n\tmkdir Demo\n\tcd Demo\n\tcurl -o jetty-all-uber.jar http://central.maven.org/maven2/org/eclipse/jetty/aggregate/jetty-all/9.3.7.v20160115/jetty-all-9.3.7.v20160115-uber.jar\n\n### HelloWorld\n\nJetty 官方文档中的 [嵌入 Jetty](http://www.eclipse.org/jetty/documentation/current/advanced-embedding.html)一章有很多通过 Jetty API 编写的实例。本教程用只包含一个 Main 方法的简单 HelloWorld 处理器运行服务器。[HelloWorld.java](https://raw.githubusercontent.com/eclipse/jetty.project/master/examples/embedded/src/main/java/org/eclipse/jetty/embedded/HelloWorld.java) 文件代码如下：\n\n\tpackage org.eclipse.jetty.embedded;\n\n\timport java.io.IOException;\n\timport javax.servlet.ServletException;\n\timport javax.servlet.http.HttpServletRequest;\n\timport javax.servlet.http.HttpServletResponse;\n\timport org.eclipse.jetty.server.Request;\n\timport org.eclipse.jetty.server.Server;\n\timport org.eclipse.jetty.server.handler.AbstractHandler;\n\n\tpublic class HelloWorld extends AbstractHandler {\n\n\t    @Override\n\t    public void handle(String target, Request baseRequest, HttpServletRequest request, HttpServletResponse response) throws IOException, ServletException {\n\t        response.setContentType(\"text/html; charset=utf-8\");\n\t        response.setStatus(HttpServletResponse.SC_OK);\n\t        response.getWriter().println(\"<h1>Hello World</h1>\");\n\t        baseRequest.setHandled(true);\n\t    }\n\n\t    public static void main(String[] args) throws Exception {\n\t        Server server = new Server(8080);\n\t        server.setHandler(new HelloWorld());\n\t        server.start();\n\t        server.join();\n\t    }\n\t}\n\n### 编译 HelloWorld\n\n下面的命令编译 HelloWorld 类：\n\n\tmkdir classes\n\tjavac -d classes -cp jetty-all-uber.jar HelloWorld.java\n\n### 运行处理器和服务器\n\n下面的命令运行 HelloWorld 实例：\n\n\tjava -cp classes:jetty-all-uber.jar org.eclipse.jetty.embedded.HelloWorld\n\n在浏览器地址栏输入：[http://localhost:8080](http://localhost:8080) 查看 HelloWorld 页面。\n","slug":"Jetty-嵌入式-HelloWorld","published":1,"updated":"2021-07-19T16:28:00.264Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphnu005titd33ur96h8x","content":"<h3 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h3><p>Jetty 是一个开源的项目，提供 HTTP 服务端、HTTP 客户端和 javax.servlet 容器。<br>下面看如何利用 Jetty 的 API 快速开发嵌入式代码。</p>\n<span id=\"more\"></span>\n\n<h3 id=\"下载-Jar-包\"><a href=\"#下载-Jar-包\" class=\"headerlink\" title=\"下载 Jar 包\"></a>下载 Jar 包</h3><p>Jetty 被分解很多 jar 包和依赖包，通过选择 jar 包的最小集合达到占用最小空间的目的。通常，最好使用像 Maven 一样的工具管理 jar 包，但本例中使用包含 Jetty 所有类的聚合 jar 包。可以用 <a href=\"https://curl.haxx.se/\">curl</a> 命令或者浏览器手动下载 <a href=\"http://central.maven.org/maven2/org/eclipse/jetty/aggregate/jetty-all/9.3.7.v20160115/jetty-all-9.3.7.v20160115-uber.jar\">jetty-all.jar</a>。</p>\n<p>像下面这样使用 curl 命令：</p>\n<pre><code>mkdir Demo\ncd Demo\ncurl -o jetty-all-uber.jar http://central.maven.org/maven2/org/eclipse/jetty/aggregate/jetty-all/9.3.7.v20160115/jetty-all-9.3.7.v20160115-uber.jar\n</code></pre>\n<h3 id=\"HelloWorld\"><a href=\"#HelloWorld\" class=\"headerlink\" title=\"HelloWorld\"></a>HelloWorld</h3><p>Jetty 官方文档中的 <a href=\"http://www.eclipse.org/jetty/documentation/current/advanced-embedding.html\">嵌入 Jetty</a>一章有很多通过 Jetty API 编写的实例。本教程用只包含一个 Main 方法的简单 HelloWorld 处理器运行服务器。<a href=\"https://raw.githubusercontent.com/eclipse/jetty.project/master/examples/embedded/src/main/java/org/eclipse/jetty/embedded/HelloWorld.java\">HelloWorld.java</a> 文件代码如下：</p>\n<pre><code>package org.eclipse.jetty.embedded;\n\nimport java.io.IOException;\nimport javax.servlet.ServletException;\nimport javax.servlet.http.HttpServletRequest;\nimport javax.servlet.http.HttpServletResponse;\nimport org.eclipse.jetty.server.Request;\nimport org.eclipse.jetty.server.Server;\nimport org.eclipse.jetty.server.handler.AbstractHandler;\n\npublic class HelloWorld extends AbstractHandler &#123;\n\n    @Override\n    public void handle(String target, Request baseRequest, HttpServletRequest request, HttpServletResponse response) throws IOException, ServletException &#123;\n        response.setContentType(&quot;text/html; charset=utf-8&quot;);\n        response.setStatus(HttpServletResponse.SC_OK);\n        response.getWriter().println(&quot;&lt;h1&gt;Hello World&lt;/h1&gt;&quot;);\n        baseRequest.setHandled(true);\n    &#125;\n\n    public static void main(String[] args) throws Exception &#123;\n        Server server = new Server(8080);\n        server.setHandler(new HelloWorld());\n        server.start();\n        server.join();\n    &#125;\n&#125;\n</code></pre>\n<h3 id=\"编译-HelloWorld\"><a href=\"#编译-HelloWorld\" class=\"headerlink\" title=\"编译 HelloWorld\"></a>编译 HelloWorld</h3><p>下面的命令编译 HelloWorld 类：</p>\n<pre><code>mkdir classes\njavac -d classes -cp jetty-all-uber.jar HelloWorld.java\n</code></pre>\n<h3 id=\"运行处理器和服务器\"><a href=\"#运行处理器和服务器\" class=\"headerlink\" title=\"运行处理器和服务器\"></a>运行处理器和服务器</h3><p>下面的命令运行 HelloWorld 实例：</p>\n<pre><code>java -cp classes:jetty-all-uber.jar org.eclipse.jetty.embedded.HelloWorld\n</code></pre>\n<p>在浏览器地址栏输入：<a href=\"http://localhost:8080/\">http://localhost:8080</a> 查看 HelloWorld 页面。</p>\n","site":{"data":{}},"excerpt":"<h3 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h3><p>Jetty 是一个开源的项目，提供 HTTP 服务端、HTTP 客户端和 javax.servlet 容器。<br>下面看如何利用 Jetty 的 API 快速开发嵌入式代码。</p>","more":"<h3 id=\"下载-Jar-包\"><a href=\"#下载-Jar-包\" class=\"headerlink\" title=\"下载 Jar 包\"></a>下载 Jar 包</h3><p>Jetty 被分解很多 jar 包和依赖包，通过选择 jar 包的最小集合达到占用最小空间的目的。通常，最好使用像 Maven 一样的工具管理 jar 包，但本例中使用包含 Jetty 所有类的聚合 jar 包。可以用 <a href=\"https://curl.haxx.se/\">curl</a> 命令或者浏览器手动下载 <a href=\"http://central.maven.org/maven2/org/eclipse/jetty/aggregate/jetty-all/9.3.7.v20160115/jetty-all-9.3.7.v20160115-uber.jar\">jetty-all.jar</a>。</p>\n<p>像下面这样使用 curl 命令：</p>\n<pre><code>mkdir Demo\ncd Demo\ncurl -o jetty-all-uber.jar http://central.maven.org/maven2/org/eclipse/jetty/aggregate/jetty-all/9.3.7.v20160115/jetty-all-9.3.7.v20160115-uber.jar\n</code></pre>\n<h3 id=\"HelloWorld\"><a href=\"#HelloWorld\" class=\"headerlink\" title=\"HelloWorld\"></a>HelloWorld</h3><p>Jetty 官方文档中的 <a href=\"http://www.eclipse.org/jetty/documentation/current/advanced-embedding.html\">嵌入 Jetty</a>一章有很多通过 Jetty API 编写的实例。本教程用只包含一个 Main 方法的简单 HelloWorld 处理器运行服务器。<a href=\"https://raw.githubusercontent.com/eclipse/jetty.project/master/examples/embedded/src/main/java/org/eclipse/jetty/embedded/HelloWorld.java\">HelloWorld.java</a> 文件代码如下：</p>\n<pre><code>package org.eclipse.jetty.embedded;\n\nimport java.io.IOException;\nimport javax.servlet.ServletException;\nimport javax.servlet.http.HttpServletRequest;\nimport javax.servlet.http.HttpServletResponse;\nimport org.eclipse.jetty.server.Request;\nimport org.eclipse.jetty.server.Server;\nimport org.eclipse.jetty.server.handler.AbstractHandler;\n\npublic class HelloWorld extends AbstractHandler &#123;\n\n    @Override\n    public void handle(String target, Request baseRequest, HttpServletRequest request, HttpServletResponse response) throws IOException, ServletException &#123;\n        response.setContentType(&quot;text/html; charset=utf-8&quot;);\n        response.setStatus(HttpServletResponse.SC_OK);\n        response.getWriter().println(&quot;&lt;h1&gt;Hello World&lt;/h1&gt;&quot;);\n        baseRequest.setHandled(true);\n    &#125;\n\n    public static void main(String[] args) throws Exception &#123;\n        Server server = new Server(8080);\n        server.setHandler(new HelloWorld());\n        server.start();\n        server.join();\n    &#125;\n&#125;\n</code></pre>\n<h3 id=\"编译-HelloWorld\"><a href=\"#编译-HelloWorld\" class=\"headerlink\" title=\"编译 HelloWorld\"></a>编译 HelloWorld</h3><p>下面的命令编译 HelloWorld 类：</p>\n<pre><code>mkdir classes\njavac -d classes -cp jetty-all-uber.jar HelloWorld.java\n</code></pre>\n<h3 id=\"运行处理器和服务器\"><a href=\"#运行处理器和服务器\" class=\"headerlink\" title=\"运行处理器和服务器\"></a>运行处理器和服务器</h3><p>下面的命令运行 HelloWorld 实例：</p>\n<pre><code>java -cp classes:jetty-all-uber.jar org.eclipse.jetty.embedded.HelloWorld\n</code></pre>\n<p>在浏览器地址栏输入：<a href=\"http://localhost:8080/\">http://localhost:8080</a> 查看 HelloWorld 页面。</p>"},{"title":"Kafka RecordTooLargeException 问题解决","date":"2017-08-17T10:50:31.000Z","_content":"\n\nProducer 向 Kafka 写入数据时遇到异常：org.apache.kafka.common.errors.RecordTooLargeException。该异常是因为单条消息大小超过限制导致的。解决方法是将限制参数调大：\n\n（1）server端：server.properties  \n    message.max.bytes 参数默认值为 1000012，调整为适合的值，如 10485760。\n\n（2）producer端：  \n    设置 Producer 的参数 max.request.size 的值与 server 端的 message.max.bytes 值一致。\n","source":"_posts/Kafka-RecordTooLargeException-问题解决.md","raw":"title: Kafka RecordTooLargeException 问题解决\ntags:\n  - 大数据\n  - Kafka\ncategory:\n  - 大数据\n  - Kafka\ndate: 2017-08-17 18:50:31\n---\n\n\nProducer 向 Kafka 写入数据时遇到异常：org.apache.kafka.common.errors.RecordTooLargeException。该异常是因为单条消息大小超过限制导致的。解决方法是将限制参数调大：\n\n（1）server端：server.properties  \n    message.max.bytes 参数默认值为 1000012，调整为适合的值，如 10485760。\n\n（2）producer端：  \n    设置 Producer 的参数 max.request.size 的值与 server 端的 message.max.bytes 值一致。\n","slug":"Kafka-RecordTooLargeException-问题解决","published":1,"updated":"2021-07-19T16:28:00.264Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphnv005witd3gw0w4tlt","content":"<p>Producer 向 Kafka 写入数据时遇到异常：org.apache.kafka.common.errors.RecordTooLargeException。该异常是因为单条消息大小超过限制导致的。解决方法是将限制参数调大：</p>\n<p>（1）server端：server.properties<br>    message.max.bytes 参数默认值为 1000012，调整为适合的值，如 10485760。</p>\n<p>（2）producer端：<br>    设置 Producer 的参数 max.request.size 的值与 server 端的 message.max.bytes 值一致。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>Producer 向 Kafka 写入数据时遇到异常：org.apache.kafka.common.errors.RecordTooLargeException。该异常是因为单条消息大小超过限制导致的。解决方法是将限制参数调大：</p>\n<p>（1）server端：server.properties<br>    message.max.bytes 参数默认值为 1000012，调整为适合的值，如 10485760。</p>\n<p>（2）producer端：<br>    设置 Producer 的参数 max.request.size 的值与 server 端的 message.max.bytes 值一致。</p>\n"},{"title":"LRU 算法","date":"2016-06-02T18:39:14.000Z","_content":"\n转载：<http://flychao88.iteye.com/blog/1977653>\n\n<!-- more -->\n\n### LRU\n#### 原理\n\nLRU 是 Least Recently Used 的缩写，即最近最少使用算法。其核心思想是“如果数据最近被访问过，那么将来被访问的几率也更高”。\n\n#### 实现\n\n最常见的实现是使用一个链表保存缓存数据，详细算法实现如下：\n![LRU 算法](/uploads/20160603/lru.png)\n\n1. 新数据插入到链表头部；\n2. 每当缓存命中（即缓存数据被访问），则将数据移到链表头部；\n3. 当链表满的时候，将链表尾部的数据丢弃。\n\n#### 分析\n\n当存在热点数据时，LRU的效率很好，但偶发性的、周期性的批量操作会导致 LRU 命中率急剧下降，缓存污染情况比较严重。该算法实现简单。命中时需要遍历链表，找到命中的数据块索引，然后需要将数据移到头部。\n\n### LRU-K\n#### 原理\n\nLRU-K 中的 K 代表最近使用的次数，因此 LRU 可以认为是 LRU-1。LRU-K 的主要目的是为了解决 LRU 算法“缓存污染”的问题，其核心思想是将“最近使用过 1 次”的判断标准扩展为“最近使用过 K 次”。\n\n#### 实现\n\n相比 LRU，LRU-K 需要多维护一个队列，用于记录所有缓存数据被访问的历史。只有当数据的访问次数达到 K 次的时候，才将数据放入缓存。当需要淘汰数据时，LRU-K 会淘汰第 K 次访问时间距当前时间最大的数据。详细实现如下：\n![LRU－K 算法](/uploads/20160603/lru-k.png)\n\n1. 数据第一次被访问，加入到访问历史列表；\n2. 如果数据在访问历史列表里后没有达到 K 次访问，则按照一定规则（FIFO，LRU）淘汰；\n3. 当访问历史队列中的数据访问次数达到 K 次后，将数据索引从历史队列删除，将数据移到缓存队列中，并缓存此数据，缓存队列重新按照时间排序；\n4. 缓存数据队列中被再次访问后，重新排序；\n5. 需要淘汰数据时，淘汰缓存队列中排在末尾的数据，即：淘汰“倒数第 K 次访问离现在最久”的数据。\n\nLRU-K 具有 LRU 的优点，同时能够避免 LRU 的缺点，实际应用中 LRU-2 是综合各种因素后最优的选择，LRU-3 或者更大的 K 值命中率会高，但适应性差，需要大量的数据访问才能将历史访问记录清除掉。\n\n#### 分析\n\nLRU-K 降低了“缓存污染”带来的问题，命中率比 LRU 要高。LRU-K 算法复杂度和代价比较高。由于 LRU-K 还需要记录那些被访问过、但还没有放入缓存的对象，因此内存消耗会比LRU要多；当数据量很大的时候，内存消耗会比较可观。LRU-K 需要基于时间进行排序（可以需要淘汰时再排序，也可以即时排序），CPU消耗比 LRU 要高。\n\n### Two queues（2Q）\n#### 原理\n\nTwo queues（以下使用 2Q 代替）算法类似于 LRU-2，不同点在于 2Q 将 LRU-2 算法中的访问历史队列（注意这不是缓存数据的）改为一个 FIFO 缓存队列，即：2Q 算法有两个缓存队列，一个是 FIFO 队列，一个是 LRU 队列。\n\n#### 实现\n\n当数据第一次访问时，2Q 算法将数据缓存在 FIFO 队列里面，当数据第二次被访问时，则将数据从 FIFO 队列移到 LRU 队列里面，两个队列各自按照自己的方法淘汰数据。详细实现如下：\n![LRU－K 算法](/uploads/20160603/2q.png)\n\n1. 新访问的数据插入到 FIFO 队列；\n2. 如果数据在FIFO队列中一直没有被再次访问，则最终按照 FIFO 规则淘汰；\n3. 如果数据在 FIFO 队列中被再次访问，则将数据移到 LRU 队列头部；\n4. 如果数据在 LRU 队列再次被访问，则将数据移到 LRU 队列头部；\n5. LRU 队列淘汰末尾的数据。\n\n注：上图中 FIFO 队列比 LRU 队列短，但并不代表这是算法要求，实际应用中两者比例没有硬性规定。\n\n#### 分析\n\n2Q 算法的命中率要高于 LRU。2Q 算法需要两个队列，但两个队列本身都比较简单。2Q 算法的代价是 FIFO 和 LRU 的代价之和。2Q 算法和 LRU-2 算法命中率类似，内存消耗也比较接近，但对于最后缓存的数据来说，2Q 会减少一次从原始存储读取数据或者计算数据的操作。\n\n### Multi Queue（MQ）\n#### 原理\n\nMQ 算法根据访问频率将数据划分为多个队列，不同的队列具有不同的访问优先级，其核心思想是：优先缓存访问次数多的数据。\n\n#### 实现\n\nMQ 算法将缓存划分为多个 LRU 队列，每个队列对应不同的访问优先级。访问优先级是根据访问次数计算出来的，例如：详细的算法结构图如下，Q0，Q1....Qk 代表不同的优先级队列，Q-history 代表从缓存中淘汰数据，但记录了数据的索引和引用次数的队列：\n![LRU－K 算法](/uploads/20160603/mq.png)\n\n如上图，算法详细描述如下：\n\n1. 新插入的数据放入 Q0；\n2. 每个队列按照 LRU 管理数据；\n3. 当数据的访问次数达到一定次数，需要提升优先级时，将数据从当前队列删除，加入到高一级队列的头部；\n4. 为了防止高优先级数据永远不被淘汰，当数据在指定的时间里访问没有被访问时，需要降低优先级，将数据从当前队列删除，加入到低一级的队列头部；\n5. 需要淘汰数据时，从最低一级队列开始按照 LRU 淘汰；每个队列淘汰数据时，将数据从缓存中删除，将数据索引加入 Q-history 头部；\n6. 如果数据在 Q-history 中被重新访问，则重新计算其优先级，移到目标队列的头部；\n7. Q-history 按照 LRU 淘汰数据的索引。\n\n#### 分析\n\nMQ 降低了“缓存污染”带来的问题，命中率比 LRU 要高。MQ 需要维护多个队列，且需要维护每个数据的访问时间，复杂度比 LRU 高。MQ 需要记录每个数据的访问时间，需要定时扫描所有队列，代价比 LRU 要高。\n\n注：虽然 MQ 的队列看起来数量比较多，但由于所有队列之和受限于缓存容量的大小，因此这里多个队列长度之和跟一个 LRU 队列是一样的，因此队列扫描性能也相近。\n\n### LRU类算法对比\n\n由于不同的访问模型导致命中率变化较大，此处对比仅基于理论定性分析，不做定量分析。\n\n命中率：LRU-2 > MQ(2) > 2Q > LRU\n复杂度：LRU-2 > MQ(2) > 2Q > LRU\n代价：LRU-2 > MQ(2) > 2Q > LRU\n\n实际应用中需要根据业务的需求和对数据的访问情况进行选择，并不是命中率越高越好。例如：虽然LRU看起来命中率会低一些，且存在”缓存污染“的问题，但由于其简单和代价小，实际应用中反而应用更多。\n\n### Java LinkedHashMap 实现\n\nJava 中最简单的 LRU 算法实现，就是利用 jdk 的 LinkedHashMap，覆写其中的 removeEldestEntry(Map.Entry) 方法即可。如果你去看 LinkedHashMap 的源码可知，LRU 算法是通过双向链表来实现，当某个位置被命中，通过调整链表的指向将该位置调整到头位置，新加入的内容直接放在链表头，如此一来，最近被命中的内容就向链表头移动，需要替换时，链表最后的位置就是最近最少使用的位置。\n\n    import java.util.ArrayList;  \n    import java.util.Collection;  \n    import java.util.LinkedHashMap;  \n    import java.util.concurrent.locks.Lock;  \n    import java.util.concurrent.locks.ReentrantLock;  \n    import java.util.Map;  \n\n    /**\n     * 类说明：利用 LinkedHashMap 实现简单的缓存， 必须实现 removeEldestEntry 方法，具体参见 JDK 文档\n     *\n     * @param <K>\n     * @param <V>\n     */\n\n    public class LRULinkedHashMap<K, V> extends LinkedHashMap<K, V> {  \n      private final int maxCapacity;\n      private static final float DEFAULT_LOAD_FACTOR = 0.75f;\n      private final Lock lock = new ReentrantLock();\n\n      public LRULinkedHashMap(int maxCapacity) {  \n        super(maxCapacity, DEFAULT_LOAD_FACTOR, true);  \n        this.maxCapacity = maxCapacity;  \n      }\n\n      @Override\n      protected boolean removeEldestEntry(java.util.Map.Entry<K, V> eldest) {  \n        return size() > maxCapacity;  \n      }\n\n      @Override\n      public boolean containsKey(Object key) {  \n        try {  \n          lock.lock();  \n          return super.containsKey(key);  \n        } finally {  \n          lock.unlock();  \n        }\n      }\n\n      @Override\n      public V get(Object key) {  \n        try {  \n          lock.lock();  \n          return super.get(key);  \n        } finally {  \n          lock.unlock();  \n        }\n      }\n\n      @Override\n      public V put(K key, V value) {  \n        try {  \n          lock.lock();  \n          return super.put(key, value);  \n        } finally {  \n          lock.unlock();  \n        }  \n      }\n\n      public int size() {  \n        try {  \n          lock.lock();  \n          return super.size();  \n        } finally {  \n          lock.unlock();  \n        }  \n      }\n\n      public void clear() {  \n        try {  \n          lock.lock();  \n          super.clear();  \n        } finally {  \n          lock.unlock();  \n        }  \n      }  \n\n      public Collection<Map.Entry<K, V>> getAll() {\n        try {  \n          lock.lock();  \n          return new ArrayList<Map.Entry<K, V>>(super.entrySet());  \n        } finally {  \n          lock.unlock();  \n        }  \n      }  \n    }\n\n### 基于双链表的 LRU 实现\n\n传统意义的 LRU 算法是为每一个 Cache 对象设置一个计数器，每次 Cache 命中则给计数器 +1，而 Cache 用完，需要淘汰旧内容，放置新内容时，就查看所有的计数器，并将最少使用的内容替换掉。\n\n它的弊端很明显，如果 Cache 的数量少，问题不会很大，但是如果 Cache 的空间过大，达到 10W 或者 100W 以上，一旦需要淘汰，则需要遍历所有计算器，其性能与资源消耗是巨大的。效率也就非常的慢了。\n\n它的原理：将 Cache 的所有位置都用双连表连接起来，当一个位置被命中之后，就将通过调整链表的指向，将该位置调整到链表头的位置，新加入的 Cache 直接加到链表头中。\n\n这样，在多次进行 Cache 操作后，最近被命中的，就会被向链表头方向移动，而没有命中的，而想链表后面移动，链表尾则表示最近最少使用的 Cache。\n\n当需要替换内容时候，链表的最后位置就是最少被命中的位置，我们只需要淘汰链表最后的部分即可。\n\n上面说了这么多的理论， 下面用代码来实现一个 LRU 策略的缓存。我们用一个对象来表示 Cache，并实现双链表：\n\n    import java.util.Hashtable;\n\n    public class LRUCache {\n\t    class CacheNode {\n        // add your code here\n      }\n\n\t    private int cacheSize;// 缓存大小\n\t    private Hashtable nodes;// 缓存容器\n\t    private int currentSize;// 当前缓存对象数量\n\t    private CacheNode first;// (实现双链表)链表头\n\t    private CacheNode last;// (实现双链表)链表尾\n    }\n\n下面给出完整的实现，这个类也被 Tomcat 所使用（ org.apache.tomcat.util.collections.LRUCache），但是在 tomcat6.x 版本中，已经被弃用，使用另外其他的缓存类来替代它。\n\n    import java.util.Hashtable;\n\n    public class LRUCache {\n\t    class CacheNode {\n\t\t    CacheNode prev;// 前一节点\n\t\t    CacheNode next;// 后一节点\n\t\t    Object value;// 值\n\t\t    Object key;// 键\n\n\t\t    CacheNode() {\n\t\t    }\n\t    }\n\n\t    public LRUCache(int i) {\n\t\t    currentSize = 0;\n\t\t    cacheSize = i;\n\t\t    nodes = new Hashtable(i);// 缓存容器\n\t    }\n\n\t    /**\n\t     * 获取缓存中对象\n\t     *\n\t     * @param key\n\t     * @return\n\t     */\n\t    public Object get(Object key) {\n\t\t    CacheNode node = (CacheNode) nodes.get(key);\n\t\t    if (node != null) {\n\t\t\t    moveToHead(node);\n\t\t\t    return node.value;\n\t\t    } else {\n\t\t\t    return null;\n\t\t    }\n\t    }\n\n\t    /**\n\t     * 添加缓存\n\t     *\n\t     * @param key\n\t     * @param value\n\t     */\n\t    public void put(Object key, Object value) {\n\t\t    CacheNode node = (CacheNode) nodes.get(key);\n\n\t\t    if (node == null) {\n\t\t\t    // 缓存容器是否已经超过大小.\n\t\t\t    if (currentSize >= cacheSize) {\n\t\t\t\t    if (last != null)// 将最少使用的删除\n\t\t\t\t\t    nodes.remove(last.key);\n\t\t\t\t    removeLast();\n\t\t\t    } else {\n\t\t\t\t    currentSize++;\n\t\t\t    }\n\n\t\t\t    node = new CacheNode();\n\t\t    }\n\t\t    node.value = value;\n\t\t    node.key = key;\n\t\t    // 将最新使用的节点放到链表头，表示最新使用的.\n\t\t    moveToHead(node);\n\t\t    nodes.put(key, node);\n\t    }\n\n\t    /**\n\t     * 将缓存删除\n\t     *\n\t     * @param key\n\t     * @return\n\t     */\n\t    public Object remove(Object key) {\n\t\t    CacheNode node = (CacheNode) nodes.get(key);\n\t\t    if (node != null) {\n\t\t\t    if (node.prev != null) {\n\t\t\t\t    node.prev.next = node.next;\n\t\t\t    }\n\t\t\t    if (node.next != null) {\n\t\t\t\t    node.next.prev = node.prev;\n\t\t\t    }\n\t\t\t    if (last == node)\n\t\t\t\t    last = node.prev;\n\t\t\t    if (first == node)\n\t\t\t\t    first = node.next;\n\t\t    }\n\t\t    return node;\n\t    }\n\n\t    public void clear() {\n\t\t    first = null;\n\t \t    last = null;\n\t    }\n\n\t    /**\n\t     * 删除链表尾部节点 表示 删除最少使用的缓存对象\n\t     */\n\t    private void removeLast() {\n\t\t    // 链表尾不为空,则将链表尾指向null. 删除连表尾（删除最少使用的缓存对象）\n\t\t    if (last != null) {\n\t\t\t    if (last.prev != null)\n\t\t\t\t    last.prev.next = null;\n\t\t\t    else\n\t\t\t\t    first = null;\n\t\t\t    last = last.prev;\n\t\t    }\n\t    }\n\n\t    /**\n\t     * 移动到链表头，表示这个节点是最新使用过的\n\t     *\n\t     * @param node\n\t     */\n\t    private void moveToHead(CacheNode node) {\n\t\t    if (node == first)\n\t\t\t    return;\n\t\t    if (node.prev != null)\n\t\t\t    node.prev.next = node.next;\n\t\t    if (node.next != null)\n\t\t\t    node.next.prev = node.prev;\n\t\t    if (last == node)\n\t\t\t    last = node.prev;\n\t\t    if (first != null) {\n\t\t\t    node.next = first;\n\t\t\t    first.prev = node;\n\t\t    }\n\t\t    first = node;\n\t\t    node.prev = null;\n\t\t    if (last == null)\n\t\t\t    last = first;\n\t    }\n\n\t    private int cacheSize;\n\t    private Hashtable nodes;// 缓存容器\n\t    private int currentSize;\n\t    private CacheNode first;// 链表头\n\t    private CacheNode last;// 链表尾\n    }\n","source":"_posts/LRU-算法.md","raw":"title: LRU 算法\ntags:\n  - 算法\ncategories:\n  - 算法\n  - LRU\ndate: 2016-06-03 02:39:14\n---\n\n转载：<http://flychao88.iteye.com/blog/1977653>\n\n<!-- more -->\n\n### LRU\n#### 原理\n\nLRU 是 Least Recently Used 的缩写，即最近最少使用算法。其核心思想是“如果数据最近被访问过，那么将来被访问的几率也更高”。\n\n#### 实现\n\n最常见的实现是使用一个链表保存缓存数据，详细算法实现如下：\n![LRU 算法](/uploads/20160603/lru.png)\n\n1. 新数据插入到链表头部；\n2. 每当缓存命中（即缓存数据被访问），则将数据移到链表头部；\n3. 当链表满的时候，将链表尾部的数据丢弃。\n\n#### 分析\n\n当存在热点数据时，LRU的效率很好，但偶发性的、周期性的批量操作会导致 LRU 命中率急剧下降，缓存污染情况比较严重。该算法实现简单。命中时需要遍历链表，找到命中的数据块索引，然后需要将数据移到头部。\n\n### LRU-K\n#### 原理\n\nLRU-K 中的 K 代表最近使用的次数，因此 LRU 可以认为是 LRU-1。LRU-K 的主要目的是为了解决 LRU 算法“缓存污染”的问题，其核心思想是将“最近使用过 1 次”的判断标准扩展为“最近使用过 K 次”。\n\n#### 实现\n\n相比 LRU，LRU-K 需要多维护一个队列，用于记录所有缓存数据被访问的历史。只有当数据的访问次数达到 K 次的时候，才将数据放入缓存。当需要淘汰数据时，LRU-K 会淘汰第 K 次访问时间距当前时间最大的数据。详细实现如下：\n![LRU－K 算法](/uploads/20160603/lru-k.png)\n\n1. 数据第一次被访问，加入到访问历史列表；\n2. 如果数据在访问历史列表里后没有达到 K 次访问，则按照一定规则（FIFO，LRU）淘汰；\n3. 当访问历史队列中的数据访问次数达到 K 次后，将数据索引从历史队列删除，将数据移到缓存队列中，并缓存此数据，缓存队列重新按照时间排序；\n4. 缓存数据队列中被再次访问后，重新排序；\n5. 需要淘汰数据时，淘汰缓存队列中排在末尾的数据，即：淘汰“倒数第 K 次访问离现在最久”的数据。\n\nLRU-K 具有 LRU 的优点，同时能够避免 LRU 的缺点，实际应用中 LRU-2 是综合各种因素后最优的选择，LRU-3 或者更大的 K 值命中率会高，但适应性差，需要大量的数据访问才能将历史访问记录清除掉。\n\n#### 分析\n\nLRU-K 降低了“缓存污染”带来的问题，命中率比 LRU 要高。LRU-K 算法复杂度和代价比较高。由于 LRU-K 还需要记录那些被访问过、但还没有放入缓存的对象，因此内存消耗会比LRU要多；当数据量很大的时候，内存消耗会比较可观。LRU-K 需要基于时间进行排序（可以需要淘汰时再排序，也可以即时排序），CPU消耗比 LRU 要高。\n\n### Two queues（2Q）\n#### 原理\n\nTwo queues（以下使用 2Q 代替）算法类似于 LRU-2，不同点在于 2Q 将 LRU-2 算法中的访问历史队列（注意这不是缓存数据的）改为一个 FIFO 缓存队列，即：2Q 算法有两个缓存队列，一个是 FIFO 队列，一个是 LRU 队列。\n\n#### 实现\n\n当数据第一次访问时，2Q 算法将数据缓存在 FIFO 队列里面，当数据第二次被访问时，则将数据从 FIFO 队列移到 LRU 队列里面，两个队列各自按照自己的方法淘汰数据。详细实现如下：\n![LRU－K 算法](/uploads/20160603/2q.png)\n\n1. 新访问的数据插入到 FIFO 队列；\n2. 如果数据在FIFO队列中一直没有被再次访问，则最终按照 FIFO 规则淘汰；\n3. 如果数据在 FIFO 队列中被再次访问，则将数据移到 LRU 队列头部；\n4. 如果数据在 LRU 队列再次被访问，则将数据移到 LRU 队列头部；\n5. LRU 队列淘汰末尾的数据。\n\n注：上图中 FIFO 队列比 LRU 队列短，但并不代表这是算法要求，实际应用中两者比例没有硬性规定。\n\n#### 分析\n\n2Q 算法的命中率要高于 LRU。2Q 算法需要两个队列，但两个队列本身都比较简单。2Q 算法的代价是 FIFO 和 LRU 的代价之和。2Q 算法和 LRU-2 算法命中率类似，内存消耗也比较接近，但对于最后缓存的数据来说，2Q 会减少一次从原始存储读取数据或者计算数据的操作。\n\n### Multi Queue（MQ）\n#### 原理\n\nMQ 算法根据访问频率将数据划分为多个队列，不同的队列具有不同的访问优先级，其核心思想是：优先缓存访问次数多的数据。\n\n#### 实现\n\nMQ 算法将缓存划分为多个 LRU 队列，每个队列对应不同的访问优先级。访问优先级是根据访问次数计算出来的，例如：详细的算法结构图如下，Q0，Q1....Qk 代表不同的优先级队列，Q-history 代表从缓存中淘汰数据，但记录了数据的索引和引用次数的队列：\n![LRU－K 算法](/uploads/20160603/mq.png)\n\n如上图，算法详细描述如下：\n\n1. 新插入的数据放入 Q0；\n2. 每个队列按照 LRU 管理数据；\n3. 当数据的访问次数达到一定次数，需要提升优先级时，将数据从当前队列删除，加入到高一级队列的头部；\n4. 为了防止高优先级数据永远不被淘汰，当数据在指定的时间里访问没有被访问时，需要降低优先级，将数据从当前队列删除，加入到低一级的队列头部；\n5. 需要淘汰数据时，从最低一级队列开始按照 LRU 淘汰；每个队列淘汰数据时，将数据从缓存中删除，将数据索引加入 Q-history 头部；\n6. 如果数据在 Q-history 中被重新访问，则重新计算其优先级，移到目标队列的头部；\n7. Q-history 按照 LRU 淘汰数据的索引。\n\n#### 分析\n\nMQ 降低了“缓存污染”带来的问题，命中率比 LRU 要高。MQ 需要维护多个队列，且需要维护每个数据的访问时间，复杂度比 LRU 高。MQ 需要记录每个数据的访问时间，需要定时扫描所有队列，代价比 LRU 要高。\n\n注：虽然 MQ 的队列看起来数量比较多，但由于所有队列之和受限于缓存容量的大小，因此这里多个队列长度之和跟一个 LRU 队列是一样的，因此队列扫描性能也相近。\n\n### LRU类算法对比\n\n由于不同的访问模型导致命中率变化较大，此处对比仅基于理论定性分析，不做定量分析。\n\n命中率：LRU-2 > MQ(2) > 2Q > LRU\n复杂度：LRU-2 > MQ(2) > 2Q > LRU\n代价：LRU-2 > MQ(2) > 2Q > LRU\n\n实际应用中需要根据业务的需求和对数据的访问情况进行选择，并不是命中率越高越好。例如：虽然LRU看起来命中率会低一些，且存在”缓存污染“的问题，但由于其简单和代价小，实际应用中反而应用更多。\n\n### Java LinkedHashMap 实现\n\nJava 中最简单的 LRU 算法实现，就是利用 jdk 的 LinkedHashMap，覆写其中的 removeEldestEntry(Map.Entry) 方法即可。如果你去看 LinkedHashMap 的源码可知，LRU 算法是通过双向链表来实现，当某个位置被命中，通过调整链表的指向将该位置调整到头位置，新加入的内容直接放在链表头，如此一来，最近被命中的内容就向链表头移动，需要替换时，链表最后的位置就是最近最少使用的位置。\n\n    import java.util.ArrayList;  \n    import java.util.Collection;  \n    import java.util.LinkedHashMap;  \n    import java.util.concurrent.locks.Lock;  \n    import java.util.concurrent.locks.ReentrantLock;  \n    import java.util.Map;  \n\n    /**\n     * 类说明：利用 LinkedHashMap 实现简单的缓存， 必须实现 removeEldestEntry 方法，具体参见 JDK 文档\n     *\n     * @param <K>\n     * @param <V>\n     */\n\n    public class LRULinkedHashMap<K, V> extends LinkedHashMap<K, V> {  \n      private final int maxCapacity;\n      private static final float DEFAULT_LOAD_FACTOR = 0.75f;\n      private final Lock lock = new ReentrantLock();\n\n      public LRULinkedHashMap(int maxCapacity) {  \n        super(maxCapacity, DEFAULT_LOAD_FACTOR, true);  \n        this.maxCapacity = maxCapacity;  \n      }\n\n      @Override\n      protected boolean removeEldestEntry(java.util.Map.Entry<K, V> eldest) {  \n        return size() > maxCapacity;  \n      }\n\n      @Override\n      public boolean containsKey(Object key) {  \n        try {  \n          lock.lock();  \n          return super.containsKey(key);  \n        } finally {  \n          lock.unlock();  \n        }\n      }\n\n      @Override\n      public V get(Object key) {  \n        try {  \n          lock.lock();  \n          return super.get(key);  \n        } finally {  \n          lock.unlock();  \n        }\n      }\n\n      @Override\n      public V put(K key, V value) {  \n        try {  \n          lock.lock();  \n          return super.put(key, value);  \n        } finally {  \n          lock.unlock();  \n        }  \n      }\n\n      public int size() {  \n        try {  \n          lock.lock();  \n          return super.size();  \n        } finally {  \n          lock.unlock();  \n        }  \n      }\n\n      public void clear() {  \n        try {  \n          lock.lock();  \n          super.clear();  \n        } finally {  \n          lock.unlock();  \n        }  \n      }  \n\n      public Collection<Map.Entry<K, V>> getAll() {\n        try {  \n          lock.lock();  \n          return new ArrayList<Map.Entry<K, V>>(super.entrySet());  \n        } finally {  \n          lock.unlock();  \n        }  \n      }  \n    }\n\n### 基于双链表的 LRU 实现\n\n传统意义的 LRU 算法是为每一个 Cache 对象设置一个计数器，每次 Cache 命中则给计数器 +1，而 Cache 用完，需要淘汰旧内容，放置新内容时，就查看所有的计数器，并将最少使用的内容替换掉。\n\n它的弊端很明显，如果 Cache 的数量少，问题不会很大，但是如果 Cache 的空间过大，达到 10W 或者 100W 以上，一旦需要淘汰，则需要遍历所有计算器，其性能与资源消耗是巨大的。效率也就非常的慢了。\n\n它的原理：将 Cache 的所有位置都用双连表连接起来，当一个位置被命中之后，就将通过调整链表的指向，将该位置调整到链表头的位置，新加入的 Cache 直接加到链表头中。\n\n这样，在多次进行 Cache 操作后，最近被命中的，就会被向链表头方向移动，而没有命中的，而想链表后面移动，链表尾则表示最近最少使用的 Cache。\n\n当需要替换内容时候，链表的最后位置就是最少被命中的位置，我们只需要淘汰链表最后的部分即可。\n\n上面说了这么多的理论， 下面用代码来实现一个 LRU 策略的缓存。我们用一个对象来表示 Cache，并实现双链表：\n\n    import java.util.Hashtable;\n\n    public class LRUCache {\n\t    class CacheNode {\n        // add your code here\n      }\n\n\t    private int cacheSize;// 缓存大小\n\t    private Hashtable nodes;// 缓存容器\n\t    private int currentSize;// 当前缓存对象数量\n\t    private CacheNode first;// (实现双链表)链表头\n\t    private CacheNode last;// (实现双链表)链表尾\n    }\n\n下面给出完整的实现，这个类也被 Tomcat 所使用（ org.apache.tomcat.util.collections.LRUCache），但是在 tomcat6.x 版本中，已经被弃用，使用另外其他的缓存类来替代它。\n\n    import java.util.Hashtable;\n\n    public class LRUCache {\n\t    class CacheNode {\n\t\t    CacheNode prev;// 前一节点\n\t\t    CacheNode next;// 后一节点\n\t\t    Object value;// 值\n\t\t    Object key;// 键\n\n\t\t    CacheNode() {\n\t\t    }\n\t    }\n\n\t    public LRUCache(int i) {\n\t\t    currentSize = 0;\n\t\t    cacheSize = i;\n\t\t    nodes = new Hashtable(i);// 缓存容器\n\t    }\n\n\t    /**\n\t     * 获取缓存中对象\n\t     *\n\t     * @param key\n\t     * @return\n\t     */\n\t    public Object get(Object key) {\n\t\t    CacheNode node = (CacheNode) nodes.get(key);\n\t\t    if (node != null) {\n\t\t\t    moveToHead(node);\n\t\t\t    return node.value;\n\t\t    } else {\n\t\t\t    return null;\n\t\t    }\n\t    }\n\n\t    /**\n\t     * 添加缓存\n\t     *\n\t     * @param key\n\t     * @param value\n\t     */\n\t    public void put(Object key, Object value) {\n\t\t    CacheNode node = (CacheNode) nodes.get(key);\n\n\t\t    if (node == null) {\n\t\t\t    // 缓存容器是否已经超过大小.\n\t\t\t    if (currentSize >= cacheSize) {\n\t\t\t\t    if (last != null)// 将最少使用的删除\n\t\t\t\t\t    nodes.remove(last.key);\n\t\t\t\t    removeLast();\n\t\t\t    } else {\n\t\t\t\t    currentSize++;\n\t\t\t    }\n\n\t\t\t    node = new CacheNode();\n\t\t    }\n\t\t    node.value = value;\n\t\t    node.key = key;\n\t\t    // 将最新使用的节点放到链表头，表示最新使用的.\n\t\t    moveToHead(node);\n\t\t    nodes.put(key, node);\n\t    }\n\n\t    /**\n\t     * 将缓存删除\n\t     *\n\t     * @param key\n\t     * @return\n\t     */\n\t    public Object remove(Object key) {\n\t\t    CacheNode node = (CacheNode) nodes.get(key);\n\t\t    if (node != null) {\n\t\t\t    if (node.prev != null) {\n\t\t\t\t    node.prev.next = node.next;\n\t\t\t    }\n\t\t\t    if (node.next != null) {\n\t\t\t\t    node.next.prev = node.prev;\n\t\t\t    }\n\t\t\t    if (last == node)\n\t\t\t\t    last = node.prev;\n\t\t\t    if (first == node)\n\t\t\t\t    first = node.next;\n\t\t    }\n\t\t    return node;\n\t    }\n\n\t    public void clear() {\n\t\t    first = null;\n\t \t    last = null;\n\t    }\n\n\t    /**\n\t     * 删除链表尾部节点 表示 删除最少使用的缓存对象\n\t     */\n\t    private void removeLast() {\n\t\t    // 链表尾不为空,则将链表尾指向null. 删除连表尾（删除最少使用的缓存对象）\n\t\t    if (last != null) {\n\t\t\t    if (last.prev != null)\n\t\t\t\t    last.prev.next = null;\n\t\t\t    else\n\t\t\t\t    first = null;\n\t\t\t    last = last.prev;\n\t\t    }\n\t    }\n\n\t    /**\n\t     * 移动到链表头，表示这个节点是最新使用过的\n\t     *\n\t     * @param node\n\t     */\n\t    private void moveToHead(CacheNode node) {\n\t\t    if (node == first)\n\t\t\t    return;\n\t\t    if (node.prev != null)\n\t\t\t    node.prev.next = node.next;\n\t\t    if (node.next != null)\n\t\t\t    node.next.prev = node.prev;\n\t\t    if (last == node)\n\t\t\t    last = node.prev;\n\t\t    if (first != null) {\n\t\t\t    node.next = first;\n\t\t\t    first.prev = node;\n\t\t    }\n\t\t    first = node;\n\t\t    node.prev = null;\n\t\t    if (last == null)\n\t\t\t    last = first;\n\t    }\n\n\t    private int cacheSize;\n\t    private Hashtable nodes;// 缓存容器\n\t    private int currentSize;\n\t    private CacheNode first;// 链表头\n\t    private CacheNode last;// 链表尾\n    }\n","slug":"LRU-算法","published":1,"updated":"2021-07-19T16:28:00.264Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphnx0060itd30ezr81et","content":"<p>转载：<a href=\"http://flychao88.iteye.com/blog/1977653\">http://flychao88.iteye.com/blog/1977653</a></p>\n<span id=\"more\"></span>\n\n<h3 id=\"LRU\"><a href=\"#LRU\" class=\"headerlink\" title=\"LRU\"></a>LRU</h3><h4 id=\"原理\"><a href=\"#原理\" class=\"headerlink\" title=\"原理\"></a>原理</h4><p>LRU 是 Least Recently Used 的缩写，即最近最少使用算法。其核心思想是“如果数据最近被访问过，那么将来被访问的几率也更高”。</p>\n<h4 id=\"实现\"><a href=\"#实现\" class=\"headerlink\" title=\"实现\"></a>实现</h4><p>最常见的实现是使用一个链表保存缓存数据，详细算法实现如下：<br><img src=\"/uploads/20160603/lru.png\" alt=\"LRU 算法\"></p>\n<ol>\n<li>新数据插入到链表头部；</li>\n<li>每当缓存命中（即缓存数据被访问），则将数据移到链表头部；</li>\n<li>当链表满的时候，将链表尾部的数据丢弃。</li>\n</ol>\n<h4 id=\"分析\"><a href=\"#分析\" class=\"headerlink\" title=\"分析\"></a>分析</h4><p>当存在热点数据时，LRU的效率很好，但偶发性的、周期性的批量操作会导致 LRU 命中率急剧下降，缓存污染情况比较严重。该算法实现简单。命中时需要遍历链表，找到命中的数据块索引，然后需要将数据移到头部。</p>\n<h3 id=\"LRU-K\"><a href=\"#LRU-K\" class=\"headerlink\" title=\"LRU-K\"></a>LRU-K</h3><h4 id=\"原理-1\"><a href=\"#原理-1\" class=\"headerlink\" title=\"原理\"></a>原理</h4><p>LRU-K 中的 K 代表最近使用的次数，因此 LRU 可以认为是 LRU-1。LRU-K 的主要目的是为了解决 LRU 算法“缓存污染”的问题，其核心思想是将“最近使用过 1 次”的判断标准扩展为“最近使用过 K 次”。</p>\n<h4 id=\"实现-1\"><a href=\"#实现-1\" class=\"headerlink\" title=\"实现\"></a>实现</h4><p>相比 LRU，LRU-K 需要多维护一个队列，用于记录所有缓存数据被访问的历史。只有当数据的访问次数达到 K 次的时候，才将数据放入缓存。当需要淘汰数据时，LRU-K 会淘汰第 K 次访问时间距当前时间最大的数据。详细实现如下：<br><img src=\"/uploads/20160603/lru-k.png\" alt=\"LRU－K 算法\"></p>\n<ol>\n<li>数据第一次被访问，加入到访问历史列表；</li>\n<li>如果数据在访问历史列表里后没有达到 K 次访问，则按照一定规则（FIFO，LRU）淘汰；</li>\n<li>当访问历史队列中的数据访问次数达到 K 次后，将数据索引从历史队列删除，将数据移到缓存队列中，并缓存此数据，缓存队列重新按照时间排序；</li>\n<li>缓存数据队列中被再次访问后，重新排序；</li>\n<li>需要淘汰数据时，淘汰缓存队列中排在末尾的数据，即：淘汰“倒数第 K 次访问离现在最久”的数据。</li>\n</ol>\n<p>LRU-K 具有 LRU 的优点，同时能够避免 LRU 的缺点，实际应用中 LRU-2 是综合各种因素后最优的选择，LRU-3 或者更大的 K 值命中率会高，但适应性差，需要大量的数据访问才能将历史访问记录清除掉。</p>\n<h4 id=\"分析-1\"><a href=\"#分析-1\" class=\"headerlink\" title=\"分析\"></a>分析</h4><p>LRU-K 降低了“缓存污染”带来的问题，命中率比 LRU 要高。LRU-K 算法复杂度和代价比较高。由于 LRU-K 还需要记录那些被访问过、但还没有放入缓存的对象，因此内存消耗会比LRU要多；当数据量很大的时候，内存消耗会比较可观。LRU-K 需要基于时间进行排序（可以需要淘汰时再排序，也可以即时排序），CPU消耗比 LRU 要高。</p>\n<h3 id=\"Two-queues（2Q）\"><a href=\"#Two-queues（2Q）\" class=\"headerlink\" title=\"Two queues（2Q）\"></a>Two queues（2Q）</h3><h4 id=\"原理-2\"><a href=\"#原理-2\" class=\"headerlink\" title=\"原理\"></a>原理</h4><p>Two queues（以下使用 2Q 代替）算法类似于 LRU-2，不同点在于 2Q 将 LRU-2 算法中的访问历史队列（注意这不是缓存数据的）改为一个 FIFO 缓存队列，即：2Q 算法有两个缓存队列，一个是 FIFO 队列，一个是 LRU 队列。</p>\n<h4 id=\"实现-2\"><a href=\"#实现-2\" class=\"headerlink\" title=\"实现\"></a>实现</h4><p>当数据第一次访问时，2Q 算法将数据缓存在 FIFO 队列里面，当数据第二次被访问时，则将数据从 FIFO 队列移到 LRU 队列里面，两个队列各自按照自己的方法淘汰数据。详细实现如下：<br><img src=\"/uploads/20160603/2q.png\" alt=\"LRU－K 算法\"></p>\n<ol>\n<li>新访问的数据插入到 FIFO 队列；</li>\n<li>如果数据在FIFO队列中一直没有被再次访问，则最终按照 FIFO 规则淘汰；</li>\n<li>如果数据在 FIFO 队列中被再次访问，则将数据移到 LRU 队列头部；</li>\n<li>如果数据在 LRU 队列再次被访问，则将数据移到 LRU 队列头部；</li>\n<li>LRU 队列淘汰末尾的数据。</li>\n</ol>\n<p>注：上图中 FIFO 队列比 LRU 队列短，但并不代表这是算法要求，实际应用中两者比例没有硬性规定。</p>\n<h4 id=\"分析-2\"><a href=\"#分析-2\" class=\"headerlink\" title=\"分析\"></a>分析</h4><p>2Q 算法的命中率要高于 LRU。2Q 算法需要两个队列，但两个队列本身都比较简单。2Q 算法的代价是 FIFO 和 LRU 的代价之和。2Q 算法和 LRU-2 算法命中率类似，内存消耗也比较接近，但对于最后缓存的数据来说，2Q 会减少一次从原始存储读取数据或者计算数据的操作。</p>\n<h3 id=\"Multi-Queue（MQ）\"><a href=\"#Multi-Queue（MQ）\" class=\"headerlink\" title=\"Multi Queue（MQ）\"></a>Multi Queue（MQ）</h3><h4 id=\"原理-3\"><a href=\"#原理-3\" class=\"headerlink\" title=\"原理\"></a>原理</h4><p>MQ 算法根据访问频率将数据划分为多个队列，不同的队列具有不同的访问优先级，其核心思想是：优先缓存访问次数多的数据。</p>\n<h4 id=\"实现-3\"><a href=\"#实现-3\" class=\"headerlink\" title=\"实现\"></a>实现</h4><p>MQ 算法将缓存划分为多个 LRU 队列，每个队列对应不同的访问优先级。访问优先级是根据访问次数计算出来的，例如：详细的算法结构图如下，Q0，Q1….Qk 代表不同的优先级队列，Q-history 代表从缓存中淘汰数据，但记录了数据的索引和引用次数的队列：<br><img src=\"/uploads/20160603/mq.png\" alt=\"LRU－K 算法\"></p>\n<p>如上图，算法详细描述如下：</p>\n<ol>\n<li>新插入的数据放入 Q0；</li>\n<li>每个队列按照 LRU 管理数据；</li>\n<li>当数据的访问次数达到一定次数，需要提升优先级时，将数据从当前队列删除，加入到高一级队列的头部；</li>\n<li>为了防止高优先级数据永远不被淘汰，当数据在指定的时间里访问没有被访问时，需要降低优先级，将数据从当前队列删除，加入到低一级的队列头部；</li>\n<li>需要淘汰数据时，从最低一级队列开始按照 LRU 淘汰；每个队列淘汰数据时，将数据从缓存中删除，将数据索引加入 Q-history 头部；</li>\n<li>如果数据在 Q-history 中被重新访问，则重新计算其优先级，移到目标队列的头部；</li>\n<li>Q-history 按照 LRU 淘汰数据的索引。</li>\n</ol>\n<h4 id=\"分析-3\"><a href=\"#分析-3\" class=\"headerlink\" title=\"分析\"></a>分析</h4><p>MQ 降低了“缓存污染”带来的问题，命中率比 LRU 要高。MQ 需要维护多个队列，且需要维护每个数据的访问时间，复杂度比 LRU 高。MQ 需要记录每个数据的访问时间，需要定时扫描所有队列，代价比 LRU 要高。</p>\n<p>注：虽然 MQ 的队列看起来数量比较多，但由于所有队列之和受限于缓存容量的大小，因此这里多个队列长度之和跟一个 LRU 队列是一样的，因此队列扫描性能也相近。</p>\n<h3 id=\"LRU类算法对比\"><a href=\"#LRU类算法对比\" class=\"headerlink\" title=\"LRU类算法对比\"></a>LRU类算法对比</h3><p>由于不同的访问模型导致命中率变化较大，此处对比仅基于理论定性分析，不做定量分析。</p>\n<p>命中率：LRU-2 &gt; MQ(2) &gt; 2Q &gt; LRU<br>复杂度：LRU-2 &gt; MQ(2) &gt; 2Q &gt; LRU<br>代价：LRU-2 &gt; MQ(2) &gt; 2Q &gt; LRU</p>\n<p>实际应用中需要根据业务的需求和对数据的访问情况进行选择，并不是命中率越高越好。例如：虽然LRU看起来命中率会低一些，且存在”缓存污染“的问题，但由于其简单和代价小，实际应用中反而应用更多。</p>\n<h3 id=\"Java-LinkedHashMap-实现\"><a href=\"#Java-LinkedHashMap-实现\" class=\"headerlink\" title=\"Java LinkedHashMap 实现\"></a>Java LinkedHashMap 实现</h3><p>Java 中最简单的 LRU 算法实现，就是利用 jdk 的 LinkedHashMap，覆写其中的 removeEldestEntry(Map.Entry) 方法即可。如果你去看 LinkedHashMap 的源码可知，LRU 算法是通过双向链表来实现，当某个位置被命中，通过调整链表的指向将该位置调整到头位置，新加入的内容直接放在链表头，如此一来，最近被命中的内容就向链表头移动，需要替换时，链表最后的位置就是最近最少使用的位置。</p>\n<pre><code>import java.util.ArrayList;  \nimport java.util.Collection;  \nimport java.util.LinkedHashMap;  \nimport java.util.concurrent.locks.Lock;  \nimport java.util.concurrent.locks.ReentrantLock;  \nimport java.util.Map;  \n\n/**\n * 类说明：利用 LinkedHashMap 实现简单的缓存， 必须实现 removeEldestEntry 方法，具体参见 JDK 文档\n *\n * @param &lt;K&gt;\n * @param &lt;V&gt;\n */\n\npublic class LRULinkedHashMap&lt;K, V&gt; extends LinkedHashMap&lt;K, V&gt; &#123;  \n  private final int maxCapacity;\n  private static final float DEFAULT_LOAD_FACTOR = 0.75f;\n  private final Lock lock = new ReentrantLock();\n\n  public LRULinkedHashMap(int maxCapacity) &#123;  \n    super(maxCapacity, DEFAULT_LOAD_FACTOR, true);  \n    this.maxCapacity = maxCapacity;  \n  &#125;\n\n  @Override\n  protected boolean removeEldestEntry(java.util.Map.Entry&lt;K, V&gt; eldest) &#123;  \n    return size() &gt; maxCapacity;  \n  &#125;\n\n  @Override\n  public boolean containsKey(Object key) &#123;  \n    try &#123;  \n      lock.lock();  \n      return super.containsKey(key);  \n    &#125; finally &#123;  \n      lock.unlock();  \n    &#125;\n  &#125;\n\n  @Override\n  public V get(Object key) &#123;  \n    try &#123;  \n      lock.lock();  \n      return super.get(key);  \n    &#125; finally &#123;  \n      lock.unlock();  \n    &#125;\n  &#125;\n\n  @Override\n  public V put(K key, V value) &#123;  \n    try &#123;  \n      lock.lock();  \n      return super.put(key, value);  \n    &#125; finally &#123;  \n      lock.unlock();  \n    &#125;  \n  &#125;\n\n  public int size() &#123;  \n    try &#123;  \n      lock.lock();  \n      return super.size();  \n    &#125; finally &#123;  \n      lock.unlock();  \n    &#125;  \n  &#125;\n\n  public void clear() &#123;  \n    try &#123;  \n      lock.lock();  \n      super.clear();  \n    &#125; finally &#123;  \n      lock.unlock();  \n    &#125;  \n  &#125;  \n\n  public Collection&lt;Map.Entry&lt;K, V&gt;&gt; getAll() &#123;\n    try &#123;  \n      lock.lock();  \n      return new ArrayList&lt;Map.Entry&lt;K, V&gt;&gt;(super.entrySet());  \n    &#125; finally &#123;  \n      lock.unlock();  \n    &#125;  \n  &#125;  \n&#125;\n</code></pre>\n<h3 id=\"基于双链表的-LRU-实现\"><a href=\"#基于双链表的-LRU-实现\" class=\"headerlink\" title=\"基于双链表的 LRU 实现\"></a>基于双链表的 LRU 实现</h3><p>传统意义的 LRU 算法是为每一个 Cache 对象设置一个计数器，每次 Cache 命中则给计数器 +1，而 Cache 用完，需要淘汰旧内容，放置新内容时，就查看所有的计数器，并将最少使用的内容替换掉。</p>\n<p>它的弊端很明显，如果 Cache 的数量少，问题不会很大，但是如果 Cache 的空间过大，达到 10W 或者 100W 以上，一旦需要淘汰，则需要遍历所有计算器，其性能与资源消耗是巨大的。效率也就非常的慢了。</p>\n<p>它的原理：将 Cache 的所有位置都用双连表连接起来，当一个位置被命中之后，就将通过调整链表的指向，将该位置调整到链表头的位置，新加入的 Cache 直接加到链表头中。</p>\n<p>这样，在多次进行 Cache 操作后，最近被命中的，就会被向链表头方向移动，而没有命中的，而想链表后面移动，链表尾则表示最近最少使用的 Cache。</p>\n<p>当需要替换内容时候，链表的最后位置就是最少被命中的位置，我们只需要淘汰链表最后的部分即可。</p>\n<p>上面说了这么多的理论， 下面用代码来实现一个 LRU 策略的缓存。我们用一个对象来表示 Cache，并实现双链表：</p>\n<pre><code>import java.util.Hashtable;\n\npublic class LRUCache &#123;\n    class CacheNode &#123;\n    // add your code here\n  &#125;\n\n    private int cacheSize;// 缓存大小\n    private Hashtable nodes;// 缓存容器\n    private int currentSize;// 当前缓存对象数量\n    private CacheNode first;// (实现双链表)链表头\n    private CacheNode last;// (实现双链表)链表尾\n&#125;\n</code></pre>\n<p>下面给出完整的实现，这个类也被 Tomcat 所使用（ org.apache.tomcat.util.collections.LRUCache），但是在 tomcat6.x 版本中，已经被弃用，使用另外其他的缓存类来替代它。</p>\n<pre><code>import java.util.Hashtable;\n\npublic class LRUCache &#123;\n    class CacheNode &#123;\n        CacheNode prev;// 前一节点\n        CacheNode next;// 后一节点\n        Object value;// 值\n        Object key;// 键\n\n        CacheNode() &#123;\n        &#125;\n    &#125;\n\n    public LRUCache(int i) &#123;\n        currentSize = 0;\n        cacheSize = i;\n        nodes = new Hashtable(i);// 缓存容器\n    &#125;\n\n    /**\n     * 获取缓存中对象\n     *\n     * @param key\n     * @return\n     */\n    public Object get(Object key) &#123;\n        CacheNode node = (CacheNode) nodes.get(key);\n        if (node != null) &#123;\n            moveToHead(node);\n            return node.value;\n        &#125; else &#123;\n            return null;\n        &#125;\n    &#125;\n\n    /**\n     * 添加缓存\n     *\n     * @param key\n     * @param value\n     */\n    public void put(Object key, Object value) &#123;\n        CacheNode node = (CacheNode) nodes.get(key);\n\n        if (node == null) &#123;\n            // 缓存容器是否已经超过大小.\n            if (currentSize &gt;= cacheSize) &#123;\n                if (last != null)// 将最少使用的删除\n                    nodes.remove(last.key);\n                removeLast();\n            &#125; else &#123;\n                currentSize++;\n            &#125;\n\n            node = new CacheNode();\n        &#125;\n        node.value = value;\n        node.key = key;\n        // 将最新使用的节点放到链表头，表示最新使用的.\n        moveToHead(node);\n        nodes.put(key, node);\n    &#125;\n\n    /**\n     * 将缓存删除\n     *\n     * @param key\n     * @return\n     */\n    public Object remove(Object key) &#123;\n        CacheNode node = (CacheNode) nodes.get(key);\n        if (node != null) &#123;\n            if (node.prev != null) &#123;\n                node.prev.next = node.next;\n            &#125;\n            if (node.next != null) &#123;\n                node.next.prev = node.prev;\n            &#125;\n            if (last == node)\n                last = node.prev;\n            if (first == node)\n                first = node.next;\n        &#125;\n        return node;\n    &#125;\n\n    public void clear() &#123;\n        first = null;\n         last = null;\n    &#125;\n\n    /**\n     * 删除链表尾部节点 表示 删除最少使用的缓存对象\n     */\n    private void removeLast() &#123;\n        // 链表尾不为空,则将链表尾指向null. 删除连表尾（删除最少使用的缓存对象）\n        if (last != null) &#123;\n            if (last.prev != null)\n                last.prev.next = null;\n            else\n                first = null;\n            last = last.prev;\n        &#125;\n    &#125;\n\n    /**\n     * 移动到链表头，表示这个节点是最新使用过的\n     *\n     * @param node\n     */\n    private void moveToHead(CacheNode node) &#123;\n        if (node == first)\n            return;\n        if (node.prev != null)\n            node.prev.next = node.next;\n        if (node.next != null)\n            node.next.prev = node.prev;\n        if (last == node)\n            last = node.prev;\n        if (first != null) &#123;\n            node.next = first;\n            first.prev = node;\n        &#125;\n        first = node;\n        node.prev = null;\n        if (last == null)\n            last = first;\n    &#125;\n\n    private int cacheSize;\n    private Hashtable nodes;// 缓存容器\n    private int currentSize;\n    private CacheNode first;// 链表头\n    private CacheNode last;// 链表尾\n&#125;\n</code></pre>\n","site":{"data":{}},"excerpt":"<p>转载：<a href=\"http://flychao88.iteye.com/blog/1977653\">http://flychao88.iteye.com/blog/1977653</a></p>","more":"<h3 id=\"LRU\"><a href=\"#LRU\" class=\"headerlink\" title=\"LRU\"></a>LRU</h3><h4 id=\"原理\"><a href=\"#原理\" class=\"headerlink\" title=\"原理\"></a>原理</h4><p>LRU 是 Least Recently Used 的缩写，即最近最少使用算法。其核心思想是“如果数据最近被访问过，那么将来被访问的几率也更高”。</p>\n<h4 id=\"实现\"><a href=\"#实现\" class=\"headerlink\" title=\"实现\"></a>实现</h4><p>最常见的实现是使用一个链表保存缓存数据，详细算法实现如下：<br><img src=\"/uploads/20160603/lru.png\" alt=\"LRU 算法\"></p>\n<ol>\n<li>新数据插入到链表头部；</li>\n<li>每当缓存命中（即缓存数据被访问），则将数据移到链表头部；</li>\n<li>当链表满的时候，将链表尾部的数据丢弃。</li>\n</ol>\n<h4 id=\"分析\"><a href=\"#分析\" class=\"headerlink\" title=\"分析\"></a>分析</h4><p>当存在热点数据时，LRU的效率很好，但偶发性的、周期性的批量操作会导致 LRU 命中率急剧下降，缓存污染情况比较严重。该算法实现简单。命中时需要遍历链表，找到命中的数据块索引，然后需要将数据移到头部。</p>\n<h3 id=\"LRU-K\"><a href=\"#LRU-K\" class=\"headerlink\" title=\"LRU-K\"></a>LRU-K</h3><h4 id=\"原理-1\"><a href=\"#原理-1\" class=\"headerlink\" title=\"原理\"></a>原理</h4><p>LRU-K 中的 K 代表最近使用的次数，因此 LRU 可以认为是 LRU-1。LRU-K 的主要目的是为了解决 LRU 算法“缓存污染”的问题，其核心思想是将“最近使用过 1 次”的判断标准扩展为“最近使用过 K 次”。</p>\n<h4 id=\"实现-1\"><a href=\"#实现-1\" class=\"headerlink\" title=\"实现\"></a>实现</h4><p>相比 LRU，LRU-K 需要多维护一个队列，用于记录所有缓存数据被访问的历史。只有当数据的访问次数达到 K 次的时候，才将数据放入缓存。当需要淘汰数据时，LRU-K 会淘汰第 K 次访问时间距当前时间最大的数据。详细实现如下：<br><img src=\"/uploads/20160603/lru-k.png\" alt=\"LRU－K 算法\"></p>\n<ol>\n<li>数据第一次被访问，加入到访问历史列表；</li>\n<li>如果数据在访问历史列表里后没有达到 K 次访问，则按照一定规则（FIFO，LRU）淘汰；</li>\n<li>当访问历史队列中的数据访问次数达到 K 次后，将数据索引从历史队列删除，将数据移到缓存队列中，并缓存此数据，缓存队列重新按照时间排序；</li>\n<li>缓存数据队列中被再次访问后，重新排序；</li>\n<li>需要淘汰数据时，淘汰缓存队列中排在末尾的数据，即：淘汰“倒数第 K 次访问离现在最久”的数据。</li>\n</ol>\n<p>LRU-K 具有 LRU 的优点，同时能够避免 LRU 的缺点，实际应用中 LRU-2 是综合各种因素后最优的选择，LRU-3 或者更大的 K 值命中率会高，但适应性差，需要大量的数据访问才能将历史访问记录清除掉。</p>\n<h4 id=\"分析-1\"><a href=\"#分析-1\" class=\"headerlink\" title=\"分析\"></a>分析</h4><p>LRU-K 降低了“缓存污染”带来的问题，命中率比 LRU 要高。LRU-K 算法复杂度和代价比较高。由于 LRU-K 还需要记录那些被访问过、但还没有放入缓存的对象，因此内存消耗会比LRU要多；当数据量很大的时候，内存消耗会比较可观。LRU-K 需要基于时间进行排序（可以需要淘汰时再排序，也可以即时排序），CPU消耗比 LRU 要高。</p>\n<h3 id=\"Two-queues（2Q）\"><a href=\"#Two-queues（2Q）\" class=\"headerlink\" title=\"Two queues（2Q）\"></a>Two queues（2Q）</h3><h4 id=\"原理-2\"><a href=\"#原理-2\" class=\"headerlink\" title=\"原理\"></a>原理</h4><p>Two queues（以下使用 2Q 代替）算法类似于 LRU-2，不同点在于 2Q 将 LRU-2 算法中的访问历史队列（注意这不是缓存数据的）改为一个 FIFO 缓存队列，即：2Q 算法有两个缓存队列，一个是 FIFO 队列，一个是 LRU 队列。</p>\n<h4 id=\"实现-2\"><a href=\"#实现-2\" class=\"headerlink\" title=\"实现\"></a>实现</h4><p>当数据第一次访问时，2Q 算法将数据缓存在 FIFO 队列里面，当数据第二次被访问时，则将数据从 FIFO 队列移到 LRU 队列里面，两个队列各自按照自己的方法淘汰数据。详细实现如下：<br><img src=\"/uploads/20160603/2q.png\" alt=\"LRU－K 算法\"></p>\n<ol>\n<li>新访问的数据插入到 FIFO 队列；</li>\n<li>如果数据在FIFO队列中一直没有被再次访问，则最终按照 FIFO 规则淘汰；</li>\n<li>如果数据在 FIFO 队列中被再次访问，则将数据移到 LRU 队列头部；</li>\n<li>如果数据在 LRU 队列再次被访问，则将数据移到 LRU 队列头部；</li>\n<li>LRU 队列淘汰末尾的数据。</li>\n</ol>\n<p>注：上图中 FIFO 队列比 LRU 队列短，但并不代表这是算法要求，实际应用中两者比例没有硬性规定。</p>\n<h4 id=\"分析-2\"><a href=\"#分析-2\" class=\"headerlink\" title=\"分析\"></a>分析</h4><p>2Q 算法的命中率要高于 LRU。2Q 算法需要两个队列，但两个队列本身都比较简单。2Q 算法的代价是 FIFO 和 LRU 的代价之和。2Q 算法和 LRU-2 算法命中率类似，内存消耗也比较接近，但对于最后缓存的数据来说，2Q 会减少一次从原始存储读取数据或者计算数据的操作。</p>\n<h3 id=\"Multi-Queue（MQ）\"><a href=\"#Multi-Queue（MQ）\" class=\"headerlink\" title=\"Multi Queue（MQ）\"></a>Multi Queue（MQ）</h3><h4 id=\"原理-3\"><a href=\"#原理-3\" class=\"headerlink\" title=\"原理\"></a>原理</h4><p>MQ 算法根据访问频率将数据划分为多个队列，不同的队列具有不同的访问优先级，其核心思想是：优先缓存访问次数多的数据。</p>\n<h4 id=\"实现-3\"><a href=\"#实现-3\" class=\"headerlink\" title=\"实现\"></a>实现</h4><p>MQ 算法将缓存划分为多个 LRU 队列，每个队列对应不同的访问优先级。访问优先级是根据访问次数计算出来的，例如：详细的算法结构图如下，Q0，Q1….Qk 代表不同的优先级队列，Q-history 代表从缓存中淘汰数据，但记录了数据的索引和引用次数的队列：<br><img src=\"/uploads/20160603/mq.png\" alt=\"LRU－K 算法\"></p>\n<p>如上图，算法详细描述如下：</p>\n<ol>\n<li>新插入的数据放入 Q0；</li>\n<li>每个队列按照 LRU 管理数据；</li>\n<li>当数据的访问次数达到一定次数，需要提升优先级时，将数据从当前队列删除，加入到高一级队列的头部；</li>\n<li>为了防止高优先级数据永远不被淘汰，当数据在指定的时间里访问没有被访问时，需要降低优先级，将数据从当前队列删除，加入到低一级的队列头部；</li>\n<li>需要淘汰数据时，从最低一级队列开始按照 LRU 淘汰；每个队列淘汰数据时，将数据从缓存中删除，将数据索引加入 Q-history 头部；</li>\n<li>如果数据在 Q-history 中被重新访问，则重新计算其优先级，移到目标队列的头部；</li>\n<li>Q-history 按照 LRU 淘汰数据的索引。</li>\n</ol>\n<h4 id=\"分析-3\"><a href=\"#分析-3\" class=\"headerlink\" title=\"分析\"></a>分析</h4><p>MQ 降低了“缓存污染”带来的问题，命中率比 LRU 要高。MQ 需要维护多个队列，且需要维护每个数据的访问时间，复杂度比 LRU 高。MQ 需要记录每个数据的访问时间，需要定时扫描所有队列，代价比 LRU 要高。</p>\n<p>注：虽然 MQ 的队列看起来数量比较多，但由于所有队列之和受限于缓存容量的大小，因此这里多个队列长度之和跟一个 LRU 队列是一样的，因此队列扫描性能也相近。</p>\n<h3 id=\"LRU类算法对比\"><a href=\"#LRU类算法对比\" class=\"headerlink\" title=\"LRU类算法对比\"></a>LRU类算法对比</h3><p>由于不同的访问模型导致命中率变化较大，此处对比仅基于理论定性分析，不做定量分析。</p>\n<p>命中率：LRU-2 &gt; MQ(2) &gt; 2Q &gt; LRU<br>复杂度：LRU-2 &gt; MQ(2) &gt; 2Q &gt; LRU<br>代价：LRU-2 &gt; MQ(2) &gt; 2Q &gt; LRU</p>\n<p>实际应用中需要根据业务的需求和对数据的访问情况进行选择，并不是命中率越高越好。例如：虽然LRU看起来命中率会低一些，且存在”缓存污染“的问题，但由于其简单和代价小，实际应用中反而应用更多。</p>\n<h3 id=\"Java-LinkedHashMap-实现\"><a href=\"#Java-LinkedHashMap-实现\" class=\"headerlink\" title=\"Java LinkedHashMap 实现\"></a>Java LinkedHashMap 实现</h3><p>Java 中最简单的 LRU 算法实现，就是利用 jdk 的 LinkedHashMap，覆写其中的 removeEldestEntry(Map.Entry) 方法即可。如果你去看 LinkedHashMap 的源码可知，LRU 算法是通过双向链表来实现，当某个位置被命中，通过调整链表的指向将该位置调整到头位置，新加入的内容直接放在链表头，如此一来，最近被命中的内容就向链表头移动，需要替换时，链表最后的位置就是最近最少使用的位置。</p>\n<pre><code>import java.util.ArrayList;  \nimport java.util.Collection;  \nimport java.util.LinkedHashMap;  \nimport java.util.concurrent.locks.Lock;  \nimport java.util.concurrent.locks.ReentrantLock;  \nimport java.util.Map;  \n\n/**\n * 类说明：利用 LinkedHashMap 实现简单的缓存， 必须实现 removeEldestEntry 方法，具体参见 JDK 文档\n *\n * @param &lt;K&gt;\n * @param &lt;V&gt;\n */\n\npublic class LRULinkedHashMap&lt;K, V&gt; extends LinkedHashMap&lt;K, V&gt; &#123;  \n  private final int maxCapacity;\n  private static final float DEFAULT_LOAD_FACTOR = 0.75f;\n  private final Lock lock = new ReentrantLock();\n\n  public LRULinkedHashMap(int maxCapacity) &#123;  \n    super(maxCapacity, DEFAULT_LOAD_FACTOR, true);  \n    this.maxCapacity = maxCapacity;  \n  &#125;\n\n  @Override\n  protected boolean removeEldestEntry(java.util.Map.Entry&lt;K, V&gt; eldest) &#123;  \n    return size() &gt; maxCapacity;  \n  &#125;\n\n  @Override\n  public boolean containsKey(Object key) &#123;  \n    try &#123;  \n      lock.lock();  \n      return super.containsKey(key);  \n    &#125; finally &#123;  \n      lock.unlock();  \n    &#125;\n  &#125;\n\n  @Override\n  public V get(Object key) &#123;  \n    try &#123;  \n      lock.lock();  \n      return super.get(key);  \n    &#125; finally &#123;  \n      lock.unlock();  \n    &#125;\n  &#125;\n\n  @Override\n  public V put(K key, V value) &#123;  \n    try &#123;  \n      lock.lock();  \n      return super.put(key, value);  \n    &#125; finally &#123;  \n      lock.unlock();  \n    &#125;  \n  &#125;\n\n  public int size() &#123;  \n    try &#123;  \n      lock.lock();  \n      return super.size();  \n    &#125; finally &#123;  \n      lock.unlock();  \n    &#125;  \n  &#125;\n\n  public void clear() &#123;  \n    try &#123;  \n      lock.lock();  \n      super.clear();  \n    &#125; finally &#123;  \n      lock.unlock();  \n    &#125;  \n  &#125;  \n\n  public Collection&lt;Map.Entry&lt;K, V&gt;&gt; getAll() &#123;\n    try &#123;  \n      lock.lock();  \n      return new ArrayList&lt;Map.Entry&lt;K, V&gt;&gt;(super.entrySet());  \n    &#125; finally &#123;  \n      lock.unlock();  \n    &#125;  \n  &#125;  \n&#125;\n</code></pre>\n<h3 id=\"基于双链表的-LRU-实现\"><a href=\"#基于双链表的-LRU-实现\" class=\"headerlink\" title=\"基于双链表的 LRU 实现\"></a>基于双链表的 LRU 实现</h3><p>传统意义的 LRU 算法是为每一个 Cache 对象设置一个计数器，每次 Cache 命中则给计数器 +1，而 Cache 用完，需要淘汰旧内容，放置新内容时，就查看所有的计数器，并将最少使用的内容替换掉。</p>\n<p>它的弊端很明显，如果 Cache 的数量少，问题不会很大，但是如果 Cache 的空间过大，达到 10W 或者 100W 以上，一旦需要淘汰，则需要遍历所有计算器，其性能与资源消耗是巨大的。效率也就非常的慢了。</p>\n<p>它的原理：将 Cache 的所有位置都用双连表连接起来，当一个位置被命中之后，就将通过调整链表的指向，将该位置调整到链表头的位置，新加入的 Cache 直接加到链表头中。</p>\n<p>这样，在多次进行 Cache 操作后，最近被命中的，就会被向链表头方向移动，而没有命中的，而想链表后面移动，链表尾则表示最近最少使用的 Cache。</p>\n<p>当需要替换内容时候，链表的最后位置就是最少被命中的位置，我们只需要淘汰链表最后的部分即可。</p>\n<p>上面说了这么多的理论， 下面用代码来实现一个 LRU 策略的缓存。我们用一个对象来表示 Cache，并实现双链表：</p>\n<pre><code>import java.util.Hashtable;\n\npublic class LRUCache &#123;\n    class CacheNode &#123;\n    // add your code here\n  &#125;\n\n    private int cacheSize;// 缓存大小\n    private Hashtable nodes;// 缓存容器\n    private int currentSize;// 当前缓存对象数量\n    private CacheNode first;// (实现双链表)链表头\n    private CacheNode last;// (实现双链表)链表尾\n&#125;\n</code></pre>\n<p>下面给出完整的实现，这个类也被 Tomcat 所使用（ org.apache.tomcat.util.collections.LRUCache），但是在 tomcat6.x 版本中，已经被弃用，使用另外其他的缓存类来替代它。</p>\n<pre><code>import java.util.Hashtable;\n\npublic class LRUCache &#123;\n    class CacheNode &#123;\n        CacheNode prev;// 前一节点\n        CacheNode next;// 后一节点\n        Object value;// 值\n        Object key;// 键\n\n        CacheNode() &#123;\n        &#125;\n    &#125;\n\n    public LRUCache(int i) &#123;\n        currentSize = 0;\n        cacheSize = i;\n        nodes = new Hashtable(i);// 缓存容器\n    &#125;\n\n    /**\n     * 获取缓存中对象\n     *\n     * @param key\n     * @return\n     */\n    public Object get(Object key) &#123;\n        CacheNode node = (CacheNode) nodes.get(key);\n        if (node != null) &#123;\n            moveToHead(node);\n            return node.value;\n        &#125; else &#123;\n            return null;\n        &#125;\n    &#125;\n\n    /**\n     * 添加缓存\n     *\n     * @param key\n     * @param value\n     */\n    public void put(Object key, Object value) &#123;\n        CacheNode node = (CacheNode) nodes.get(key);\n\n        if (node == null) &#123;\n            // 缓存容器是否已经超过大小.\n            if (currentSize &gt;= cacheSize) &#123;\n                if (last != null)// 将最少使用的删除\n                    nodes.remove(last.key);\n                removeLast();\n            &#125; else &#123;\n                currentSize++;\n            &#125;\n\n            node = new CacheNode();\n        &#125;\n        node.value = value;\n        node.key = key;\n        // 将最新使用的节点放到链表头，表示最新使用的.\n        moveToHead(node);\n        nodes.put(key, node);\n    &#125;\n\n    /**\n     * 将缓存删除\n     *\n     * @param key\n     * @return\n     */\n    public Object remove(Object key) &#123;\n        CacheNode node = (CacheNode) nodes.get(key);\n        if (node != null) &#123;\n            if (node.prev != null) &#123;\n                node.prev.next = node.next;\n            &#125;\n            if (node.next != null) &#123;\n                node.next.prev = node.prev;\n            &#125;\n            if (last == node)\n                last = node.prev;\n            if (first == node)\n                first = node.next;\n        &#125;\n        return node;\n    &#125;\n\n    public void clear() &#123;\n        first = null;\n         last = null;\n    &#125;\n\n    /**\n     * 删除链表尾部节点 表示 删除最少使用的缓存对象\n     */\n    private void removeLast() &#123;\n        // 链表尾不为空,则将链表尾指向null. 删除连表尾（删除最少使用的缓存对象）\n        if (last != null) &#123;\n            if (last.prev != null)\n                last.prev.next = null;\n            else\n                first = null;\n            last = last.prev;\n        &#125;\n    &#125;\n\n    /**\n     * 移动到链表头，表示这个节点是最新使用过的\n     *\n     * @param node\n     */\n    private void moveToHead(CacheNode node) &#123;\n        if (node == first)\n            return;\n        if (node.prev != null)\n            node.prev.next = node.next;\n        if (node.next != null)\n            node.next.prev = node.prev;\n        if (last == node)\n            last = node.prev;\n        if (first != null) &#123;\n            node.next = first;\n            first.prev = node;\n        &#125;\n        first = node;\n        node.prev = null;\n        if (last == null)\n            last = first;\n    &#125;\n\n    private int cacheSize;\n    private Hashtable nodes;// 缓存容器\n    private int currentSize;\n    private CacheNode first;// 链表头\n    private CacheNode last;// 链表尾\n&#125;\n</code></pre>"},{"title":"Linux Iptables 示例一则","date":"2020-11-30T06:06:24.000Z","_content":"\n关于 Iptables 的介绍个人强烈推荐：http://www.zsythink.net/archives/tag/iptables/page/2/。这位兄弟介绍的很详细。\n\n我个人的需求是在同一个网络内从网络上把测试主机与正式环境主机间的网络进行隔离。我的思路是采用 OUTPUT filter 表，采用黑名单方式禁用正式环境网络的访问。如果确实需要访问正式环境的某个IP和端口，则添加特殊规则放行。根据 Iptables 的规则匹配顺序原则，放行特定IP和端口的规则要在网段禁用规则之前。以下是我的名列列表：\n\n\tiptables -t filter -P OUTPUT ACCEPT\n\tiptables -F OUTPUT\n\tiptables -t filter -I OUTPUT -d 192.168.72.0/24 -j DROP\n\tiptables -t filter -I OUTPUT -d 192.168.72.1/32 -j ACCEPT\n\tservice iptables save\n\t\n1. 第一条命令是修改OUTPUT链的默认策略为ACCEPT，这样做的优点是方便通过yum安装软件时不需要设置一堆放行规则，当然网络入口的防火墙需要做好安全防护。\n2. 第二条命令是情况OUTPUT链的所有规则，重新进行配置。\n3. 第三条命令是插入一条filter规则，禁用当前服务器对192.168.72网段的访问。\n4. 第四条命令是在filter表插入放行规则，允许当前服务器访问网关192.168.72.1。\n\n注意，因为filter规则添加采用的是 -I 插入模式，每次都是在表的最前名插入，所有第三、四的顺序不能颠倒。如果采用的是 -A 追加模式，则第三、四命令需要颠倒。\n","source":"_posts/Linux-Iptables-示例一则.md","raw":"title: Linux Iptables 示例一则\ndate: 2020-11-30 14:06:24\ntags:\n- Linux\n- 防火墙\n- Iptables\ncategories:\n- 操作系统\n- Linux\n---\n\n关于 Iptables 的介绍个人强烈推荐：http://www.zsythink.net/archives/tag/iptables/page/2/。这位兄弟介绍的很详细。\n\n我个人的需求是在同一个网络内从网络上把测试主机与正式环境主机间的网络进行隔离。我的思路是采用 OUTPUT filter 表，采用黑名单方式禁用正式环境网络的访问。如果确实需要访问正式环境的某个IP和端口，则添加特殊规则放行。根据 Iptables 的规则匹配顺序原则，放行特定IP和端口的规则要在网段禁用规则之前。以下是我的名列列表：\n\n\tiptables -t filter -P OUTPUT ACCEPT\n\tiptables -F OUTPUT\n\tiptables -t filter -I OUTPUT -d 192.168.72.0/24 -j DROP\n\tiptables -t filter -I OUTPUT -d 192.168.72.1/32 -j ACCEPT\n\tservice iptables save\n\t\n1. 第一条命令是修改OUTPUT链的默认策略为ACCEPT，这样做的优点是方便通过yum安装软件时不需要设置一堆放行规则，当然网络入口的防火墙需要做好安全防护。\n2. 第二条命令是情况OUTPUT链的所有规则，重新进行配置。\n3. 第三条命令是插入一条filter规则，禁用当前服务器对192.168.72网段的访问。\n4. 第四条命令是在filter表插入放行规则，允许当前服务器访问网关192.168.72.1。\n\n注意，因为filter规则添加采用的是 -I 插入模式，每次都是在表的最前名插入，所有第三、四的顺序不能颠倒。如果采用的是 -A 追加模式，则第三、四命令需要颠倒。\n","slug":"Linux-Iptables-示例一则","published":1,"updated":"2021-07-19T16:28:00.012Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphnz0064itd3cbcl80j3","content":"<p>关于 Iptables 的介绍个人强烈推荐：<a href=\"http://www.zsythink.net/archives/tag/iptables/page/2/%E3%80%82%E8%BF%99%E4%BD%8D%E5%85%84%E5%BC%9F%E4%BB%8B%E7%BB%8D%E7%9A%84%E5%BE%88%E8%AF%A6%E7%BB%86%E3%80%82\">http://www.zsythink.net/archives/tag/iptables/page/2/。这位兄弟介绍的很详细。</a></p>\n<p>我个人的需求是在同一个网络内从网络上把测试主机与正式环境主机间的网络进行隔离。我的思路是采用 OUTPUT filter 表，采用黑名单方式禁用正式环境网络的访问。如果确实需要访问正式环境的某个IP和端口，则添加特殊规则放行。根据 Iptables 的规则匹配顺序原则，放行特定IP和端口的规则要在网段禁用规则之前。以下是我的名列列表：</p>\n<pre><code>iptables -t filter -P OUTPUT ACCEPT\niptables -F OUTPUT\niptables -t filter -I OUTPUT -d 192.168.72.0/24 -j DROP\niptables -t filter -I OUTPUT -d 192.168.72.1/32 -j ACCEPT\nservice iptables save\n</code></pre>\n<ol>\n<li>第一条命令是修改OUTPUT链的默认策略为ACCEPT，这样做的优点是方便通过yum安装软件时不需要设置一堆放行规则，当然网络入口的防火墙需要做好安全防护。</li>\n<li>第二条命令是情况OUTPUT链的所有规则，重新进行配置。</li>\n<li>第三条命令是插入一条filter规则，禁用当前服务器对192.168.72网段的访问。</li>\n<li>第四条命令是在filter表插入放行规则，允许当前服务器访问网关192.168.72.1。</li>\n</ol>\n<p>注意，因为filter规则添加采用的是 -I 插入模式，每次都是在表的最前名插入，所有第三、四的顺序不能颠倒。如果采用的是 -A 追加模式，则第三、四命令需要颠倒。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>关于 Iptables 的介绍个人强烈推荐：<a href=\"http://www.zsythink.net/archives/tag/iptables/page/2/%E3%80%82%E8%BF%99%E4%BD%8D%E5%85%84%E5%BC%9F%E4%BB%8B%E7%BB%8D%E7%9A%84%E5%BE%88%E8%AF%A6%E7%BB%86%E3%80%82\">http://www.zsythink.net/archives/tag/iptables/page/2/。这位兄弟介绍的很详细。</a></p>\n<p>我个人的需求是在同一个网络内从网络上把测试主机与正式环境主机间的网络进行隔离。我的思路是采用 OUTPUT filter 表，采用黑名单方式禁用正式环境网络的访问。如果确实需要访问正式环境的某个IP和端口，则添加特殊规则放行。根据 Iptables 的规则匹配顺序原则，放行特定IP和端口的规则要在网段禁用规则之前。以下是我的名列列表：</p>\n<pre><code>iptables -t filter -P OUTPUT ACCEPT\niptables -F OUTPUT\niptables -t filter -I OUTPUT -d 192.168.72.0/24 -j DROP\niptables -t filter -I OUTPUT -d 192.168.72.1/32 -j ACCEPT\nservice iptables save\n</code></pre>\n<ol>\n<li>第一条命令是修改OUTPUT链的默认策略为ACCEPT，这样做的优点是方便通过yum安装软件时不需要设置一堆放行规则，当然网络入口的防火墙需要做好安全防护。</li>\n<li>第二条命令是情况OUTPUT链的所有规则，重新进行配置。</li>\n<li>第三条命令是插入一条filter规则，禁用当前服务器对192.168.72网段的访问。</li>\n<li>第四条命令是在filter表插入放行规则，允许当前服务器访问网关192.168.72.1。</li>\n</ol>\n<p>注意，因为filter规则添加采用的是 -I 插入模式，每次都是在表的最前名插入，所有第三、四的顺序不能颠倒。如果采用的是 -A 追加模式，则第三、四命令需要颠倒。</p>\n"},{"title":"Linux 命令后 -- 的含义","date":"2017-02-04T19:39:53.000Z","_content":"\nUnix/Linux 下各种命令的参数，都是以 - （后面跟单字符参数，比如 -r） 或者 -- （后面紧跟多字符参数，比如 --recursive）来表示。 在所有参数后面是文件名或者目录。大多情况下（文件名第一个字符不是 -）这样是没有任何问题的，但是如果文件名第一个字符是 - 时（比如 -myfile)，命令就无法区分此时 -myfile 到底是文件名还是参数名。此时解决方法就是用 -- 来表示命令参数结束了，后面哪怕是以 - 开头的字符串，也被当成文件名处理。\n\n比如 rm -r -- -mydir 就会递归的（-r）删除 -mydir 这个目录。\n","source":"_posts/Linux-命令后-的含义.md","raw":"title: Linux 命令后 -- 的含义\ntags:\n  - Linux\ncategories:\n  - 语言\n  - Shell\ndate: 2017-02-05 03:39:53\n---\n\nUnix/Linux 下各种命令的参数，都是以 - （后面跟单字符参数，比如 -r） 或者 -- （后面紧跟多字符参数，比如 --recursive）来表示。 在所有参数后面是文件名或者目录。大多情况下（文件名第一个字符不是 -）这样是没有任何问题的，但是如果文件名第一个字符是 - 时（比如 -myfile)，命令就无法区分此时 -myfile 到底是文件名还是参数名。此时解决方法就是用 -- 来表示命令参数结束了，后面哪怕是以 - 开头的字符串，也被当成文件名处理。\n\n比如 rm -r -- -mydir 就会递归的（-r）删除 -mydir 这个目录。\n","slug":"Linux-命令后-的含义","published":1,"updated":"2021-07-19T16:28:00.276Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmpho00067itd3ggst3mhl","content":"<p>Unix/Linux 下各种命令的参数，都是以 - （后面跟单字符参数，比如 -r） 或者 – （后面紧跟多字符参数，比如 –recursive）来表示。 在所有参数后面是文件名或者目录。大多情况下（文件名第一个字符不是 -）这样是没有任何问题的，但是如果文件名第一个字符是 - 时（比如 -myfile)，命令就无法区分此时 -myfile 到底是文件名还是参数名。此时解决方法就是用 – 来表示命令参数结束了，后面哪怕是以 - 开头的字符串，也被当成文件名处理。</p>\n<p>比如 rm -r – -mydir 就会递归的（-r）删除 -mydir 这个目录。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>Unix/Linux 下各种命令的参数，都是以 - （后面跟单字符参数，比如 -r） 或者 – （后面紧跟多字符参数，比如 –recursive）来表示。 在所有参数后面是文件名或者目录。大多情况下（文件名第一个字符不是 -）这样是没有任何问题的，但是如果文件名第一个字符是 - 时（比如 -myfile)，命令就无法区分此时 -myfile 到底是文件名还是参数名。此时解决方法就是用 – 来表示命令参数结束了，后面哪怕是以 - 开头的字符串，也被当成文件名处理。</p>\n<p>比如 rm -r – -mydir 就会递归的（-r）删除 -mydir 这个目录。</p>\n"},{"title":"Linux 命令行导出 Emacs ORG 文档为 HTML","date":"2018-12-27T17:44:30.000Z","_content":"\nEmacs 版本 25.2。使用以下命令将 org 文档导出 html：\n\n    emacs {orgFile} --batch --eval \"(require 'ox)\" --eval \"(org-html-export-to-html)\"\n\n批量导出目录下的 org 文档：\n\n    for orgFile in `ls *.org`\n    do\n      emacs ${orgFile} --batch --eval \"(require 'ox)\" --eval \"(org-html-export-to-html)\"\n    done\n","source":"_posts/Linux-命令行导出-Emacs-ORG-文档为-HTML.md","raw":"title: Linux 命令行导出 Emacs ORG 文档为 HTML\ndate: 2018-12-28 01:44:30\ntags:\n- Linux\n- Emacs\ncategories:\n- 开发工具\n- Emacs\n---\n\nEmacs 版本 25.2。使用以下命令将 org 文档导出 html：\n\n    emacs {orgFile} --batch --eval \"(require 'ox)\" --eval \"(org-html-export-to-html)\"\n\n批量导出目录下的 org 文档：\n\n    for orgFile in `ls *.org`\n    do\n      emacs ${orgFile} --batch --eval \"(require 'ox)\" --eval \"(org-html-export-to-html)\"\n    done\n","slug":"Linux-命令行导出-Emacs-ORG-文档为-HTML","published":1,"updated":"2021-07-19T16:28:00.280Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmpho2006aitd3b3uzgwwi","content":"<p>Emacs 版本 25.2。使用以下命令将 org 文档导出 html：</p>\n<pre><code>emacs &#123;orgFile&#125; --batch --eval &quot;(require &#39;ox)&quot; --eval &quot;(org-html-export-to-html)&quot;\n</code></pre>\n<p>批量导出目录下的 org 文档：</p>\n<pre><code>for orgFile in `ls *.org`\ndo\n  emacs $&#123;orgFile&#125; --batch --eval &quot;(require &#39;ox)&quot; --eval &quot;(org-html-export-to-html)&quot;\ndone\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>Emacs 版本 25.2。使用以下命令将 org 文档导出 html：</p>\n<pre><code>emacs &#123;orgFile&#125; --batch --eval &quot;(require &#39;ox)&quot; --eval &quot;(org-html-export-to-html)&quot;\n</code></pre>\n<p>批量导出目录下的 org 文档：</p>\n<pre><code>for orgFile in `ls *.org`\ndo\n  emacs $&#123;orgFile&#125; --batch --eval &quot;(require &#39;ox)&quot; --eval &quot;(org-html-export-to-html)&quot;\ndone\n</code></pre>\n"},{"title":"Linux 远程 kill 进程及 $ 处理","date":"2017-03-14T06:40:03.000Z","_content":"\n\n在远程执行的命令中如果包含 $ 时需要转义，如下：\n\n    ssh root@remote_host \"ps -ef|grep process_name | grep -v 'grep ' | awk '{print $2}'\"\n\n以上语句的本意是得到进程的 ID，但实际输出会是进程的信息。应该使用如下形式：\n\n    ssh root@remote_host \"ps -ef|grep process_name | grep -v 'grep ' | awk '{print \\$2}'\"\n\n<!-- more -->\n\n>如果远程执行命令是在脚本文件中，则需要多重 转义。如：  \n>sh exec.sh \"ps -ef|grep process_name | grep -v 'grep ' | awk '{print \\\\\\\\\\$2}'\"  \n>exec.sh 内容如下：  \n>ssh root@remote_host << EOF  \n>  eval \"${command}\"  \n>  exit  \n>EOF\n\n使用以下命令可以远程 kill 进程：\n\n  ssh root@remote_host \"ps -ef|grep process_name | grep -v 'grep ' | awk '{print \\$2}' | xargs kill -9\"","source":"_posts/Linux-远程-kill-进程及-处理.md","raw":"title: Linux 远程 kill 进程及 $ 处理\ntags:\n  - Linux\ncategories:\n  - 操作系统\n  - Linux\ndate: 2017-03-14 14:40:03\n---\n\n\n在远程执行的命令中如果包含 $ 时需要转义，如下：\n\n    ssh root@remote_host \"ps -ef|grep process_name | grep -v 'grep ' | awk '{print $2}'\"\n\n以上语句的本意是得到进程的 ID，但实际输出会是进程的信息。应该使用如下形式：\n\n    ssh root@remote_host \"ps -ef|grep process_name | grep -v 'grep ' | awk '{print \\$2}'\"\n\n<!-- more -->\n\n>如果远程执行命令是在脚本文件中，则需要多重 转义。如：  \n>sh exec.sh \"ps -ef|grep process_name | grep -v 'grep ' | awk '{print \\\\\\\\\\$2}'\"  \n>exec.sh 内容如下：  \n>ssh root@remote_host << EOF  \n>  eval \"${command}\"  \n>  exit  \n>EOF\n\n使用以下命令可以远程 kill 进程：\n\n  ssh root@remote_host \"ps -ef|grep process_name | grep -v 'grep ' | awk '{print \\$2}' | xargs kill -9\"","slug":"Linux-远程-kill-进程及-处理","published":1,"updated":"2021-07-19T16:28:00.280Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmpho3006eitd36u51b1gi","content":"<p>在远程执行的命令中如果包含 $ 时需要转义，如下：</p>\n<pre><code>ssh root@remote_host &quot;ps -ef|grep process_name | grep -v &#39;grep &#39; | awk &#39;&#123;print $2&#125;&#39;&quot;\n</code></pre>\n<p>以上语句的本意是得到进程的 ID，但实际输出会是进程的信息。应该使用如下形式：</p>\n<pre><code>ssh root@remote_host &quot;ps -ef|grep process_name | grep -v &#39;grep &#39; | awk &#39;&#123;print \\$2&#125;&#39;&quot;\n</code></pre>\n<span id=\"more\"></span>\n\n<blockquote>\n<p>如果远程执行命令是在脚本文件中，则需要多重 转义。如：<br>sh exec.sh “ps -ef|grep process_name | grep -v ‘grep ‘ | awk ‘{print \\\\$2}’”<br>exec.sh 内容如下：<br>ssh root@remote_host &lt;&lt; EOF<br> eval “${command}”<br> exit<br>EOF</p>\n</blockquote>\n<p>使用以下命令可以远程 kill 进程：</p>\n<p>  ssh root@remote_host “ps -ef|grep process_name | grep -v ‘grep ‘ | awk ‘{print $2}’ | xargs kill -9”</p>\n","site":{"data":{}},"excerpt":"<p>在远程执行的命令中如果包含 $ 时需要转义，如下：</p>\n<pre><code>ssh root@remote_host &quot;ps -ef|grep process_name | grep -v &#39;grep &#39; | awk &#39;&#123;print $2&#125;&#39;&quot;\n</code></pre>\n<p>以上语句的本意是得到进程的 ID，但实际输出会是进程的信息。应该使用如下形式：</p>\n<pre><code>ssh root@remote_host &quot;ps -ef|grep process_name | grep -v &#39;grep &#39; | awk &#39;&#123;print \\$2&#125;&#39;&quot;\n</code></pre>","more":"<blockquote>\n<p>如果远程执行命令是在脚本文件中，则需要多重 转义。如：<br>sh exec.sh “ps -ef|grep process_name | grep -v ‘grep ‘ | awk ‘{print \\\\$2}’”<br>exec.sh 内容如下：<br>ssh root@remote_host &lt;&lt; EOF<br> eval “${command}”<br> exit<br>EOF</p>\n</blockquote>\n<p>使用以下命令可以远程 kill 进程：</p>\n<p>  ssh root@remote_host “ps -ef|grep process_name | grep -v ‘grep ‘ | awk ‘{print $2}’ | xargs kill -9”</p>"},{"title":"Linux下Nodejs应用service配置","date":"2016-05-15T16:12:51.000Z","_content":"\nLinux 的 service 命令用于对系统服务进行管理，比如启动（start）、停止（stop）、重启（restart）、查看状态（status）等。service 命令本身是一个 shell 脚本，它在 /etc/init.d/ 目录查找指定的服务脚本，然后调用该服务脚本来完成任务。\n\n<!-- more -->\n\n下面以基于 Nodejs 开发的名称为 data-inspector 的应用为例，说明 Linux service 的配置。\n\n第一步：在 /frin/DataInspector 下创建指向 node 命令的软链接 data-inspector，执行 ll 命令查看该文件的信息如下：\n\n    lrwxrwxrwx  1 root root     21 12月 30 15:42 data-inspector -> /usr/bin/node\n\n> 这样做的目的是为自己的应用进程起一个不同的名字。\n\n第二步：在 /etc/init.d/ 下创建文件 data-inspector，设置权限为 755，内容如下：\n\n    #!/bin/sh\n    #\n    # data-inspector\n    #\n    # description: data-inspector\n    # processname: data-inspector\n\n    case \"$1\" in\n      start)\n        echo \"Starting data-inspector\"\n        cd /frin/DataInspector\n        rm -f data-inspector.log\n        nohup ./data-inspector ./bin/www > data-inspector.log 2>&1 &\n        sleep 1s\n        echo \"started data-inspector\"\n        ;;\n      stop)\n        PID=`pidof data-inspector`\n        echo \"Stopping data-inspector\"\n        if [ ! -z \"$PID\" ]; then\n          kill -9 $PID\n        fi\n        echo \"stoped data-inspector\"\n        ;;\n      restart)\n        $0 stop\n        $0 start\n      ;;\n    *)\n       echo \"Usage: service data-inspector {start|stop|restart}\"\n    esac\n\n    exit 0\n\n> 注意，如果 node ／usr/bin 或 /usr/sbin 下，服务会因为找不到 node 命令失败。可以在 /usr/bin 下创建 node 的软链接。\n\n完成之后执行以下命令可以对服务进行启动、停止、重启：\n\n    $service data-inspector start\n    $service data-inspector stop\n    $service data-inspector restart\n","source":"_posts/Linux下Nodejs应用service配置.md","raw":"title: Linux下Nodejs应用service配置\ntags:\n  - Linux\n  - Node.js\ncategories:\n  - 语言\n  - Node.js\ndate: 2016-05-16 00:12:51\n---\n\nLinux 的 service 命令用于对系统服务进行管理，比如启动（start）、停止（stop）、重启（restart）、查看状态（status）等。service 命令本身是一个 shell 脚本，它在 /etc/init.d/ 目录查找指定的服务脚本，然后调用该服务脚本来完成任务。\n\n<!-- more -->\n\n下面以基于 Nodejs 开发的名称为 data-inspector 的应用为例，说明 Linux service 的配置。\n\n第一步：在 /frin/DataInspector 下创建指向 node 命令的软链接 data-inspector，执行 ll 命令查看该文件的信息如下：\n\n    lrwxrwxrwx  1 root root     21 12月 30 15:42 data-inspector -> /usr/bin/node\n\n> 这样做的目的是为自己的应用进程起一个不同的名字。\n\n第二步：在 /etc/init.d/ 下创建文件 data-inspector，设置权限为 755，内容如下：\n\n    #!/bin/sh\n    #\n    # data-inspector\n    #\n    # description: data-inspector\n    # processname: data-inspector\n\n    case \"$1\" in\n      start)\n        echo \"Starting data-inspector\"\n        cd /frin/DataInspector\n        rm -f data-inspector.log\n        nohup ./data-inspector ./bin/www > data-inspector.log 2>&1 &\n        sleep 1s\n        echo \"started data-inspector\"\n        ;;\n      stop)\n        PID=`pidof data-inspector`\n        echo \"Stopping data-inspector\"\n        if [ ! -z \"$PID\" ]; then\n          kill -9 $PID\n        fi\n        echo \"stoped data-inspector\"\n        ;;\n      restart)\n        $0 stop\n        $0 start\n      ;;\n    *)\n       echo \"Usage: service data-inspector {start|stop|restart}\"\n    esac\n\n    exit 0\n\n> 注意，如果 node ／usr/bin 或 /usr/sbin 下，服务会因为找不到 node 命令失败。可以在 /usr/bin 下创建 node 的软链接。\n\n完成之后执行以下命令可以对服务进行启动、停止、重启：\n\n    $service data-inspector start\n    $service data-inspector stop\n    $service data-inspector restart\n","slug":"Linux下Nodejs应用service配置","published":1,"updated":"2021-07-19T16:28:00.280Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmpho5006iitd380js8v4l","content":"<p>Linux 的 service 命令用于对系统服务进行管理，比如启动（start）、停止（stop）、重启（restart）、查看状态（status）等。service 命令本身是一个 shell 脚本，它在 /etc/init.d/ 目录查找指定的服务脚本，然后调用该服务脚本来完成任务。</p>\n<span id=\"more\"></span>\n\n<p>下面以基于 Nodejs 开发的名称为 data-inspector 的应用为例，说明 Linux service 的配置。</p>\n<p>第一步：在 /frin/DataInspector 下创建指向 node 命令的软链接 data-inspector，执行 ll 命令查看该文件的信息如下：</p>\n<pre><code>lrwxrwxrwx  1 root root     21 12月 30 15:42 data-inspector -&gt; /usr/bin/node\n</code></pre>\n<blockquote>\n<p>这样做的目的是为自己的应用进程起一个不同的名字。</p>\n</blockquote>\n<p>第二步：在 /etc/init.d/ 下创建文件 data-inspector，设置权限为 755，内容如下：</p>\n<pre><code>#!/bin/sh\n#\n# data-inspector\n#\n# description: data-inspector\n# processname: data-inspector\n\ncase &quot;$1&quot; in\n  start)\n    echo &quot;Starting data-inspector&quot;\n    cd /frin/DataInspector\n    rm -f data-inspector.log\n    nohup ./data-inspector ./bin/www &gt; data-inspector.log 2&gt;&amp;1 &amp;\n    sleep 1s\n    echo &quot;started data-inspector&quot;\n    ;;\n  stop)\n    PID=`pidof data-inspector`\n    echo &quot;Stopping data-inspector&quot;\n    if [ ! -z &quot;$PID&quot; ]; then\n      kill -9 $PID\n    fi\n    echo &quot;stoped data-inspector&quot;\n    ;;\n  restart)\n    $0 stop\n    $0 start\n  ;;\n*)\n   echo &quot;Usage: service data-inspector &#123;start|stop|restart&#125;&quot;\nesac\n\nexit 0\n</code></pre>\n<blockquote>\n<p>注意，如果 node ／usr/bin 或 /usr/sbin 下，服务会因为找不到 node 命令失败。可以在 /usr/bin 下创建 node 的软链接。</p>\n</blockquote>\n<p>完成之后执行以下命令可以对服务进行启动、停止、重启：</p>\n<pre><code>$service data-inspector start\n$service data-inspector stop\n$service data-inspector restart\n</code></pre>\n","site":{"data":{}},"excerpt":"<p>Linux 的 service 命令用于对系统服务进行管理，比如启动（start）、停止（stop）、重启（restart）、查看状态（status）等。service 命令本身是一个 shell 脚本，它在 /etc/init.d/ 目录查找指定的服务脚本，然后调用该服务脚本来完成任务。</p>","more":"<p>下面以基于 Nodejs 开发的名称为 data-inspector 的应用为例，说明 Linux service 的配置。</p>\n<p>第一步：在 /frin/DataInspector 下创建指向 node 命令的软链接 data-inspector，执行 ll 命令查看该文件的信息如下：</p>\n<pre><code>lrwxrwxrwx  1 root root     21 12月 30 15:42 data-inspector -&gt; /usr/bin/node\n</code></pre>\n<blockquote>\n<p>这样做的目的是为自己的应用进程起一个不同的名字。</p>\n</blockquote>\n<p>第二步：在 /etc/init.d/ 下创建文件 data-inspector，设置权限为 755，内容如下：</p>\n<pre><code>#!/bin/sh\n#\n# data-inspector\n#\n# description: data-inspector\n# processname: data-inspector\n\ncase &quot;$1&quot; in\n  start)\n    echo &quot;Starting data-inspector&quot;\n    cd /frin/DataInspector\n    rm -f data-inspector.log\n    nohup ./data-inspector ./bin/www &gt; data-inspector.log 2&gt;&amp;1 &amp;\n    sleep 1s\n    echo &quot;started data-inspector&quot;\n    ;;\n  stop)\n    PID=`pidof data-inspector`\n    echo &quot;Stopping data-inspector&quot;\n    if [ ! -z &quot;$PID&quot; ]; then\n      kill -9 $PID\n    fi\n    echo &quot;stoped data-inspector&quot;\n    ;;\n  restart)\n    $0 stop\n    $0 start\n  ;;\n*)\n   echo &quot;Usage: service data-inspector &#123;start|stop|restart&#125;&quot;\nesac\n\nexit 0\n</code></pre>\n<blockquote>\n<p>注意，如果 node ／usr/bin 或 /usr/sbin 下，服务会因为找不到 node 命令失败。可以在 /usr/bin 下创建 node 的软链接。</p>\n</blockquote>\n<p>完成之后执行以下命令可以对服务进行启动、停止、重启：</p>\n<pre><code>$service data-inspector start\n$service data-inspector stop\n$service data-inspector restart\n</code></pre>"},{"title":"Mac OS JAVA_HOME 设置","date":"2016-08-28T03:36:14.000Z","_content":"\n\n在 Mac OS 上使用 DMG 文件安装了 [Jdk8](http://www.oracle.com/technetwork/java/javase/downloads/index.html) 之后，在默认路径下找不到 JDK 的 HOME 路径：\n\n    $ which java\n    /usr/bin/java\n    $ ls -l /usr/bin/java\n    lrwxr-xr-x  1 root  wheel  74 12  6  2015 /usr/bin/java -> /System/Library/Frameworks/JavaVM.framework/Versions/Current/Commands/java\n    $ ls -l /System/Library/Frameworks/JavaVM.framework/Versions\n    total 8\n    drwxr-xr-x  10 root  wheel  340  5  9 20:45 A\n    lrwxr-xr-x   1 root  wheel    1 12  6  2015 Current -> A\n    $ ls -l /System/Library/Frameworks/JavaVM.framework/Versions/A/\n    total 80\n    drwxr-xr-x  47 root  wheel    1598 10 18  2015 Commands\n    drwxr-xr-x   4 root  wheel     136 10 18  2015 Frameworks\n    drwxr-xr-x  14 root  wheel     476  8  2  2015 Headers\n    drwxr-xr-x   3 root  wheel     102  8 23  2015 JavaPluginCocoa.bundle\n    -rwxr-xr-x   1 root  wheel  109488 10 18  2015 JavaVM\n    drwxr-xr-x   3 root  wheel     102  8  2  2015 Modules\n    drwxr-xr-x  45 root  wheel    1530 10 18  2015 Resources\n    drwxr-xr-x   3 root  wheel     102  8 23  2015 _CodeSignature\n\n<!-- more -->\n\n在 Mac OS 下可以使用 /usr/libexec/java_home 命令来定位 JAVA_HOME：\n\n    $ /usr/libexec/java_home\n    /Library/Java/JavaVirtualMachines/jdk1.8.0_91.jdk/Contents/Home\n\n设置 JAVA_HOME：\n\n    export JAVA_HOME=`/usr/libexec/java_home`\n\n检查 JAVA_HOME：\n\n    $ echo $JAVA_HOME\n    /Library/Java/JavaVirtualMachines/jdk1.8.0_91.jdk/Contents/Home\n\n如果安装了多个版本的 JDK，可以使用 -V 命令选项列出所有版本的 JAVA_HOME：\n\n    $ /usr/libexec/java_home -V\n    Matching Java Virtual Machines (1):\n        1.8.0_91, x86_64:\t\"Java SE 8\"\t/Library/Java/JavaVirtualMachines/jdk1.8.0_91.jdk/Contents/Home\n\n    /Library/Java/JavaVirtualMachines/jdk1.8.0_91.jdk/Contents/Home\n","source":"_posts/Mac-OS-JAVA-HOME-设置.md","raw":"title: Mac OS JAVA_HOME 设置\ntags:\n  - Java\n  - Mac OS\ncategories:\n  - 语言\n  - Java\ndate: 2016-08-28 11:36:14\n---\n\n\n在 Mac OS 上使用 DMG 文件安装了 [Jdk8](http://www.oracle.com/technetwork/java/javase/downloads/index.html) 之后，在默认路径下找不到 JDK 的 HOME 路径：\n\n    $ which java\n    /usr/bin/java\n    $ ls -l /usr/bin/java\n    lrwxr-xr-x  1 root  wheel  74 12  6  2015 /usr/bin/java -> /System/Library/Frameworks/JavaVM.framework/Versions/Current/Commands/java\n    $ ls -l /System/Library/Frameworks/JavaVM.framework/Versions\n    total 8\n    drwxr-xr-x  10 root  wheel  340  5  9 20:45 A\n    lrwxr-xr-x   1 root  wheel    1 12  6  2015 Current -> A\n    $ ls -l /System/Library/Frameworks/JavaVM.framework/Versions/A/\n    total 80\n    drwxr-xr-x  47 root  wheel    1598 10 18  2015 Commands\n    drwxr-xr-x   4 root  wheel     136 10 18  2015 Frameworks\n    drwxr-xr-x  14 root  wheel     476  8  2  2015 Headers\n    drwxr-xr-x   3 root  wheel     102  8 23  2015 JavaPluginCocoa.bundle\n    -rwxr-xr-x   1 root  wheel  109488 10 18  2015 JavaVM\n    drwxr-xr-x   3 root  wheel     102  8  2  2015 Modules\n    drwxr-xr-x  45 root  wheel    1530 10 18  2015 Resources\n    drwxr-xr-x   3 root  wheel     102  8 23  2015 _CodeSignature\n\n<!-- more -->\n\n在 Mac OS 下可以使用 /usr/libexec/java_home 命令来定位 JAVA_HOME：\n\n    $ /usr/libexec/java_home\n    /Library/Java/JavaVirtualMachines/jdk1.8.0_91.jdk/Contents/Home\n\n设置 JAVA_HOME：\n\n    export JAVA_HOME=`/usr/libexec/java_home`\n\n检查 JAVA_HOME：\n\n    $ echo $JAVA_HOME\n    /Library/Java/JavaVirtualMachines/jdk1.8.0_91.jdk/Contents/Home\n\n如果安装了多个版本的 JDK，可以使用 -V 命令选项列出所有版本的 JAVA_HOME：\n\n    $ /usr/libexec/java_home -V\n    Matching Java Virtual Machines (1):\n        1.8.0_91, x86_64:\t\"Java SE 8\"\t/Library/Java/JavaVirtualMachines/jdk1.8.0_91.jdk/Contents/Home\n\n    /Library/Java/JavaVirtualMachines/jdk1.8.0_91.jdk/Contents/Home\n","slug":"Mac-OS-JAVA-HOME-设置","published":1,"updated":"2021-07-19T16:28:00.280Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmpho6006mitd37z9ghqyv","content":"<p>在 Mac OS 上使用 DMG 文件安装了 <a href=\"http://www.oracle.com/technetwork/java/javase/downloads/index.html\">Jdk8</a> 之后，在默认路径下找不到 JDK 的 HOME 路径：</p>\n<pre><code>$ which java\n/usr/bin/java\n$ ls -l /usr/bin/java\nlrwxr-xr-x  1 root  wheel  74 12  6  2015 /usr/bin/java -&gt; /System/Library/Frameworks/JavaVM.framework/Versions/Current/Commands/java\n$ ls -l /System/Library/Frameworks/JavaVM.framework/Versions\ntotal 8\ndrwxr-xr-x  10 root  wheel  340  5  9 20:45 A\nlrwxr-xr-x   1 root  wheel    1 12  6  2015 Current -&gt; A\n$ ls -l /System/Library/Frameworks/JavaVM.framework/Versions/A/\ntotal 80\ndrwxr-xr-x  47 root  wheel    1598 10 18  2015 Commands\ndrwxr-xr-x   4 root  wheel     136 10 18  2015 Frameworks\ndrwxr-xr-x  14 root  wheel     476  8  2  2015 Headers\ndrwxr-xr-x   3 root  wheel     102  8 23  2015 JavaPluginCocoa.bundle\n-rwxr-xr-x   1 root  wheel  109488 10 18  2015 JavaVM\ndrwxr-xr-x   3 root  wheel     102  8  2  2015 Modules\ndrwxr-xr-x  45 root  wheel    1530 10 18  2015 Resources\ndrwxr-xr-x   3 root  wheel     102  8 23  2015 _CodeSignature\n</code></pre>\n<span id=\"more\"></span>\n\n<p>在 Mac OS 下可以使用 /usr/libexec/java_home 命令来定位 JAVA_HOME：</p>\n<pre><code>$ /usr/libexec/java_home\n/Library/Java/JavaVirtualMachines/jdk1.8.0_91.jdk/Contents/Home\n</code></pre>\n<p>设置 JAVA_HOME：</p>\n<pre><code>export JAVA_HOME=`/usr/libexec/java_home`\n</code></pre>\n<p>检查 JAVA_HOME：</p>\n<pre><code>$ echo $JAVA_HOME\n/Library/Java/JavaVirtualMachines/jdk1.8.0_91.jdk/Contents/Home\n</code></pre>\n<p>如果安装了多个版本的 JDK，可以使用 -V 命令选项列出所有版本的 JAVA_HOME：</p>\n<pre><code>$ /usr/libexec/java_home -V\nMatching Java Virtual Machines (1):\n    1.8.0_91, x86_64:    &quot;Java SE 8&quot;    /Library/Java/JavaVirtualMachines/jdk1.8.0_91.jdk/Contents/Home\n\n/Library/Java/JavaVirtualMachines/jdk1.8.0_91.jdk/Contents/Home\n</code></pre>\n","site":{"data":{}},"excerpt":"<p>在 Mac OS 上使用 DMG 文件安装了 <a href=\"http://www.oracle.com/technetwork/java/javase/downloads/index.html\">Jdk8</a> 之后，在默认路径下找不到 JDK 的 HOME 路径：</p>\n<pre><code>$ which java\n/usr/bin/java\n$ ls -l /usr/bin/java\nlrwxr-xr-x  1 root  wheel  74 12  6  2015 /usr/bin/java -&gt; /System/Library/Frameworks/JavaVM.framework/Versions/Current/Commands/java\n$ ls -l /System/Library/Frameworks/JavaVM.framework/Versions\ntotal 8\ndrwxr-xr-x  10 root  wheel  340  5  9 20:45 A\nlrwxr-xr-x   1 root  wheel    1 12  6  2015 Current -&gt; A\n$ ls -l /System/Library/Frameworks/JavaVM.framework/Versions/A/\ntotal 80\ndrwxr-xr-x  47 root  wheel    1598 10 18  2015 Commands\ndrwxr-xr-x   4 root  wheel     136 10 18  2015 Frameworks\ndrwxr-xr-x  14 root  wheel     476  8  2  2015 Headers\ndrwxr-xr-x   3 root  wheel     102  8 23  2015 JavaPluginCocoa.bundle\n-rwxr-xr-x   1 root  wheel  109488 10 18  2015 JavaVM\ndrwxr-xr-x   3 root  wheel     102  8  2  2015 Modules\ndrwxr-xr-x  45 root  wheel    1530 10 18  2015 Resources\ndrwxr-xr-x   3 root  wheel     102  8 23  2015 _CodeSignature\n</code></pre>","more":"<p>在 Mac OS 下可以使用 /usr/libexec/java_home 命令来定位 JAVA_HOME：</p>\n<pre><code>$ /usr/libexec/java_home\n/Library/Java/JavaVirtualMachines/jdk1.8.0_91.jdk/Contents/Home\n</code></pre>\n<p>设置 JAVA_HOME：</p>\n<pre><code>export JAVA_HOME=`/usr/libexec/java_home`\n</code></pre>\n<p>检查 JAVA_HOME：</p>\n<pre><code>$ echo $JAVA_HOME\n/Library/Java/JavaVirtualMachines/jdk1.8.0_91.jdk/Contents/Home\n</code></pre>\n<p>如果安装了多个版本的 JDK，可以使用 -V 命令选项列出所有版本的 JAVA_HOME：</p>\n<pre><code>$ /usr/libexec/java_home -V\nMatching Java Virtual Machines (1):\n    1.8.0_91, x86_64:    &quot;Java SE 8&quot;    /Library/Java/JavaVirtualMachines/jdk1.8.0_91.jdk/Contents/Home\n\n/Library/Java/JavaVirtualMachines/jdk1.8.0_91.jdk/Contents/Home\n</code></pre>"},{"title":"Mac OS Shell 远程执行 Shell 命令","date":"2016-06-26T10:57:04.000Z","_content":"\n之前写过一个 Linux 下远程执行 Shell 脚本：\n\n    #!/bin/bash\n\n    SLAVES=(192.168.1.133 192.168.1.134)\n\n    for slave in ${SLAVES[@]}\n    do\n      echo \"==================$slave======================\"\n      ssh root@$slave << EOF\n        ls -l /usr/\n      EOF\n    done\n\n当然，所有的 slave 都配置过免密码登陆。\n\n<!-- more -->\n\n但是，这个脚本在 Mac OS 下至性的时候一直报错，信息如下：\n\n    t.sh: line 12: syntax error: unexpected end of file\n\n经过尝试发现在 Mac OS 下，远程执行命令前不能有空格，修改后脚本如下：\n\n#!/bin/bash\n\nSLAVES=(192.168.1.133 192.168.1.134)\n\nfor slave in ${SLAVES[@]}\ndo\n  echo \"==================$slave======================\"\nssh root@$slave << EOF\n  ls -l /usr/\nEOF\ndone\n\n再次执行成功。\n","source":"_posts/Mac-OS-Shell-远程执行-Shell-命令.md","raw":"title: Mac OS Shell 远程执行 Shell 命令\ntags:\n  - Mac OS\n  - Shell\ncategories:\n  - 操作系统\n  - Mac OS\ndate: 2016-06-26 18:57:04\n---\n\n之前写过一个 Linux 下远程执行 Shell 脚本：\n\n    #!/bin/bash\n\n    SLAVES=(192.168.1.133 192.168.1.134)\n\n    for slave in ${SLAVES[@]}\n    do\n      echo \"==================$slave======================\"\n      ssh root@$slave << EOF\n        ls -l /usr/\n      EOF\n    done\n\n当然，所有的 slave 都配置过免密码登陆。\n\n<!-- more -->\n\n但是，这个脚本在 Mac OS 下至性的时候一直报错，信息如下：\n\n    t.sh: line 12: syntax error: unexpected end of file\n\n经过尝试发现在 Mac OS 下，远程执行命令前不能有空格，修改后脚本如下：\n\n#!/bin/bash\n\nSLAVES=(192.168.1.133 192.168.1.134)\n\nfor slave in ${SLAVES[@]}\ndo\n  echo \"==================$slave======================\"\nssh root@$slave << EOF\n  ls -l /usr/\nEOF\ndone\n\n再次执行成功。\n","slug":"Mac-OS-Shell-远程执行-Shell-命令","published":1,"updated":"2021-07-19T16:28:00.280Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmpho8006qitd3dijtahm4","content":"<p>之前写过一个 Linux 下远程执行 Shell 脚本：</p>\n<pre><code>#!/bin/bash\n\nSLAVES=(192.168.1.133 192.168.1.134)\n\nfor slave in $&#123;SLAVES[@]&#125;\ndo\n  echo &quot;==================$slave======================&quot;\n  ssh root@$slave &lt;&lt; EOF\n    ls -l /usr/\n  EOF\ndone\n</code></pre>\n<p>当然，所有的 slave 都配置过免密码登陆。</p>\n<span id=\"more\"></span>\n\n<p>但是，这个脚本在 Mac OS 下至性的时候一直报错，信息如下：</p>\n<pre><code>t.sh: line 12: syntax error: unexpected end of file\n</code></pre>\n<p>经过尝试发现在 Mac OS 下，远程执行命令前不能有空格，修改后脚本如下：</p>\n<p>#!/bin/bash</p>\n<p>SLAVES=(192.168.1.133 192.168.1.134)</p>\n<p>for slave in ${SLAVES[@]}<br>do<br>  echo “==================$slave======================”<br>ssh root@$slave &lt;&lt; EOF<br>  ls -l /usr/<br>EOF<br>done</p>\n<p>再次执行成功。</p>\n","site":{"data":{}},"excerpt":"<p>之前写过一个 Linux 下远程执行 Shell 脚本：</p>\n<pre><code>#!/bin/bash\n\nSLAVES=(192.168.1.133 192.168.1.134)\n\nfor slave in $&#123;SLAVES[@]&#125;\ndo\n  echo &quot;==================$slave======================&quot;\n  ssh root@$slave &lt;&lt; EOF\n    ls -l /usr/\n  EOF\ndone\n</code></pre>\n<p>当然，所有的 slave 都配置过免密码登陆。</p>","more":"<p>但是，这个脚本在 Mac OS 下至性的时候一直报错，信息如下：</p>\n<pre><code>t.sh: line 12: syntax error: unexpected end of file\n</code></pre>\n<p>经过尝试发现在 Mac OS 下，远程执行命令前不能有空格，修改后脚本如下：</p>\n<p>#!/bin/bash</p>\n<p>SLAVES=(192.168.1.133 192.168.1.134)</p>\n<p>for slave in ${SLAVES[@]}<br>do<br>  echo “==================$slave======================”<br>ssh root@$slave &lt;&lt; EOF<br>  ls -l /usr/<br>EOF<br>done</p>\n<p>再次执行成功。</p>"},{"title":"Mac OS 安装 Emacs","date":"2016-07-17T03:55:37.000Z","_content":"\n\n在 Mac OS X 上可以使用 [Homebrew](http://brew.sh/) 安装 Emacs：\n\n    $ brew install emacs --with-cocoa\n\n或者用 [MacPorts](https://www.macports.org/)：\n\n    $ sudo port install emacs-app\n\n[OSX Emacs](https://emacsformacosx.com/) 网站提供了通用的二进制包。\n\n> 前两种方法安装后只能通过命令行启动 Emacs，界面是终端界面。\n> 二进制安装可以将 Emacs 安装到 Applications 中，通过 Spotlight 搜索快速启动，并有图形界面。","source":"_posts/Mac-OS-安装-Emacs.md","raw":"title: Mac OS 安装 Emacs\ntags:\n  - Mac OS\n  - Emacs\ncategories:\n  - 开发工具\n  - Emacs\ndate: 2016-07-17 11:55:37\n---\n\n\n在 Mac OS X 上可以使用 [Homebrew](http://brew.sh/) 安装 Emacs：\n\n    $ brew install emacs --with-cocoa\n\n或者用 [MacPorts](https://www.macports.org/)：\n\n    $ sudo port install emacs-app\n\n[OSX Emacs](https://emacsformacosx.com/) 网站提供了通用的二进制包。\n\n> 前两种方法安装后只能通过命令行启动 Emacs，界面是终端界面。\n> 二进制安装可以将 Emacs 安装到 Applications 中，通过 Spotlight 搜索快速启动，并有图形界面。","slug":"Mac-OS-安装-Emacs","published":1,"updated":"2021-07-19T16:28:00.280Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmpho9006uitd3h9rw247k","content":"<p>在 Mac OS X 上可以使用 <a href=\"http://brew.sh/\">Homebrew</a> 安装 Emacs：</p>\n<pre><code>$ brew install emacs --with-cocoa\n</code></pre>\n<p>或者用 <a href=\"https://www.macports.org/\">MacPorts</a>：</p>\n<pre><code>$ sudo port install emacs-app\n</code></pre>\n<p><a href=\"https://emacsformacosx.com/\">OSX Emacs</a> 网站提供了通用的二进制包。</p>\n<blockquote>\n<p>前两种方法安装后只能通过命令行启动 Emacs，界面是终端界面。<br>二进制安装可以将 Emacs 安装到 Applications 中，通过 Spotlight 搜索快速启动，并有图形界面。</p>\n</blockquote>\n","site":{"data":{}},"excerpt":"","more":"<p>在 Mac OS X 上可以使用 <a href=\"http://brew.sh/\">Homebrew</a> 安装 Emacs：</p>\n<pre><code>$ brew install emacs --with-cocoa\n</code></pre>\n<p>或者用 <a href=\"https://www.macports.org/\">MacPorts</a>：</p>\n<pre><code>$ sudo port install emacs-app\n</code></pre>\n<p><a href=\"https://emacsformacosx.com/\">OSX Emacs</a> 网站提供了通用的二进制包。</p>\n<blockquote>\n<p>前两种方法安装后只能通过命令行启动 Emacs，界面是终端界面。<br>二进制安装可以将 Emacs 安装到 Applications 中，通过 Spotlight 搜索快速启动，并有图形界面。</p>\n</blockquote>\n"},{"title":"Mac OS 安装 VirtualBox","date":"2016-05-24T14:50:38.000Z","_content":"\n#### 下载\n从 Orale 官网下载 dmg 文件。下载网址：http://www.oracle.com/technetwork/server-storage/virtualbox/downloads/index.html?ssSourceSiteId=ocomen\n\n<!-- more -->\n\n#### 安装\n\n双击下载的 dmg 文件，按照说明进行安装，选项按照默认完成安装。\n![安装界面](/uploads/20160522/virtualbox-install-dmg.png)\n\n#### 打开\n\n可以从 Applications 中双击图标打开，或者使用“Spotlight 搜索”（ctrl ＋ space 快捷键）搜索打开。\n\n1. Applications 图标：\n![Applications 图标](／uploads/20160522/virtualbox-run-apps.png)\n2. Spotlight 快捷打开：\n![Spotlight 快捷打开](/uploads/20160522/virtualbox-run-spotlight.png)\n\n#### 说明\n\n安装 Ubuntu Server 后，在 VirtualBox 界面不能全屏显示，尝试解决没有找到有效的方法（有现成方法的同学可以指教 ^\\_^）。其实可以不用非在 VirtualBox 中全屏，因为 Server 就是作为服务器来用的，完全可以在客户端通过 ssh 登陆来使用。\n","source":"_posts/Mac-OS-安装-VirtualBox.md","raw":"title: Mac OS 安装 VirtualBox\ntags:\n  - Mac OS\n  - VirtualBox\ncategories:\n  - 操作系统\n  - 虚拟机\ndate: 2016-05-24 22:50:38\n---\n\n#### 下载\n从 Orale 官网下载 dmg 文件。下载网址：http://www.oracle.com/technetwork/server-storage/virtualbox/downloads/index.html?ssSourceSiteId=ocomen\n\n<!-- more -->\n\n#### 安装\n\n双击下载的 dmg 文件，按照说明进行安装，选项按照默认完成安装。\n![安装界面](/uploads/20160522/virtualbox-install-dmg.png)\n\n#### 打开\n\n可以从 Applications 中双击图标打开，或者使用“Spotlight 搜索”（ctrl ＋ space 快捷键）搜索打开。\n\n1. Applications 图标：\n![Applications 图标](／uploads/20160522/virtualbox-run-apps.png)\n2. Spotlight 快捷打开：\n![Spotlight 快捷打开](/uploads/20160522/virtualbox-run-spotlight.png)\n\n#### 说明\n\n安装 Ubuntu Server 后，在 VirtualBox 界面不能全屏显示，尝试解决没有找到有效的方法（有现成方法的同学可以指教 ^\\_^）。其实可以不用非在 VirtualBox 中全屏，因为 Server 就是作为服务器来用的，完全可以在客户端通过 ssh 登陆来使用。\n","slug":"Mac-OS-安装-VirtualBox","published":1,"updated":"2021-07-19T16:28:00.280Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphoa006yitd38w039cfv","content":"<h4 id=\"下载\"><a href=\"#下载\" class=\"headerlink\" title=\"下载\"></a>下载</h4><p>从 Orale 官网下载 dmg 文件。下载网址：<a href=\"http://www.oracle.com/technetwork/server-storage/virtualbox/downloads/index.html?ssSourceSiteId=ocomen\">http://www.oracle.com/technetwork/server-storage/virtualbox/downloads/index.html?ssSourceSiteId=ocomen</a></p>\n<span id=\"more\"></span>\n\n<h4 id=\"安装\"><a href=\"#安装\" class=\"headerlink\" title=\"安装\"></a>安装</h4><p>双击下载的 dmg 文件，按照说明进行安装，选项按照默认完成安装。<br><img src=\"/uploads/20160522/virtualbox-install-dmg.png\" alt=\"安装界面\"></p>\n<h4 id=\"打开\"><a href=\"#打开\" class=\"headerlink\" title=\"打开\"></a>打开</h4><p>可以从 Applications 中双击图标打开，或者使用“Spotlight 搜索”（ctrl ＋ space 快捷键）搜索打开。</p>\n<ol>\n<li>Applications 图标：<br><img src=\"%EF%BC%8Fuploads/20160522/virtualbox-run-apps.png\" alt=\"Applications 图标\"></li>\n<li>Spotlight 快捷打开：<br><img src=\"/uploads/20160522/virtualbox-run-spotlight.png\" alt=\"Spotlight 快捷打开\"></li>\n</ol>\n<h4 id=\"说明\"><a href=\"#说明\" class=\"headerlink\" title=\"说明\"></a>说明</h4><p>安装 Ubuntu Server 后，在 VirtualBox 界面不能全屏显示，尝试解决没有找到有效的方法（有现成方法的同学可以指教 ^_^）。其实可以不用非在 VirtualBox 中全屏，因为 Server 就是作为服务器来用的，完全可以在客户端通过 ssh 登陆来使用。</p>\n","site":{"data":{}},"excerpt":"<h4 id=\"下载\"><a href=\"#下载\" class=\"headerlink\" title=\"下载\"></a>下载</h4><p>从 Orale 官网下载 dmg 文件。下载网址：<a href=\"http://www.oracle.com/technetwork/server-storage/virtualbox/downloads/index.html?ssSourceSiteId=ocomen\">http://www.oracle.com/technetwork/server-storage/virtualbox/downloads/index.html?ssSourceSiteId=ocomen</a></p>","more":"<h4 id=\"安装\"><a href=\"#安装\" class=\"headerlink\" title=\"安装\"></a>安装</h4><p>双击下载的 dmg 文件，按照说明进行安装，选项按照默认完成安装。<br><img src=\"/uploads/20160522/virtualbox-install-dmg.png\" alt=\"安装界面\"></p>\n<h4 id=\"打开\"><a href=\"#打开\" class=\"headerlink\" title=\"打开\"></a>打开</h4><p>可以从 Applications 中双击图标打开，或者使用“Spotlight 搜索”（ctrl ＋ space 快捷键）搜索打开。</p>\n<ol>\n<li>Applications 图标：<br><img src=\"%EF%BC%8Fuploads/20160522/virtualbox-run-apps.png\" alt=\"Applications 图标\"></li>\n<li>Spotlight 快捷打开：<br><img src=\"/uploads/20160522/virtualbox-run-spotlight.png\" alt=\"Spotlight 快捷打开\"></li>\n</ol>\n<h4 id=\"说明\"><a href=\"#说明\" class=\"headerlink\" title=\"说明\"></a>说明</h4><p>安装 Ubuntu Server 后，在 VirtualBox 界面不能全屏显示，尝试解决没有找到有效的方法（有现成方法的同学可以指教 ^_^）。其实可以不用非在 VirtualBox 中全屏，因为 Server 就是作为服务器来用的，完全可以在客户端通过 ssh 登陆来使用。</p>"},{"title":"Mac OS 常用快捷键","date":"2016-06-05T09:30:54.000Z","_content":"\n\n- 显示桌面：command + F3\n- 剪切、粘贴：先 command + C，再 command + option + V\n- 终端 Shell 新建标签：command + T\n- 屏幕快照：command + shift + 3\n- 区域截屏：command + shift + 4\n","source":"_posts/Mac-OS-常用快捷键.md","raw":"title: Mac OS 常用快捷键\ntags:\n  - Mac OS\ncategories:\n  - 操作系统\n  - Mac OS\ndate: 2016-06-05 17:30:54\n---\n\n\n- 显示桌面：command + F3\n- 剪切、粘贴：先 command + C，再 command + option + V\n- 终端 Shell 新建标签：command + T\n- 屏幕快照：command + shift + 3\n- 区域截屏：command + shift + 4\n","slug":"Mac-OS-常用快捷键","published":1,"updated":"2021-07-19T16:28:00.280Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphoc0072itd328th18wp","content":"<ul>\n<li>显示桌面：command + F3</li>\n<li>剪切、粘贴：先 command + C，再 command + option + V</li>\n<li>终端 Shell 新建标签：command + T</li>\n<li>屏幕快照：command + shift + 3</li>\n<li>区域截屏：command + shift + 4</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<ul>\n<li>显示桌面：command + F3</li>\n<li>剪切、粘贴：先 command + C，再 command + option + V</li>\n<li>终端 Shell 新建标签：command + T</li>\n<li>屏幕快照：command + shift + 3</li>\n<li>区域截屏：command + shift + 4</li>\n</ul>\n"},{"title":"MapReduce 读取 Hive ORC ArrayIndexOutOfBoundsException: 1024 异常解决","date":"2019-05-07T02:26:47.000Z","_content":"\n在 MR 处理 ORC 的时候遇到如下异常：\n\n    Exception in thread \"main\" java.lang.ArrayIndexOutOfBoundsException: 1024\n    \tat org.apache.orc.impl.RunLengthIntegerReaderV2.nextVector    (RunLengthIntegerReaderV2.java:369)\n    \tat org.apache.orc.impl.TreeReaderFactory$BytesColumnVectorUtil.commonReadByteArrays    (TreeReaderFactory.java:1231)\n    \tat org.apache.orc.impl.TreeReaderFactory$BytesColumnVectorUtil.readOrcByteArrays    (TreeReaderFactory.java:1268)\n    \tat org.apache.orc.impl.TreeReaderFactory$StringDirectTreeReader.nextVector    (TreeReaderFactory.java:1368)\n    \tat org.apache.orc.impl.TreeReaderFactory$StringTreeReader.nextVector    (TreeReaderFactory.java:1212)\n    \tat org.apache.orc.impl.TreeReaderFactory$ListTreeReader.nextVector    (TreeReaderFactory.java:1902)\n    \tat org.apache.orc.impl.TreeReaderFactory$StructTreeReader.nextBatch    (TreeReaderFactory.java:1737)\n    \tat org.apache.orc.impl.RecordReaderImpl.nextBatch(RecordReaderImpl.java:1045)\n    \tat org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ensureBatch(RecordReaderImpl.java:77)\n    \tat org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.hasNext(RecordReaderImpl.java:89)\n\n通过搜索发现这个 Bug 在 Hive 2.1.1 版本中已经修复。我使用的就是这个版本，检查对应的源代码发现代码是已经按照下面的 Patch 修复过得：https://issues.apache.org/jira/browse/HIVE-14483 \n\n通过反编译发现我最终打包后的代码中使用的是未修复 Bug 的代码版本。通过依赖包发现依赖的以下模块中也包含 ORC 的 Jar：\n\n    <dependency>\n\t    <groupId>org.apache.orc</groupId>\n\t\t<artifactId>orc-mapreduce</artifactId>\n\t\t<version>1.1.0</version>\n\t</dependency>\n\n解决方法是将 orc-mapreduce 包升级到 1.1.2 版本，依赖配置如下：\n\n    <dependency>\n\t    <groupId>org.apache.orc</groupId>\n\t\t<artifactId>orc-mapreduce</artifactId>\n\t\t<version>1.1.2</version>\n\t</dependency>","source":"_posts/MapReduce-读取-Hive-ORC-ArrayIndexOutOfBoundsException-1024-异常解决.md","raw":"title: 'MapReduce 读取 Hive ORC ArrayIndexOutOfBoundsException: 1024 异常解决'\ndate: 2019-05-07 10:26:47\ntags:\n- Hadoop\n- Hive\n- ORC\n---\n\n在 MR 处理 ORC 的时候遇到如下异常：\n\n    Exception in thread \"main\" java.lang.ArrayIndexOutOfBoundsException: 1024\n    \tat org.apache.orc.impl.RunLengthIntegerReaderV2.nextVector    (RunLengthIntegerReaderV2.java:369)\n    \tat org.apache.orc.impl.TreeReaderFactory$BytesColumnVectorUtil.commonReadByteArrays    (TreeReaderFactory.java:1231)\n    \tat org.apache.orc.impl.TreeReaderFactory$BytesColumnVectorUtil.readOrcByteArrays    (TreeReaderFactory.java:1268)\n    \tat org.apache.orc.impl.TreeReaderFactory$StringDirectTreeReader.nextVector    (TreeReaderFactory.java:1368)\n    \tat org.apache.orc.impl.TreeReaderFactory$StringTreeReader.nextVector    (TreeReaderFactory.java:1212)\n    \tat org.apache.orc.impl.TreeReaderFactory$ListTreeReader.nextVector    (TreeReaderFactory.java:1902)\n    \tat org.apache.orc.impl.TreeReaderFactory$StructTreeReader.nextBatch    (TreeReaderFactory.java:1737)\n    \tat org.apache.orc.impl.RecordReaderImpl.nextBatch(RecordReaderImpl.java:1045)\n    \tat org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ensureBatch(RecordReaderImpl.java:77)\n    \tat org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.hasNext(RecordReaderImpl.java:89)\n\n通过搜索发现这个 Bug 在 Hive 2.1.1 版本中已经修复。我使用的就是这个版本，检查对应的源代码发现代码是已经按照下面的 Patch 修复过得：https://issues.apache.org/jira/browse/HIVE-14483 \n\n通过反编译发现我最终打包后的代码中使用的是未修复 Bug 的代码版本。通过依赖包发现依赖的以下模块中也包含 ORC 的 Jar：\n\n    <dependency>\n\t    <groupId>org.apache.orc</groupId>\n\t\t<artifactId>orc-mapreduce</artifactId>\n\t\t<version>1.1.0</version>\n\t</dependency>\n\n解决方法是将 orc-mapreduce 包升级到 1.1.2 版本，依赖配置如下：\n\n    <dependency>\n\t    <groupId>org.apache.orc</groupId>\n\t\t<artifactId>orc-mapreduce</artifactId>\n\t\t<version>1.1.2</version>\n\t</dependency>","slug":"MapReduce-读取-Hive-ORC-ArrayIndexOutOfBoundsException-1024-异常解决","published":1,"updated":"2021-07-19T16:28:00.280Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphod0075itd34igp8gc5","content":"<p>在 MR 处理 ORC 的时候遇到如下异常：</p>\n<pre><code>Exception in thread &quot;main&quot; java.lang.ArrayIndexOutOfBoundsException: 1024\n    at org.apache.orc.impl.RunLengthIntegerReaderV2.nextVector    (RunLengthIntegerReaderV2.java:369)\n    at org.apache.orc.impl.TreeReaderFactory$BytesColumnVectorUtil.commonReadByteArrays    (TreeReaderFactory.java:1231)\n    at org.apache.orc.impl.TreeReaderFactory$BytesColumnVectorUtil.readOrcByteArrays    (TreeReaderFactory.java:1268)\n    at org.apache.orc.impl.TreeReaderFactory$StringDirectTreeReader.nextVector    (TreeReaderFactory.java:1368)\n    at org.apache.orc.impl.TreeReaderFactory$StringTreeReader.nextVector    (TreeReaderFactory.java:1212)\n    at org.apache.orc.impl.TreeReaderFactory$ListTreeReader.nextVector    (TreeReaderFactory.java:1902)\n    at org.apache.orc.impl.TreeReaderFactory$StructTreeReader.nextBatch    (TreeReaderFactory.java:1737)\n    at org.apache.orc.impl.RecordReaderImpl.nextBatch(RecordReaderImpl.java:1045)\n    at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ensureBatch(RecordReaderImpl.java:77)\n    at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.hasNext(RecordReaderImpl.java:89)\n</code></pre>\n<p>通过搜索发现这个 Bug 在 Hive 2.1.1 版本中已经修复。我使用的就是这个版本，检查对应的源代码发现代码是已经按照下面的 Patch 修复过得：<a href=\"https://issues.apache.org/jira/browse/HIVE-14483\">https://issues.apache.org/jira/browse/HIVE-14483</a> </p>\n<p>通过反编译发现我最终打包后的代码中使用的是未修复 Bug 的代码版本。通过依赖包发现依赖的以下模块中也包含 ORC 的 Jar：</p>\n<pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.apache.orc&lt;/groupId&gt;\n    &lt;artifactId&gt;orc-mapreduce&lt;/artifactId&gt;\n    &lt;version&gt;1.1.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>\n<p>解决方法是将 orc-mapreduce 包升级到 1.1.2 版本，依赖配置如下：</p>\n<pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.apache.orc&lt;/groupId&gt;\n    &lt;artifactId&gt;orc-mapreduce&lt;/artifactId&gt;\n    &lt;version&gt;1.1.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>在 MR 处理 ORC 的时候遇到如下异常：</p>\n<pre><code>Exception in thread &quot;main&quot; java.lang.ArrayIndexOutOfBoundsException: 1024\n    at org.apache.orc.impl.RunLengthIntegerReaderV2.nextVector    (RunLengthIntegerReaderV2.java:369)\n    at org.apache.orc.impl.TreeReaderFactory$BytesColumnVectorUtil.commonReadByteArrays    (TreeReaderFactory.java:1231)\n    at org.apache.orc.impl.TreeReaderFactory$BytesColumnVectorUtil.readOrcByteArrays    (TreeReaderFactory.java:1268)\n    at org.apache.orc.impl.TreeReaderFactory$StringDirectTreeReader.nextVector    (TreeReaderFactory.java:1368)\n    at org.apache.orc.impl.TreeReaderFactory$StringTreeReader.nextVector    (TreeReaderFactory.java:1212)\n    at org.apache.orc.impl.TreeReaderFactory$ListTreeReader.nextVector    (TreeReaderFactory.java:1902)\n    at org.apache.orc.impl.TreeReaderFactory$StructTreeReader.nextBatch    (TreeReaderFactory.java:1737)\n    at org.apache.orc.impl.RecordReaderImpl.nextBatch(RecordReaderImpl.java:1045)\n    at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.ensureBatch(RecordReaderImpl.java:77)\n    at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.hasNext(RecordReaderImpl.java:89)\n</code></pre>\n<p>通过搜索发现这个 Bug 在 Hive 2.1.1 版本中已经修复。我使用的就是这个版本，检查对应的源代码发现代码是已经按照下面的 Patch 修复过得：<a href=\"https://issues.apache.org/jira/browse/HIVE-14483\">https://issues.apache.org/jira/browse/HIVE-14483</a> </p>\n<p>通过反编译发现我最终打包后的代码中使用的是未修复 Bug 的代码版本。通过依赖包发现依赖的以下模块中也包含 ORC 的 Jar：</p>\n<pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.apache.orc&lt;/groupId&gt;\n    &lt;artifactId&gt;orc-mapreduce&lt;/artifactId&gt;\n    &lt;version&gt;1.1.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>\n<p>解决方法是将 orc-mapreduce 包升级到 1.1.2 版本，依赖配置如下：</p>\n<pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.apache.orc&lt;/groupId&gt;\n    &lt;artifactId&gt;orc-mapreduce&lt;/artifactId&gt;\n    &lt;version&gt;1.1.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>\n"},{"title":"Markdown 转义","date":"2016-05-15T02:50:12.000Z","_content":"\n\n#### 什么时候需要转义\n并不是所有遇到 Markdown 语法字符的时候都需要转义，看下面的例子：\n\n在 Markdown 中想要表示字符串‘\\s’，那么代码要怎么写呢？是‘\\s’还是‘\\\\\\s’？后一种写法是很明显的，但是前一种写法也是可以的。原因是，Markdown 会根据转义字符‘\\’之后的字符判断是否需要执行转义。\\s 不是一个需要执行转义的表达式，所以转义字符‘\\’按照原字符表达显示。这种特性可以让我们在类似的情况下少输入一些字符。\n\n<!-- more -->\n\n#### 转义 >\n\n遇到以下情况时如何转义：\n\n    <String>\n\n是 \\<String\\&gt; 吗？试一下就知道这样是不行的。在这种情况下通过下面的方式处理：\n\n    <String&git;\n","source":"_posts/Markdown-转义.md","raw":"title: Markdown 转义\ntags:\n  - Markdown\ncategories:\n  - 语言\n  - Markdown\ndate: 2016-05-15 10:50:12\n---\n\n\n#### 什么时候需要转义\n并不是所有遇到 Markdown 语法字符的时候都需要转义，看下面的例子：\n\n在 Markdown 中想要表示字符串‘\\s’，那么代码要怎么写呢？是‘\\s’还是‘\\\\\\s’？后一种写法是很明显的，但是前一种写法也是可以的。原因是，Markdown 会根据转义字符‘\\’之后的字符判断是否需要执行转义。\\s 不是一个需要执行转义的表达式，所以转义字符‘\\’按照原字符表达显示。这种特性可以让我们在类似的情况下少输入一些字符。\n\n<!-- more -->\n\n#### 转义 >\n\n遇到以下情况时如何转义：\n\n    <String>\n\n是 \\<String\\&gt; 吗？试一下就知道这样是不行的。在这种情况下通过下面的方式处理：\n\n    <String&git;\n","slug":"Markdown-转义","published":1,"updated":"2021-07-19T16:28:00.280Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphoe0077itd38ck4brmw","content":"<h4 id=\"什么时候需要转义\"><a href=\"#什么时候需要转义\" class=\"headerlink\" title=\"什么时候需要转义\"></a>什么时候需要转义</h4><p>并不是所有遇到 Markdown 语法字符的时候都需要转义，看下面的例子：</p>\n<p>在 Markdown 中想要表示字符串‘\\s’，那么代码要怎么写呢？是‘\\s’还是‘\\\\s’？后一种写法是很明显的，但是前一种写法也是可以的。原因是，Markdown 会根据转义字符‘\\’之后的字符判断是否需要执行转义。\\s 不是一个需要执行转义的表达式，所以转义字符‘\\’按照原字符表达显示。这种特性可以让我们在类似的情况下少输入一些字符。</p>\n<span id=\"more\"></span>\n\n<h4 id=\"转义-gt\"><a href=\"#转义-gt\" class=\"headerlink\" title=\"转义 &gt;\"></a>转义 &gt;</h4><p>遇到以下情况时如何转义：</p>\n<pre><code>&lt;String&gt;\n</code></pre>\n<p>是 &lt;String&amp;gt; 吗？试一下就知道这样是不行的。在这种情况下通过下面的方式处理：</p>\n<pre><code>&lt;String&amp;git;\n</code></pre>\n","site":{"data":{}},"excerpt":"<h4 id=\"什么时候需要转义\"><a href=\"#什么时候需要转义\" class=\"headerlink\" title=\"什么时候需要转义\"></a>什么时候需要转义</h4><p>并不是所有遇到 Markdown 语法字符的时候都需要转义，看下面的例子：</p>\n<p>在 Markdown 中想要表示字符串‘\\s’，那么代码要怎么写呢？是‘\\s’还是‘\\\\s’？后一种写法是很明显的，但是前一种写法也是可以的。原因是，Markdown 会根据转义字符‘\\’之后的字符判断是否需要执行转义。\\s 不是一个需要执行转义的表达式，所以转义字符‘\\’按照原字符表达显示。这种特性可以让我们在类似的情况下少输入一些字符。</p>","more":"<h4 id=\"转义-gt\"><a href=\"#转义-gt\" class=\"headerlink\" title=\"转义 &gt;\"></a>转义 &gt;</h4><p>遇到以下情况时如何转义：</p>\n<pre><code>&lt;String&gt;\n</code></pre>\n<p>是 &lt;String&amp;gt; 吗？试一下就知道这样是不行的。在这种情况下通过下面的方式处理：</p>\n<pre><code>&lt;String&amp;git;\n</code></pre>"},{"title":"Markdown基本语法","date":"2015-11-15T03:17:04.000Z","_content":"\n# Markdown基本语法\n---\n## 1. 基本语法\n### 代码\n\t单个回车【这里有一个回车符->】\n\t视为空格。\n\t连续回车【这里有两个回车符->】\n\n\t视为分段。\n\t行尾加两个空格【这里有两个空格和一个回车符->】  \n\t即可段内换行。\n\t*这是斜体*\n\t**这是粗体**\n\t\\为转义字符 【这是一个斜杠 \\\\ 】\n\n<!-- more -->\n\n### 效果\n单个回车【这里有一个回车符->】\n视为空格。  \n连续回车【这里有两个回车符->】\n\n视为分段。  \n行尾加两个空格【这里有两个空格和一个回车符->】  \n即可段内换行。  \n*这是斜体*  \n**这是粗体**  \n\\\\为转义字符【这是一个\\\\】  \n\n---\n## 2. 标题\n### 代码\n    # 这是一级标题\n\t## 这是二级标题\n\t### 这是三级标题\n\t#### 这是四级标题\n\t##### 这是五级标题\n    ###### 这是六级标题\n\t在行下面加 = 表示大标题\n\t在行下面加 - 表示小标题\n### 效果\n# 这是一级标题\n## 这是二级标题\n### 这是三级标题\n#### 这是四级标题\n##### 这是五级标题\n###### 这是六级标题\n<h1>在行下面加 = 表示大标题</h1>\n<h2>在行下面加 - 表示小标题<h2>\n### 说明\n在行首添加 1 到 6 个 # ，表示 1 到 6 级标题。 Markdown 共有6级标题。在文字行下面加 = 或者 - 分别表示大标题和小标题。\n\n---\n## 3. 列表\n### 代码\n\t- 无序列表项目1\n\t- 无序列表项目2\n\t- 无序列表项目3\n\t\n\t两个列表不能相邻，否则会认为是一个列表。\n\n\t1. 有序列表项目1\n\t2. 有序列表项目2\n\t3. 有序列表项目3\n\t\n\t下面是嵌套列表：\n\n\t- 外层列表项目1\n\t+ 内层列表项目11\n\t+ 内层列表项目12\n\t- 外层列表项目2\n\t+ 内层列表项目21\n\t+ 内层列表项目22\n\t\n### 效果\n- 无序列表项目1\n- 无序列表项目2\n- 无序列表项目3\n\n两个列表不能相邻，否则会认为是一个列表。\n\n1. 有序列表项目1\n2. 有序列表项目2\n3. 有序列表项目3\n\n下面是嵌套列表：\n\n- 外层列表项目1\n + 内层列表项目11\n + 内层列表项目12\n- 外层列表项目2\n + 内层列表项目21\n + 内层列表项目22\n\n### 说明\n注意，两个列表不能相邻，否则会认为是一个列表；内层列表项目前需要有缩进。\n\n---\n## 4. 分割线\n### 代码\n\t---\n\t上面是一条分割线\n### 效果\n---\n上面是一条分割线\n\n---\n## 5. 程序代码\n### 代码\n\tJava：\n\n\t\tpublic class CodeView {\n\t\t\tpublic static void main(String[] args) {\n\t\t\t\tSystem.out.println(\"Hello World!\");\n\t\t\t}\n\t\t}\n### 效果\nJava：\n\n\tpublic class CodeView {\n\t\tpublic static void main(String[] args) {\n\t\t\tSystem.out.println(\"Hello World!\");\n\t\t}\n\t}\n### 说明\n行的开头空4个空格或者tab符，表示程序代码。\n\n---\n### 6. 引用\n#### 代码\n\t>这是引用文字\n#### 效果\n>这是引用文字\n\n---\n## 7. 其他\n### 代码\n\t<http://example.com/>\n\t[链接文字](http://example.com/ \"标题文字\")\n\t![图片文字](/uploads/avatar.jpg \"图片文字\")\n### 效果\n<http://example.com/>  \n[链接文字](http://www.ituring.com.cn \"标题文字\")\n![图片文字](/uploads/avatar.jpg \"图片文字\")\n\n### 说明\n- 直接显示链接地址，则用 <> 包围链接即可。\n- 链接要显示的文字放到 [] 中。\n- 链接地址放到 [] 后紧跟的 ()中。\n- 要显示链接标题文字（鼠标悬浮时显示的内容）则在链接后加空格，且标题文字放到\"\"中。\n- 图片跟链接一样，只是在最前面加 ! 符号。\n","source":"_posts/Markdown基本语法.md","raw":"title: Markdown基本语法\ndate: 2015-11-15 11:17:04\ncategories:\n  - 语言\n  - Markdown\ntags:\n  - Markdown\n\n---\n\n# Markdown基本语法\n---\n## 1. 基本语法\n### 代码\n\t单个回车【这里有一个回车符->】\n\t视为空格。\n\t连续回车【这里有两个回车符->】\n\n\t视为分段。\n\t行尾加两个空格【这里有两个空格和一个回车符->】  \n\t即可段内换行。\n\t*这是斜体*\n\t**这是粗体**\n\t\\为转义字符 【这是一个斜杠 \\\\ 】\n\n<!-- more -->\n\n### 效果\n单个回车【这里有一个回车符->】\n视为空格。  \n连续回车【这里有两个回车符->】\n\n视为分段。  \n行尾加两个空格【这里有两个空格和一个回车符->】  \n即可段内换行。  \n*这是斜体*  \n**这是粗体**  \n\\\\为转义字符【这是一个\\\\】  \n\n---\n## 2. 标题\n### 代码\n    # 这是一级标题\n\t## 这是二级标题\n\t### 这是三级标题\n\t#### 这是四级标题\n\t##### 这是五级标题\n    ###### 这是六级标题\n\t在行下面加 = 表示大标题\n\t在行下面加 - 表示小标题\n### 效果\n# 这是一级标题\n## 这是二级标题\n### 这是三级标题\n#### 这是四级标题\n##### 这是五级标题\n###### 这是六级标题\n<h1>在行下面加 = 表示大标题</h1>\n<h2>在行下面加 - 表示小标题<h2>\n### 说明\n在行首添加 1 到 6 个 # ，表示 1 到 6 级标题。 Markdown 共有6级标题。在文字行下面加 = 或者 - 分别表示大标题和小标题。\n\n---\n## 3. 列表\n### 代码\n\t- 无序列表项目1\n\t- 无序列表项目2\n\t- 无序列表项目3\n\t\n\t两个列表不能相邻，否则会认为是一个列表。\n\n\t1. 有序列表项目1\n\t2. 有序列表项目2\n\t3. 有序列表项目3\n\t\n\t下面是嵌套列表：\n\n\t- 外层列表项目1\n\t+ 内层列表项目11\n\t+ 内层列表项目12\n\t- 外层列表项目2\n\t+ 内层列表项目21\n\t+ 内层列表项目22\n\t\n### 效果\n- 无序列表项目1\n- 无序列表项目2\n- 无序列表项目3\n\n两个列表不能相邻，否则会认为是一个列表。\n\n1. 有序列表项目1\n2. 有序列表项目2\n3. 有序列表项目3\n\n下面是嵌套列表：\n\n- 外层列表项目1\n + 内层列表项目11\n + 内层列表项目12\n- 外层列表项目2\n + 内层列表项目21\n + 内层列表项目22\n\n### 说明\n注意，两个列表不能相邻，否则会认为是一个列表；内层列表项目前需要有缩进。\n\n---\n## 4. 分割线\n### 代码\n\t---\n\t上面是一条分割线\n### 效果\n---\n上面是一条分割线\n\n---\n## 5. 程序代码\n### 代码\n\tJava：\n\n\t\tpublic class CodeView {\n\t\t\tpublic static void main(String[] args) {\n\t\t\t\tSystem.out.println(\"Hello World!\");\n\t\t\t}\n\t\t}\n### 效果\nJava：\n\n\tpublic class CodeView {\n\t\tpublic static void main(String[] args) {\n\t\t\tSystem.out.println(\"Hello World!\");\n\t\t}\n\t}\n### 说明\n行的开头空4个空格或者tab符，表示程序代码。\n\n---\n### 6. 引用\n#### 代码\n\t>这是引用文字\n#### 效果\n>这是引用文字\n\n---\n## 7. 其他\n### 代码\n\t<http://example.com/>\n\t[链接文字](http://example.com/ \"标题文字\")\n\t![图片文字](/uploads/avatar.jpg \"图片文字\")\n### 效果\n<http://example.com/>  \n[链接文字](http://www.ituring.com.cn \"标题文字\")\n![图片文字](/uploads/avatar.jpg \"图片文字\")\n\n### 说明\n- 直接显示链接地址，则用 <> 包围链接即可。\n- 链接要显示的文字放到 [] 中。\n- 链接地址放到 [] 后紧跟的 ()中。\n- 要显示链接标题文字（鼠标悬浮时显示的内容）则在链接后加空格，且标题文字放到\"\"中。\n- 图片跟链接一样，只是在最前面加 ! 符号。\n","slug":"Markdown基本语法","published":1,"updated":"2021-07-19T16:28:00.284Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphog007bitd32wo67mkj","content":"<h1 id=\"Markdown基本语法\"><a href=\"#Markdown基本语法\" class=\"headerlink\" title=\"Markdown基本语法\"></a>Markdown基本语法</h1><hr>\n<h2 id=\"1-基本语法\"><a href=\"#1-基本语法\" class=\"headerlink\" title=\"1. 基本语法\"></a>1. 基本语法</h2><h3 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h3><pre><code>单个回车【这里有一个回车符-&gt;】\n视为空格。\n连续回车【这里有两个回车符-&gt;】\n\n视为分段。\n行尾加两个空格【这里有两个空格和一个回车符-&gt;】  \n即可段内换行。\n*这是斜体*\n**这是粗体**\n\\为转义字符 【这是一个斜杠 \\\\ 】\n</code></pre>\n<span id=\"more\"></span>\n\n<h3 id=\"效果\"><a href=\"#效果\" class=\"headerlink\" title=\"效果\"></a>效果</h3><p>单个回车【这里有一个回车符-&gt;】<br>视为空格。<br>连续回车【这里有两个回车符-&gt;】</p>\n<p>视为分段。<br>行尾加两个空格【这里有两个空格和一个回车符-&gt;】<br>即可段内换行。<br><em>这是斜体</em><br><strong>这是粗体</strong><br>\\为转义字符【这是一个\\】  </p>\n<hr>\n<h2 id=\"2-标题\"><a href=\"#2-标题\" class=\"headerlink\" title=\"2. 标题\"></a>2. 标题</h2><h3 id=\"代码-1\"><a href=\"#代码-1\" class=\"headerlink\" title=\"代码\"></a>代码</h3><pre><code># 这是一级标题\n## 这是二级标题\n### 这是三级标题\n#### 这是四级标题\n##### 这是五级标题\n###### 这是六级标题\n在行下面加 = 表示大标题\n在行下面加 - 表示小标题\n</code></pre>\n<h3 id=\"效果-1\"><a href=\"#效果-1\" class=\"headerlink\" title=\"效果\"></a>效果</h3><h1 id=\"这是一级标题\"><a href=\"#这是一级标题\" class=\"headerlink\" title=\"这是一级标题\"></a>这是一级标题</h1><h2 id=\"这是二级标题\"><a href=\"#这是二级标题\" class=\"headerlink\" title=\"这是二级标题\"></a>这是二级标题</h2><h3 id=\"这是三级标题\"><a href=\"#这是三级标题\" class=\"headerlink\" title=\"这是三级标题\"></a>这是三级标题</h3><h4 id=\"这是四级标题\"><a href=\"#这是四级标题\" class=\"headerlink\" title=\"这是四级标题\"></a>这是四级标题</h4><h5 id=\"这是五级标题\"><a href=\"#这是五级标题\" class=\"headerlink\" title=\"这是五级标题\"></a>这是五级标题</h5><h6 id=\"这是六级标题\"><a href=\"#这是六级标题\" class=\"headerlink\" title=\"这是六级标题\"></a>这是六级标题</h6><h1>在行下面加 = 表示大标题</h1>\n<h2>在行下面加 - 表示小标题<h2>\n### 说明\n在行首添加 1 到 6 个 # ，表示 1 到 6 级标题。 Markdown 共有6级标题。在文字行下面加 = 或者 - 分别表示大标题和小标题。\n\n<hr>\n<h2 id=\"3-列表\"><a href=\"#3-列表\" class=\"headerlink\" title=\"3. 列表\"></a>3. 列表</h2><h3 id=\"代码-2\"><a href=\"#代码-2\" class=\"headerlink\" title=\"代码\"></a>代码</h3><pre><code>- 无序列表项目1\n- 无序列表项目2\n- 无序列表项目3\n\n两个列表不能相邻，否则会认为是一个列表。\n\n1. 有序列表项目1\n2. 有序列表项目2\n3. 有序列表项目3\n\n下面是嵌套列表：\n\n- 外层列表项目1\n+ 内层列表项目11\n+ 内层列表项目12\n- 外层列表项目2\n+ 内层列表项目21\n+ 内层列表项目22\n</code></pre>\n<h3 id=\"效果-2\"><a href=\"#效果-2\" class=\"headerlink\" title=\"效果\"></a>效果</h3><ul>\n<li>无序列表项目1</li>\n<li>无序列表项目2</li>\n<li>无序列表项目3</li>\n</ul>\n<p>两个列表不能相邻，否则会认为是一个列表。</p>\n<ol>\n<li>有序列表项目1</li>\n<li>有序列表项目2</li>\n<li>有序列表项目3</li>\n</ol>\n<p>下面是嵌套列表：</p>\n<ul>\n<li>外层列表项目1</li>\n</ul>\n<ul>\n<li>内层列表项目11</li>\n<li>内层列表项目12</li>\n</ul>\n<ul>\n<li>外层列表项目2</li>\n</ul>\n<ul>\n<li>内层列表项目21</li>\n<li>内层列表项目22</li>\n</ul>\n<h3 id=\"说明\"><a href=\"#说明\" class=\"headerlink\" title=\"说明\"></a>说明</h3><p>注意，两个列表不能相邻，否则会认为是一个列表；内层列表项目前需要有缩进。</p>\n<hr>\n<h2 id=\"4-分割线\"><a href=\"#4-分割线\" class=\"headerlink\" title=\"4. 分割线\"></a>4. 分割线</h2><h3 id=\"代码-3\"><a href=\"#代码-3\" class=\"headerlink\" title=\"代码\"></a>代码</h3><pre><code>---\n上面是一条分割线\n</code></pre>\n<h3 id=\"效果-3\"><a href=\"#效果-3\" class=\"headerlink\" title=\"效果\"></a>效果</h3><hr>\n<p>上面是一条分割线</p>\n<hr>\n<h2 id=\"5-程序代码\"><a href=\"#5-程序代码\" class=\"headerlink\" title=\"5. 程序代码\"></a>5. 程序代码</h2><h3 id=\"代码-4\"><a href=\"#代码-4\" class=\"headerlink\" title=\"代码\"></a>代码</h3><pre><code>Java：\n\n    public class CodeView &#123;\n        public static void main(String[] args) &#123;\n            System.out.println(&quot;Hello World!&quot;);\n        &#125;\n    &#125;\n</code></pre>\n<h3 id=\"效果-4\"><a href=\"#效果-4\" class=\"headerlink\" title=\"效果\"></a>效果</h3><p>Java：</p>\n<pre><code>public class CodeView &#123;\n    public static void main(String[] args) &#123;\n        System.out.println(&quot;Hello World!&quot;);\n    &#125;\n&#125;\n</code></pre>\n<h3 id=\"说明-1\"><a href=\"#说明-1\" class=\"headerlink\" title=\"说明\"></a>说明</h3><p>行的开头空4个空格或者tab符，表示程序代码。</p>\n<hr>\n<h3 id=\"6-引用\"><a href=\"#6-引用\" class=\"headerlink\" title=\"6. 引用\"></a>6. 引用</h3><h4 id=\"代码-5\"><a href=\"#代码-5\" class=\"headerlink\" title=\"代码\"></a>代码</h4><pre><code>&gt;这是引用文字\n</code></pre>\n<h4 id=\"效果-5\"><a href=\"#效果-5\" class=\"headerlink\" title=\"效果\"></a>效果</h4><blockquote>\n<p>这是引用文字</p>\n</blockquote>\n<hr>\n<h2 id=\"7-其他\"><a href=\"#7-其他\" class=\"headerlink\" title=\"7. 其他\"></a>7. 其他</h2><h3 id=\"代码-6\"><a href=\"#代码-6\" class=\"headerlink\" title=\"代码\"></a>代码</h3><pre><code>&lt;http://example.com/&gt;\n[链接文字](http://example.com/ &quot;标题文字&quot;)\n![图片文字](/uploads/avatar.jpg &quot;图片文字&quot;)\n</code></pre>\n<h3 id=\"效果-6\"><a href=\"#效果-6\" class=\"headerlink\" title=\"效果\"></a>效果</h3><p><a href=\"http://example.com/\">http://example.com/</a><br><a href=\"http://www.ituring.com.cn/\" title=\"标题文字\">链接文字</a><br><img src=\"/uploads/avatar.jpg\" alt=\"图片文字\" title=\"图片文字\"></p>\n<h3 id=\"说明-2\"><a href=\"#说明-2\" class=\"headerlink\" title=\"说明\"></a>说明</h3><ul>\n<li>直接显示链接地址，则用 &lt;&gt; 包围链接即可。</li>\n<li>链接要显示的文字放到 [] 中。</li>\n<li>链接地址放到 [] 后紧跟的 ()中。</li>\n<li>要显示链接标题文字（鼠标悬浮时显示的内容）则在链接后加空格，且标题文字放到””中。</li>\n<li>图片跟链接一样，只是在最前面加 ! 符号。</li>\n</ul>\n","site":{"data":{}},"excerpt":"<h1 id=\"Markdown基本语法\"><a href=\"#Markdown基本语法\" class=\"headerlink\" title=\"Markdown基本语法\"></a>Markdown基本语法</h1><hr>\n<h2 id=\"1-基本语法\"><a href=\"#1-基本语法\" class=\"headerlink\" title=\"1. 基本语法\"></a>1. 基本语法</h2><h3 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h3><pre><code>单个回车【这里有一个回车符-&gt;】\n视为空格。\n连续回车【这里有两个回车符-&gt;】\n\n视为分段。\n行尾加两个空格【这里有两个空格和一个回车符-&gt;】  \n即可段内换行。\n*这是斜体*\n**这是粗体**\n\\为转义字符 【这是一个斜杠 \\\\ 】\n</code></pre>","more":"<h3 id=\"效果\"><a href=\"#效果\" class=\"headerlink\" title=\"效果\"></a>效果</h3><p>单个回车【这里有一个回车符-&gt;】<br>视为空格。<br>连续回车【这里有两个回车符-&gt;】</p>\n<p>视为分段。<br>行尾加两个空格【这里有两个空格和一个回车符-&gt;】<br>即可段内换行。<br><em>这是斜体</em><br><strong>这是粗体</strong><br>\\为转义字符【这是一个\\】  </p>\n<hr>\n<h2 id=\"2-标题\"><a href=\"#2-标题\" class=\"headerlink\" title=\"2. 标题\"></a>2. 标题</h2><h3 id=\"代码-1\"><a href=\"#代码-1\" class=\"headerlink\" title=\"代码\"></a>代码</h3><pre><code># 这是一级标题\n## 这是二级标题\n### 这是三级标题\n#### 这是四级标题\n##### 这是五级标题\n###### 这是六级标题\n在行下面加 = 表示大标题\n在行下面加 - 表示小标题\n</code></pre>\n<h3 id=\"效果-1\"><a href=\"#效果-1\" class=\"headerlink\" title=\"效果\"></a>效果</h3><h1 id=\"这是一级标题\"><a href=\"#这是一级标题\" class=\"headerlink\" title=\"这是一级标题\"></a>这是一级标题</h1><h2 id=\"这是二级标题\"><a href=\"#这是二级标题\" class=\"headerlink\" title=\"这是二级标题\"></a>这是二级标题</h2><h3 id=\"这是三级标题\"><a href=\"#这是三级标题\" class=\"headerlink\" title=\"这是三级标题\"></a>这是三级标题</h3><h4 id=\"这是四级标题\"><a href=\"#这是四级标题\" class=\"headerlink\" title=\"这是四级标题\"></a>这是四级标题</h4><h5 id=\"这是五级标题\"><a href=\"#这是五级标题\" class=\"headerlink\" title=\"这是五级标题\"></a>这是五级标题</h5><h6 id=\"这是六级标题\"><a href=\"#这是六级标题\" class=\"headerlink\" title=\"这是六级标题\"></a>这是六级标题</h6><h1>在行下面加 = 表示大标题</h1>\n<h2>在行下面加 - 表示小标题<h2>\n### 说明\n在行首添加 1 到 6 个 # ，表示 1 到 6 级标题。 Markdown 共有6级标题。在文字行下面加 = 或者 - 分别表示大标题和小标题。\n\n<hr>\n<h2 id=\"3-列表\"><a href=\"#3-列表\" class=\"headerlink\" title=\"3. 列表\"></a>3. 列表</h2><h3 id=\"代码-2\"><a href=\"#代码-2\" class=\"headerlink\" title=\"代码\"></a>代码</h3><pre><code>- 无序列表项目1\n- 无序列表项目2\n- 无序列表项目3\n\n两个列表不能相邻，否则会认为是一个列表。\n\n1. 有序列表项目1\n2. 有序列表项目2\n3. 有序列表项目3\n\n下面是嵌套列表：\n\n- 外层列表项目1\n+ 内层列表项目11\n+ 内层列表项目12\n- 外层列表项目2\n+ 内层列表项目21\n+ 内层列表项目22\n</code></pre>\n<h3 id=\"效果-2\"><a href=\"#效果-2\" class=\"headerlink\" title=\"效果\"></a>效果</h3><ul>\n<li>无序列表项目1</li>\n<li>无序列表项目2</li>\n<li>无序列表项目3</li>\n</ul>\n<p>两个列表不能相邻，否则会认为是一个列表。</p>\n<ol>\n<li>有序列表项目1</li>\n<li>有序列表项目2</li>\n<li>有序列表项目3</li>\n</ol>\n<p>下面是嵌套列表：</p>\n<ul>\n<li>外层列表项目1</li>\n</ul>\n<ul>\n<li>内层列表项目11</li>\n<li>内层列表项目12</li>\n</ul>\n<ul>\n<li>外层列表项目2</li>\n</ul>\n<ul>\n<li>内层列表项目21</li>\n<li>内层列表项目22</li>\n</ul>\n<h3 id=\"说明\"><a href=\"#说明\" class=\"headerlink\" title=\"说明\"></a>说明</h3><p>注意，两个列表不能相邻，否则会认为是一个列表；内层列表项目前需要有缩进。</p>\n<hr>\n<h2 id=\"4-分割线\"><a href=\"#4-分割线\" class=\"headerlink\" title=\"4. 分割线\"></a>4. 分割线</h2><h3 id=\"代码-3\"><a href=\"#代码-3\" class=\"headerlink\" title=\"代码\"></a>代码</h3><pre><code>---\n上面是一条分割线\n</code></pre>\n<h3 id=\"效果-3\"><a href=\"#效果-3\" class=\"headerlink\" title=\"效果\"></a>效果</h3><hr>\n<p>上面是一条分割线</p>\n<hr>\n<h2 id=\"5-程序代码\"><a href=\"#5-程序代码\" class=\"headerlink\" title=\"5. 程序代码\"></a>5. 程序代码</h2><h3 id=\"代码-4\"><a href=\"#代码-4\" class=\"headerlink\" title=\"代码\"></a>代码</h3><pre><code>Java：\n\n    public class CodeView &#123;\n        public static void main(String[] args) &#123;\n            System.out.println(&quot;Hello World!&quot;);\n        &#125;\n    &#125;\n</code></pre>\n<h3 id=\"效果-4\"><a href=\"#效果-4\" class=\"headerlink\" title=\"效果\"></a>效果</h3><p>Java：</p>\n<pre><code>public class CodeView &#123;\n    public static void main(String[] args) &#123;\n        System.out.println(&quot;Hello World!&quot;);\n    &#125;\n&#125;\n</code></pre>\n<h3 id=\"说明-1\"><a href=\"#说明-1\" class=\"headerlink\" title=\"说明\"></a>说明</h3><p>行的开头空4个空格或者tab符，表示程序代码。</p>\n<hr>\n<h3 id=\"6-引用\"><a href=\"#6-引用\" class=\"headerlink\" title=\"6. 引用\"></a>6. 引用</h3><h4 id=\"代码-5\"><a href=\"#代码-5\" class=\"headerlink\" title=\"代码\"></a>代码</h4><pre><code>&gt;这是引用文字\n</code></pre>\n<h4 id=\"效果-5\"><a href=\"#效果-5\" class=\"headerlink\" title=\"效果\"></a>效果</h4><blockquote>\n<p>这是引用文字</p>\n</blockquote>\n<hr>\n<h2 id=\"7-其他\"><a href=\"#7-其他\" class=\"headerlink\" title=\"7. 其他\"></a>7. 其他</h2><h3 id=\"代码-6\"><a href=\"#代码-6\" class=\"headerlink\" title=\"代码\"></a>代码</h3><pre><code>&lt;http://example.com/&gt;\n[链接文字](http://example.com/ &quot;标题文字&quot;)\n![图片文字](/uploads/avatar.jpg &quot;图片文字&quot;)\n</code></pre>\n<h3 id=\"效果-6\"><a href=\"#效果-6\" class=\"headerlink\" title=\"效果\"></a>效果</h3><p><a href=\"http://example.com/\">http://example.com/</a><br><a href=\"http://www.ituring.com.cn/\" title=\"标题文字\">链接文字</a><br><img src=\"/uploads/avatar.jpg\" alt=\"图片文字\" title=\"图片文字\"></p>\n<h3 id=\"说明-2\"><a href=\"#说明-2\" class=\"headerlink\" title=\"说明\"></a>说明</h3><ul>\n<li>直接显示链接地址，则用 &lt;&gt; 包围链接即可。</li>\n<li>链接要显示的文字放到 [] 中。</li>\n<li>链接地址放到 [] 后紧跟的 ()中。</li>\n<li>要显示链接标题文字（鼠标悬浮时显示的内容）则在链接后加空格，且标题文字放到””中。</li>\n<li>图片跟链接一样，只是在最前面加 ! 符号。</li>\n</ul>"},{"title":"Maven: Too many files with unapproved license","date":"2020-11-25T07:15:35.000Z","_content":"\n异常信息如下：\n\n\t[ERROR] Failed to execute goal org.apache.rat:apache-rat-plugin:0.11:check (default) on project ranger: Too many files with unapproved license: 1 See RAT report in: /home/zhangjc/ysten/git/apache-ranger-1.2.0/target/rat.txt -> [Help 1]\n\torg.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.rat:apache-rat-plugin:0.11:check (default) on project ranger: Too many files with unapproved license: 1 See RAT report in: /home/zhangjc/ysten/git/apache-ranger-1.2.0/target/rat.txt\n\t    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:215)\n\t    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:156)\n\t    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:148)\n\t    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)\n\t    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)\n\t    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56)\n\t    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)\n\t    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:305)\n\t    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192)\n\t    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105)\n\t    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:957)\n\t    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:289)\n\t    at org.apache.maven.cli.MavenCli.main (MavenCli.java:193)\n\t    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\n\t    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\n\t    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\n\t    at java.lang.reflect.Method.invoke (Method.java:498)\n\t    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)\n\t    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)\n\t    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406)\n\t    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)\n\t   \n解决方法：增加 -Drat.skip=true 参数 ，跳过licensing 检查。","source":"_posts/Maven-Too-many-files-with-unapproved-license.md","raw":"title: 'Maven: Too many files with unapproved license'\ndate: 2020-11-25 15:15:35\ntags:\n- Maven\n- Java\ncategories:\n- 开发工具\n- Maven\n---\n\n异常信息如下：\n\n\t[ERROR] Failed to execute goal org.apache.rat:apache-rat-plugin:0.11:check (default) on project ranger: Too many files with unapproved license: 1 See RAT report in: /home/zhangjc/ysten/git/apache-ranger-1.2.0/target/rat.txt -> [Help 1]\n\torg.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.rat:apache-rat-plugin:0.11:check (default) on project ranger: Too many files with unapproved license: 1 See RAT report in: /home/zhangjc/ysten/git/apache-ranger-1.2.0/target/rat.txt\n\t    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:215)\n\t    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:156)\n\t    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:148)\n\t    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)\n\t    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)\n\t    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56)\n\t    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)\n\t    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:305)\n\t    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192)\n\t    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105)\n\t    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:957)\n\t    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:289)\n\t    at org.apache.maven.cli.MavenCli.main (MavenCli.java:193)\n\t    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\n\t    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\n\t    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\n\t    at java.lang.reflect.Method.invoke (Method.java:498)\n\t    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)\n\t    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)\n\t    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406)\n\t    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)\n\t   \n解决方法：增加 -Drat.skip=true 参数 ，跳过licensing 检查。","slug":"Maven-Too-many-files-with-unapproved-license","published":1,"updated":"2021-07-19T16:27:59.980Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphog007ditd3er0lajck","content":"<p>异常信息如下：</p>\n<pre><code>[ERROR] Failed to execute goal org.apache.rat:apache-rat-plugin:0.11:check (default) on project ranger: Too many files with unapproved license: 1 See RAT report in: /home/zhangjc/ysten/git/apache-ranger-1.2.0/target/rat.txt -&gt; [Help 1]\norg.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.rat:apache-rat-plugin:0.11:check (default) on project ranger: Too many files with unapproved license: 1 See RAT report in: /home/zhangjc/ysten/git/apache-ranger-1.2.0/target/rat.txt\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:215)\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:156)\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:148)\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)\n    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56)\n    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:305)\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192)\n    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105)\n    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:957)\n    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:289)\n    at org.apache.maven.cli.MavenCli.main (MavenCli.java:193)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke (Method.java:498)\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)\n    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406)\n    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)\n   \n</code></pre>\n<p>解决方法：增加 -Drat.skip=true 参数 ，跳过licensing 检查。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>异常信息如下：</p>\n<pre><code>[ERROR] Failed to execute goal org.apache.rat:apache-rat-plugin:0.11:check (default) on project ranger: Too many files with unapproved license: 1 See RAT report in: /home/zhangjc/ysten/git/apache-ranger-1.2.0/target/rat.txt -&gt; [Help 1]\norg.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.rat:apache-rat-plugin:0.11:check (default) on project ranger: Too many files with unapproved license: 1 See RAT report in: /home/zhangjc/ysten/git/apache-ranger-1.2.0/target/rat.txt\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:215)\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:156)\n    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:148)\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)\n    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)\n    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56)\n    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:305)\n    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192)\n    at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105)\n    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:957)\n    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:289)\n    at org.apache.maven.cli.MavenCli.main (MavenCli.java:193)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke (Method.java:498)\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)\n    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)\n    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406)\n    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)\n   \n</code></pre>\n<p>解决方法：增加 -Drat.skip=true 参数 ，跳过licensing 检查。</p>\n"},{"title":"Missing artifact jdk.tools:jdk.tools:jar:1.6","date":"2015-11-17T06:06:14.000Z","_content":"Eclipse maven工程 Missing artifact jdk.tools:jdk.tools:jar:1.6 可以用以下方法解决。思路是配置 maven 依赖本地的 tools.jar ，需要本地安装有 jdk 1.6 。\n\n<!-- more -->\n\n### 下载安装JDK 1.6\nOracle 官网下载JDK 1.6 的链接不太容易找到，可以在 [Java SE Site Map](http://www.oracle.com/technetwork/java/javase/sitemap-jsp-139155.html) 页面找到以下位置，点击即可看到 JDK 所有版本的列表：\n\n![java se site map](/uploads/java-se-site-map.png)\n\n### 配置 maven 依赖\n\n\t<dependency>\n\t\t<groupId>jdk.tools</groupId>\n\t\t<artifactId>jdk.tools</artifactId>\n\t\t<version>1.6</version>\n\t\t<scope>system</scope>\n\t\t<systemPath>${JAVA_HOME}/lib/tools.jar</systemPath>\n\t</dependency-->\n\n注意，如果使用 ${JAVA_HOME} 配置，系统环境变量必须要配置 JAVA_HOME 。也可以直接使用本地路径，如下：\n\n\t<dependency>\n\t\t<groupId>jdk.tools</groupId>\n\t\t<artifactId>jdk.tools</artifactId>\n\t\t<version>1.6</version>\n\t\t<scope>system</scope>\n\t\t<systemPath>C:/Program Files/Java/jdk1.6.0_45/lib/tools.jar</systemPath>\n\t</dependency>\n","source":"_posts/Missing-artifact-jdk-tools-jdk-tools-jar-1-6.md","raw":"title: 'Missing artifact jdk.tools:jdk.tools:jar:1.6'\ndate: 2015-11-17 14:06:14\ntags:\n  - Maven\n  - Eclipse\ncategories:\n  - 开发工具\n  - Maven\n---\nEclipse maven工程 Missing artifact jdk.tools:jdk.tools:jar:1.6 可以用以下方法解决。思路是配置 maven 依赖本地的 tools.jar ，需要本地安装有 jdk 1.6 。\n\n<!-- more -->\n\n### 下载安装JDK 1.6\nOracle 官网下载JDK 1.6 的链接不太容易找到，可以在 [Java SE Site Map](http://www.oracle.com/technetwork/java/javase/sitemap-jsp-139155.html) 页面找到以下位置，点击即可看到 JDK 所有版本的列表：\n\n![java se site map](/uploads/java-se-site-map.png)\n\n### 配置 maven 依赖\n\n\t<dependency>\n\t\t<groupId>jdk.tools</groupId>\n\t\t<artifactId>jdk.tools</artifactId>\n\t\t<version>1.6</version>\n\t\t<scope>system</scope>\n\t\t<systemPath>${JAVA_HOME}/lib/tools.jar</systemPath>\n\t</dependency-->\n\n注意，如果使用 ${JAVA_HOME} 配置，系统环境变量必须要配置 JAVA_HOME 。也可以直接使用本地路径，如下：\n\n\t<dependency>\n\t\t<groupId>jdk.tools</groupId>\n\t\t<artifactId>jdk.tools</artifactId>\n\t\t<version>1.6</version>\n\t\t<scope>system</scope>\n\t\t<systemPath>C:/Program Files/Java/jdk1.6.0_45/lib/tools.jar</systemPath>\n\t</dependency>\n","slug":"Missing-artifact-jdk-tools-jdk-tools-jar-1-6","published":1,"updated":"2021-07-19T16:28:00.284Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphoi007hitd37fyy9dpi","content":"<p>Eclipse maven工程 Missing artifact jdk.tools:jdk.tools:jar:1.6 可以用以下方法解决。思路是配置 maven 依赖本地的 tools.jar ，需要本地安装有 jdk 1.6 。</p>\n<span id=\"more\"></span>\n\n<h3 id=\"下载安装JDK-1-6\"><a href=\"#下载安装JDK-1-6\" class=\"headerlink\" title=\"下载安装JDK 1.6\"></a>下载安装JDK 1.6</h3><p>Oracle 官网下载JDK 1.6 的链接不太容易找到，可以在 <a href=\"http://www.oracle.com/technetwork/java/javase/sitemap-jsp-139155.html\">Java SE Site Map</a> 页面找到以下位置，点击即可看到 JDK 所有版本的列表：</p>\n<p><img src=\"/uploads/java-se-site-map.png\" alt=\"java se site map\"></p>\n<h3 id=\"配置-maven-依赖\"><a href=\"#配置-maven-依赖\" class=\"headerlink\" title=\"配置 maven 依赖\"></a>配置 maven 依赖</h3><pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;jdk.tools&lt;/groupId&gt;\n    &lt;artifactId&gt;jdk.tools&lt;/artifactId&gt;\n    &lt;version&gt;1.6&lt;/version&gt;\n    &lt;scope&gt;system&lt;/scope&gt;\n    &lt;systemPath&gt;$&#123;JAVA_HOME&#125;/lib/tools.jar&lt;/systemPath&gt;\n&lt;/dependency--&gt;\n</code></pre>\n<p>注意，如果使用 ${JAVA_HOME} 配置，系统环境变量必须要配置 JAVA_HOME 。也可以直接使用本地路径，如下：</p>\n<pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;jdk.tools&lt;/groupId&gt;\n    &lt;artifactId&gt;jdk.tools&lt;/artifactId&gt;\n    &lt;version&gt;1.6&lt;/version&gt;\n    &lt;scope&gt;system&lt;/scope&gt;\n    &lt;systemPath&gt;C:/Program Files/Java/jdk1.6.0_45/lib/tools.jar&lt;/systemPath&gt;\n&lt;/dependency&gt;\n</code></pre>\n","site":{"data":{}},"excerpt":"<p>Eclipse maven工程 Missing artifact jdk.tools:jdk.tools:jar:1.6 可以用以下方法解决。思路是配置 maven 依赖本地的 tools.jar ，需要本地安装有 jdk 1.6 。</p>","more":"<h3 id=\"下载安装JDK-1-6\"><a href=\"#下载安装JDK-1-6\" class=\"headerlink\" title=\"下载安装JDK 1.6\"></a>下载安装JDK 1.6</h3><p>Oracle 官网下载JDK 1.6 的链接不太容易找到，可以在 <a href=\"http://www.oracle.com/technetwork/java/javase/sitemap-jsp-139155.html\">Java SE Site Map</a> 页面找到以下位置，点击即可看到 JDK 所有版本的列表：</p>\n<p><img src=\"/uploads/java-se-site-map.png\" alt=\"java se site map\"></p>\n<h3 id=\"配置-maven-依赖\"><a href=\"#配置-maven-依赖\" class=\"headerlink\" title=\"配置 maven 依赖\"></a>配置 maven 依赖</h3><pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;jdk.tools&lt;/groupId&gt;\n    &lt;artifactId&gt;jdk.tools&lt;/artifactId&gt;\n    &lt;version&gt;1.6&lt;/version&gt;\n    &lt;scope&gt;system&lt;/scope&gt;\n    &lt;systemPath&gt;$&#123;JAVA_HOME&#125;/lib/tools.jar&lt;/systemPath&gt;\n&lt;/dependency--&gt;\n</code></pre>\n<p>注意，如果使用 ${JAVA_HOME} 配置，系统环境变量必须要配置 JAVA_HOME 。也可以直接使用本地路径，如下：</p>\n<pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;jdk.tools&lt;/groupId&gt;\n    &lt;artifactId&gt;jdk.tools&lt;/artifactId&gt;\n    &lt;version&gt;1.6&lt;/version&gt;\n    &lt;scope&gt;system&lt;/scope&gt;\n    &lt;systemPath&gt;C:/Program Files/Java/jdk1.6.0_45/lib/tools.jar&lt;/systemPath&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"title":"MongoDB between ... and ... 操作","date":"2016-07-11T06:15:52.000Z","_content":"\n\nMongoDB 中类似 SQL 的 between and 操作可以采用如下语法：\n\n    db.collection.find( { field: { $gt: value1, $lt: value2 } } );\n","source":"_posts/MongoDB-between-and-操作.md","raw":"title: MongoDB between ... and ... 操作\ntags:\n  - MongoDB\ncategories:\n  - 数据库\n  - MongoDB\ndate: 2016-07-11 14:15:52\n---\n\n\nMongoDB 中类似 SQL 的 between and 操作可以采用如下语法：\n\n    db.collection.find( { field: { $gt: value1, $lt: value2 } } );\n","slug":"MongoDB-between-and-操作","published":1,"updated":"2021-07-19T16:28:00.284Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphoj007jitd3h15vfk2p","content":"<p>MongoDB 中类似 SQL 的 between and 操作可以采用如下语法：</p>\n<pre><code>db.collection.find( &#123; field: &#123; $gt: value1, $lt: value2 &#125; &#125; );\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>MongoDB 中类似 SQL 的 between and 操作可以采用如下语法：</p>\n<pre><code>db.collection.find( &#123; field: &#123; $gt: value1, $lt: value2 &#125; &#125; );\n</code></pre>\n"},{"title":"MongoDB中sum的方法","date":"2015-11-15T08:01:43.000Z","_content":"MongoDB中求和的方法有以下两种：\n\n<!-- more -->\n\n### 聚合管道（Aggregation Pipelines）\n以下是MongoDB官网的示例，很清楚。注意，用来统计的 field 的数据类型得是数值类型，不能是字符串，否则统计结果为 0。\n\n![aggregation-pipeline](/uploads/aggregation-pipeline.png)\n\n### Map-Reduce\n以下是MongoDB官网的示例，也很清楚。注意，Map-Reduce的数据结果保存在一个新的Collection中；Collection的名字就是out的值。在该示例中即 order_totals。\n\n![map-reduct](/uploads/map-reduce.png)\n\n如果要 sum 的 field 需要数据类型转换，可以在 Map 阶段完成。如下数据：\n\n\tdb.orders_str.insert({cust_id:\"A123\",amount:\"500\",status:\"A\"})\n\tdb.orders_str.insert({cust_id:\"A123\",amount:\"250\",status:\"A\"})\n\tdb.orders_str.insert({cust_id:\"B212\",amount:\"200\",status:\"A\"})\n\tdb.orders_str.insert({cust_id:\"A123\",amount:\"300\",status:\"D\"})\n\nMap-Reduce：\n\n\tdb.orders_str.mapReduce(\n\t\tfunction(){\n\t\t\tvar cnt = parseInt(this.amount);\n\t\t\temit(this.cust_id,cnt);\n\t\t},\n\t\tfunction(key,values){\n\t\t\treturn Array.sum(values);\n\t\t},\n\t\t{\n\t\t\tquery:{status:\"A\"},\n\t\t\tout: \"order_totals_str\"\n\t\t}\n\t)\n\n输出结果：\n\n\tdb.order_totals_str.find({})\n\n{ \"_id\" : \"A123\", \"value\" : 750 }  \n{ \"_id\" : \"B212\", \"value\" : 200 }  \nDONE！!\n","source":"_posts/MongoDB-sum.md","raw":"title: MongoDB中sum的方法\ndate: 2015-11-15 16:01:43\ntags:\n  - MongoDB\ncategories:\n  - 数据库\n  - MongoDB\n---\nMongoDB中求和的方法有以下两种：\n\n<!-- more -->\n\n### 聚合管道（Aggregation Pipelines）\n以下是MongoDB官网的示例，很清楚。注意，用来统计的 field 的数据类型得是数值类型，不能是字符串，否则统计结果为 0。\n\n![aggregation-pipeline](/uploads/aggregation-pipeline.png)\n\n### Map-Reduce\n以下是MongoDB官网的示例，也很清楚。注意，Map-Reduce的数据结果保存在一个新的Collection中；Collection的名字就是out的值。在该示例中即 order_totals。\n\n![map-reduct](/uploads/map-reduce.png)\n\n如果要 sum 的 field 需要数据类型转换，可以在 Map 阶段完成。如下数据：\n\n\tdb.orders_str.insert({cust_id:\"A123\",amount:\"500\",status:\"A\"})\n\tdb.orders_str.insert({cust_id:\"A123\",amount:\"250\",status:\"A\"})\n\tdb.orders_str.insert({cust_id:\"B212\",amount:\"200\",status:\"A\"})\n\tdb.orders_str.insert({cust_id:\"A123\",amount:\"300\",status:\"D\"})\n\nMap-Reduce：\n\n\tdb.orders_str.mapReduce(\n\t\tfunction(){\n\t\t\tvar cnt = parseInt(this.amount);\n\t\t\temit(this.cust_id,cnt);\n\t\t},\n\t\tfunction(key,values){\n\t\t\treturn Array.sum(values);\n\t\t},\n\t\t{\n\t\t\tquery:{status:\"A\"},\n\t\t\tout: \"order_totals_str\"\n\t\t}\n\t)\n\n输出结果：\n\n\tdb.order_totals_str.find({})\n\n{ \"_id\" : \"A123\", \"value\" : 750 }  \n{ \"_id\" : \"B212\", \"value\" : 200 }  \nDONE！!\n","slug":"MongoDB-sum","published":1,"updated":"2021-07-19T16:28:00.284Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphol007nitd36uvshncz","content":"<p>MongoDB中求和的方法有以下两种：</p>\n<span id=\"more\"></span>\n\n<h3 id=\"聚合管道（Aggregation-Pipelines）\"><a href=\"#聚合管道（Aggregation-Pipelines）\" class=\"headerlink\" title=\"聚合管道（Aggregation Pipelines）\"></a>聚合管道（Aggregation Pipelines）</h3><p>以下是MongoDB官网的示例，很清楚。注意，用来统计的 field 的数据类型得是数值类型，不能是字符串，否则统计结果为 0。</p>\n<p><img src=\"/uploads/aggregation-pipeline.png\" alt=\"aggregation-pipeline\"></p>\n<h3 id=\"Map-Reduce\"><a href=\"#Map-Reduce\" class=\"headerlink\" title=\"Map-Reduce\"></a>Map-Reduce</h3><p>以下是MongoDB官网的示例，也很清楚。注意，Map-Reduce的数据结果保存在一个新的Collection中；Collection的名字就是out的值。在该示例中即 order_totals。</p>\n<p><img src=\"/uploads/map-reduce.png\" alt=\"map-reduct\"></p>\n<p>如果要 sum 的 field 需要数据类型转换，可以在 Map 阶段完成。如下数据：</p>\n<pre><code>db.orders_str.insert(&#123;cust_id:&quot;A123&quot;,amount:&quot;500&quot;,status:&quot;A&quot;&#125;)\ndb.orders_str.insert(&#123;cust_id:&quot;A123&quot;,amount:&quot;250&quot;,status:&quot;A&quot;&#125;)\ndb.orders_str.insert(&#123;cust_id:&quot;B212&quot;,amount:&quot;200&quot;,status:&quot;A&quot;&#125;)\ndb.orders_str.insert(&#123;cust_id:&quot;A123&quot;,amount:&quot;300&quot;,status:&quot;D&quot;&#125;)\n</code></pre>\n<p>Map-Reduce：</p>\n<pre><code>db.orders_str.mapReduce(\n    function()&#123;\n        var cnt = parseInt(this.amount);\n        emit(this.cust_id,cnt);\n    &#125;,\n    function(key,values)&#123;\n        return Array.sum(values);\n    &#125;,\n    &#123;\n        query:&#123;status:&quot;A&quot;&#125;,\n        out: &quot;order_totals_str&quot;\n    &#125;\n)\n</code></pre>\n<p>输出结果：</p>\n<pre><code>db.order_totals_str.find(&#123;&#125;)\n</code></pre>\n<p>{ “_id” : “A123”, “value” : 750 }<br>{ “_id” : “B212”, “value” : 200 }<br>DONE！!</p>\n","site":{"data":{}},"excerpt":"<p>MongoDB中求和的方法有以下两种：</p>","more":"<h3 id=\"聚合管道（Aggregation-Pipelines）\"><a href=\"#聚合管道（Aggregation-Pipelines）\" class=\"headerlink\" title=\"聚合管道（Aggregation Pipelines）\"></a>聚合管道（Aggregation Pipelines）</h3><p>以下是MongoDB官网的示例，很清楚。注意，用来统计的 field 的数据类型得是数值类型，不能是字符串，否则统计结果为 0。</p>\n<p><img src=\"/uploads/aggregation-pipeline.png\" alt=\"aggregation-pipeline\"></p>\n<h3 id=\"Map-Reduce\"><a href=\"#Map-Reduce\" class=\"headerlink\" title=\"Map-Reduce\"></a>Map-Reduce</h3><p>以下是MongoDB官网的示例，也很清楚。注意，Map-Reduce的数据结果保存在一个新的Collection中；Collection的名字就是out的值。在该示例中即 order_totals。</p>\n<p><img src=\"/uploads/map-reduce.png\" alt=\"map-reduct\"></p>\n<p>如果要 sum 的 field 需要数据类型转换，可以在 Map 阶段完成。如下数据：</p>\n<pre><code>db.orders_str.insert(&#123;cust_id:&quot;A123&quot;,amount:&quot;500&quot;,status:&quot;A&quot;&#125;)\ndb.orders_str.insert(&#123;cust_id:&quot;A123&quot;,amount:&quot;250&quot;,status:&quot;A&quot;&#125;)\ndb.orders_str.insert(&#123;cust_id:&quot;B212&quot;,amount:&quot;200&quot;,status:&quot;A&quot;&#125;)\ndb.orders_str.insert(&#123;cust_id:&quot;A123&quot;,amount:&quot;300&quot;,status:&quot;D&quot;&#125;)\n</code></pre>\n<p>Map-Reduce：</p>\n<pre><code>db.orders_str.mapReduce(\n    function()&#123;\n        var cnt = parseInt(this.amount);\n        emit(this.cust_id,cnt);\n    &#125;,\n    function(key,values)&#123;\n        return Array.sum(values);\n    &#125;,\n    &#123;\n        query:&#123;status:&quot;A&quot;&#125;,\n        out: &quot;order_totals_str&quot;\n    &#125;\n)\n</code></pre>\n<p>输出结果：</p>\n<pre><code>db.order_totals_str.find(&#123;&#125;)\n</code></pre>\n<p>{ “_id” : “A123”, “value” : 750 }<br>{ “_id” : “B212”, “value” : 200 }<br>DONE！!</p>"},{"title":"MongoDB 常用查询","date":"2016-04-20T09:02:48.000Z","_content":"\n\n1. 当前 MongoDB 版本：\n\n    db.version();\n\n<!-- more -->\n\n2. Web控制台\n\n  Mongodb 自带了 Web 控制台，默认和数据服务一同开启。他的端口在 Mongodb 数据库服务器端口的基础上加 1000 ，如果是默认的Mongodb数据服务端口  27017 ，则相应的Web端口为 28017。这个页面可以看到：\n\n  － 当前Mongodb的所有连接。\n  － 各个数据库和Collection的访问统计，包括：Reads, Writes, Queries, GetMores ,Inserts, Updates, Removes。\n  － 写锁的状态。\n  － 以及日志文件的最后几百行。\n\n  参考截图：![MongoDB Web 控制台](/uploads/20160511/mongoDB-web-console.png)\n","source":"_posts/MongoDB-常用查询.md","raw":"title: MongoDB 常用查询\ntags:\n  - MongoDB\ncategories:\n  - 数据库\n  - MongoDB\ndate: 2016-04-20 17:02:48\n---\n\n\n1. 当前 MongoDB 版本：\n\n    db.version();\n\n<!-- more -->\n\n2. Web控制台\n\n  Mongodb 自带了 Web 控制台，默认和数据服务一同开启。他的端口在 Mongodb 数据库服务器端口的基础上加 1000 ，如果是默认的Mongodb数据服务端口  27017 ，则相应的Web端口为 28017。这个页面可以看到：\n\n  － 当前Mongodb的所有连接。\n  － 各个数据库和Collection的访问统计，包括：Reads, Writes, Queries, GetMores ,Inserts, Updates, Removes。\n  － 写锁的状态。\n  － 以及日志文件的最后几百行。\n\n  参考截图：![MongoDB Web 控制台](/uploads/20160511/mongoDB-web-console.png)\n","slug":"MongoDB-常用查询","published":1,"updated":"2021-07-19T16:28:00.284Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphom007qitd39n815o1r","content":"<ol>\n<li><p>当前 MongoDB 版本：</p>\n<p> db.version();</p>\n</li>\n</ol>\n<span id=\"more\"></span>\n\n<ol start=\"2\">\n<li>Web控制台</li>\n</ol>\n<p>  Mongodb 自带了 Web 控制台，默认和数据服务一同开启。他的端口在 Mongodb 数据库服务器端口的基础上加 1000 ，如果是默认的Mongodb数据服务端口  27017 ，则相应的Web端口为 28017。这个页面可以看到：</p>\n<p>  － 当前Mongodb的所有连接。<br>  － 各个数据库和Collection的访问统计，包括：Reads, Writes, Queries, GetMores ,Inserts, Updates, Removes。<br>  － 写锁的状态。<br>  － 以及日志文件的最后几百行。</p>\n<p>  参考截图：<img src=\"/uploads/20160511/mongoDB-web-console.png\" alt=\"MongoDB Web 控制台\"></p>\n","site":{"data":{}},"excerpt":"<ol>\n<li><p>当前 MongoDB 版本：</p>\n<p> db.version();</p>\n</li>\n</ol>","more":"<ol start=\"2\">\n<li>Web控制台</li>\n</ol>\n<p>  Mongodb 自带了 Web 控制台，默认和数据服务一同开启。他的端口在 Mongodb 数据库服务器端口的基础上加 1000 ，如果是默认的Mongodb数据服务端口  27017 ，则相应的Web端口为 28017。这个页面可以看到：</p>\n<p>  － 当前Mongodb的所有连接。<br>  － 各个数据库和Collection的访问统计，包括：Reads, Writes, Queries, GetMores ,Inserts, Updates, Removes。<br>  － 写锁的状态。<br>  － 以及日志文件的最后几百行。</p>\n<p>  参考截图：<img src=\"/uploads/20160511/mongoDB-web-console.png\" alt=\"MongoDB Web 控制台\"></p>"},{"title":"MongoDB 数据导出工具 mongoexport","date":"2019-03-27T03:11:51.000Z","_content":"\n导出示例：\n\n    $ mongoexport --host 192.168.72.60 --db realtime_statistic_backup --collection all_play_stats_summary --fields \"_id.timestamp,total_uv\" --type csv --out uv.csv --sort \"{'_id.timestamp': -1}\" --query \"{'_id.province_id':'sc','_id.city_id':'all', '_id.display_name': 'all'}\"\n\n<!-- more -->\n\n- 3.4.6 版本后支持 --uri 方式连接 MongoDB\n- --fields 指定要导出的 field，如果是嵌套的 json，可以通过点（.）操作指定内层的 field\n","source":"_posts/MongoDB-数据导出工具-mongoexport.md","raw":"title: MongoDB 数据导出工具 mongoexport\ndate: 2019-03-27 11:11:51\ntags:\n- MongoDB\ncategories:\n- 数据库\n- MongoDB\n---\n\n导出示例：\n\n    $ mongoexport --host 192.168.72.60 --db realtime_statistic_backup --collection all_play_stats_summary --fields \"_id.timestamp,total_uv\" --type csv --out uv.csv --sort \"{'_id.timestamp': -1}\" --query \"{'_id.province_id':'sc','_id.city_id':'all', '_id.display_name': 'all'}\"\n\n<!-- more -->\n\n- 3.4.6 版本后支持 --uri 方式连接 MongoDB\n- --fields 指定要导出的 field，如果是嵌套的 json，可以通过点（.）操作指定内层的 field\n","slug":"MongoDB-数据导出工具-mongoexport","published":1,"updated":"2021-07-19T16:28:00.284Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphoq007vitd3fcb00t7j","content":"<p>导出示例：</p>\n<pre><code>$ mongoexport --host 192.168.72.60 --db realtime_statistic_backup --collection all_play_stats_summary --fields &quot;_id.timestamp,total_uv&quot; --type csv --out uv.csv --sort &quot;&#123;&#39;_id.timestamp&#39;: -1&#125;&quot; --query &quot;&#123;&#39;_id.province_id&#39;:&#39;sc&#39;,&#39;_id.city_id&#39;:&#39;all&#39;, &#39;_id.display_name&#39;: &#39;all&#39;&#125;&quot;\n</code></pre>\n<span id=\"more\"></span>\n\n<ul>\n<li>3.4.6 版本后支持 –uri 方式连接 MongoDB</li>\n<li>–fields 指定要导出的 field，如果是嵌套的 json，可以通过点（.）操作指定内层的 field</li>\n</ul>\n","site":{"data":{}},"excerpt":"<p>导出示例：</p>\n<pre><code>$ mongoexport --host 192.168.72.60 --db realtime_statistic_backup --collection all_play_stats_summary --fields &quot;_id.timestamp,total_uv&quot; --type csv --out uv.csv --sort &quot;&#123;&#39;_id.timestamp&#39;: -1&#125;&quot; --query &quot;&#123;&#39;_id.province_id&#39;:&#39;sc&#39;,&#39;_id.city_id&#39;:&#39;all&#39;, &#39;_id.display_name&#39;: &#39;all&#39;&#125;&quot;\n</code></pre>","more":"<ul>\n<li>3.4.6 版本后支持 –uri 方式连接 MongoDB</li>\n<li>–fields 指定要导出的 field，如果是嵌套的 json，可以通过点（.）操作指定内层的 field</li>\n</ul>"},{"title":"MongoDB 查询和投影操作符","date":"2016-05-15T10:58:31.000Z","_content":"\n### 查询选择器\n#### 比较操作符\n\n想了解不同 BSON 类型值的比较，参见 [BSON比较顺序详细说明](https://docs.mongodb.org/manual/reference/bson-types/#bson-types-comparison-order)。\n\n[$eq](https://docs.mongodb.com/manual/reference/operator/query/eq/#op._S_eq) 匹配跟指定值相等的值  \n[$gt](https://docs.mongodb.com/manual/reference/operator/query/gt/#op._S_gt) 匹配比指定值大的值  \n[$gte](https://docs.mongodb.com/manual/reference/operator/query/gte/#op._S_gte) 匹配比指定值大或者相等的值  \n[$lt](https://docs.mongodb.com/manual/reference/operator/query/lt/#op._S_lt) 匹配比指定值小的值  \n[$lte](https://docs.mongodb.com/manual/reference/operator/query/lte/#op._S_lte) 匹配比指定值小或者相等的值  \n[$ne](https://docs.mongodb.com/manual/reference/operator/query/ne/#op._S_ne) 匹配跟指定值不相等的值  \n[$in](https://docs.mongodb.com/manual/reference/operator/query/in/#op._S_in) 匹配在指定数组中的任意值  \n[$nin](https://docs.mongodb.com/manual/reference/operator/query/nin/#op._S_nin) 匹配不在指定数组中的值\n\n<!-- more -->\n\n#### 逻辑操作符\n\n[$or](https://docs.mongodb.com/manual/reference/operator/query/or/#op._S_or) 用逻辑 **OR** 操作符连接查询子句，返回所有匹配任一个条件子句的文档。  \n[$and](https://docs.mongodb.com/manual/reference/operator/query/and/#op._S_and) 用逻辑 **AND** 操作符连接查询子句，返回所有同时匹配两个条件子句的文档。  \n[$not](https://docs.mongodb.com/manual/reference/operator/query/not/#op._S_not) 反转查询条件的效果，返回*不匹配*查询条件的文档。  \n[$nor](https://docs.mongodb.com/manual/reference/operator/query/nor/#op._S_nor) 用逻辑 **NOR** 操作符连接查询子句，返回两个条件都匹配失败的所有文档。\n\n#### 元素操作符\n\n[$exists](https://docs.mongodb.com/manual/reference/operator/query/exists/#op._S_exists) 匹配有指定属性的文档。  \n[$type](https://docs.mongodb.com/manual/reference/operator/query/type/#op._S_type) 选择属性是指定类型的文档。\n\n#### 评估操作符\n\n[$mod](https://docs.mongodb.com/manual/reference/operator/query/mod/#op._S_mod) 对一个属性的值执行模数运算，选择有指定结果的文档。  \n[$regex](https://docs.mongodb.com/manual/reference/operator/query/regex/#op._S_regex) 选择值匹配指定规则表达式的文档。  \n[$text](https://docs.mongodb.com/manual/reference/operator/query/text/#op._S_text) 执行文本搜索。  \n[$where](https://docs.mongodb.com/manual/reference/operator/query/where/#op._S_where) 匹配满足一个 JavaScript 表达式的文档。\n\n#### 空间操作符\n\n[$geoWithin](https://docs.mongodb.com/manual/reference/operator/query/geoWithin/#op._S_geoWithin) 选择包围 [GeoJSON  几何图形](https://docs.mongodb.com/manual/reference/geojson/#geospatial-indexes-store-geojson) 的几何图形。[二维球形](https://docs.mongodb.com/manual/core/2dsphere/)和[二维索引](https://docs.mongodb.com/manual/core/2d/)支持 [$geoWithin](https://docs.mongodb.com/manual/reference/operator/query/geoWithin/#op._S_geoWithin)。  \n[$geoIntersects](https://docs.mongodb.com/manual/reference/operator/query/geoIntersects/#op._S_geoIntersects) 选择与一个 [GeoJSON 几何图形](https://docs.mongodb.com/manual/reference/geojson/#geospatial-indexes-store-geojson) 相交的几何图形。[二维球形](https://docs.mongodb.com/manual/core/2dsphere/)支持 [$geoIntersects](https://docs.mongodb.com/manual/reference/operator/query/geoIntersects/#op._S_geoIntersects)。  \n[$near](https://docs.mongodb.com/manual/reference/operator/query/near/#op._S_near) 返回跟一个点邻近的空间对象。需要一个空间索引。[二维球形](https://docs.mongodb.com/manual/core/2dsphere/)和[二维索引](https://docs.mongodb.com/manual/core/2d/)支持 [$near](https://docs.mongodb.com/manual/reference/operator/query/near/#op._S_near)。  \n[$nearSphere](https://docs.mongodb.com/manual/reference/operator/query/nearSphere/#op._S_nearSphere) 返回球形上跟一个点邻近的空间对象。[二维球形](https://docs.mongodb.com/manual/core/2dsphere/)和[二维索引](https://docs.mongodb.com/manual/core/2d/)支持 [$nearSphere](https://docs.mongodb.com/manual/reference/operator/query/nearSphere/#op._S_nearSphere)。\n\n#### 数组操作符\n\n[$all](https://docs.mongodb.com/manual/reference/operator/query/all/#op._S_all) 匹配查询中指定的所有元素的数组。  \n[$elemMatch](https://docs.mongodb.com/manual/reference/operator/query/elemMatch/#op._S_elemMatch) 选择那些属性匹配所有 [$elemMatch](https://docs.mongodb.com/manual/reference/operator/query/elemMatch/#op._S_elemMatch) 条件指定的全部属性的数组。  \n[$size](https://docs.mongodb.com/manual/reference/operator/query/size/#op._S_size) 选择符合指定属性个数的数组。\n\n#### 位运算操作符\n\n[$bitsAllSet](https://docs.mongodb.com/manual/reference/operator/query/bitsAllSet/#op._S_bitsAllSet) 匹配的数字或二进制值，其中这组位的位置都有一个值为1。  \n[$bitsAnySet](https://docs.mongodb.com/manual/reference/operator/query/bitsAnySet/#op._S_bitsAnySet) 匹配的数字或二进制值，其中这组位的任意一个位置有一个值为1。  \n[$bitsAllClear](https://docs.mongodb.com/manual/reference/operator/query/bitsAllClear/#op._S_bitsAllClear) 匹配的数字或二进制值，其中这组位的位置都有一个值为0。  \n[$bitsAnyClear](https://docs.mongodb.com/manual/reference/operator/query/bitsAnyClear/#op._S_bitsAnyClear) 匹配的数字或二进制值，其中这组位的任意一个位置有一个值为0。\n\n#### 注释操作符\n\n[$comment](https://docs.mongodb.com/manual/reference/operator/query/comment/#op._S_comment) 给查询断言添加注释。\n\n### 投影操作符\n\n[$](https://docs.mongodb.com/manual/reference/operator/projection/positional/#proj._S_) 展现第一个元素匹配查询条件的数组。  \n[$elemMatch](https://docs.mongodb.com/manual/reference/operator/projection/elemMatch/#proj._S_elemMatch) 展现第一个元素匹配指定的 [$elemMatch](https://docs.mongodb.com/manual/reference/operator/projection/elemMatch/#proj._S_elemMatch) 条件的数组。  \n[$meta](https://docs.mongodb.com/manual/reference/operator/projection/meta/#proj._S_meta) 展现 [$text](https://docs.mongodb.com/manual/reference/operator/query/text/#op._S_text) 操作中赋值给文档的分数。  \n[$slice](https://docs.mongodb.com/manual/reference/operator/projection/slice/#proj._S_slice) 限制从一个数组展示的元素的个数。支持 skip 和 limit。\n","source":"_posts/MongoDB-查询和投影操作符.md","raw":"title: MongoDB 查询和投影操作符\ntags:\n  - MongoDB\ncategories:\n  - 数据库\n  - MongoDB\ndate: 2016-05-15 18:58:31\n---\n\n### 查询选择器\n#### 比较操作符\n\n想了解不同 BSON 类型值的比较，参见 [BSON比较顺序详细说明](https://docs.mongodb.org/manual/reference/bson-types/#bson-types-comparison-order)。\n\n[$eq](https://docs.mongodb.com/manual/reference/operator/query/eq/#op._S_eq) 匹配跟指定值相等的值  \n[$gt](https://docs.mongodb.com/manual/reference/operator/query/gt/#op._S_gt) 匹配比指定值大的值  \n[$gte](https://docs.mongodb.com/manual/reference/operator/query/gte/#op._S_gte) 匹配比指定值大或者相等的值  \n[$lt](https://docs.mongodb.com/manual/reference/operator/query/lt/#op._S_lt) 匹配比指定值小的值  \n[$lte](https://docs.mongodb.com/manual/reference/operator/query/lte/#op._S_lte) 匹配比指定值小或者相等的值  \n[$ne](https://docs.mongodb.com/manual/reference/operator/query/ne/#op._S_ne) 匹配跟指定值不相等的值  \n[$in](https://docs.mongodb.com/manual/reference/operator/query/in/#op._S_in) 匹配在指定数组中的任意值  \n[$nin](https://docs.mongodb.com/manual/reference/operator/query/nin/#op._S_nin) 匹配不在指定数组中的值\n\n<!-- more -->\n\n#### 逻辑操作符\n\n[$or](https://docs.mongodb.com/manual/reference/operator/query/or/#op._S_or) 用逻辑 **OR** 操作符连接查询子句，返回所有匹配任一个条件子句的文档。  \n[$and](https://docs.mongodb.com/manual/reference/operator/query/and/#op._S_and) 用逻辑 **AND** 操作符连接查询子句，返回所有同时匹配两个条件子句的文档。  \n[$not](https://docs.mongodb.com/manual/reference/operator/query/not/#op._S_not) 反转查询条件的效果，返回*不匹配*查询条件的文档。  \n[$nor](https://docs.mongodb.com/manual/reference/operator/query/nor/#op._S_nor) 用逻辑 **NOR** 操作符连接查询子句，返回两个条件都匹配失败的所有文档。\n\n#### 元素操作符\n\n[$exists](https://docs.mongodb.com/manual/reference/operator/query/exists/#op._S_exists) 匹配有指定属性的文档。  \n[$type](https://docs.mongodb.com/manual/reference/operator/query/type/#op._S_type) 选择属性是指定类型的文档。\n\n#### 评估操作符\n\n[$mod](https://docs.mongodb.com/manual/reference/operator/query/mod/#op._S_mod) 对一个属性的值执行模数运算，选择有指定结果的文档。  \n[$regex](https://docs.mongodb.com/manual/reference/operator/query/regex/#op._S_regex) 选择值匹配指定规则表达式的文档。  \n[$text](https://docs.mongodb.com/manual/reference/operator/query/text/#op._S_text) 执行文本搜索。  \n[$where](https://docs.mongodb.com/manual/reference/operator/query/where/#op._S_where) 匹配满足一个 JavaScript 表达式的文档。\n\n#### 空间操作符\n\n[$geoWithin](https://docs.mongodb.com/manual/reference/operator/query/geoWithin/#op._S_geoWithin) 选择包围 [GeoJSON  几何图形](https://docs.mongodb.com/manual/reference/geojson/#geospatial-indexes-store-geojson) 的几何图形。[二维球形](https://docs.mongodb.com/manual/core/2dsphere/)和[二维索引](https://docs.mongodb.com/manual/core/2d/)支持 [$geoWithin](https://docs.mongodb.com/manual/reference/operator/query/geoWithin/#op._S_geoWithin)。  \n[$geoIntersects](https://docs.mongodb.com/manual/reference/operator/query/geoIntersects/#op._S_geoIntersects) 选择与一个 [GeoJSON 几何图形](https://docs.mongodb.com/manual/reference/geojson/#geospatial-indexes-store-geojson) 相交的几何图形。[二维球形](https://docs.mongodb.com/manual/core/2dsphere/)支持 [$geoIntersects](https://docs.mongodb.com/manual/reference/operator/query/geoIntersects/#op._S_geoIntersects)。  \n[$near](https://docs.mongodb.com/manual/reference/operator/query/near/#op._S_near) 返回跟一个点邻近的空间对象。需要一个空间索引。[二维球形](https://docs.mongodb.com/manual/core/2dsphere/)和[二维索引](https://docs.mongodb.com/manual/core/2d/)支持 [$near](https://docs.mongodb.com/manual/reference/operator/query/near/#op._S_near)。  \n[$nearSphere](https://docs.mongodb.com/manual/reference/operator/query/nearSphere/#op._S_nearSphere) 返回球形上跟一个点邻近的空间对象。[二维球形](https://docs.mongodb.com/manual/core/2dsphere/)和[二维索引](https://docs.mongodb.com/manual/core/2d/)支持 [$nearSphere](https://docs.mongodb.com/manual/reference/operator/query/nearSphere/#op._S_nearSphere)。\n\n#### 数组操作符\n\n[$all](https://docs.mongodb.com/manual/reference/operator/query/all/#op._S_all) 匹配查询中指定的所有元素的数组。  \n[$elemMatch](https://docs.mongodb.com/manual/reference/operator/query/elemMatch/#op._S_elemMatch) 选择那些属性匹配所有 [$elemMatch](https://docs.mongodb.com/manual/reference/operator/query/elemMatch/#op._S_elemMatch) 条件指定的全部属性的数组。  \n[$size](https://docs.mongodb.com/manual/reference/operator/query/size/#op._S_size) 选择符合指定属性个数的数组。\n\n#### 位运算操作符\n\n[$bitsAllSet](https://docs.mongodb.com/manual/reference/operator/query/bitsAllSet/#op._S_bitsAllSet) 匹配的数字或二进制值，其中这组位的位置都有一个值为1。  \n[$bitsAnySet](https://docs.mongodb.com/manual/reference/operator/query/bitsAnySet/#op._S_bitsAnySet) 匹配的数字或二进制值，其中这组位的任意一个位置有一个值为1。  \n[$bitsAllClear](https://docs.mongodb.com/manual/reference/operator/query/bitsAllClear/#op._S_bitsAllClear) 匹配的数字或二进制值，其中这组位的位置都有一个值为0。  \n[$bitsAnyClear](https://docs.mongodb.com/manual/reference/operator/query/bitsAnyClear/#op._S_bitsAnyClear) 匹配的数字或二进制值，其中这组位的任意一个位置有一个值为0。\n\n#### 注释操作符\n\n[$comment](https://docs.mongodb.com/manual/reference/operator/query/comment/#op._S_comment) 给查询断言添加注释。\n\n### 投影操作符\n\n[$](https://docs.mongodb.com/manual/reference/operator/projection/positional/#proj._S_) 展现第一个元素匹配查询条件的数组。  \n[$elemMatch](https://docs.mongodb.com/manual/reference/operator/projection/elemMatch/#proj._S_elemMatch) 展现第一个元素匹配指定的 [$elemMatch](https://docs.mongodb.com/manual/reference/operator/projection/elemMatch/#proj._S_elemMatch) 条件的数组。  \n[$meta](https://docs.mongodb.com/manual/reference/operator/projection/meta/#proj._S_meta) 展现 [$text](https://docs.mongodb.com/manual/reference/operator/query/text/#op._S_text) 操作中赋值给文档的分数。  \n[$slice](https://docs.mongodb.com/manual/reference/operator/projection/slice/#proj._S_slice) 限制从一个数组展示的元素的个数。支持 skip 和 limit。\n","slug":"MongoDB-查询和投影操作符","published":1,"updated":"2021-07-19T16:28:00.284Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphos007yitd3fad6g3i0","content":"<h3 id=\"查询选择器\"><a href=\"#查询选择器\" class=\"headerlink\" title=\"查询选择器\"></a>查询选择器</h3><h4 id=\"比较操作符\"><a href=\"#比较操作符\" class=\"headerlink\" title=\"比较操作符\"></a>比较操作符</h4><p>想了解不同 BSON 类型值的比较，参见 <a href=\"https://docs.mongodb.org/manual/reference/bson-types/#bson-types-comparison-order\">BSON比较顺序详细说明</a>。</p>\n<p><a href=\"https://docs.mongodb.com/manual/reference/operator/query/eq/#op._S_eq\">$eq</a> 匹配跟指定值相等的值<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/gt/#op._S_gt\">$gt</a> 匹配比指定值大的值<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/gte/#op._S_gte\">$gte</a> 匹配比指定值大或者相等的值<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/lt/#op._S_lt\">$lt</a> 匹配比指定值小的值<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/lte/#op._S_lte\">$lte</a> 匹配比指定值小或者相等的值<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/ne/#op._S_ne\">$ne</a> 匹配跟指定值不相等的值<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/in/#op._S_in\">$in</a> 匹配在指定数组中的任意值<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/nin/#op._S_nin\">$nin</a> 匹配不在指定数组中的值</p>\n<span id=\"more\"></span>\n\n<h4 id=\"逻辑操作符\"><a href=\"#逻辑操作符\" class=\"headerlink\" title=\"逻辑操作符\"></a>逻辑操作符</h4><p><a href=\"https://docs.mongodb.com/manual/reference/operator/query/or/#op._S_or\">$or</a> 用逻辑 <strong>OR</strong> 操作符连接查询子句，返回所有匹配任一个条件子句的文档。<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/and/#op._S_and\">$and</a> 用逻辑 <strong>AND</strong> 操作符连接查询子句，返回所有同时匹配两个条件子句的文档。<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/not/#op._S_not\">$not</a> 反转查询条件的效果，返回<em>不匹配</em>查询条件的文档。<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/nor/#op._S_nor\">$nor</a> 用逻辑 <strong>NOR</strong> 操作符连接查询子句，返回两个条件都匹配失败的所有文档。</p>\n<h4 id=\"元素操作符\"><a href=\"#元素操作符\" class=\"headerlink\" title=\"元素操作符\"></a>元素操作符</h4><p><a href=\"https://docs.mongodb.com/manual/reference/operator/query/exists/#op._S_exists\">$exists</a> 匹配有指定属性的文档。<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/type/#op._S_type\">$type</a> 选择属性是指定类型的文档。</p>\n<h4 id=\"评估操作符\"><a href=\"#评估操作符\" class=\"headerlink\" title=\"评估操作符\"></a>评估操作符</h4><p><a href=\"https://docs.mongodb.com/manual/reference/operator/query/mod/#op._S_mod\">$mod</a> 对一个属性的值执行模数运算，选择有指定结果的文档。<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/regex/#op._S_regex\">$regex</a> 选择值匹配指定规则表达式的文档。<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/text/#op._S_text\">$text</a> 执行文本搜索。<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/where/#op._S_where\">$where</a> 匹配满足一个 JavaScript 表达式的文档。</p>\n<h4 id=\"空间操作符\"><a href=\"#空间操作符\" class=\"headerlink\" title=\"空间操作符\"></a>空间操作符</h4><p><a href=\"https://docs.mongodb.com/manual/reference/operator/query/geoWithin/#op._S_geoWithin\">$geoWithin</a> 选择包围 <a href=\"https://docs.mongodb.com/manual/reference/geojson/#geospatial-indexes-store-geojson\">GeoJSON  几何图形</a> 的几何图形。<a href=\"https://docs.mongodb.com/manual/core/2dsphere/\">二维球形</a>和<a href=\"https://docs.mongodb.com/manual/core/2d/\">二维索引</a>支持 <a href=\"https://docs.mongodb.com/manual/reference/operator/query/geoWithin/#op._S_geoWithin\">$geoWithin</a>。<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/geoIntersects/#op._S_geoIntersects\">$geoIntersects</a> 选择与一个 <a href=\"https://docs.mongodb.com/manual/reference/geojson/#geospatial-indexes-store-geojson\">GeoJSON 几何图形</a> 相交的几何图形。<a href=\"https://docs.mongodb.com/manual/core/2dsphere/\">二维球形</a>支持 <a href=\"https://docs.mongodb.com/manual/reference/operator/query/geoIntersects/#op._S_geoIntersects\">$geoIntersects</a>。<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/near/#op._S_near\">$near</a> 返回跟一个点邻近的空间对象。需要一个空间索引。<a href=\"https://docs.mongodb.com/manual/core/2dsphere/\">二维球形</a>和<a href=\"https://docs.mongodb.com/manual/core/2d/\">二维索引</a>支持 <a href=\"https://docs.mongodb.com/manual/reference/operator/query/near/#op._S_near\">$near</a>。<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/nearSphere/#op._S_nearSphere\">$nearSphere</a> 返回球形上跟一个点邻近的空间对象。<a href=\"https://docs.mongodb.com/manual/core/2dsphere/\">二维球形</a>和<a href=\"https://docs.mongodb.com/manual/core/2d/\">二维索引</a>支持 <a href=\"https://docs.mongodb.com/manual/reference/operator/query/nearSphere/#op._S_nearSphere\">$nearSphere</a>。</p>\n<h4 id=\"数组操作符\"><a href=\"#数组操作符\" class=\"headerlink\" title=\"数组操作符\"></a>数组操作符</h4><p><a href=\"https://docs.mongodb.com/manual/reference/operator/query/all/#op._S_all\">$all</a> 匹配查询中指定的所有元素的数组。<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/elemMatch/#op._S_elemMatch\">$elemMatch</a> 选择那些属性匹配所有 <a href=\"https://docs.mongodb.com/manual/reference/operator/query/elemMatch/#op._S_elemMatch\">$elemMatch</a> 条件指定的全部属性的数组。<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/size/#op._S_size\">$size</a> 选择符合指定属性个数的数组。</p>\n<h4 id=\"位运算操作符\"><a href=\"#位运算操作符\" class=\"headerlink\" title=\"位运算操作符\"></a>位运算操作符</h4><p><a href=\"https://docs.mongodb.com/manual/reference/operator/query/bitsAllSet/#op._S_bitsAllSet\">$bitsAllSet</a> 匹配的数字或二进制值，其中这组位的位置都有一个值为1。<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/bitsAnySet/#op._S_bitsAnySet\">$bitsAnySet</a> 匹配的数字或二进制值，其中这组位的任意一个位置有一个值为1。<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/bitsAllClear/#op._S_bitsAllClear\">$bitsAllClear</a> 匹配的数字或二进制值，其中这组位的位置都有一个值为0。<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/bitsAnyClear/#op._S_bitsAnyClear\">$bitsAnyClear</a> 匹配的数字或二进制值，其中这组位的任意一个位置有一个值为0。</p>\n<h4 id=\"注释操作符\"><a href=\"#注释操作符\" class=\"headerlink\" title=\"注释操作符\"></a>注释操作符</h4><p><a href=\"https://docs.mongodb.com/manual/reference/operator/query/comment/#op._S_comment\">$comment</a> 给查询断言添加注释。</p>\n<h3 id=\"投影操作符\"><a href=\"#投影操作符\" class=\"headerlink\" title=\"投影操作符\"></a>投影操作符</h3><p><a href=\"https://docs.mongodb.com/manual/reference/operator/projection/positional/#proj._S_\">$</a> 展现第一个元素匹配查询条件的数组。<br><a href=\"https://docs.mongodb.com/manual/reference/operator/projection/elemMatch/#proj._S_elemMatch\">$elemMatch</a> 展现第一个元素匹配指定的 <a href=\"https://docs.mongodb.com/manual/reference/operator/projection/elemMatch/#proj._S_elemMatch\">$elemMatch</a> 条件的数组。<br><a href=\"https://docs.mongodb.com/manual/reference/operator/projection/meta/#proj._S_meta\">$meta</a> 展现 <a href=\"https://docs.mongodb.com/manual/reference/operator/query/text/#op._S_text\">$text</a> 操作中赋值给文档的分数。<br><a href=\"https://docs.mongodb.com/manual/reference/operator/projection/slice/#proj._S_slice\">$slice</a> 限制从一个数组展示的元素的个数。支持 skip 和 limit。</p>\n","site":{"data":{}},"excerpt":"<h3 id=\"查询选择器\"><a href=\"#查询选择器\" class=\"headerlink\" title=\"查询选择器\"></a>查询选择器</h3><h4 id=\"比较操作符\"><a href=\"#比较操作符\" class=\"headerlink\" title=\"比较操作符\"></a>比较操作符</h4><p>想了解不同 BSON 类型值的比较，参见 <a href=\"https://docs.mongodb.org/manual/reference/bson-types/#bson-types-comparison-order\">BSON比较顺序详细说明</a>。</p>\n<p><a href=\"https://docs.mongodb.com/manual/reference/operator/query/eq/#op._S_eq\">$eq</a> 匹配跟指定值相等的值<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/gt/#op._S_gt\">$gt</a> 匹配比指定值大的值<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/gte/#op._S_gte\">$gte</a> 匹配比指定值大或者相等的值<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/lt/#op._S_lt\">$lt</a> 匹配比指定值小的值<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/lte/#op._S_lte\">$lte</a> 匹配比指定值小或者相等的值<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/ne/#op._S_ne\">$ne</a> 匹配跟指定值不相等的值<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/in/#op._S_in\">$in</a> 匹配在指定数组中的任意值<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/nin/#op._S_nin\">$nin</a> 匹配不在指定数组中的值</p>","more":"<h4 id=\"逻辑操作符\"><a href=\"#逻辑操作符\" class=\"headerlink\" title=\"逻辑操作符\"></a>逻辑操作符</h4><p><a href=\"https://docs.mongodb.com/manual/reference/operator/query/or/#op._S_or\">$or</a> 用逻辑 <strong>OR</strong> 操作符连接查询子句，返回所有匹配任一个条件子句的文档。<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/and/#op._S_and\">$and</a> 用逻辑 <strong>AND</strong> 操作符连接查询子句，返回所有同时匹配两个条件子句的文档。<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/not/#op._S_not\">$not</a> 反转查询条件的效果，返回<em>不匹配</em>查询条件的文档。<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/nor/#op._S_nor\">$nor</a> 用逻辑 <strong>NOR</strong> 操作符连接查询子句，返回两个条件都匹配失败的所有文档。</p>\n<h4 id=\"元素操作符\"><a href=\"#元素操作符\" class=\"headerlink\" title=\"元素操作符\"></a>元素操作符</h4><p><a href=\"https://docs.mongodb.com/manual/reference/operator/query/exists/#op._S_exists\">$exists</a> 匹配有指定属性的文档。<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/type/#op._S_type\">$type</a> 选择属性是指定类型的文档。</p>\n<h4 id=\"评估操作符\"><a href=\"#评估操作符\" class=\"headerlink\" title=\"评估操作符\"></a>评估操作符</h4><p><a href=\"https://docs.mongodb.com/manual/reference/operator/query/mod/#op._S_mod\">$mod</a> 对一个属性的值执行模数运算，选择有指定结果的文档。<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/regex/#op._S_regex\">$regex</a> 选择值匹配指定规则表达式的文档。<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/text/#op._S_text\">$text</a> 执行文本搜索。<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/where/#op._S_where\">$where</a> 匹配满足一个 JavaScript 表达式的文档。</p>\n<h4 id=\"空间操作符\"><a href=\"#空间操作符\" class=\"headerlink\" title=\"空间操作符\"></a>空间操作符</h4><p><a href=\"https://docs.mongodb.com/manual/reference/operator/query/geoWithin/#op._S_geoWithin\">$geoWithin</a> 选择包围 <a href=\"https://docs.mongodb.com/manual/reference/geojson/#geospatial-indexes-store-geojson\">GeoJSON  几何图形</a> 的几何图形。<a href=\"https://docs.mongodb.com/manual/core/2dsphere/\">二维球形</a>和<a href=\"https://docs.mongodb.com/manual/core/2d/\">二维索引</a>支持 <a href=\"https://docs.mongodb.com/manual/reference/operator/query/geoWithin/#op._S_geoWithin\">$geoWithin</a>。<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/geoIntersects/#op._S_geoIntersects\">$geoIntersects</a> 选择与一个 <a href=\"https://docs.mongodb.com/manual/reference/geojson/#geospatial-indexes-store-geojson\">GeoJSON 几何图形</a> 相交的几何图形。<a href=\"https://docs.mongodb.com/manual/core/2dsphere/\">二维球形</a>支持 <a href=\"https://docs.mongodb.com/manual/reference/operator/query/geoIntersects/#op._S_geoIntersects\">$geoIntersects</a>。<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/near/#op._S_near\">$near</a> 返回跟一个点邻近的空间对象。需要一个空间索引。<a href=\"https://docs.mongodb.com/manual/core/2dsphere/\">二维球形</a>和<a href=\"https://docs.mongodb.com/manual/core/2d/\">二维索引</a>支持 <a href=\"https://docs.mongodb.com/manual/reference/operator/query/near/#op._S_near\">$near</a>。<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/nearSphere/#op._S_nearSphere\">$nearSphere</a> 返回球形上跟一个点邻近的空间对象。<a href=\"https://docs.mongodb.com/manual/core/2dsphere/\">二维球形</a>和<a href=\"https://docs.mongodb.com/manual/core/2d/\">二维索引</a>支持 <a href=\"https://docs.mongodb.com/manual/reference/operator/query/nearSphere/#op._S_nearSphere\">$nearSphere</a>。</p>\n<h4 id=\"数组操作符\"><a href=\"#数组操作符\" class=\"headerlink\" title=\"数组操作符\"></a>数组操作符</h4><p><a href=\"https://docs.mongodb.com/manual/reference/operator/query/all/#op._S_all\">$all</a> 匹配查询中指定的所有元素的数组。<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/elemMatch/#op._S_elemMatch\">$elemMatch</a> 选择那些属性匹配所有 <a href=\"https://docs.mongodb.com/manual/reference/operator/query/elemMatch/#op._S_elemMatch\">$elemMatch</a> 条件指定的全部属性的数组。<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/size/#op._S_size\">$size</a> 选择符合指定属性个数的数组。</p>\n<h4 id=\"位运算操作符\"><a href=\"#位运算操作符\" class=\"headerlink\" title=\"位运算操作符\"></a>位运算操作符</h4><p><a href=\"https://docs.mongodb.com/manual/reference/operator/query/bitsAllSet/#op._S_bitsAllSet\">$bitsAllSet</a> 匹配的数字或二进制值，其中这组位的位置都有一个值为1。<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/bitsAnySet/#op._S_bitsAnySet\">$bitsAnySet</a> 匹配的数字或二进制值，其中这组位的任意一个位置有一个值为1。<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/bitsAllClear/#op._S_bitsAllClear\">$bitsAllClear</a> 匹配的数字或二进制值，其中这组位的位置都有一个值为0。<br><a href=\"https://docs.mongodb.com/manual/reference/operator/query/bitsAnyClear/#op._S_bitsAnyClear\">$bitsAnyClear</a> 匹配的数字或二进制值，其中这组位的任意一个位置有一个值为0。</p>\n<h4 id=\"注释操作符\"><a href=\"#注释操作符\" class=\"headerlink\" title=\"注释操作符\"></a>注释操作符</h4><p><a href=\"https://docs.mongodb.com/manual/reference/operator/query/comment/#op._S_comment\">$comment</a> 给查询断言添加注释。</p>\n<h3 id=\"投影操作符\"><a href=\"#投影操作符\" class=\"headerlink\" title=\"投影操作符\"></a>投影操作符</h3><p><a href=\"https://docs.mongodb.com/manual/reference/operator/projection/positional/#proj._S_\">$</a> 展现第一个元素匹配查询条件的数组。<br><a href=\"https://docs.mongodb.com/manual/reference/operator/projection/elemMatch/#proj._S_elemMatch\">$elemMatch</a> 展现第一个元素匹配指定的 <a href=\"https://docs.mongodb.com/manual/reference/operator/projection/elemMatch/#proj._S_elemMatch\">$elemMatch</a> 条件的数组。<br><a href=\"https://docs.mongodb.com/manual/reference/operator/projection/meta/#proj._S_meta\">$meta</a> 展现 <a href=\"https://docs.mongodb.com/manual/reference/operator/query/text/#op._S_text\">$text</a> 操作中赋值给文档的分数。<br><a href=\"https://docs.mongodb.com/manual/reference/operator/projection/slice/#proj._S_slice\">$slice</a> 限制从一个数组展示的元素的个数。支持 skip 和 limit。</p>"},{"title":"MongoDB：listDatabases failed : not master and slaveOk=false","date":"2018-09-27T12:14:24.000Z","_content":"\n#### 异常描述\n\n如果在 MongoDB 的 SECONDARY 上查询数据时会报如下错误信息：\n\n    > show databases;\n    2018-09-20T17:40:55.377+0800 E QUERY [thread1] Error: listDatabases failed:{ \"ok\" : 0, \"errmsg\" : \"not master and slaveOk=false\", \"code\" : 13435 } :\n    _getErrorWithCode@src/mongo/shell/utils.js:25:13\n    Mongo.prototype.getDBs@src/mongo/shell/mongo.js:62:1\n    shellHelper.show@src/mongo/shell/utils.js:781:19\n    shellHelper@src/mongo/shell/utils.js:671:15\n    @(shellhelp2):1:1\n\n#### Mongo Shell 设置\n\n如果在 Mongo Shell 中可以通过下面的命令允许从 SECONDARY 上查询数据：\n\n    > rs.slaveOk();\n    \n#### JDBC 设置\n\n示例代码如下：\n\n    MongoClientOptions opts = (new MongoClientOptions.Builder().readPreference(ReadPreference.secondary())).build();\n\tMongoClient mongoClient = new MongoClient(${host}, opts);\n","source":"_posts/MongoDB：listDatabases-failed-not-master-and-slaveOk-false.md","raw":"title: 'MongoDB：listDatabases failed : not master and slaveOk=false'\ntags:\n  - MongoDB\ncategories:\n  - 数据库\n  - MongoDB\ndate: 2018-09-27 20:14:24\n---\n\n#### 异常描述\n\n如果在 MongoDB 的 SECONDARY 上查询数据时会报如下错误信息：\n\n    > show databases;\n    2018-09-20T17:40:55.377+0800 E QUERY [thread1] Error: listDatabases failed:{ \"ok\" : 0, \"errmsg\" : \"not master and slaveOk=false\", \"code\" : 13435 } :\n    _getErrorWithCode@src/mongo/shell/utils.js:25:13\n    Mongo.prototype.getDBs@src/mongo/shell/mongo.js:62:1\n    shellHelper.show@src/mongo/shell/utils.js:781:19\n    shellHelper@src/mongo/shell/utils.js:671:15\n    @(shellhelp2):1:1\n\n#### Mongo Shell 设置\n\n如果在 Mongo Shell 中可以通过下面的命令允许从 SECONDARY 上查询数据：\n\n    > rs.slaveOk();\n    \n#### JDBC 设置\n\n示例代码如下：\n\n    MongoClientOptions opts = (new MongoClientOptions.Builder().readPreference(ReadPreference.secondary())).build();\n\tMongoClient mongoClient = new MongoClient(${host}, opts);\n","slug":"MongoDB：listDatabases-failed-not-master-and-slaveOk-false","published":1,"updated":"2021-07-19T16:28:00.284Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphot0082itd39xw9ddg3","content":"<h4 id=\"异常描述\"><a href=\"#异常描述\" class=\"headerlink\" title=\"异常描述\"></a>异常描述</h4><p>如果在 MongoDB 的 SECONDARY 上查询数据时会报如下错误信息：</p>\n<pre><code>&gt; show databases;\n2018-09-20T17:40:55.377+0800 E QUERY [thread1] Error: listDatabases failed:&#123; &quot;ok&quot; : 0, &quot;errmsg&quot; : &quot;not master and slaveOk=false&quot;, &quot;code&quot; : 13435 &#125; :\n_getErrorWithCode@src/mongo/shell/utils.js:25:13\nMongo.prototype.getDBs@src/mongo/shell/mongo.js:62:1\nshellHelper.show@src/mongo/shell/utils.js:781:19\nshellHelper@src/mongo/shell/utils.js:671:15\n@(shellhelp2):1:1\n</code></pre>\n<h4 id=\"Mongo-Shell-设置\"><a href=\"#Mongo-Shell-设置\" class=\"headerlink\" title=\"Mongo Shell 设置\"></a>Mongo Shell 设置</h4><p>如果在 Mongo Shell 中可以通过下面的命令允许从 SECONDARY 上查询数据：</p>\n<pre><code>&gt; rs.slaveOk();\n</code></pre>\n<h4 id=\"JDBC-设置\"><a href=\"#JDBC-设置\" class=\"headerlink\" title=\"JDBC 设置\"></a>JDBC 设置</h4><p>示例代码如下：</p>\n<pre><code>MongoClientOptions opts = (new MongoClientOptions.Builder().readPreference(ReadPreference.secondary())).build();\nMongoClient mongoClient = new MongoClient($&#123;host&#125;, opts);\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"异常描述\"><a href=\"#异常描述\" class=\"headerlink\" title=\"异常描述\"></a>异常描述</h4><p>如果在 MongoDB 的 SECONDARY 上查询数据时会报如下错误信息：</p>\n<pre><code>&gt; show databases;\n2018-09-20T17:40:55.377+0800 E QUERY [thread1] Error: listDatabases failed:&#123; &quot;ok&quot; : 0, &quot;errmsg&quot; : &quot;not master and slaveOk=false&quot;, &quot;code&quot; : 13435 &#125; :\n_getErrorWithCode@src/mongo/shell/utils.js:25:13\nMongo.prototype.getDBs@src/mongo/shell/mongo.js:62:1\nshellHelper.show@src/mongo/shell/utils.js:781:19\nshellHelper@src/mongo/shell/utils.js:671:15\n@(shellhelp2):1:1\n</code></pre>\n<h4 id=\"Mongo-Shell-设置\"><a href=\"#Mongo-Shell-设置\" class=\"headerlink\" title=\"Mongo Shell 设置\"></a>Mongo Shell 设置</h4><p>如果在 Mongo Shell 中可以通过下面的命令允许从 SECONDARY 上查询数据：</p>\n<pre><code>&gt; rs.slaveOk();\n</code></pre>\n<h4 id=\"JDBC-设置\"><a href=\"#JDBC-设置\" class=\"headerlink\" title=\"JDBC 设置\"></a>JDBC 设置</h4><p>示例代码如下：</p>\n<pre><code>MongoClientOptions opts = (new MongoClientOptions.Builder().readPreference(ReadPreference.secondary())).build();\nMongoClient mongoClient = new MongoClient($&#123;host&#125;, opts);\n</code></pre>\n"},{"title":"MySQL JDBC 连接异常：javax.net.ssl.SSLException: closing inbound before receiving peer's close_notify","date":"2020-11-27T01:40:17.000Z","_content":"\n异常信息如下：\n\n\tThu Nov 26 23:54:22 CST 2020 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.\n\tThu Nov 26 23:54:23 CST 2020 WARN: Caught while disconnecting...\n\t\n\tEXCEPTION STACK TRACE:\n\t\n\t\n\t\n\t** BEGIN NESTED EXCEPTION ** \n\t\n\tjavax.net.ssl.SSLException\n\tMESSAGE: closing inbound before receiving peer's close_notify\n\t\n\tSTACKTRACE:\n\t\n\tjavax.net.ssl.SSLException: closing inbound before receiving peer's close_notify\n\t\tat sun.security.ssl.Alert.createSSLException(Alert.java:133)\n\t\tat sun.security.ssl.Alert.createSSLException(Alert.java:117)\n\t\tat sun.security.ssl.TransportContext.fatal(TransportContext.java:314)\n\t\tat sun.security.ssl.TransportContext.fatal(TransportContext.java:270)\n\t\tat sun.security.ssl.TransportContext.fatal(TransportContext.java:261)\n\t\tat sun.security.ssl.SSLSocketImpl.shutdownInput(SSLSocketImpl.java:656)\n\t\tat sun.security.ssl.SSLSocketImpl.shutdownInput(SSLSocketImpl.java:635)\n\t\tat com.mysql.jdbc.MysqlIO.quit(MysqlIO.java:2246)\n\t\tat com.mysql.jdbc.ConnectionImpl.realClose(ConnectionImpl.java:4201)\n\t\tat com.mysql.jdbc.ConnectionImpl.close(ConnectionImpl.java:1472)\n\t\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:353)\n\t\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)\n\t\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\n\t\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:483)\n\t\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:296)\n\t\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\t\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\t\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\t\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\t\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)\n\t\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\n\t\tat org.datanucleus.NucleusContextHelper.createStoreManagerForProperties(NucleusContextHelper.java:133)\n\t\tat org.datanucleus.PersistenceNucleusContextImpl.initialise(PersistenceNucleusContextImpl.java:420)\n\t\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:821)\n\t\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:338)\n\t\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:217)\n\t\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\t\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\t\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\t\tat java.lang.reflect.Method.invoke(Method.java:498)\n\t\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\n\t\tat java.security.AccessController.doPrivileged(Native Method)\n\t\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\n\t\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\n\t\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\n\t\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\n\t\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:515)\n\t\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:544)\n\t\tat org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:399)\n\t\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:336)\n\t\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:297)\n\t\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76)\n\t\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n\t\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)\n\t\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)\n\t\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:599)\n\t\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:564)\n\t\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:626)\n\t\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:416)\n\t\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:78)\n\t\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:84)\n\t\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6490)\n\t\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:238)\n\t\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)\n\t\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\t\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\t\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\t\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\t\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1652)\n\t\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:80)\n\t\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:130)\n\t\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:101)\n\t\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3367)\n\t\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3406)\n\t\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3386)\n\t\tat org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:3640)\n\t\tat org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:236)\n\t\tat org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:221)\n\t\tat org.apache.hadoop.hive.ql.metadata.Hive.<init>(Hive.java:366)\n\t\tat org.apache.hadoop.hive.ql.metadata.Hive.create(Hive.java:310)\n\t\tat org.apache.hadoop.hive.ql.metadata.Hive.getInternal(Hive.java:290)\n\t\tat org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:266)\n\t\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:558)\n\t\tat org.apache.hadoop.hive.ql.session.SessionState.beginStart(SessionState.java:531)\n\t\tat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:705)\n\t\tat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:641)\n\t\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\t\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\t\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\t\tat java.lang.reflect.Method.invoke(Method.java:498)\n\t\tat org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n\t\tat org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n\t\n\t\n\t** END NESTED EXCEPTION **\n\n解决方法：在 JDBC 连接串中添加 useSSL=false 配置，如：jdbc:mysql://192.168.72.212/hive?createDatabaseIfNotExist=true&amp;characterEncoding=utf-8&amp;useSSL=false\n\n注意，XML 中的 \\& 需要改写为：\\&amp;\n\n","source":"_posts/MySQL-JDBC-连接异常：javax-net-ssl-SSLException-closing-inbound-before-receiving-peer-s-close-notify.md","raw":"title: >-\n  MySQL JDBC 连接异常：javax.net.ssl.SSLException: closing inbound before receiving\n  peer's close_notify\ndate: 2020-11-27 09:40:17\ntags:\n- MySQL\ncategories:\n- 数据库\n- MySQL\n---\n\n异常信息如下：\n\n\tThu Nov 26 23:54:22 CST 2020 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.\n\tThu Nov 26 23:54:23 CST 2020 WARN: Caught while disconnecting...\n\t\n\tEXCEPTION STACK TRACE:\n\t\n\t\n\t\n\t** BEGIN NESTED EXCEPTION ** \n\t\n\tjavax.net.ssl.SSLException\n\tMESSAGE: closing inbound before receiving peer's close_notify\n\t\n\tSTACKTRACE:\n\t\n\tjavax.net.ssl.SSLException: closing inbound before receiving peer's close_notify\n\t\tat sun.security.ssl.Alert.createSSLException(Alert.java:133)\n\t\tat sun.security.ssl.Alert.createSSLException(Alert.java:117)\n\t\tat sun.security.ssl.TransportContext.fatal(TransportContext.java:314)\n\t\tat sun.security.ssl.TransportContext.fatal(TransportContext.java:270)\n\t\tat sun.security.ssl.TransportContext.fatal(TransportContext.java:261)\n\t\tat sun.security.ssl.SSLSocketImpl.shutdownInput(SSLSocketImpl.java:656)\n\t\tat sun.security.ssl.SSLSocketImpl.shutdownInput(SSLSocketImpl.java:635)\n\t\tat com.mysql.jdbc.MysqlIO.quit(MysqlIO.java:2246)\n\t\tat com.mysql.jdbc.ConnectionImpl.realClose(ConnectionImpl.java:4201)\n\t\tat com.mysql.jdbc.ConnectionImpl.close(ConnectionImpl.java:1472)\n\t\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:353)\n\t\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)\n\t\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\n\t\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:483)\n\t\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:296)\n\t\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\t\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\t\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\t\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\t\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)\n\t\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\n\t\tat org.datanucleus.NucleusContextHelper.createStoreManagerForProperties(NucleusContextHelper.java:133)\n\t\tat org.datanucleus.PersistenceNucleusContextImpl.initialise(PersistenceNucleusContextImpl.java:420)\n\t\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:821)\n\t\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:338)\n\t\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:217)\n\t\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\t\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\t\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\t\tat java.lang.reflect.Method.invoke(Method.java:498)\n\t\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\n\t\tat java.security.AccessController.doPrivileged(Native Method)\n\t\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\n\t\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\n\t\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\n\t\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\n\t\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:515)\n\t\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:544)\n\t\tat org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:399)\n\t\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:336)\n\t\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:297)\n\t\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76)\n\t\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n\t\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)\n\t\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)\n\t\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:599)\n\t\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:564)\n\t\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:626)\n\t\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:416)\n\t\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:78)\n\t\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:84)\n\t\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6490)\n\t\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:238)\n\t\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)\n\t\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\t\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\t\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\t\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\t\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1652)\n\t\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:80)\n\t\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:130)\n\t\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:101)\n\t\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3367)\n\t\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3406)\n\t\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3386)\n\t\tat org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:3640)\n\t\tat org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:236)\n\t\tat org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:221)\n\t\tat org.apache.hadoop.hive.ql.metadata.Hive.<init>(Hive.java:366)\n\t\tat org.apache.hadoop.hive.ql.metadata.Hive.create(Hive.java:310)\n\t\tat org.apache.hadoop.hive.ql.metadata.Hive.getInternal(Hive.java:290)\n\t\tat org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:266)\n\t\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:558)\n\t\tat org.apache.hadoop.hive.ql.session.SessionState.beginStart(SessionState.java:531)\n\t\tat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:705)\n\t\tat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:641)\n\t\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\t\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\t\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\t\tat java.lang.reflect.Method.invoke(Method.java:498)\n\t\tat org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n\t\tat org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n\t\n\t\n\t** END NESTED EXCEPTION **\n\n解决方法：在 JDBC 连接串中添加 useSSL=false 配置，如：jdbc:mysql://192.168.72.212/hive?createDatabaseIfNotExist=true&amp;characterEncoding=utf-8&amp;useSSL=false\n\n注意，XML 中的 \\& 需要改写为：\\&amp;\n\n","slug":"MySQL-JDBC-连接异常：javax-net-ssl-SSLException-closing-inbound-before-receiving-peer-s-close-notify","published":1,"updated":"2021-07-19T16:28:00.032Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphou0086itd3cxqo7vpr","content":"<p>异常信息如下：</p>\n<pre><code>Thu Nov 26 23:54:22 CST 2020 WARN: Establishing SSL connection without server&#39;s identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn&#39;t set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to &#39;false&#39;. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.\nThu Nov 26 23:54:23 CST 2020 WARN: Caught while disconnecting...\n\nEXCEPTION STACK TRACE:\n\n\n\n** BEGIN NESTED EXCEPTION ** \n\njavax.net.ssl.SSLException\nMESSAGE: closing inbound before receiving peer&#39;s close_notify\n\nSTACKTRACE:\n\njavax.net.ssl.SSLException: closing inbound before receiving peer&#39;s close_notify\n    at sun.security.ssl.Alert.createSSLException(Alert.java:133)\n    at sun.security.ssl.Alert.createSSLException(Alert.java:117)\n    at sun.security.ssl.TransportContext.fatal(TransportContext.java:314)\n    at sun.security.ssl.TransportContext.fatal(TransportContext.java:270)\n    at sun.security.ssl.TransportContext.fatal(TransportContext.java:261)\n    at sun.security.ssl.SSLSocketImpl.shutdownInput(SSLSocketImpl.java:656)\n    at sun.security.ssl.SSLSocketImpl.shutdownInput(SSLSocketImpl.java:635)\n    at com.mysql.jdbc.MysqlIO.quit(MysqlIO.java:2246)\n    at com.mysql.jdbc.ConnectionImpl.realClose(ConnectionImpl.java:4201)\n    at com.mysql.jdbc.ConnectionImpl.close(ConnectionImpl.java:1472)\n    at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:353)\n    at com.jolbox.bonecp.BoneCP.&lt;init&gt;(BoneCP.java:416)\n    at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\n    at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:483)\n    at org.datanucleus.store.rdbms.RDBMSStoreManager.&lt;init&gt;(RDBMSStoreManager.java:296)\n    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n    at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)\n    at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\n    at org.datanucleus.NucleusContextHelper.createStoreManagerForProperties(NucleusContextHelper.java:133)\n    at org.datanucleus.PersistenceNucleusContextImpl.initialise(PersistenceNucleusContextImpl.java:420)\n    at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:821)\n    at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:338)\n    at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:217)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\n    at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\n    at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\n    at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\n    at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:515)\n    at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:544)\n    at org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:399)\n    at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:336)\n    at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:297)\n    at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76)\n    at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n    at org.apache.hadoop.hive.metastore.RawStoreProxy.&lt;init&gt;(RawStoreProxy.java:58)\n    at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)\n    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:599)\n    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:564)\n    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:626)\n    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:416)\n    at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:78)\n    at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:84)\n    at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6490)\n    at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:238)\n    at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:70)\n    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n    at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1652)\n    at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:80)\n    at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:130)\n    at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:101)\n    at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3367)\n    at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3406)\n    at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3386)\n    at org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:3640)\n    at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:236)\n    at org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:221)\n    at org.apache.hadoop.hive.ql.metadata.Hive.&lt;init&gt;(Hive.java:366)\n    at org.apache.hadoop.hive.ql.metadata.Hive.create(Hive.java:310)\n    at org.apache.hadoop.hive.ql.metadata.Hive.getInternal(Hive.java:290)\n    at org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:266)\n    at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:558)\n    at org.apache.hadoop.hive.ql.session.SessionState.beginStart(SessionState.java:531)\n    at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:705)\n    at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:641)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n    at org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n\n\n** END NESTED EXCEPTION **\n</code></pre>\n<p>解决方法：在 JDBC 连接串中添加 useSSL=false 配置，如：jdbc:mysql://192.168.72.212/hive?createDatabaseIfNotExist=true&amp;characterEncoding=utf-8&amp;useSSL=false</p>\n<p>注意，XML 中的 &amp; 需要改写为：&amp;amp;</p>\n","site":{"data":{}},"excerpt":"","more":"<p>异常信息如下：</p>\n<pre><code>Thu Nov 26 23:54:22 CST 2020 WARN: Establishing SSL connection without server&#39;s identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn&#39;t set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to &#39;false&#39;. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.\nThu Nov 26 23:54:23 CST 2020 WARN: Caught while disconnecting...\n\nEXCEPTION STACK TRACE:\n\n\n\n** BEGIN NESTED EXCEPTION ** \n\njavax.net.ssl.SSLException\nMESSAGE: closing inbound before receiving peer&#39;s close_notify\n\nSTACKTRACE:\n\njavax.net.ssl.SSLException: closing inbound before receiving peer&#39;s close_notify\n    at sun.security.ssl.Alert.createSSLException(Alert.java:133)\n    at sun.security.ssl.Alert.createSSLException(Alert.java:117)\n    at sun.security.ssl.TransportContext.fatal(TransportContext.java:314)\n    at sun.security.ssl.TransportContext.fatal(TransportContext.java:270)\n    at sun.security.ssl.TransportContext.fatal(TransportContext.java:261)\n    at sun.security.ssl.SSLSocketImpl.shutdownInput(SSLSocketImpl.java:656)\n    at sun.security.ssl.SSLSocketImpl.shutdownInput(SSLSocketImpl.java:635)\n    at com.mysql.jdbc.MysqlIO.quit(MysqlIO.java:2246)\n    at com.mysql.jdbc.ConnectionImpl.realClose(ConnectionImpl.java:4201)\n    at com.mysql.jdbc.ConnectionImpl.close(ConnectionImpl.java:1472)\n    at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:353)\n    at com.jolbox.bonecp.BoneCP.&lt;init&gt;(BoneCP.java:416)\n    at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\n    at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:483)\n    at org.datanucleus.store.rdbms.RDBMSStoreManager.&lt;init&gt;(RDBMSStoreManager.java:296)\n    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n    at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)\n    at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\n    at org.datanucleus.NucleusContextHelper.createStoreManagerForProperties(NucleusContextHelper.java:133)\n    at org.datanucleus.PersistenceNucleusContextImpl.initialise(PersistenceNucleusContextImpl.java:420)\n    at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:821)\n    at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:338)\n    at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:217)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\n    at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\n    at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\n    at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\n    at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:515)\n    at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:544)\n    at org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:399)\n    at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:336)\n    at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:297)\n    at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76)\n    at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n    at org.apache.hadoop.hive.metastore.RawStoreProxy.&lt;init&gt;(RawStoreProxy.java:58)\n    at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)\n    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:599)\n    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:564)\n    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:626)\n    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:416)\n    at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:78)\n    at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:84)\n    at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6490)\n    at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:238)\n    at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:70)\n    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n    at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1652)\n    at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:80)\n    at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:130)\n    at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:101)\n    at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3367)\n    at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3406)\n    at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3386)\n    at org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:3640)\n    at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:236)\n    at org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:221)\n    at org.apache.hadoop.hive.ql.metadata.Hive.&lt;init&gt;(Hive.java:366)\n    at org.apache.hadoop.hive.ql.metadata.Hive.create(Hive.java:310)\n    at org.apache.hadoop.hive.ql.metadata.Hive.getInternal(Hive.java:290)\n    at org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:266)\n    at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:558)\n    at org.apache.hadoop.hive.ql.session.SessionState.beginStart(SessionState.java:531)\n    at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:705)\n    at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:641)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n    at org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n\n\n** END NESTED EXCEPTION **\n</code></pre>\n<p>解决方法：在 JDBC 连接串中添加 useSSL=false 配置，如：jdbc:mysql://192.168.72.212/hive?createDatabaseIfNotExist=true&amp;characterEncoding=utf-8&amp;useSSL=false</p>\n<p>注意，XML 中的 &amp; 需要改写为：&amp;amp;</p>\n"},{"title":"MySQL 修改 max_allowed_packet","date":"2018-10-11T17:25:22.000Z","_content":"\n\n通过 global 参数设置：\n\n    set global max_allowed_packet = 32*1024*1024;\n\n注意，下面的写法是不正确的：\n\n    set global max_allowed_packet = 32m;\n\n如果要重启也生效的话，在 my.cnf 中添加如下配置：\n\n    max_allowed_packet=32m\n","source":"_posts/MySQL-修改-max-allowed-packet.md","raw":"title: MySQL 修改 max_allowed_packet\ntags:\n  - MySQL\ncategories:\n  - 数据库\n  - MySQL\ndate: 2018-10-12 01:25:22\n---\n\n\n通过 global 参数设置：\n\n    set global max_allowed_packet = 32*1024*1024;\n\n注意，下面的写法是不正确的：\n\n    set global max_allowed_packet = 32m;\n\n如果要重启也生效的话，在 my.cnf 中添加如下配置：\n\n    max_allowed_packet=32m\n","slug":"MySQL-修改-max-allowed-packet","published":1,"updated":"2021-07-19T16:28:00.284Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphov008aitd37v3h5i6w","content":"<p>通过 global 参数设置：</p>\n<pre><code>set global max_allowed_packet = 32*1024*1024;\n</code></pre>\n<p>注意，下面的写法是不正确的：</p>\n<pre><code>set global max_allowed_packet = 32m;\n</code></pre>\n<p>如果要重启也生效的话，在 my.cnf 中添加如下配置：</p>\n<pre><code>max_allowed_packet=32m\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>通过 global 参数设置：</p>\n<pre><code>set global max_allowed_packet = 32*1024*1024;\n</code></pre>\n<p>注意，下面的写法是不正确的：</p>\n<pre><code>set global max_allowed_packet = 32m;\n</code></pre>\n<p>如果要重启也生效的话，在 my.cnf 中添加如下配置：</p>\n<pre><code>max_allowed_packet=32m\n</code></pre>\n"},{"title":"MySQL 表分区修改操作不支持 IF [NOT] EXISTS","date":"2016-05-19T14:38:42.000Z","_content":"\n### MySQL 删除表分区不支持 IF [NOT] EXISTS\n\n最近的工作中需要用到 MySQL 的分区表。表根据每天的日期分区，如：20160518、20160519 等。需要支持数据重新写入分区，即如果对应的分区下已经存在数据，则先清理再写入。IF [NOT] EXISTS 是一种判定表是否存在简便方式，非常适合这种场景。但查看了 MySQL 的官方文档，PARTITION 的添加、删除操作不支持该操作。\n\n<!-- more -->\n\n[ALTER TABLE Partition Operations](http://dev.mysql.com/doc/refman/5.7/en/alter-table-partition-operations.html)\n\n![ADD PARTITION and DROP PARTITION do not currently support IF [NOT] EXISTS.](/uploads/20160518/mysql-alter-partition.png)\n\n### 解决方法\n\n通过 MySQL 的 INFORMATION_SCHEMA 库下的 [PARTITIONS](http://dev.mysql.com/doc/refman/5.7/en/partitions-table.html) 表查询表分区信息确定分区是否存在。该表的使用见下面的例子：\n\n    mysql> CREATE TABLE th (\n        ->     c1 INT,\n        ->     c2 VARCHAR(20)\n        -> )\n        -> PARTITION BY HASH(c1)\n        -> PARTITIONS 2;\n    Query OK, 0 rows affected (0.00 sec)\n\n可以用下面的查询表 th 的分区情况：\n\n    mysql> SELECT TABLE_NAME,PARTITION_NAME,TABLE_ROWS,AVG_ROW_LENGTH,DATA_LENGTH\n         >   FROM INFORMATION_SCHEMA.PARTITIONS\n         >   WHERE TABLE_SCHEMA = 'p' AND TABLE_NAME ='th';\n\n结果如下：![mysql-information-schema-partitions](/uploads/20160518/mysql-information-schema-partitions.png)\n","source":"_posts/MySQL-表分区修改操作不支持-IF-NOT-EXISTS.md","raw":"title: 'MySQL 表分区修改操作不支持 IF [NOT] EXISTS'\ntags:\n  - MySQL\ncategories:\n  - 数据库\n  - MySQL\ndate: 2016-05-19 22:38:42\n---\n\n### MySQL 删除表分区不支持 IF [NOT] EXISTS\n\n最近的工作中需要用到 MySQL 的分区表。表根据每天的日期分区，如：20160518、20160519 等。需要支持数据重新写入分区，即如果对应的分区下已经存在数据，则先清理再写入。IF [NOT] EXISTS 是一种判定表是否存在简便方式，非常适合这种场景。但查看了 MySQL 的官方文档，PARTITION 的添加、删除操作不支持该操作。\n\n<!-- more -->\n\n[ALTER TABLE Partition Operations](http://dev.mysql.com/doc/refman/5.7/en/alter-table-partition-operations.html)\n\n![ADD PARTITION and DROP PARTITION do not currently support IF [NOT] EXISTS.](/uploads/20160518/mysql-alter-partition.png)\n\n### 解决方法\n\n通过 MySQL 的 INFORMATION_SCHEMA 库下的 [PARTITIONS](http://dev.mysql.com/doc/refman/5.7/en/partitions-table.html) 表查询表分区信息确定分区是否存在。该表的使用见下面的例子：\n\n    mysql> CREATE TABLE th (\n        ->     c1 INT,\n        ->     c2 VARCHAR(20)\n        -> )\n        -> PARTITION BY HASH(c1)\n        -> PARTITIONS 2;\n    Query OK, 0 rows affected (0.00 sec)\n\n可以用下面的查询表 th 的分区情况：\n\n    mysql> SELECT TABLE_NAME,PARTITION_NAME,TABLE_ROWS,AVG_ROW_LENGTH,DATA_LENGTH\n         >   FROM INFORMATION_SCHEMA.PARTITIONS\n         >   WHERE TABLE_SCHEMA = 'p' AND TABLE_NAME ='th';\n\n结果如下：![mysql-information-schema-partitions](/uploads/20160518/mysql-information-schema-partitions.png)\n","slug":"MySQL-表分区修改操作不支持-IF-NOT-EXISTS","published":1,"updated":"2021-07-19T16:28:00.284Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphox008eitd3fxzv0xhk","content":"<h3 id=\"MySQL-删除表分区不支持-IF-NOT-EXISTS\"><a href=\"#MySQL-删除表分区不支持-IF-NOT-EXISTS\" class=\"headerlink\" title=\"MySQL 删除表分区不支持 IF [NOT] EXISTS\"></a>MySQL 删除表分区不支持 IF [NOT] EXISTS</h3><p>最近的工作中需要用到 MySQL 的分区表。表根据每天的日期分区，如：20160518、20160519 等。需要支持数据重新写入分区，即如果对应的分区下已经存在数据，则先清理再写入。IF [NOT] EXISTS 是一种判定表是否存在简便方式，非常适合这种场景。但查看了 MySQL 的官方文档，PARTITION 的添加、删除操作不支持该操作。</p>\n<span id=\"more\"></span>\n\n<p><a href=\"http://dev.mysql.com/doc/refman/5.7/en/alter-table-partition-operations.html\">ALTER TABLE Partition Operations</a></p>\n<p><img src=\"/uploads/20160518/mysql-alter-partition.png\" alt=\"ADD PARTITION and DROP PARTITION do not currently support IF [NOT] EXISTS.\"></p>\n<h3 id=\"解决方法\"><a href=\"#解决方法\" class=\"headerlink\" title=\"解决方法\"></a>解决方法</h3><p>通过 MySQL 的 INFORMATION_SCHEMA 库下的 <a href=\"http://dev.mysql.com/doc/refman/5.7/en/partitions-table.html\">PARTITIONS</a> 表查询表分区信息确定分区是否存在。该表的使用见下面的例子：</p>\n<pre><code>mysql&gt; CREATE TABLE th (\n    -&gt;     c1 INT,\n    -&gt;     c2 VARCHAR(20)\n    -&gt; )\n    -&gt; PARTITION BY HASH(c1)\n    -&gt; PARTITIONS 2;\nQuery OK, 0 rows affected (0.00 sec)\n</code></pre>\n<p>可以用下面的查询表 th 的分区情况：</p>\n<pre><code>mysql&gt; SELECT TABLE_NAME,PARTITION_NAME,TABLE_ROWS,AVG_ROW_LENGTH,DATA_LENGTH\n     &gt;   FROM INFORMATION_SCHEMA.PARTITIONS\n     &gt;   WHERE TABLE_SCHEMA = &#39;p&#39; AND TABLE_NAME =&#39;th&#39;;\n</code></pre>\n<p>结果如下：<img src=\"/uploads/20160518/mysql-information-schema-partitions.png\" alt=\"mysql-information-schema-partitions\"></p>\n","site":{"data":{}},"excerpt":"<h3 id=\"MySQL-删除表分区不支持-IF-NOT-EXISTS\"><a href=\"#MySQL-删除表分区不支持-IF-NOT-EXISTS\" class=\"headerlink\" title=\"MySQL 删除表分区不支持 IF [NOT] EXISTS\"></a>MySQL 删除表分区不支持 IF [NOT] EXISTS</h3><p>最近的工作中需要用到 MySQL 的分区表。表根据每天的日期分区，如：20160518、20160519 等。需要支持数据重新写入分区，即如果对应的分区下已经存在数据，则先清理再写入。IF [NOT] EXISTS 是一种判定表是否存在简便方式，非常适合这种场景。但查看了 MySQL 的官方文档，PARTITION 的添加、删除操作不支持该操作。</p>","more":"<p><a href=\"http://dev.mysql.com/doc/refman/5.7/en/alter-table-partition-operations.html\">ALTER TABLE Partition Operations</a></p>\n<p><img src=\"/uploads/20160518/mysql-alter-partition.png\" alt=\"ADD PARTITION and DROP PARTITION do not currently support IF [NOT] EXISTS.\"></p>\n<h3 id=\"解决方法\"><a href=\"#解决方法\" class=\"headerlink\" title=\"解决方法\"></a>解决方法</h3><p>通过 MySQL 的 INFORMATION_SCHEMA 库下的 <a href=\"http://dev.mysql.com/doc/refman/5.7/en/partitions-table.html\">PARTITIONS</a> 表查询表分区信息确定分区是否存在。该表的使用见下面的例子：</p>\n<pre><code>mysql&gt; CREATE TABLE th (\n    -&gt;     c1 INT,\n    -&gt;     c2 VARCHAR(20)\n    -&gt; )\n    -&gt; PARTITION BY HASH(c1)\n    -&gt; PARTITIONS 2;\nQuery OK, 0 rows affected (0.00 sec)\n</code></pre>\n<p>可以用下面的查询表 th 的分区情况：</p>\n<pre><code>mysql&gt; SELECT TABLE_NAME,PARTITION_NAME,TABLE_ROWS,AVG_ROW_LENGTH,DATA_LENGTH\n     &gt;   FROM INFORMATION_SCHEMA.PARTITIONS\n     &gt;   WHERE TABLE_SCHEMA = &#39;p&#39; AND TABLE_NAME =&#39;th&#39;;\n</code></pre>\n<p>结果如下：<img src=\"/uploads/20160518/mysql-information-schema-partitions.png\" alt=\"mysql-information-schema-partitions\"></p>"},{"title":"Nginx 配置支持 HTTPS 代理","date":"2019-03-12T07:39:01.000Z","_content":"\n本文描述的是 Nginx HTTPS 反向代理的情况（即后端服务是 HTTP 的）。\n\n<!-- more -->\n\n#### 使用 openssl 配置 ssl 证书\n\n##### 生成服务器端的私钥（key 文件）：\n\n    # openssl genrsa -des3 -out server.key 2048\n    Generating RSA private key, 2048 bit long modulus\n    .............+++\n    ..........+++\n    e is 65537 (0x10001)\n    Enter pass phrase for server.key:\n\n输入两次密码后 key 文件生成完毕。\n\n##### 生成 CSR（Certificate Signing Request）文件：\n\n    # openssl req -new -key server.key -out server.crs\n\n根据提示输入需要的信息后 CSR 文件生成完毕。\n\n##### 生成自签名的 CA 文件：\n\n    # openssl x509 -req -days 3650 -in server.crs -signkey server.key -out ca.crt\n    Signature ok\n    subject=/C=cn/ST=beijing/L=beijing/O=test/OU=test/CN=test/emailAddress=test@test.com\n    Getting Private key\n    Enter pass phrase for server.key:\n\n输入 key 文件的密码后 CA 文件生成完毕。\n\n#### Nginx 配置\n\n    server {\n        listen 443;\n        ssl on;\n        ssl_certificate ca.crt;\n        ssl_certificate_key server.key;\n\n        location / {\n            proxy_pass http://192.168.36.144:11000;\n         }\n    }\n\n重新加载 Nginx 配置文件后，发现 Nginx 代理的 443 端口未正常启动。查看 Nginx error 日志发现以下异常信息：\n\n    2019/03/12 16:49:26 [emerg] 85175#0: SSL_CTX_use_PrivateKey_file(\"/etc/nginx/server.key\") failed (SSL: error:0906406D:PEM routines:PEM_def_callback:problems getting password error:0906A068:PEM routines:PEM_do_header:bad password read error:140B0009:SSL routines:SSL_CTX_use_PrivateKey_file:PEM lib)\n\n通过以下方式解决：\n\n    # mv server.key server.key.org\n    # openssl rsa -in server.key.org -out server.key\n\n重新加载 Nginx 配置文件。此时也不需要再输入 key 文件的密码了。\n","source":"_posts/Nginx-配置支持-HTTPS-代理.md","raw":"title: Nginx 配置支持 HTTPS 代理\ndate: 2019-03-12 15:39:01\ntags:\n- Nginx\ncategories:\n- 开发工具\n- Nginx\n---\n\n本文描述的是 Nginx HTTPS 反向代理的情况（即后端服务是 HTTP 的）。\n\n<!-- more -->\n\n#### 使用 openssl 配置 ssl 证书\n\n##### 生成服务器端的私钥（key 文件）：\n\n    # openssl genrsa -des3 -out server.key 2048\n    Generating RSA private key, 2048 bit long modulus\n    .............+++\n    ..........+++\n    e is 65537 (0x10001)\n    Enter pass phrase for server.key:\n\n输入两次密码后 key 文件生成完毕。\n\n##### 生成 CSR（Certificate Signing Request）文件：\n\n    # openssl req -new -key server.key -out server.crs\n\n根据提示输入需要的信息后 CSR 文件生成完毕。\n\n##### 生成自签名的 CA 文件：\n\n    # openssl x509 -req -days 3650 -in server.crs -signkey server.key -out ca.crt\n    Signature ok\n    subject=/C=cn/ST=beijing/L=beijing/O=test/OU=test/CN=test/emailAddress=test@test.com\n    Getting Private key\n    Enter pass phrase for server.key:\n\n输入 key 文件的密码后 CA 文件生成完毕。\n\n#### Nginx 配置\n\n    server {\n        listen 443;\n        ssl on;\n        ssl_certificate ca.crt;\n        ssl_certificate_key server.key;\n\n        location / {\n            proxy_pass http://192.168.36.144:11000;\n         }\n    }\n\n重新加载 Nginx 配置文件后，发现 Nginx 代理的 443 端口未正常启动。查看 Nginx error 日志发现以下异常信息：\n\n    2019/03/12 16:49:26 [emerg] 85175#0: SSL_CTX_use_PrivateKey_file(\"/etc/nginx/server.key\") failed (SSL: error:0906406D:PEM routines:PEM_def_callback:problems getting password error:0906A068:PEM routines:PEM_do_header:bad password read error:140B0009:SSL routines:SSL_CTX_use_PrivateKey_file:PEM lib)\n\n通过以下方式解决：\n\n    # mv server.key server.key.org\n    # openssl rsa -in server.key.org -out server.key\n\n重新加载 Nginx 配置文件。此时也不需要再输入 key 文件的密码了。\n","slug":"Nginx-配置支持-HTTPS-代理","published":1,"updated":"2021-07-19T16:28:00.284Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphoy008iitd35d2fbgj9","content":"<p>本文描述的是 Nginx HTTPS 反向代理的情况（即后端服务是 HTTP 的）。</p>\n<span id=\"more\"></span>\n\n<h4 id=\"使用-openssl-配置-ssl-证书\"><a href=\"#使用-openssl-配置-ssl-证书\" class=\"headerlink\" title=\"使用 openssl 配置 ssl 证书\"></a>使用 openssl 配置 ssl 证书</h4><h5 id=\"生成服务器端的私钥（key-文件）：\"><a href=\"#生成服务器端的私钥（key-文件）：\" class=\"headerlink\" title=\"生成服务器端的私钥（key 文件）：\"></a>生成服务器端的私钥（key 文件）：</h5><pre><code># openssl genrsa -des3 -out server.key 2048\nGenerating RSA private key, 2048 bit long modulus\n.............+++\n..........+++\ne is 65537 (0x10001)\nEnter pass phrase for server.key:\n</code></pre>\n<p>输入两次密码后 key 文件生成完毕。</p>\n<h5 id=\"生成-CSR（Certificate-Signing-Request）文件：\"><a href=\"#生成-CSR（Certificate-Signing-Request）文件：\" class=\"headerlink\" title=\"生成 CSR（Certificate Signing Request）文件：\"></a>生成 CSR（Certificate Signing Request）文件：</h5><pre><code># openssl req -new -key server.key -out server.crs\n</code></pre>\n<p>根据提示输入需要的信息后 CSR 文件生成完毕。</p>\n<h5 id=\"生成自签名的-CA-文件：\"><a href=\"#生成自签名的-CA-文件：\" class=\"headerlink\" title=\"生成自签名的 CA 文件：\"></a>生成自签名的 CA 文件：</h5><pre><code># openssl x509 -req -days 3650 -in server.crs -signkey server.key -out ca.crt\nSignature ok\nsubject=/C=cn/ST=beijing/L=beijing/O=test/OU=test/CN=test/emailAddress=test@test.com\nGetting Private key\nEnter pass phrase for server.key:\n</code></pre>\n<p>输入 key 文件的密码后 CA 文件生成完毕。</p>\n<h4 id=\"Nginx-配置\"><a href=\"#Nginx-配置\" class=\"headerlink\" title=\"Nginx 配置\"></a>Nginx 配置</h4><pre><code>server &#123;\n    listen 443;\n    ssl on;\n    ssl_certificate ca.crt;\n    ssl_certificate_key server.key;\n\n    location / &#123;\n        proxy_pass http://192.168.36.144:11000;\n     &#125;\n&#125;\n</code></pre>\n<p>重新加载 Nginx 配置文件后，发现 Nginx 代理的 443 端口未正常启动。查看 Nginx error 日志发现以下异常信息：</p>\n<pre><code>2019/03/12 16:49:26 [emerg] 85175#0: SSL_CTX_use_PrivateKey_file(&quot;/etc/nginx/server.key&quot;) failed (SSL: error:0906406D:PEM routines:PEM_def_callback:problems getting password error:0906A068:PEM routines:PEM_do_header:bad password read error:140B0009:SSL routines:SSL_CTX_use_PrivateKey_file:PEM lib)\n</code></pre>\n<p>通过以下方式解决：</p>\n<pre><code># mv server.key server.key.org\n# openssl rsa -in server.key.org -out server.key\n</code></pre>\n<p>重新加载 Nginx 配置文件。此时也不需要再输入 key 文件的密码了。</p>\n","site":{"data":{}},"excerpt":"<p>本文描述的是 Nginx HTTPS 反向代理的情况（即后端服务是 HTTP 的）。</p>","more":"<h4 id=\"使用-openssl-配置-ssl-证书\"><a href=\"#使用-openssl-配置-ssl-证书\" class=\"headerlink\" title=\"使用 openssl 配置 ssl 证书\"></a>使用 openssl 配置 ssl 证书</h4><h5 id=\"生成服务器端的私钥（key-文件）：\"><a href=\"#生成服务器端的私钥（key-文件）：\" class=\"headerlink\" title=\"生成服务器端的私钥（key 文件）：\"></a>生成服务器端的私钥（key 文件）：</h5><pre><code># openssl genrsa -des3 -out server.key 2048\nGenerating RSA private key, 2048 bit long modulus\n.............+++\n..........+++\ne is 65537 (0x10001)\nEnter pass phrase for server.key:\n</code></pre>\n<p>输入两次密码后 key 文件生成完毕。</p>\n<h5 id=\"生成-CSR（Certificate-Signing-Request）文件：\"><a href=\"#生成-CSR（Certificate-Signing-Request）文件：\" class=\"headerlink\" title=\"生成 CSR（Certificate Signing Request）文件：\"></a>生成 CSR（Certificate Signing Request）文件：</h5><pre><code># openssl req -new -key server.key -out server.crs\n</code></pre>\n<p>根据提示输入需要的信息后 CSR 文件生成完毕。</p>\n<h5 id=\"生成自签名的-CA-文件：\"><a href=\"#生成自签名的-CA-文件：\" class=\"headerlink\" title=\"生成自签名的 CA 文件：\"></a>生成自签名的 CA 文件：</h5><pre><code># openssl x509 -req -days 3650 -in server.crs -signkey server.key -out ca.crt\nSignature ok\nsubject=/C=cn/ST=beijing/L=beijing/O=test/OU=test/CN=test/emailAddress=test@test.com\nGetting Private key\nEnter pass phrase for server.key:\n</code></pre>\n<p>输入 key 文件的密码后 CA 文件生成完毕。</p>\n<h4 id=\"Nginx-配置\"><a href=\"#Nginx-配置\" class=\"headerlink\" title=\"Nginx 配置\"></a>Nginx 配置</h4><pre><code>server &#123;\n    listen 443;\n    ssl on;\n    ssl_certificate ca.crt;\n    ssl_certificate_key server.key;\n\n    location / &#123;\n        proxy_pass http://192.168.36.144:11000;\n     &#125;\n&#125;\n</code></pre>\n<p>重新加载 Nginx 配置文件后，发现 Nginx 代理的 443 端口未正常启动。查看 Nginx error 日志发现以下异常信息：</p>\n<pre><code>2019/03/12 16:49:26 [emerg] 85175#0: SSL_CTX_use_PrivateKey_file(&quot;/etc/nginx/server.key&quot;) failed (SSL: error:0906406D:PEM routines:PEM_def_callback:problems getting password error:0906A068:PEM routines:PEM_do_header:bad password read error:140B0009:SSL routines:SSL_CTX_use_PrivateKey_file:PEM lib)\n</code></pre>\n<p>通过以下方式解决：</p>\n<pre><code># mv server.key server.key.org\n# openssl rsa -in server.key.org -out server.key\n</code></pre>\n<p>重新加载 Nginx 配置文件。此时也不需要再输入 key 文件的密码了。</p>"},{"title":"Node JS 安装","date":"2015-11-25T08:45:45.000Z","_content":"\n### 系统环境\n\t[root@vm-10-176-30-167 ~]# cat /etc/redhat-release\n\tCentOS release 6.6\n\n<!-- more -->\n\n### 下载 nodejs\n下载地址：<https://nodejs.org/en/download/>  \n我下载的是**Linux Binaries (.tar.gz) 64-bit**\n### 解压\n\t[root@vm-10-176-30-167 letv]# tar xzvf node-v4.2.2-linux-x64.tar.gz\n\t[root@vm-10-176-30-167 letv]# mv node-v4.2.2-linux-x64 nodejs\n为了缩短目录名字，将解压后的目录重命名为 nodejs。\n### 配置 path\n\t[root@vm-10-176-30-167 letv]# vi /etc/profile\n添加以下内容，并保存退出。\n\tPATH=${PATH}:/root/nodejs/bin\n\texport PATH\n### 测试 nodejs\n\t[root@vm-10-176-30-167 letv]# node -v\n\tv4.2.2\n\t[root@vm-10-176-30-167 letv]# npm -v\n\t2.14.7\n此时，node 和 npm 安装完成。\n### 安装 Express\n创建一个目录，然后进入此目录并将其作为当前工作目录。\n\n\t[root@vm-10-176-30-167 letv]# mkdir DataInspector\n\t[root@vm-10-176-30-167 letv]# cd DataInspector/\n通过 npm init 命令为应用创建一个 package.json 文件。\n\n\t[root@vm-10-176-30-167 DataInspector]# npm init\n此命令将要求你输入几个参数，例如此应用的名称和版本。 应用名称不能包含大写字母，如果默认应用名称包含大写字母会提示错误，并要求重新录入：\n\n\tname: (DataInspector)\n\tSorry, name can no longer contain capital letters.\n\tname: (DataInspector) data_inspector\n\n参数可以直接按“回车”键接受默认设置即可，下面这个除外：\n\n\tentry point: (index.js)\n键入 app.js 或者你所希望的名称，这是当前应用的入口文件。如果你希望采用默认的 index.js 文件名，只需按“回车”键即可。此处，我将应用入库文件名改为 app.js：\n\n\tentry point: (index.js) app.js\n\n接下来安装 Express 并将其保存到依赖列表中：\n\n\t[root@vm-10-176-30-167 DataInspector]# npm install express --save\n如果只是临时安装 Express，不想将它添加到依赖列表中，只需略去 --save 参数即可：\n\n\t[root@vm-10-176-30-167 DataInspector]# npm install express\n\n>安装 Node 模块时，如果指定了 --save 参数，那么此模块将被添加到 package.json 文件中 dependencies 依赖列表中。 然后通过 npm install 命令即可自动安装依赖列表中所列出的所有模块。\n\n### Express Hello World\n创建 app.js 文件：\n\n\t[root@vm-10-176-30-167 DataInspector]# vi app.js\n复制以下代码到 app.js：\n\n\tvar express = require('express');\n\tvar app = express();\n\n\tapp.get('/', function (req, res) {\n\t\tres.send('Hello World!');\n\t});\n\n\tvar server = app.listen(3000, function () {\n\t  \tvar host = server.address().address;\n\t  \tvar port = server.address().port;\n\n\t  \tconsole.log('Example app listening at http://%s:%s', host, port);\n\t});\n上面的代码启动一个服务并监听从 3000 端口进入的所有连接请求。他将对所有 (/) URL 或 ***路由*** 返回 “Hello World!” 字符串。对于其他所有路径全部返回 **404 Not Found**。\n>req (请求) 和 res (响应) 与 Node 提供的对象完全一致，因此，你可以调用 req.pipe()、req.on('data', callback) 以及任何 Node 提供的方法。\n\n通过如下命令启动此应用：\n\n\t[root@vm-10-176-30-167 DataInspector]# node app.js\n\tExample app listening at http://:::3000\n然后在浏览器中打开 <http://10.176.30.167:3000/> 并查看输出结果。如果是在个人电脑上，在浏览器中打开 <http://localhost:3000/> 并查看数据结果。如果看到以下输出结果，则安装成功：\n\n![express hello world](/uploads/20151125/nodejs1.png)\n","source":"_posts/Node-JS-环境搭建.md","raw":"title: Node JS 安装\ntags:\n  - Node.js\ncategories:\n  - 开发\n  - 环境搭建\ndate: 2015-11-25 16:45:45\n---\n\n### 系统环境\n\t[root@vm-10-176-30-167 ~]# cat /etc/redhat-release\n\tCentOS release 6.6\n\n<!-- more -->\n\n### 下载 nodejs\n下载地址：<https://nodejs.org/en/download/>  \n我下载的是**Linux Binaries (.tar.gz) 64-bit**\n### 解压\n\t[root@vm-10-176-30-167 letv]# tar xzvf node-v4.2.2-linux-x64.tar.gz\n\t[root@vm-10-176-30-167 letv]# mv node-v4.2.2-linux-x64 nodejs\n为了缩短目录名字，将解压后的目录重命名为 nodejs。\n### 配置 path\n\t[root@vm-10-176-30-167 letv]# vi /etc/profile\n添加以下内容，并保存退出。\n\tPATH=${PATH}:/root/nodejs/bin\n\texport PATH\n### 测试 nodejs\n\t[root@vm-10-176-30-167 letv]# node -v\n\tv4.2.2\n\t[root@vm-10-176-30-167 letv]# npm -v\n\t2.14.7\n此时，node 和 npm 安装完成。\n### 安装 Express\n创建一个目录，然后进入此目录并将其作为当前工作目录。\n\n\t[root@vm-10-176-30-167 letv]# mkdir DataInspector\n\t[root@vm-10-176-30-167 letv]# cd DataInspector/\n通过 npm init 命令为应用创建一个 package.json 文件。\n\n\t[root@vm-10-176-30-167 DataInspector]# npm init\n此命令将要求你输入几个参数，例如此应用的名称和版本。 应用名称不能包含大写字母，如果默认应用名称包含大写字母会提示错误，并要求重新录入：\n\n\tname: (DataInspector)\n\tSorry, name can no longer contain capital letters.\n\tname: (DataInspector) data_inspector\n\n参数可以直接按“回车”键接受默认设置即可，下面这个除外：\n\n\tentry point: (index.js)\n键入 app.js 或者你所希望的名称，这是当前应用的入口文件。如果你希望采用默认的 index.js 文件名，只需按“回车”键即可。此处，我将应用入库文件名改为 app.js：\n\n\tentry point: (index.js) app.js\n\n接下来安装 Express 并将其保存到依赖列表中：\n\n\t[root@vm-10-176-30-167 DataInspector]# npm install express --save\n如果只是临时安装 Express，不想将它添加到依赖列表中，只需略去 --save 参数即可：\n\n\t[root@vm-10-176-30-167 DataInspector]# npm install express\n\n>安装 Node 模块时，如果指定了 --save 参数，那么此模块将被添加到 package.json 文件中 dependencies 依赖列表中。 然后通过 npm install 命令即可自动安装依赖列表中所列出的所有模块。\n\n### Express Hello World\n创建 app.js 文件：\n\n\t[root@vm-10-176-30-167 DataInspector]# vi app.js\n复制以下代码到 app.js：\n\n\tvar express = require('express');\n\tvar app = express();\n\n\tapp.get('/', function (req, res) {\n\t\tres.send('Hello World!');\n\t});\n\n\tvar server = app.listen(3000, function () {\n\t  \tvar host = server.address().address;\n\t  \tvar port = server.address().port;\n\n\t  \tconsole.log('Example app listening at http://%s:%s', host, port);\n\t});\n上面的代码启动一个服务并监听从 3000 端口进入的所有连接请求。他将对所有 (/) URL 或 ***路由*** 返回 “Hello World!” 字符串。对于其他所有路径全部返回 **404 Not Found**。\n>req (请求) 和 res (响应) 与 Node 提供的对象完全一致，因此，你可以调用 req.pipe()、req.on('data', callback) 以及任何 Node 提供的方法。\n\n通过如下命令启动此应用：\n\n\t[root@vm-10-176-30-167 DataInspector]# node app.js\n\tExample app listening at http://:::3000\n然后在浏览器中打开 <http://10.176.30.167:3000/> 并查看输出结果。如果是在个人电脑上，在浏览器中打开 <http://localhost:3000/> 并查看数据结果。如果看到以下输出结果，则安装成功：\n\n![express hello world](/uploads/20151125/nodejs1.png)\n","slug":"Node-JS-环境搭建","published":1,"updated":"2021-07-19T16:28:00.284Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphoz008mitd39ld42mgw","content":"<h3 id=\"系统环境\"><a href=\"#系统环境\" class=\"headerlink\" title=\"系统环境\"></a>系统环境</h3><pre><code>[root@vm-10-176-30-167 ~]# cat /etc/redhat-release\nCentOS release 6.6\n</code></pre>\n<span id=\"more\"></span>\n\n<h3 id=\"下载-nodejs\"><a href=\"#下载-nodejs\" class=\"headerlink\" title=\"下载 nodejs\"></a>下载 nodejs</h3><p>下载地址：<a href=\"https://nodejs.org/en/download/\">https://nodejs.org/en/download/</a><br>我下载的是<strong>Linux Binaries (.tar.gz) 64-bit</strong></p>\n<h3 id=\"解压\"><a href=\"#解压\" class=\"headerlink\" title=\"解压\"></a>解压</h3><pre><code>[root@vm-10-176-30-167 letv]# tar xzvf node-v4.2.2-linux-x64.tar.gz\n[root@vm-10-176-30-167 letv]# mv node-v4.2.2-linux-x64 nodejs\n</code></pre>\n<p>为了缩短目录名字，将解压后的目录重命名为 nodejs。</p>\n<h3 id=\"配置-path\"><a href=\"#配置-path\" class=\"headerlink\" title=\"配置 path\"></a>配置 path</h3><pre><code>[root@vm-10-176-30-167 letv]# vi /etc/profile\n</code></pre>\n<p>添加以下内容，并保存退出。<br>    PATH=${PATH}:/root/nodejs/bin<br>    export PATH</p>\n<h3 id=\"测试-nodejs\"><a href=\"#测试-nodejs\" class=\"headerlink\" title=\"测试 nodejs\"></a>测试 nodejs</h3><pre><code>[root@vm-10-176-30-167 letv]# node -v\nv4.2.2\n[root@vm-10-176-30-167 letv]# npm -v\n2.14.7\n</code></pre>\n<p>此时，node 和 npm 安装完成。</p>\n<h3 id=\"安装-Express\"><a href=\"#安装-Express\" class=\"headerlink\" title=\"安装 Express\"></a>安装 Express</h3><p>创建一个目录，然后进入此目录并将其作为当前工作目录。</p>\n<pre><code>[root@vm-10-176-30-167 letv]# mkdir DataInspector\n[root@vm-10-176-30-167 letv]# cd DataInspector/\n</code></pre>\n<p>通过 npm init 命令为应用创建一个 package.json 文件。</p>\n<pre><code>[root@vm-10-176-30-167 DataInspector]# npm init\n</code></pre>\n<p>此命令将要求你输入几个参数，例如此应用的名称和版本。 应用名称不能包含大写字母，如果默认应用名称包含大写字母会提示错误，并要求重新录入：</p>\n<pre><code>name: (DataInspector)\nSorry, name can no longer contain capital letters.\nname: (DataInspector) data_inspector\n</code></pre>\n<p>参数可以直接按“回车”键接受默认设置即可，下面这个除外：</p>\n<pre><code>entry point: (index.js)\n</code></pre>\n<p>键入 app.js 或者你所希望的名称，这是当前应用的入口文件。如果你希望采用默认的 index.js 文件名，只需按“回车”键即可。此处，我将应用入库文件名改为 app.js：</p>\n<pre><code>entry point: (index.js) app.js\n</code></pre>\n<p>接下来安装 Express 并将其保存到依赖列表中：</p>\n<pre><code>[root@vm-10-176-30-167 DataInspector]# npm install express --save\n</code></pre>\n<p>如果只是临时安装 Express，不想将它添加到依赖列表中，只需略去 –save 参数即可：</p>\n<pre><code>[root@vm-10-176-30-167 DataInspector]# npm install express\n</code></pre>\n<blockquote>\n<p>安装 Node 模块时，如果指定了 –save 参数，那么此模块将被添加到 package.json 文件中 dependencies 依赖列表中。 然后通过 npm install 命令即可自动安装依赖列表中所列出的所有模块。</p>\n</blockquote>\n<h3 id=\"Express-Hello-World\"><a href=\"#Express-Hello-World\" class=\"headerlink\" title=\"Express Hello World\"></a>Express Hello World</h3><p>创建 app.js 文件：</p>\n<pre><code>[root@vm-10-176-30-167 DataInspector]# vi app.js\n</code></pre>\n<p>复制以下代码到 app.js：</p>\n<pre><code>var express = require(&#39;express&#39;);\nvar app = express();\n\napp.get(&#39;/&#39;, function (req, res) &#123;\n    res.send(&#39;Hello World!&#39;);\n&#125;);\n\nvar server = app.listen(3000, function () &#123;\n      var host = server.address().address;\n      var port = server.address().port;\n\n      console.log(&#39;Example app listening at http://%s:%s&#39;, host, port);\n&#125;);\n</code></pre>\n<p>上面的代码启动一个服务并监听从 3000 端口进入的所有连接请求。他将对所有 (/) URL 或 <em><strong>路由</strong></em> 返回 “Hello World!” 字符串。对于其他所有路径全部返回 <strong>404 Not Found</strong>。</p>\n<blockquote>\n<p>req (请求) 和 res (响应) 与 Node 提供的对象完全一致，因此，你可以调用 req.pipe()、req.on(‘data’, callback) 以及任何 Node 提供的方法。</p>\n</blockquote>\n<p>通过如下命令启动此应用：</p>\n<pre><code>[root@vm-10-176-30-167 DataInspector]# node app.js\nExample app listening at http://:::3000\n</code></pre>\n<p>然后在浏览器中打开 <a href=\"http://10.176.30.167:3000/\">http://10.176.30.167:3000/</a> 并查看输出结果。如果是在个人电脑上，在浏览器中打开 <a href=\"http://localhost:3000/\">http://localhost:3000/</a> 并查看数据结果。如果看到以下输出结果，则安装成功：</p>\n<p><img src=\"/uploads/20151125/nodejs1.png\" alt=\"express hello world\"></p>\n","site":{"data":{}},"excerpt":"<h3 id=\"系统环境\"><a href=\"#系统环境\" class=\"headerlink\" title=\"系统环境\"></a>系统环境</h3><pre><code>[root@vm-10-176-30-167 ~]# cat /etc/redhat-release\nCentOS release 6.6\n</code></pre>","more":"<h3 id=\"下载-nodejs\"><a href=\"#下载-nodejs\" class=\"headerlink\" title=\"下载 nodejs\"></a>下载 nodejs</h3><p>下载地址：<a href=\"https://nodejs.org/en/download/\">https://nodejs.org/en/download/</a><br>我下载的是<strong>Linux Binaries (.tar.gz) 64-bit</strong></p>\n<h3 id=\"解压\"><a href=\"#解压\" class=\"headerlink\" title=\"解压\"></a>解压</h3><pre><code>[root@vm-10-176-30-167 letv]# tar xzvf node-v4.2.2-linux-x64.tar.gz\n[root@vm-10-176-30-167 letv]# mv node-v4.2.2-linux-x64 nodejs\n</code></pre>\n<p>为了缩短目录名字，将解压后的目录重命名为 nodejs。</p>\n<h3 id=\"配置-path\"><a href=\"#配置-path\" class=\"headerlink\" title=\"配置 path\"></a>配置 path</h3><pre><code>[root@vm-10-176-30-167 letv]# vi /etc/profile\n</code></pre>\n<p>添加以下内容，并保存退出。<br>    PATH=${PATH}:/root/nodejs/bin<br>    export PATH</p>\n<h3 id=\"测试-nodejs\"><a href=\"#测试-nodejs\" class=\"headerlink\" title=\"测试 nodejs\"></a>测试 nodejs</h3><pre><code>[root@vm-10-176-30-167 letv]# node -v\nv4.2.2\n[root@vm-10-176-30-167 letv]# npm -v\n2.14.7\n</code></pre>\n<p>此时，node 和 npm 安装完成。</p>\n<h3 id=\"安装-Express\"><a href=\"#安装-Express\" class=\"headerlink\" title=\"安装 Express\"></a>安装 Express</h3><p>创建一个目录，然后进入此目录并将其作为当前工作目录。</p>\n<pre><code>[root@vm-10-176-30-167 letv]# mkdir DataInspector\n[root@vm-10-176-30-167 letv]# cd DataInspector/\n</code></pre>\n<p>通过 npm init 命令为应用创建一个 package.json 文件。</p>\n<pre><code>[root@vm-10-176-30-167 DataInspector]# npm init\n</code></pre>\n<p>此命令将要求你输入几个参数，例如此应用的名称和版本。 应用名称不能包含大写字母，如果默认应用名称包含大写字母会提示错误，并要求重新录入：</p>\n<pre><code>name: (DataInspector)\nSorry, name can no longer contain capital letters.\nname: (DataInspector) data_inspector\n</code></pre>\n<p>参数可以直接按“回车”键接受默认设置即可，下面这个除外：</p>\n<pre><code>entry point: (index.js)\n</code></pre>\n<p>键入 app.js 或者你所希望的名称，这是当前应用的入口文件。如果你希望采用默认的 index.js 文件名，只需按“回车”键即可。此处，我将应用入库文件名改为 app.js：</p>\n<pre><code>entry point: (index.js) app.js\n</code></pre>\n<p>接下来安装 Express 并将其保存到依赖列表中：</p>\n<pre><code>[root@vm-10-176-30-167 DataInspector]# npm install express --save\n</code></pre>\n<p>如果只是临时安装 Express，不想将它添加到依赖列表中，只需略去 –save 参数即可：</p>\n<pre><code>[root@vm-10-176-30-167 DataInspector]# npm install express\n</code></pre>\n<blockquote>\n<p>安装 Node 模块时，如果指定了 –save 参数，那么此模块将被添加到 package.json 文件中 dependencies 依赖列表中。 然后通过 npm install 命令即可自动安装依赖列表中所列出的所有模块。</p>\n</blockquote>\n<h3 id=\"Express-Hello-World\"><a href=\"#Express-Hello-World\" class=\"headerlink\" title=\"Express Hello World\"></a>Express Hello World</h3><p>创建 app.js 文件：</p>\n<pre><code>[root@vm-10-176-30-167 DataInspector]# vi app.js\n</code></pre>\n<p>复制以下代码到 app.js：</p>\n<pre><code>var express = require(&#39;express&#39;);\nvar app = express();\n\napp.get(&#39;/&#39;, function (req, res) &#123;\n    res.send(&#39;Hello World!&#39;);\n&#125;);\n\nvar server = app.listen(3000, function () &#123;\n      var host = server.address().address;\n      var port = server.address().port;\n\n      console.log(&#39;Example app listening at http://%s:%s&#39;, host, port);\n&#125;);\n</code></pre>\n<p>上面的代码启动一个服务并监听从 3000 端口进入的所有连接请求。他将对所有 (/) URL 或 <em><strong>路由</strong></em> 返回 “Hello World!” 字符串。对于其他所有路径全部返回 <strong>404 Not Found</strong>。</p>\n<blockquote>\n<p>req (请求) 和 res (响应) 与 Node 提供的对象完全一致，因此，你可以调用 req.pipe()、req.on(‘data’, callback) 以及任何 Node 提供的方法。</p>\n</blockquote>\n<p>通过如下命令启动此应用：</p>\n<pre><code>[root@vm-10-176-30-167 DataInspector]# node app.js\nExample app listening at http://:::3000\n</code></pre>\n<p>然后在浏览器中打开 <a href=\"http://10.176.30.167:3000/\">http://10.176.30.167:3000/</a> 并查看输出结果。如果是在个人电脑上，在浏览器中打开 <a href=\"http://localhost:3000/\">http://localhost:3000/</a> 并查看数据结果。如果看到以下输出结果，则安装成功：</p>\n<p><img src=\"/uploads/20151125/nodejs1.png\" alt=\"express hello world\"></p>"},{"title":"Python3 ImportError: cannot import name 'XXX' from 'XXX'","date":"2019-12-11T10:52:53.000Z","_content":"\n例如如下错误：\n\n    $ python3 git.py \n    Traceback (most recent call last):\n      File \"git.py\", line 1, in <module>\n        from git import Repo\n      File \"/home/a/git.py\", line 1, in <module>\n        from git import Repo\n    ImportError: cannot import name 'Repo' from 'git' (/home/a/git.py)\n\n这种错误基本都是因为 python 脚本的名称与模块的名称重复导致的。像我这个例子中就是因为 git.py 与 GitPython 模块中的文件名一样导致的。","source":"_posts/Python3-ImportError-cannot-import-name-XXX-from-XXX.md","raw":"title: 'Python3 ImportError: cannot import name ''XXX'' from ''XXX'''\ndate: 2019-12-11 18:52:53\ntags:\n- Python3\ncategories:\n- 语言\n- Python3\n---\n\n例如如下错误：\n\n    $ python3 git.py \n    Traceback (most recent call last):\n      File \"git.py\", line 1, in <module>\n        from git import Repo\n      File \"/home/a/git.py\", line 1, in <module>\n        from git import Repo\n    ImportError: cannot import name 'Repo' from 'git' (/home/a/git.py)\n\n这种错误基本都是因为 python 脚本的名称与模块的名称重复导致的。像我这个例子中就是因为 git.py 与 GitPython 模块中的文件名一样导致的。","slug":"Python3-ImportError-cannot-import-name-XXX-from-XXX","published":1,"updated":"2021-07-19T16:28:00.352Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphp0008qitd3455i3p9h","content":"<p>例如如下错误：</p>\n<pre><code>$ python3 git.py \nTraceback (most recent call last):\n  File &quot;git.py&quot;, line 1, in &lt;module&gt;\n    from git import Repo\n  File &quot;/home/a/git.py&quot;, line 1, in &lt;module&gt;\n    from git import Repo\nImportError: cannot import name &#39;Repo&#39; from &#39;git&#39; (/home/a/git.py)\n</code></pre>\n<p>这种错误基本都是因为 python 脚本的名称与模块的名称重复导致的。像我这个例子中就是因为 git.py 与 GitPython 模块中的文件名一样导致的。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>例如如下错误：</p>\n<pre><code>$ python3 git.py \nTraceback (most recent call last):\n  File &quot;git.py&quot;, line 1, in &lt;module&gt;\n    from git import Repo\n  File &quot;/home/a/git.py&quot;, line 1, in &lt;module&gt;\n    from git import Repo\nImportError: cannot import name &#39;Repo&#39; from &#39;git&#39; (/home/a/git.py)\n</code></pre>\n<p>这种错误基本都是因为 python 脚本的名称与模块的名称重复导致的。像我这个例子中就是因为 git.py 与 GitPython 模块中的文件名一样导致的。</p>\n"},{"title":"Python3 创建虚拟环境","date":"2019-01-13T12:09:35.000Z","_content":"Python 虚拟环境主要是为了解决 Python 多版本及模块间版本兼容的问题。创建虚拟环境的方法很简单，使用下面的命令即可：\n\n    python3 -m venv PySparkEnv\n\nPySparkEnv 即虚拟环境的主目录。如果要启用这个虚拟环境的话，使用以下命令：\n\n    source PySparkEnv/bin/activate\n","source":"_posts/Python3-创建虚拟环境.md","raw":"title: Python3 创建虚拟环境\ndate: 2019-01-13 20:09:35\ntags:\n- Python3\ncategories:\n- 语言\n- Python3\n---\nPython 虚拟环境主要是为了解决 Python 多版本及模块间版本兼容的问题。创建虚拟环境的方法很简单，使用下面的命令即可：\n\n    python3 -m venv PySparkEnv\n\nPySparkEnv 即虚拟环境的主目录。如果要启用这个虚拟环境的话，使用以下命令：\n\n    source PySparkEnv/bin/activate\n","slug":"Python3-创建虚拟环境","published":1,"updated":"2021-07-19T16:28:00.284Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphp2008uitd3hxcd7o64","content":"<p>Python 虚拟环境主要是为了解决 Python 多版本及模块间版本兼容的问题。创建虚拟环境的方法很简单，使用下面的命令即可：</p>\n<pre><code>python3 -m venv PySparkEnv\n</code></pre>\n<p>PySparkEnv 即虚拟环境的主目录。如果要启用这个虚拟环境的话，使用以下命令：</p>\n<pre><code>source PySparkEnv/bin/activate\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>Python 虚拟环境主要是为了解决 Python 多版本及模块间版本兼容的问题。创建虚拟环境的方法很简单，使用下面的命令即可：</p>\n<pre><code>python3 -m venv PySparkEnv\n</code></pre>\n<p>PySparkEnv 即虚拟环境的主目录。如果要启用这个虚拟环境的话，使用以下命令：</p>\n<pre><code>source PySparkEnv/bin/activate\n</code></pre>\n"},{"title":"Python3 命令行交互不能使用方向键","date":"2019-12-21T09:25:50.000Z","categorires":["语言","Python3"],"_content":"\n自定义安装 Python3 后在命令行使用方向键时出现以下问题：\n\n    $ python3\n    Python 3.7.4 (default, Dec 11 2019, 17:40:08) \n    [GCC 7.4.0] on linux\n    Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n    >>> ^[[A^[[B^[[C^[[D\n\n解决方法是安装 readline：\n\n    pip3 install readline\n","source":"_posts/Python3-命令行交互不能使用方向键.md","raw":"title: Python3 命令行交互不能使用方向键\ndate: 2019-12-21 17:25:50\ntags:\n- Python3\ncategorires:\n- 语言\n- Python3\n---\n\n自定义安装 Python3 后在命令行使用方向键时出现以下问题：\n\n    $ python3\n    Python 3.7.4 (default, Dec 11 2019, 17:40:08) \n    [GCC 7.4.0] on linux\n    Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n    >>> ^[[A^[[B^[[C^[[D\n\n解决方法是安装 readline：\n\n    pip3 install readline\n","slug":"Python3-命令行交互不能使用方向键","published":1,"updated":"2021-07-19T16:28:00.328Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphp3008yitd357dp3lfk","content":"<p>自定义安装 Python3 后在命令行使用方向键时出现以下问题：</p>\n<pre><code>$ python3\nPython 3.7.4 (default, Dec 11 2019, 17:40:08) \n[GCC 7.4.0] on linux\nType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.\n&gt;&gt;&gt; ^[[A^[[B^[[C^[[D\n</code></pre>\n<p>解决方法是安装 readline：</p>\n<pre><code>pip3 install readline\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>自定义安装 Python3 后在命令行使用方向键时出现以下问题：</p>\n<pre><code>$ python3\nPython 3.7.4 (default, Dec 11 2019, 17:40:08) \n[GCC 7.4.0] on linux\nType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.\n&gt;&gt;&gt; ^[[A^[[B^[[C^[[D\n</code></pre>\n<p>解决方法是安装 readline：</p>\n<pre><code>pip3 install readline\n</code></pre>\n"},{"title":"Python3 操作 MongoDB 批量 upsert","date":"2018-11-19T08:18:25.000Z","_content":"\n\n代码如下：\n\n    mongoClient = MongoClient('mongodb://172.16.72.213:27017/')\n    opsDb = mongoClient.ops\n    azScheduled = opsDb.azScheduledFlow\n    \n    bulkOpers = []\n    for flow in scheduledFlows.values():\n        bulkOpers.append(UpdateOne({'opsDt': opsDt, 'projectId': flow['projectId'], 'projectName': flow['projectName'], 'flowName': flow['flowName']}, {'$set': {'opsDateTime': opsDtStr, 'status': flow['status'], 'startTime': flow['startTime'], 'endTime': flow['endTime'], 'elapsed': flow['elapsed']}}, upsert=True))\n    \n    azScheduled.bulk_write(bulkOpers)\n","source":"_posts/Python3-操作-MongoDB-批量-upsert.md","raw":"title: Python3 操作 MongoDB 批量 upsert\ntags:\n  - Python3\n  - MongoDB\ncategories:\n  - 语言\n  - Python3\ndate: 2018-11-19 16:18:25\n---\n\n\n代码如下：\n\n    mongoClient = MongoClient('mongodb://172.16.72.213:27017/')\n    opsDb = mongoClient.ops\n    azScheduled = opsDb.azScheduledFlow\n    \n    bulkOpers = []\n    for flow in scheduledFlows.values():\n        bulkOpers.append(UpdateOne({'opsDt': opsDt, 'projectId': flow['projectId'], 'projectName': flow['projectName'], 'flowName': flow['flowName']}, {'$set': {'opsDateTime': opsDtStr, 'status': flow['status'], 'startTime': flow['startTime'], 'endTime': flow['endTime'], 'elapsed': flow['elapsed']}}, upsert=True))\n    \n    azScheduled.bulk_write(bulkOpers)\n","slug":"Python3-操作-MongoDB-批量-upsert","published":1,"updated":"2021-07-19T16:28:00.284Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphp50092itd3gxuahxu5","content":"<p>代码如下：</p>\n<pre><code>mongoClient = MongoClient(&#39;mongodb://172.16.72.213:27017/&#39;)\nopsDb = mongoClient.ops\nazScheduled = opsDb.azScheduledFlow\n\nbulkOpers = []\nfor flow in scheduledFlows.values():\n    bulkOpers.append(UpdateOne(&#123;&#39;opsDt&#39;: opsDt, &#39;projectId&#39;: flow[&#39;projectId&#39;], &#39;projectName&#39;: flow[&#39;projectName&#39;], &#39;flowName&#39;: flow[&#39;flowName&#39;]&#125;, &#123;&#39;$set&#39;: &#123;&#39;opsDateTime&#39;: opsDtStr, &#39;status&#39;: flow[&#39;status&#39;], &#39;startTime&#39;: flow[&#39;startTime&#39;], &#39;endTime&#39;: flow[&#39;endTime&#39;], &#39;elapsed&#39;: flow[&#39;elapsed&#39;]&#125;&#125;, upsert=True))\n\nazScheduled.bulk_write(bulkOpers)\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>代码如下：</p>\n<pre><code>mongoClient = MongoClient(&#39;mongodb://172.16.72.213:27017/&#39;)\nopsDb = mongoClient.ops\nazScheduled = opsDb.azScheduledFlow\n\nbulkOpers = []\nfor flow in scheduledFlows.values():\n    bulkOpers.append(UpdateOne(&#123;&#39;opsDt&#39;: opsDt, &#39;projectId&#39;: flow[&#39;projectId&#39;], &#39;projectName&#39;: flow[&#39;projectName&#39;], &#39;flowName&#39;: flow[&#39;flowName&#39;]&#125;, &#123;&#39;$set&#39;: &#123;&#39;opsDateTime&#39;: opsDtStr, &#39;status&#39;: flow[&#39;status&#39;], &#39;startTime&#39;: flow[&#39;startTime&#39;], &#39;endTime&#39;: flow[&#39;endTime&#39;], &#39;elapsed&#39;: flow[&#39;elapsed&#39;]&#125;&#125;, upsert=True))\n\nazScheduled.bulk_write(bulkOpers)\n</code></pre>\n"},{"title":"Python3 连接 MySQL 并且读取 Blob 字段信息","date":"2018-11-15T06:33:28.000Z","_content":"\n\n#### 安装驱动\n\n    $ pip3 install mysql-connector-python\n    Command 'pip3' not found, but can be installed with:\n    sudo apt install python3-pip\n    \n根据提示信息安装 pip3。\n\n根据 MySQL 官网建议应该安装 8.0 的驱动。我的安装：mysql-connector-python-8.0.13、protobuf-3.6.1、setuptools-40.6.2、six-1.11.0。\n\n#### 读取MySQL数据\n\n以读取 Azkaban 中的 triggers 表数据为例。代码如下：\n\n    #!/usr/bin/python3\n    \n    import mysql.connector\n    import gzip\n    \n    config = {\n        'user': 'roHive',\n        'password': 'hive@bigdata!23',\n        'host': '172.16.72.22',\n        'database': 'azkaban3',\n        'raise_on_warnings': True,\n        'charset': 'latin1'\n    }\n    \n    cnx = mysql.connector.connect(**config)\n    cursor = cnx.cursor()\n    query = (\"SELECT trigger_id, data FROM azkaban3.triggers\")\n    cursor.execute(query)\n    \n    for (triggerId, triggerData) in cursor:\n        print(f'triggerId={triggerId}')\n    \n    cursor.close()\n    cnx.close()\n    \nAzkaban 的 triggers 表中的 data 字段是 BLOB 类型。因为我的 Azkaban MySQL 库采用的是 latin1 编码，如果连接时不设置字符集在读取 data 字段数据时在读取 BLOB 类型字段时会报如下错误：\n\n    UnicodeDecodeError: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte\n\n因为 data 字段是采用 gzip 压缩的，所以需要解压，代码如下：\n\n    for (triggerId, triggerData) in cursor:\n        print(gzip.decompress(bytes(triggerData, encoding='latin1')))\n","source":"_posts/Python3-连接-MySQL-并且读取-Blob-字段信息.md","raw":"title: Python3 连接 MySQL 并且读取 Blob 字段信息\ntags:\n  - Python3\n  - MySQL\n  - Azkaban\ncategories:\n  - 语言\n  - Python3\ndate: 2018-11-15 14:33:28\n---\n\n\n#### 安装驱动\n\n    $ pip3 install mysql-connector-python\n    Command 'pip3' not found, but can be installed with:\n    sudo apt install python3-pip\n    \n根据提示信息安装 pip3。\n\n根据 MySQL 官网建议应该安装 8.0 的驱动。我的安装：mysql-connector-python-8.0.13、protobuf-3.6.1、setuptools-40.6.2、six-1.11.0。\n\n#### 读取MySQL数据\n\n以读取 Azkaban 中的 triggers 表数据为例。代码如下：\n\n    #!/usr/bin/python3\n    \n    import mysql.connector\n    import gzip\n    \n    config = {\n        'user': 'roHive',\n        'password': 'hive@bigdata!23',\n        'host': '172.16.72.22',\n        'database': 'azkaban3',\n        'raise_on_warnings': True,\n        'charset': 'latin1'\n    }\n    \n    cnx = mysql.connector.connect(**config)\n    cursor = cnx.cursor()\n    query = (\"SELECT trigger_id, data FROM azkaban3.triggers\")\n    cursor.execute(query)\n    \n    for (triggerId, triggerData) in cursor:\n        print(f'triggerId={triggerId}')\n    \n    cursor.close()\n    cnx.close()\n    \nAzkaban 的 triggers 表中的 data 字段是 BLOB 类型。因为我的 Azkaban MySQL 库采用的是 latin1 编码，如果连接时不设置字符集在读取 data 字段数据时在读取 BLOB 类型字段时会报如下错误：\n\n    UnicodeDecodeError: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte\n\n因为 data 字段是采用 gzip 压缩的，所以需要解压，代码如下：\n\n    for (triggerId, triggerData) in cursor:\n        print(gzip.decompress(bytes(triggerData, encoding='latin1')))\n","slug":"Python3-连接-MySQL-并且读取-Blob-字段信息","published":1,"updated":"2021-07-19T16:28:00.288Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphp60096itd3flyu178b","content":"<h4 id=\"安装驱动\"><a href=\"#安装驱动\" class=\"headerlink\" title=\"安装驱动\"></a>安装驱动</h4><pre><code>$ pip3 install mysql-connector-python\nCommand &#39;pip3&#39; not found, but can be installed with:\nsudo apt install python3-pip\n</code></pre>\n<p>根据提示信息安装 pip3。</p>\n<p>根据 MySQL 官网建议应该安装 8.0 的驱动。我的安装：mysql-connector-python-8.0.13、protobuf-3.6.1、setuptools-40.6.2、six-1.11.0。</p>\n<h4 id=\"读取MySQL数据\"><a href=\"#读取MySQL数据\" class=\"headerlink\" title=\"读取MySQL数据\"></a>读取MySQL数据</h4><p>以读取 Azkaban 中的 triggers 表数据为例。代码如下：</p>\n<pre><code>#!/usr/bin/python3\n\nimport mysql.connector\nimport gzip\n\nconfig = &#123;\n    &#39;user&#39;: &#39;roHive&#39;,\n    &#39;password&#39;: &#39;hive@bigdata!23&#39;,\n    &#39;host&#39;: &#39;172.16.72.22&#39;,\n    &#39;database&#39;: &#39;azkaban3&#39;,\n    &#39;raise_on_warnings&#39;: True,\n    &#39;charset&#39;: &#39;latin1&#39;\n&#125;\n\ncnx = mysql.connector.connect(**config)\ncursor = cnx.cursor()\nquery = (&quot;SELECT trigger_id, data FROM azkaban3.triggers&quot;)\ncursor.execute(query)\n\nfor (triggerId, triggerData) in cursor:\n    print(f&#39;triggerId=&#123;triggerId&#125;&#39;)\n\ncursor.close()\ncnx.close()\n</code></pre>\n<p>Azkaban 的 triggers 表中的 data 字段是 BLOB 类型。因为我的 Azkaban MySQL 库采用的是 latin1 编码，如果连接时不设置字符集在读取 data 字段数据时在读取 BLOB 类型字段时会报如下错误：</p>\n<pre><code>UnicodeDecodeError: &#39;utf-8&#39; codec can&#39;t decode byte 0x8b in position 1: invalid start byte\n</code></pre>\n<p>因为 data 字段是采用 gzip 压缩的，所以需要解压，代码如下：</p>\n<pre><code>for (triggerId, triggerData) in cursor:\n    print(gzip.decompress(bytes(triggerData, encoding=&#39;latin1&#39;)))\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"安装驱动\"><a href=\"#安装驱动\" class=\"headerlink\" title=\"安装驱动\"></a>安装驱动</h4><pre><code>$ pip3 install mysql-connector-python\nCommand &#39;pip3&#39; not found, but can be installed with:\nsudo apt install python3-pip\n</code></pre>\n<p>根据提示信息安装 pip3。</p>\n<p>根据 MySQL 官网建议应该安装 8.0 的驱动。我的安装：mysql-connector-python-8.0.13、protobuf-3.6.1、setuptools-40.6.2、six-1.11.0。</p>\n<h4 id=\"读取MySQL数据\"><a href=\"#读取MySQL数据\" class=\"headerlink\" title=\"读取MySQL数据\"></a>读取MySQL数据</h4><p>以读取 Azkaban 中的 triggers 表数据为例。代码如下：</p>\n<pre><code>#!/usr/bin/python3\n\nimport mysql.connector\nimport gzip\n\nconfig = &#123;\n    &#39;user&#39;: &#39;roHive&#39;,\n    &#39;password&#39;: &#39;hive@bigdata!23&#39;,\n    &#39;host&#39;: &#39;172.16.72.22&#39;,\n    &#39;database&#39;: &#39;azkaban3&#39;,\n    &#39;raise_on_warnings&#39;: True,\n    &#39;charset&#39;: &#39;latin1&#39;\n&#125;\n\ncnx = mysql.connector.connect(**config)\ncursor = cnx.cursor()\nquery = (&quot;SELECT trigger_id, data FROM azkaban3.triggers&quot;)\ncursor.execute(query)\n\nfor (triggerId, triggerData) in cursor:\n    print(f&#39;triggerId=&#123;triggerId&#125;&#39;)\n\ncursor.close()\ncnx.close()\n</code></pre>\n<p>Azkaban 的 triggers 表中的 data 字段是 BLOB 类型。因为我的 Azkaban MySQL 库采用的是 latin1 编码，如果连接时不设置字符集在读取 data 字段数据时在读取 BLOB 类型字段时会报如下错误：</p>\n<pre><code>UnicodeDecodeError: &#39;utf-8&#39; codec can&#39;t decode byte 0x8b in position 1: invalid start byte\n</code></pre>\n<p>因为 data 字段是采用 gzip 压缩的，所以需要解压，代码如下：</p>\n<pre><code>for (triggerId, triggerData) in cursor:\n    print(gzip.decompress(bytes(triggerData, encoding=&#39;latin1&#39;)))\n</code></pre>\n"},{"title":"Python3连接MongoDB并写入数据","date":"2018-11-15T09:09:30.000Z","_content":"\n\n#### 安装 PyMongo\n\n    $ pip3 install pymongo\n    Successfully installed pymongo-3.7.2\n\n#### 连接MongoDB并且批量插入操作\n\n    #!/usr/bin/python3\n\n    import mysql.connector\n    import gzip\n    import json\n    from pymongo import MongoClient\n    from datetime import datetime\n    \n    opsDateTime = datetime.now().isoformat(timespec='seconds')\n    \n    config = {\n        'user': 'roHive',\n        'password': 'hive@bigdata!23',\n        'host': '172.16.72.22',\n        'database': 'azkaban3',\n        'raise_on_warnings': True,\n        'charset': 'latin1'\n    }\n    \n    cnx = mysql.connector.connect(**config)\n    cursor = cnx.cursor()\n    query = (\"SELECT trigger_id, data FROM azkaban3.triggers\")\n    cursor.execute(query)\n    \n    scheduledList = []\n    for (triggerId, triggerData) in cursor:\n        triggerJson = json.loads(gzip.decompress(bytes(triggerData, encoding='latin1')))\n        actionJson = triggerJson['actions'][0]['actionJson']\n        projectName = actionJson['projectName']\n        flowName = actionJson['flowName']\n        scheduledFlow = {'opsDateTime': opsDateTime, 'projectName': projectName, 'flowName': flowName}\n        scheduledList.append(scheduledFlow)\n    \n    cursor.close()\n    cnx.close()\n    \n    mongoClient = MongoClient('mongodb://172.16.72.213:27017/')\n    opsDb = mongoClient.ops\n    azScheduled = opsDb.azScheduledFlow\n    azScheduled.insert_many(scheduledList)\n    \n","source":"_posts/Python3连接MongoDB并写入数据.md","raw":"title: Python3连接MongoDB并写入数据\ntags:\n  - Python3\n  - MongoDB\ncategories:\n  - 语言\n  - Python3\ndate: 2018-11-15 17:09:30\n---\n\n\n#### 安装 PyMongo\n\n    $ pip3 install pymongo\n    Successfully installed pymongo-3.7.2\n\n#### 连接MongoDB并且批量插入操作\n\n    #!/usr/bin/python3\n\n    import mysql.connector\n    import gzip\n    import json\n    from pymongo import MongoClient\n    from datetime import datetime\n    \n    opsDateTime = datetime.now().isoformat(timespec='seconds')\n    \n    config = {\n        'user': 'roHive',\n        'password': 'hive@bigdata!23',\n        'host': '172.16.72.22',\n        'database': 'azkaban3',\n        'raise_on_warnings': True,\n        'charset': 'latin1'\n    }\n    \n    cnx = mysql.connector.connect(**config)\n    cursor = cnx.cursor()\n    query = (\"SELECT trigger_id, data FROM azkaban3.triggers\")\n    cursor.execute(query)\n    \n    scheduledList = []\n    for (triggerId, triggerData) in cursor:\n        triggerJson = json.loads(gzip.decompress(bytes(triggerData, encoding='latin1')))\n        actionJson = triggerJson['actions'][0]['actionJson']\n        projectName = actionJson['projectName']\n        flowName = actionJson['flowName']\n        scheduledFlow = {'opsDateTime': opsDateTime, 'projectName': projectName, 'flowName': flowName}\n        scheduledList.append(scheduledFlow)\n    \n    cursor.close()\n    cnx.close()\n    \n    mongoClient = MongoClient('mongodb://172.16.72.213:27017/')\n    opsDb = mongoClient.ops\n    azScheduled = opsDb.azScheduledFlow\n    azScheduled.insert_many(scheduledList)\n    \n","slug":"Python3连接MongoDB并写入数据","published":1,"updated":"2021-07-19T16:28:00.288Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphp7009aitd30zi77fga","content":"<h4 id=\"安装-PyMongo\"><a href=\"#安装-PyMongo\" class=\"headerlink\" title=\"安装 PyMongo\"></a>安装 PyMongo</h4><pre><code>$ pip3 install pymongo\nSuccessfully installed pymongo-3.7.2\n</code></pre>\n<h4 id=\"连接MongoDB并且批量插入操作\"><a href=\"#连接MongoDB并且批量插入操作\" class=\"headerlink\" title=\"连接MongoDB并且批量插入操作\"></a>连接MongoDB并且批量插入操作</h4><pre><code>#!/usr/bin/python3\n\nimport mysql.connector\nimport gzip\nimport json\nfrom pymongo import MongoClient\nfrom datetime import datetime\n\nopsDateTime = datetime.now().isoformat(timespec=&#39;seconds&#39;)\n\nconfig = &#123;\n    &#39;user&#39;: &#39;roHive&#39;,\n    &#39;password&#39;: &#39;hive@bigdata!23&#39;,\n    &#39;host&#39;: &#39;172.16.72.22&#39;,\n    &#39;database&#39;: &#39;azkaban3&#39;,\n    &#39;raise_on_warnings&#39;: True,\n    &#39;charset&#39;: &#39;latin1&#39;\n&#125;\n\ncnx = mysql.connector.connect(**config)\ncursor = cnx.cursor()\nquery = (&quot;SELECT trigger_id, data FROM azkaban3.triggers&quot;)\ncursor.execute(query)\n\nscheduledList = []\nfor (triggerId, triggerData) in cursor:\n    triggerJson = json.loads(gzip.decompress(bytes(triggerData, encoding=&#39;latin1&#39;)))\n    actionJson = triggerJson[&#39;actions&#39;][0][&#39;actionJson&#39;]\n    projectName = actionJson[&#39;projectName&#39;]\n    flowName = actionJson[&#39;flowName&#39;]\n    scheduledFlow = &#123;&#39;opsDateTime&#39;: opsDateTime, &#39;projectName&#39;: projectName, &#39;flowName&#39;: flowName&#125;\n    scheduledList.append(scheduledFlow)\n\ncursor.close()\ncnx.close()\n\nmongoClient = MongoClient(&#39;mongodb://172.16.72.213:27017/&#39;)\nopsDb = mongoClient.ops\nazScheduled = opsDb.azScheduledFlow\nazScheduled.insert_many(scheduledList)\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"安装-PyMongo\"><a href=\"#安装-PyMongo\" class=\"headerlink\" title=\"安装 PyMongo\"></a>安装 PyMongo</h4><pre><code>$ pip3 install pymongo\nSuccessfully installed pymongo-3.7.2\n</code></pre>\n<h4 id=\"连接MongoDB并且批量插入操作\"><a href=\"#连接MongoDB并且批量插入操作\" class=\"headerlink\" title=\"连接MongoDB并且批量插入操作\"></a>连接MongoDB并且批量插入操作</h4><pre><code>#!/usr/bin/python3\n\nimport mysql.connector\nimport gzip\nimport json\nfrom pymongo import MongoClient\nfrom datetime import datetime\n\nopsDateTime = datetime.now().isoformat(timespec=&#39;seconds&#39;)\n\nconfig = &#123;\n    &#39;user&#39;: &#39;roHive&#39;,\n    &#39;password&#39;: &#39;hive@bigdata!23&#39;,\n    &#39;host&#39;: &#39;172.16.72.22&#39;,\n    &#39;database&#39;: &#39;azkaban3&#39;,\n    &#39;raise_on_warnings&#39;: True,\n    &#39;charset&#39;: &#39;latin1&#39;\n&#125;\n\ncnx = mysql.connector.connect(**config)\ncursor = cnx.cursor()\nquery = (&quot;SELECT trigger_id, data FROM azkaban3.triggers&quot;)\ncursor.execute(query)\n\nscheduledList = []\nfor (triggerId, triggerData) in cursor:\n    triggerJson = json.loads(gzip.decompress(bytes(triggerData, encoding=&#39;latin1&#39;)))\n    actionJson = triggerJson[&#39;actions&#39;][0][&#39;actionJson&#39;]\n    projectName = actionJson[&#39;projectName&#39;]\n    flowName = actionJson[&#39;flowName&#39;]\n    scheduledFlow = &#123;&#39;opsDateTime&#39;: opsDateTime, &#39;projectName&#39;: projectName, &#39;flowName&#39;: flowName&#125;\n    scheduledList.append(scheduledFlow)\n\ncursor.close()\ncnx.close()\n\nmongoClient = MongoClient(&#39;mongodb://172.16.72.213:27017/&#39;)\nopsDb = mongoClient.ops\nazScheduled = opsDb.azScheduledFlow\nazScheduled.insert_many(scheduledList)\n</code></pre>\n"},{"title":"REDUCE capability required is more than the supported max container capability in the cluster","date":"2017-03-03T02:56:11.000Z","_content":"\n今天在运行 MR 程序时遇到以下信息：\n\n    17/03/03 10:09:45 INFO mapreduce.Job: Job job_1488363995041_0002 failed with state KILLED due to: REDUCE capability required is more than the supported max container capability in the cluster. Killing the Job. reduceResourceRequest: <memory:4096, vCores:1> maxContainerCapability:<memory:3096, vCores:8>\n\n<!-- more -->\n\n原因是 Reduce Task 在申请资源时超过了设置的最大可申请内容量。检查 yarn-site.xml 中的配置项 yarn.scheduler.maximum-allocation-mb，如下：\n\n    <property>\n      <name>yarn.scheduler.maximum-allocation-mb</name>\n      <value>3096</value> \n    </property>\n\n修改配置为 8192（默认配置也是 8192），分发配置，并重启 ResourceManager。再次运行 MR 成功。\n\n**yarn.scheduler.minimum-allocation-mb / yarn.scheduler.maximum-allocation-mb**\n\n参数解释：单个可申请的最小/最大内存资源量。比如设置为1024和3072，则运行MapRedce作业时，每个Task最少可申请1024MB内存，最多可申请3072MB内存。\n\n默认值：1024 / 8192","source":"_posts/REDUCE-capability-required-is-more-than-the-supported-max-container-capability-in-the-cluster.md","raw":"title: >-\n  REDUCE capability required is more than the supported max container capability\n  in the cluster\ntags:\n  - Hadoop\n  - Yarn\ncategories:\n  - 大数据\n  - Hadoop\ndate: 2017-03-03 10:56:11\n---\n\n今天在运行 MR 程序时遇到以下信息：\n\n    17/03/03 10:09:45 INFO mapreduce.Job: Job job_1488363995041_0002 failed with state KILLED due to: REDUCE capability required is more than the supported max container capability in the cluster. Killing the Job. reduceResourceRequest: <memory:4096, vCores:1> maxContainerCapability:<memory:3096, vCores:8>\n\n<!-- more -->\n\n原因是 Reduce Task 在申请资源时超过了设置的最大可申请内容量。检查 yarn-site.xml 中的配置项 yarn.scheduler.maximum-allocation-mb，如下：\n\n    <property>\n      <name>yarn.scheduler.maximum-allocation-mb</name>\n      <value>3096</value> \n    </property>\n\n修改配置为 8192（默认配置也是 8192），分发配置，并重启 ResourceManager。再次运行 MR 成功。\n\n**yarn.scheduler.minimum-allocation-mb / yarn.scheduler.maximum-allocation-mb**\n\n参数解释：单个可申请的最小/最大内存资源量。比如设置为1024和3072，则运行MapRedce作业时，每个Task最少可申请1024MB内存，最多可申请3072MB内存。\n\n默认值：1024 / 8192","slug":"REDUCE-capability-required-is-more-than-the-supported-max-container-capability-in-the-cluster","published":1,"updated":"2021-07-19T16:28:00.288Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphp9009eitd33wle7qim","content":"<p>今天在运行 MR 程序时遇到以下信息：</p>\n<pre><code>17/03/03 10:09:45 INFO mapreduce.Job: Job job_1488363995041_0002 failed with state KILLED due to: REDUCE capability required is more than the supported max container capability in the cluster. Killing the Job. reduceResourceRequest: &lt;memory:4096, vCores:1&gt; maxContainerCapability:&lt;memory:3096, vCores:8&gt;\n</code></pre>\n<span id=\"more\"></span>\n\n<p>原因是 Reduce Task 在申请资源时超过了设置的最大可申请内容量。检查 yarn-site.xml 中的配置项 yarn.scheduler.maximum-allocation-mb，如下：</p>\n<pre><code>&lt;property&gt;\n  &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;\n  &lt;value&gt;3096&lt;/value&gt; \n&lt;/property&gt;\n</code></pre>\n<p>修改配置为 8192（默认配置也是 8192），分发配置，并重启 ResourceManager。再次运行 MR 成功。</p>\n<p><strong>yarn.scheduler.minimum-allocation-mb / yarn.scheduler.maximum-allocation-mb</strong></p>\n<p>参数解释：单个可申请的最小/最大内存资源量。比如设置为1024和3072，则运行MapRedce作业时，每个Task最少可申请1024MB内存，最多可申请3072MB内存。</p>\n<p>默认值：1024 / 8192</p>\n","site":{"data":{}},"excerpt":"<p>今天在运行 MR 程序时遇到以下信息：</p>\n<pre><code>17/03/03 10:09:45 INFO mapreduce.Job: Job job_1488363995041_0002 failed with state KILLED due to: REDUCE capability required is more than the supported max container capability in the cluster. Killing the Job. reduceResourceRequest: &lt;memory:4096, vCores:1&gt; maxContainerCapability:&lt;memory:3096, vCores:8&gt;\n</code></pre>","more":"<p>原因是 Reduce Task 在申请资源时超过了设置的最大可申请内容量。检查 yarn-site.xml 中的配置项 yarn.scheduler.maximum-allocation-mb，如下：</p>\n<pre><code>&lt;property&gt;\n  &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;\n  &lt;value&gt;3096&lt;/value&gt; \n&lt;/property&gt;\n</code></pre>\n<p>修改配置为 8192（默认配置也是 8192），分发配置，并重启 ResourceManager。再次运行 MR 成功。</p>\n<p><strong>yarn.scheduler.minimum-allocation-mb / yarn.scheduler.maximum-allocation-mb</strong></p>\n<p>参数解释：单个可申请的最小/最大内存资源量。比如设置为1024和3072，则运行MapRedce作业时，每个Task最少可申请1024MB内存，最多可申请3072MB内存。</p>\n<p>默认值：1024 / 8192</p>"},{"title":"Ranger 2.1.0 Admin安装","date":"2021-07-02T02:24:38.000Z","_content":"\n# Creating MySQL user ranger2 failed\n\n```Text\n2021-07-01 18:44:53,480  [E] Creating MySQL user ranger2 failed..\n2021-07-01 18:44:53,496  [E] DB schema setup failed! Please contact Administrator.\n```\n\n原因是Ranger安装程序默认会使用MySQL的root账号创建所需要的账号，也就是DBA过程。如果不需要安装程序自动创建，则将install.properties中的以下配置前的#号去掉：\n\n```Text\nsetup_mode=SeparateDBA\n```\n\n# Specified key was too long; max key length is 767 bytes\n\n- 第一种报错：\n\n```Text\nError executing: CREATE TABLE `x_portal_user` (   `id` bigint(20) NOT NULL AUTO_INCREMENT,   `create_time` datetime DEFAULT NULL,   `update_time` datetime DEFAULT NULL,   `added_by_id` bigint(20) DEFAULT NULL,   `upd_by_id` bigint(20) DEFAULT NULL,   `first_name` varchar(1022) DEFAULT NULL,   `last_name` varchar(1022) DEFAULT NULL,   `pub_scr_name` varchar(2048) DEFAULT NULL,   `login_id` varchar(767) DEFAULT NULL,   `password` varchar(512) NOT NULL,   `email` varchar(512) DEFAULT NULL,   `status` int(11) NOT NULL DEFAULT '0',   `user_src` int(11) NOT NULL DEFAULT '0',   `notes` varchar(4000) DEFAULT NULL,   `other_attributes` varchar(4000) DEFAULT NULL,   PRIMARY KEY (`id`),   UNIQUE KEY `x_portal_user_UK_login_id` (`login_id`),   UNIQUE KEY `x_portal_user_UK_email` (`email`),   KEY `x_portal_user_FK_added_by_id` (`added_by_id`),   KEY `x_portal_user_FK_upd_by_id` (`upd_by_id`),   KEY `x_portal_user_cr_time` (`create_time`),   KEY `x_portal_user_up_time` (`update_time`),   KEY `x_portal_user_name` (`first_name`(767)),   KEY `x_portal_user_email` (`email`),   CONSTRAINT `x_portal_user_FK_added_by_id` FOREIGN KEY (`added_by_id`) REFERENCES `x_portal_user` (`id`),   CONSTRAINT `x_portal_user_FK_upd_by_id` FOREIGN KEY (`upd_by_id`) REFERENCES`x_portal_user` (`id`) ) ROW_FORMAT=DYNAMIC;\njava.sql.SQLSyntaxErrorException: Specified key was too long; max key length is 767 bytes\nSQLException : SQL state: 42000 java.sql.SQLSyntaxErrorException: Specified key was too long; max key length is 767 bytes ErrorCode: 1071\n2021-07-02 10:44:54,411  [E] ranger_core_db_mysql.sql file import failed!\n```\n\n修改对应字段的长度可以解决。注意UTF8每个字符4个字节。\n\n- 第二种报错：\n\n```Text\nError executing: CREATE TABLE `x_portal_user` (   `id` bigint(20) NOT NULL AUTO_INCREMENT,   `create_time` datetime DEFAULT NULL,   `update_time` datetime DEFAULT NULL,   `added_by_id` bigint(20) DEFAULT NULL,   `upd_by_id` bigint(20) DEFAULT NULL,   `first_name` varchar(128) DEFAULT NULL,   `last_name` varchar(1022) DEFAULT NULL,   `pub_scr_name` varchar(2048) DEFAULT NULL,  `login_id` varchar(128) DEFAULT NULL,   `password` varchar(512) NOT NULL,   `email` varchar(128) DEFAULT NULL,   `status` int(11) NOT NULL DEFAULT '0',   `user_src` int(11) NOT NULL DEFAULT '0',   `notes` varchar(4000) DEFAULT NULL,   `other_attributes` varchar(4000) DEFAULT NULL,   PRIMARY KEY (`id`),   UNIQUE KEY `x_portal_user_UK_login_id` (`login_id`),   UNIQUE KEY `x_portal_user_UK_email` (`email`),   KEY `x_portal_user_FK_added_by_id` (`added_by_id`),   KEY `x_portal_user_FK_upd_by_id` (`upd_by_id`),   KEY `x_portal_user_cr_time` (`create_time`),   KEY `x_portal_user_up_time` (`update_time`),   KEY `x_portal_user_name` (`first_name`(767)),   KEY `x_portal_user_email` (`email`),   CONSTRAINT `x_portal_user_FK_added_by_id` FOREIGN KEY (`added_by_id`) REFERENCES `x_portal_user` (`id`),   CONSTRAINT `x_portal_user_FK_upd_by_id` FOREIGN KEY (`upd_by_id`) REFERENCES `x_portal_user` (`id`) ) ROW_FORMAT=DYNAMIC;\njava.sql.SQLException: Incorrect prefix key; the used key part isn't a string, the used length is longer than the key part, or the storage engine doesn't support unique prefix keys\nSQLException : SQL state: HY000 java.sql.SQLException: Incorrect prefix key; the used key part isn't a string, the used length is longer than the key part, or the storage engine doesn't support unique prefix keys ErrorCode: 1089\n2021-07-02 11:51:23,763  [E] ranger_core_db_mysql.sql file import failed!\n```\n\n去掉prefix key的定义即可。\n\n修改后的建表语句如下：\n\n```SQL\nCREATE TABLE `x_portal_user` (\n  `id` bigint(20) NOT NULL AUTO_INCREMENT,\n  `create_time` datetime DEFAULT NULL,\n  `update_time` datetime DEFAULT NULL,\n  `added_by_id` bigint(20) DEFAULT NULL,\n  `upd_by_id` bigint(20) DEFAULT NULL,\n  `first_name` varchar(128) DEFAULT NULL,\n  `last_name` varchar(1022) DEFAULT NULL,\n  `pub_scr_name` varchar(2048) DEFAULT NULL,\n  `login_id` varchar(128) DEFAULT NULL,\n  `password` varchar(512) NOT NULL,\n  `email` varchar(128) DEFAULT NULL,\n  `status` int(11) NOT NULL DEFAULT '0',\n  `user_src` int(11) NOT NULL DEFAULT '0',\n  `notes` varchar(4000) DEFAULT NULL,\n  `other_attributes` varchar(4000) DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  UNIQUE KEY `x_portal_user_UK_login_id` (`login_id`),\n  UNIQUE KEY `x_portal_user_UK_email` (`email`),\n  KEY `x_portal_user_FK_added_by_id` (`added_by_id`),\n  KEY `x_portal_user_FK_upd_by_id` (`upd_by_id`),\n  KEY `x_portal_user_cr_time` (`create_time`),\n  KEY `x_portal_user_up_time` (`update_time`),\n  KEY `x_portal_user_name` (`first_name`),\n  KEY `x_portal_user_email` (`email`),\n  CONSTRAINT `x_portal_user_FK_added_by_id` FOREIGN KEY (`added_by_id`) REFERENCES `x_portal_user` (`id`),\n  CONSTRAINT `x_portal_user_FK_upd_by_id` FOREIGN KEY (`upd_by_id`) REFERENCES `x_portal_user` (`id`)\n) ROW_FORMAT=DYNAMIC;\n```\n\n> 对整个建表脚本中的SQL都需要对应的修改。","source":"_posts/Ranger-2-1-0-Admin安装.md","raw":"title: Ranger 2.1.0 Admin安装\ndate: 2021-07-02 10:24:38\ntags:\n- 大数据\n- Ranger\ncategories:\n- 大数据\n- Ranger\n---\n\n# Creating MySQL user ranger2 failed\n\n```Text\n2021-07-01 18:44:53,480  [E] Creating MySQL user ranger2 failed..\n2021-07-01 18:44:53,496  [E] DB schema setup failed! Please contact Administrator.\n```\n\n原因是Ranger安装程序默认会使用MySQL的root账号创建所需要的账号，也就是DBA过程。如果不需要安装程序自动创建，则将install.properties中的以下配置前的#号去掉：\n\n```Text\nsetup_mode=SeparateDBA\n```\n\n# Specified key was too long; max key length is 767 bytes\n\n- 第一种报错：\n\n```Text\nError executing: CREATE TABLE `x_portal_user` (   `id` bigint(20) NOT NULL AUTO_INCREMENT,   `create_time` datetime DEFAULT NULL,   `update_time` datetime DEFAULT NULL,   `added_by_id` bigint(20) DEFAULT NULL,   `upd_by_id` bigint(20) DEFAULT NULL,   `first_name` varchar(1022) DEFAULT NULL,   `last_name` varchar(1022) DEFAULT NULL,   `pub_scr_name` varchar(2048) DEFAULT NULL,   `login_id` varchar(767) DEFAULT NULL,   `password` varchar(512) NOT NULL,   `email` varchar(512) DEFAULT NULL,   `status` int(11) NOT NULL DEFAULT '0',   `user_src` int(11) NOT NULL DEFAULT '0',   `notes` varchar(4000) DEFAULT NULL,   `other_attributes` varchar(4000) DEFAULT NULL,   PRIMARY KEY (`id`),   UNIQUE KEY `x_portal_user_UK_login_id` (`login_id`),   UNIQUE KEY `x_portal_user_UK_email` (`email`),   KEY `x_portal_user_FK_added_by_id` (`added_by_id`),   KEY `x_portal_user_FK_upd_by_id` (`upd_by_id`),   KEY `x_portal_user_cr_time` (`create_time`),   KEY `x_portal_user_up_time` (`update_time`),   KEY `x_portal_user_name` (`first_name`(767)),   KEY `x_portal_user_email` (`email`),   CONSTRAINT `x_portal_user_FK_added_by_id` FOREIGN KEY (`added_by_id`) REFERENCES `x_portal_user` (`id`),   CONSTRAINT `x_portal_user_FK_upd_by_id` FOREIGN KEY (`upd_by_id`) REFERENCES`x_portal_user` (`id`) ) ROW_FORMAT=DYNAMIC;\njava.sql.SQLSyntaxErrorException: Specified key was too long; max key length is 767 bytes\nSQLException : SQL state: 42000 java.sql.SQLSyntaxErrorException: Specified key was too long; max key length is 767 bytes ErrorCode: 1071\n2021-07-02 10:44:54,411  [E] ranger_core_db_mysql.sql file import failed!\n```\n\n修改对应字段的长度可以解决。注意UTF8每个字符4个字节。\n\n- 第二种报错：\n\n```Text\nError executing: CREATE TABLE `x_portal_user` (   `id` bigint(20) NOT NULL AUTO_INCREMENT,   `create_time` datetime DEFAULT NULL,   `update_time` datetime DEFAULT NULL,   `added_by_id` bigint(20) DEFAULT NULL,   `upd_by_id` bigint(20) DEFAULT NULL,   `first_name` varchar(128) DEFAULT NULL,   `last_name` varchar(1022) DEFAULT NULL,   `pub_scr_name` varchar(2048) DEFAULT NULL,  `login_id` varchar(128) DEFAULT NULL,   `password` varchar(512) NOT NULL,   `email` varchar(128) DEFAULT NULL,   `status` int(11) NOT NULL DEFAULT '0',   `user_src` int(11) NOT NULL DEFAULT '0',   `notes` varchar(4000) DEFAULT NULL,   `other_attributes` varchar(4000) DEFAULT NULL,   PRIMARY KEY (`id`),   UNIQUE KEY `x_portal_user_UK_login_id` (`login_id`),   UNIQUE KEY `x_portal_user_UK_email` (`email`),   KEY `x_portal_user_FK_added_by_id` (`added_by_id`),   KEY `x_portal_user_FK_upd_by_id` (`upd_by_id`),   KEY `x_portal_user_cr_time` (`create_time`),   KEY `x_portal_user_up_time` (`update_time`),   KEY `x_portal_user_name` (`first_name`(767)),   KEY `x_portal_user_email` (`email`),   CONSTRAINT `x_portal_user_FK_added_by_id` FOREIGN KEY (`added_by_id`) REFERENCES `x_portal_user` (`id`),   CONSTRAINT `x_portal_user_FK_upd_by_id` FOREIGN KEY (`upd_by_id`) REFERENCES `x_portal_user` (`id`) ) ROW_FORMAT=DYNAMIC;\njava.sql.SQLException: Incorrect prefix key; the used key part isn't a string, the used length is longer than the key part, or the storage engine doesn't support unique prefix keys\nSQLException : SQL state: HY000 java.sql.SQLException: Incorrect prefix key; the used key part isn't a string, the used length is longer than the key part, or the storage engine doesn't support unique prefix keys ErrorCode: 1089\n2021-07-02 11:51:23,763  [E] ranger_core_db_mysql.sql file import failed!\n```\n\n去掉prefix key的定义即可。\n\n修改后的建表语句如下：\n\n```SQL\nCREATE TABLE `x_portal_user` (\n  `id` bigint(20) NOT NULL AUTO_INCREMENT,\n  `create_time` datetime DEFAULT NULL,\n  `update_time` datetime DEFAULT NULL,\n  `added_by_id` bigint(20) DEFAULT NULL,\n  `upd_by_id` bigint(20) DEFAULT NULL,\n  `first_name` varchar(128) DEFAULT NULL,\n  `last_name` varchar(1022) DEFAULT NULL,\n  `pub_scr_name` varchar(2048) DEFAULT NULL,\n  `login_id` varchar(128) DEFAULT NULL,\n  `password` varchar(512) NOT NULL,\n  `email` varchar(128) DEFAULT NULL,\n  `status` int(11) NOT NULL DEFAULT '0',\n  `user_src` int(11) NOT NULL DEFAULT '0',\n  `notes` varchar(4000) DEFAULT NULL,\n  `other_attributes` varchar(4000) DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  UNIQUE KEY `x_portal_user_UK_login_id` (`login_id`),\n  UNIQUE KEY `x_portal_user_UK_email` (`email`),\n  KEY `x_portal_user_FK_added_by_id` (`added_by_id`),\n  KEY `x_portal_user_FK_upd_by_id` (`upd_by_id`),\n  KEY `x_portal_user_cr_time` (`create_time`),\n  KEY `x_portal_user_up_time` (`update_time`),\n  KEY `x_portal_user_name` (`first_name`),\n  KEY `x_portal_user_email` (`email`),\n  CONSTRAINT `x_portal_user_FK_added_by_id` FOREIGN KEY (`added_by_id`) REFERENCES `x_portal_user` (`id`),\n  CONSTRAINT `x_portal_user_FK_upd_by_id` FOREIGN KEY (`upd_by_id`) REFERENCES `x_portal_user` (`id`)\n) ROW_FORMAT=DYNAMIC;\n```\n\n> 对整个建表脚本中的SQL都需要对应的修改。","slug":"Ranger-2-1-0-Admin安装","published":1,"updated":"2021-07-19T16:28:00.188Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphpa009iitd3awb0fihr","content":"<h1 id=\"Creating-MySQL-user-ranger2-failed\"><a href=\"#Creating-MySQL-user-ranger2-failed\" class=\"headerlink\" title=\"Creating MySQL user ranger2 failed\"></a>Creating MySQL user ranger2 failed</h1><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">2021-07-01 18:44:53,480  [E] Creating MySQL user ranger2 failed..</span><br><span class=\"line\">2021-07-01 18:44:53,496  [E] DB schema setup failed! Please contact Administrator.</span><br></pre></td></tr></table></figure>\n\n<p>原因是Ranger安装程序默认会使用MySQL的root账号创建所需要的账号，也就是DBA过程。如果不需要安装程序自动创建，则将install.properties中的以下配置前的#号去掉：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">setup_mode=SeparateDBA</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Specified-key-was-too-long-max-key-length-is-767-bytes\"><a href=\"#Specified-key-was-too-long-max-key-length-is-767-bytes\" class=\"headerlink\" title=\"Specified key was too long; max key length is 767 bytes\"></a>Specified key was too long; max key length is 767 bytes</h1><ul>\n<li>第一种报错：</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Error executing: CREATE TABLE `x_portal_user` (   `id` bigint(20) NOT NULL AUTO_INCREMENT,   `create_time` datetime DEFAULT NULL,   `update_time` datetime DEFAULT NULL,   `added_by_id` bigint(20) DEFAULT NULL,   `upd_by_id` bigint(20) DEFAULT NULL,   `first_name` varchar(1022) DEFAULT NULL,   `last_name` varchar(1022) DEFAULT NULL,   `pub_scr_name` varchar(2048) DEFAULT NULL,   `login_id` varchar(767) DEFAULT NULL,   `password` varchar(512) NOT NULL,   `email` varchar(512) DEFAULT NULL,   `status` int(11) NOT NULL DEFAULT &#x27;0&#x27;,   `user_src` int(11) NOT NULL DEFAULT &#x27;0&#x27;,   `notes` varchar(4000) DEFAULT NULL,   `other_attributes` varchar(4000) DEFAULT NULL,   PRIMARY KEY (`id`),   UNIQUE KEY `x_portal_user_UK_login_id` (`login_id`),   UNIQUE KEY `x_portal_user_UK_email` (`email`),   KEY `x_portal_user_FK_added_by_id` (`added_by_id`),   KEY `x_portal_user_FK_upd_by_id` (`upd_by_id`),   KEY `x_portal_user_cr_time` (`create_time`),   KEY `x_portal_user_up_time` (`update_time`),   KEY `x_portal_user_name` (`first_name`(767)),   KEY `x_portal_user_email` (`email`),   CONSTRAINT `x_portal_user_FK_added_by_id` FOREIGN KEY (`added_by_id`) REFERENCES `x_portal_user` (`id`),   CONSTRAINT `x_portal_user_FK_upd_by_id` FOREIGN KEY (`upd_by_id`) REFERENCES`x_portal_user` (`id`) ) ROW_FORMAT=DYNAMIC;</span><br><span class=\"line\">java.sql.SQLSyntaxErrorException: Specified key was too long; max key length is 767 bytes</span><br><span class=\"line\">SQLException : SQL state: 42000 java.sql.SQLSyntaxErrorException: Specified key was too long; max key length is 767 bytes ErrorCode: 1071</span><br><span class=\"line\">2021-07-02 10:44:54,411  [E] ranger_core_db_mysql.sql file import failed!</span><br></pre></td></tr></table></figure>\n\n<p>修改对应字段的长度可以解决。注意UTF8每个字符4个字节。</p>\n<ul>\n<li>第二种报错：</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Error executing: CREATE TABLE `x_portal_user` (   `id` bigint(20) NOT NULL AUTO_INCREMENT,   `create_time` datetime DEFAULT NULL,   `update_time` datetime DEFAULT NULL,   `added_by_id` bigint(20) DEFAULT NULL,   `upd_by_id` bigint(20) DEFAULT NULL,   `first_name` varchar(128) DEFAULT NULL,   `last_name` varchar(1022) DEFAULT NULL,   `pub_scr_name` varchar(2048) DEFAULT NULL,  `login_id` varchar(128) DEFAULT NULL,   `password` varchar(512) NOT NULL,   `email` varchar(128) DEFAULT NULL,   `status` int(11) NOT NULL DEFAULT &#x27;0&#x27;,   `user_src` int(11) NOT NULL DEFAULT &#x27;0&#x27;,   `notes` varchar(4000) DEFAULT NULL,   `other_attributes` varchar(4000) DEFAULT NULL,   PRIMARY KEY (`id`),   UNIQUE KEY `x_portal_user_UK_login_id` (`login_id`),   UNIQUE KEY `x_portal_user_UK_email` (`email`),   KEY `x_portal_user_FK_added_by_id` (`added_by_id`),   KEY `x_portal_user_FK_upd_by_id` (`upd_by_id`),   KEY `x_portal_user_cr_time` (`create_time`),   KEY `x_portal_user_up_time` (`update_time`),   KEY `x_portal_user_name` (`first_name`(767)),   KEY `x_portal_user_email` (`email`),   CONSTRAINT `x_portal_user_FK_added_by_id` FOREIGN KEY (`added_by_id`) REFERENCES `x_portal_user` (`id`),   CONSTRAINT `x_portal_user_FK_upd_by_id` FOREIGN KEY (`upd_by_id`) REFERENCES `x_portal_user` (`id`) ) ROW_FORMAT=DYNAMIC;</span><br><span class=\"line\">java.sql.SQLException: Incorrect prefix key; the used key part isn&#x27;t a string, the used length is longer than the key part, or the storage engine doesn&#x27;t support unique prefix keys</span><br><span class=\"line\">SQLException : SQL state: HY000 java.sql.SQLException: Incorrect prefix key; the used key part isn&#x27;t a string, the used length is longer than the key part, or the storage engine doesn&#x27;t support unique prefix keys ErrorCode: 1089</span><br><span class=\"line\">2021-07-02 11:51:23,763  [E] ranger_core_db_mysql.sql file import failed!</span><br></pre></td></tr></table></figure>\n\n<p>去掉prefix key的定义即可。</p>\n<p>修改后的建表语句如下：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> `x_portal_user` (</span><br><span class=\"line\">  `id` <span class=\"type\">bigint</span>(<span class=\"number\">20</span>) <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> AUTO_INCREMENT,</span><br><span class=\"line\">  `create_time` datetime <span class=\"keyword\">DEFAULT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `update_time` datetime <span class=\"keyword\">DEFAULT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `added_by_id` <span class=\"type\">bigint</span>(<span class=\"number\">20</span>) <span class=\"keyword\">DEFAULT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `upd_by_id` <span class=\"type\">bigint</span>(<span class=\"number\">20</span>) <span class=\"keyword\">DEFAULT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `first_name` <span class=\"type\">varchar</span>(<span class=\"number\">128</span>) <span class=\"keyword\">DEFAULT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `last_name` <span class=\"type\">varchar</span>(<span class=\"number\">1022</span>) <span class=\"keyword\">DEFAULT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `pub_scr_name` <span class=\"type\">varchar</span>(<span class=\"number\">2048</span>) <span class=\"keyword\">DEFAULT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `login_id` <span class=\"type\">varchar</span>(<span class=\"number\">128</span>) <span class=\"keyword\">DEFAULT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `password` <span class=\"type\">varchar</span>(<span class=\"number\">512</span>) <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `email` <span class=\"type\">varchar</span>(<span class=\"number\">128</span>) <span class=\"keyword\">DEFAULT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `status` <span class=\"type\">int</span>(<span class=\"number\">11</span>) <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> <span class=\"keyword\">DEFAULT</span> <span class=\"string\">&#x27;0&#x27;</span>,</span><br><span class=\"line\">  `user_src` <span class=\"type\">int</span>(<span class=\"number\">11</span>) <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> <span class=\"keyword\">DEFAULT</span> <span class=\"string\">&#x27;0&#x27;</span>,</span><br><span class=\"line\">  `notes` <span class=\"type\">varchar</span>(<span class=\"number\">4000</span>) <span class=\"keyword\">DEFAULT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `other_attributes` <span class=\"type\">varchar</span>(<span class=\"number\">4000</span>) <span class=\"keyword\">DEFAULT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  <span class=\"keyword\">PRIMARY</span> KEY (`id`),</span><br><span class=\"line\">  <span class=\"keyword\">UNIQUE</span> KEY `x_portal_user_UK_login_id` (`login_id`),</span><br><span class=\"line\">  <span class=\"keyword\">UNIQUE</span> KEY `x_portal_user_UK_email` (`email`),</span><br><span class=\"line\">  KEY `x_portal_user_FK_added_by_id` (`added_by_id`),</span><br><span class=\"line\">  KEY `x_portal_user_FK_upd_by_id` (`upd_by_id`),</span><br><span class=\"line\">  KEY `x_portal_user_cr_time` (`create_time`),</span><br><span class=\"line\">  KEY `x_portal_user_up_time` (`update_time`),</span><br><span class=\"line\">  KEY `x_portal_user_name` (`first_name`),</span><br><span class=\"line\">  KEY `x_portal_user_email` (`email`),</span><br><span class=\"line\">  <span class=\"keyword\">CONSTRAINT</span> `x_portal_user_FK_added_by_id` <span class=\"keyword\">FOREIGN</span> KEY (`added_by_id`) <span class=\"keyword\">REFERENCES</span> `x_portal_user` (`id`),</span><br><span class=\"line\">  <span class=\"keyword\">CONSTRAINT</span> `x_portal_user_FK_upd_by_id` <span class=\"keyword\">FOREIGN</span> KEY (`upd_by_id`) <span class=\"keyword\">REFERENCES</span> `x_portal_user` (`id`)</span><br><span class=\"line\">) ROW_FORMAT<span class=\"operator\">=</span><span class=\"keyword\">DYNAMIC</span>;</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p>对整个建表脚本中的SQL都需要对应的修改。</p>\n</blockquote>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Creating-MySQL-user-ranger2-failed\"><a href=\"#Creating-MySQL-user-ranger2-failed\" class=\"headerlink\" title=\"Creating MySQL user ranger2 failed\"></a>Creating MySQL user ranger2 failed</h1><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">2021-07-01 18:44:53,480  [E] Creating MySQL user ranger2 failed..</span><br><span class=\"line\">2021-07-01 18:44:53,496  [E] DB schema setup failed! Please contact Administrator.</span><br></pre></td></tr></table></figure>\n\n<p>原因是Ranger安装程序默认会使用MySQL的root账号创建所需要的账号，也就是DBA过程。如果不需要安装程序自动创建，则将install.properties中的以下配置前的#号去掉：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">setup_mode=SeparateDBA</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Specified-key-was-too-long-max-key-length-is-767-bytes\"><a href=\"#Specified-key-was-too-long-max-key-length-is-767-bytes\" class=\"headerlink\" title=\"Specified key was too long; max key length is 767 bytes\"></a>Specified key was too long; max key length is 767 bytes</h1><ul>\n<li>第一种报错：</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Error executing: CREATE TABLE `x_portal_user` (   `id` bigint(20) NOT NULL AUTO_INCREMENT,   `create_time` datetime DEFAULT NULL,   `update_time` datetime DEFAULT NULL,   `added_by_id` bigint(20) DEFAULT NULL,   `upd_by_id` bigint(20) DEFAULT NULL,   `first_name` varchar(1022) DEFAULT NULL,   `last_name` varchar(1022) DEFAULT NULL,   `pub_scr_name` varchar(2048) DEFAULT NULL,   `login_id` varchar(767) DEFAULT NULL,   `password` varchar(512) NOT NULL,   `email` varchar(512) DEFAULT NULL,   `status` int(11) NOT NULL DEFAULT &#x27;0&#x27;,   `user_src` int(11) NOT NULL DEFAULT &#x27;0&#x27;,   `notes` varchar(4000) DEFAULT NULL,   `other_attributes` varchar(4000) DEFAULT NULL,   PRIMARY KEY (`id`),   UNIQUE KEY `x_portal_user_UK_login_id` (`login_id`),   UNIQUE KEY `x_portal_user_UK_email` (`email`),   KEY `x_portal_user_FK_added_by_id` (`added_by_id`),   KEY `x_portal_user_FK_upd_by_id` (`upd_by_id`),   KEY `x_portal_user_cr_time` (`create_time`),   KEY `x_portal_user_up_time` (`update_time`),   KEY `x_portal_user_name` (`first_name`(767)),   KEY `x_portal_user_email` (`email`),   CONSTRAINT `x_portal_user_FK_added_by_id` FOREIGN KEY (`added_by_id`) REFERENCES `x_portal_user` (`id`),   CONSTRAINT `x_portal_user_FK_upd_by_id` FOREIGN KEY (`upd_by_id`) REFERENCES`x_portal_user` (`id`) ) ROW_FORMAT=DYNAMIC;</span><br><span class=\"line\">java.sql.SQLSyntaxErrorException: Specified key was too long; max key length is 767 bytes</span><br><span class=\"line\">SQLException : SQL state: 42000 java.sql.SQLSyntaxErrorException: Specified key was too long; max key length is 767 bytes ErrorCode: 1071</span><br><span class=\"line\">2021-07-02 10:44:54,411  [E] ranger_core_db_mysql.sql file import failed!</span><br></pre></td></tr></table></figure>\n\n<p>修改对应字段的长度可以解决。注意UTF8每个字符4个字节。</p>\n<ul>\n<li>第二种报错：</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Error executing: CREATE TABLE `x_portal_user` (   `id` bigint(20) NOT NULL AUTO_INCREMENT,   `create_time` datetime DEFAULT NULL,   `update_time` datetime DEFAULT NULL,   `added_by_id` bigint(20) DEFAULT NULL,   `upd_by_id` bigint(20) DEFAULT NULL,   `first_name` varchar(128) DEFAULT NULL,   `last_name` varchar(1022) DEFAULT NULL,   `pub_scr_name` varchar(2048) DEFAULT NULL,  `login_id` varchar(128) DEFAULT NULL,   `password` varchar(512) NOT NULL,   `email` varchar(128) DEFAULT NULL,   `status` int(11) NOT NULL DEFAULT &#x27;0&#x27;,   `user_src` int(11) NOT NULL DEFAULT &#x27;0&#x27;,   `notes` varchar(4000) DEFAULT NULL,   `other_attributes` varchar(4000) DEFAULT NULL,   PRIMARY KEY (`id`),   UNIQUE KEY `x_portal_user_UK_login_id` (`login_id`),   UNIQUE KEY `x_portal_user_UK_email` (`email`),   KEY `x_portal_user_FK_added_by_id` (`added_by_id`),   KEY `x_portal_user_FK_upd_by_id` (`upd_by_id`),   KEY `x_portal_user_cr_time` (`create_time`),   KEY `x_portal_user_up_time` (`update_time`),   KEY `x_portal_user_name` (`first_name`(767)),   KEY `x_portal_user_email` (`email`),   CONSTRAINT `x_portal_user_FK_added_by_id` FOREIGN KEY (`added_by_id`) REFERENCES `x_portal_user` (`id`),   CONSTRAINT `x_portal_user_FK_upd_by_id` FOREIGN KEY (`upd_by_id`) REFERENCES `x_portal_user` (`id`) ) ROW_FORMAT=DYNAMIC;</span><br><span class=\"line\">java.sql.SQLException: Incorrect prefix key; the used key part isn&#x27;t a string, the used length is longer than the key part, or the storage engine doesn&#x27;t support unique prefix keys</span><br><span class=\"line\">SQLException : SQL state: HY000 java.sql.SQLException: Incorrect prefix key; the used key part isn&#x27;t a string, the used length is longer than the key part, or the storage engine doesn&#x27;t support unique prefix keys ErrorCode: 1089</span><br><span class=\"line\">2021-07-02 11:51:23,763  [E] ranger_core_db_mysql.sql file import failed!</span><br></pre></td></tr></table></figure>\n\n<p>去掉prefix key的定义即可。</p>\n<p>修改后的建表语句如下：</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">TABLE</span> `x_portal_user` (</span><br><span class=\"line\">  `id` <span class=\"type\">bigint</span>(<span class=\"number\">20</span>) <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> AUTO_INCREMENT,</span><br><span class=\"line\">  `create_time` datetime <span class=\"keyword\">DEFAULT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `update_time` datetime <span class=\"keyword\">DEFAULT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `added_by_id` <span class=\"type\">bigint</span>(<span class=\"number\">20</span>) <span class=\"keyword\">DEFAULT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `upd_by_id` <span class=\"type\">bigint</span>(<span class=\"number\">20</span>) <span class=\"keyword\">DEFAULT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `first_name` <span class=\"type\">varchar</span>(<span class=\"number\">128</span>) <span class=\"keyword\">DEFAULT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `last_name` <span class=\"type\">varchar</span>(<span class=\"number\">1022</span>) <span class=\"keyword\">DEFAULT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `pub_scr_name` <span class=\"type\">varchar</span>(<span class=\"number\">2048</span>) <span class=\"keyword\">DEFAULT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `login_id` <span class=\"type\">varchar</span>(<span class=\"number\">128</span>) <span class=\"keyword\">DEFAULT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `password` <span class=\"type\">varchar</span>(<span class=\"number\">512</span>) <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `email` <span class=\"type\">varchar</span>(<span class=\"number\">128</span>) <span class=\"keyword\">DEFAULT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `status` <span class=\"type\">int</span>(<span class=\"number\">11</span>) <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> <span class=\"keyword\">DEFAULT</span> <span class=\"string\">&#x27;0&#x27;</span>,</span><br><span class=\"line\">  `user_src` <span class=\"type\">int</span>(<span class=\"number\">11</span>) <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span> <span class=\"keyword\">DEFAULT</span> <span class=\"string\">&#x27;0&#x27;</span>,</span><br><span class=\"line\">  `notes` <span class=\"type\">varchar</span>(<span class=\"number\">4000</span>) <span class=\"keyword\">DEFAULT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  `other_attributes` <span class=\"type\">varchar</span>(<span class=\"number\">4000</span>) <span class=\"keyword\">DEFAULT</span> <span class=\"keyword\">NULL</span>,</span><br><span class=\"line\">  <span class=\"keyword\">PRIMARY</span> KEY (`id`),</span><br><span class=\"line\">  <span class=\"keyword\">UNIQUE</span> KEY `x_portal_user_UK_login_id` (`login_id`),</span><br><span class=\"line\">  <span class=\"keyword\">UNIQUE</span> KEY `x_portal_user_UK_email` (`email`),</span><br><span class=\"line\">  KEY `x_portal_user_FK_added_by_id` (`added_by_id`),</span><br><span class=\"line\">  KEY `x_portal_user_FK_upd_by_id` (`upd_by_id`),</span><br><span class=\"line\">  KEY `x_portal_user_cr_time` (`create_time`),</span><br><span class=\"line\">  KEY `x_portal_user_up_time` (`update_time`),</span><br><span class=\"line\">  KEY `x_portal_user_name` (`first_name`),</span><br><span class=\"line\">  KEY `x_portal_user_email` (`email`),</span><br><span class=\"line\">  <span class=\"keyword\">CONSTRAINT</span> `x_portal_user_FK_added_by_id` <span class=\"keyword\">FOREIGN</span> KEY (`added_by_id`) <span class=\"keyword\">REFERENCES</span> `x_portal_user` (`id`),</span><br><span class=\"line\">  <span class=\"keyword\">CONSTRAINT</span> `x_portal_user_FK_upd_by_id` <span class=\"keyword\">FOREIGN</span> KEY (`upd_by_id`) <span class=\"keyword\">REFERENCES</span> `x_portal_user` (`id`)</span><br><span class=\"line\">) ROW_FORMAT<span class=\"operator\">=</span><span class=\"keyword\">DYNAMIC</span>;</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p>对整个建表脚本中的SQL都需要对应的修改。</p>\n</blockquote>\n"},{"title":"Ranger Admin安装MySQL初始化问题解决","date":"2020-12-01T08:47:49.000Z","_content":"\n告警信息及如何解决见我的另外一篇博客：http://zhang-jc.github.io/2020/11/27/MySQL-JDBC-%E8%BF%9E%E6%8E%A5%E5%BC%82%E5%B8%B8%EF%BC%9Ajavax-net-ssl-SSLException-closing-inbound-before-receiving-peer-s-close-notify/\n\n但是Ranger的安装配置install.properties中未启用ssl，配置信息如下：\n\n\t#SSL config\n\tdb_ssl_enabled=false\n\tdb_ssl_required=false\n\tdb_ssl_verifyServerCertificate=false\n\t#db_ssl_auth_type=1-way|2-way, where 1-way represents standard one way ssl authentication and 2-way represents mutual ssl authentication\n\tdb_ssl_auth_type=2-way\n\tjavax_net_ssl_keyStore=\n\tjavax_net_ssl_keyStorePassword=\n\tjavax_net_ssl_trustStore=\n\tjavax_net_ssl_trustStorePassword=\n\t\n跟踪源代码发现安装过程只处理了启用ssl的情况，但是新版本的mysql默认是需要启用ssl的。修改Ranger安装源代码（apache-ranger-1.2.0/security-admin/scripts/db_setup.py）增加下面代码中的221、222行，如下：\n\n    210         def get_jisql_cmd(self, user, password ,db_name):\n    211                 path = RANGER_ADMIN_HOME\n    212                 db_ssl_param=''\n    213                 db_ssl_cert_param=''\n    214                 if self.db_ssl_enabled == 'true':\n    215                         db_ssl_param=\"?useSSL=%s&requireSSL=%s&verifyServerCertificate=%s\" %(self.db_ssl_enabled,self.db_ssl_required,self.db_ssl_verifyServerCertificate)\n    216                         if self.db_ssl_verifyServerCertificate == 'true':\n    217                                 if self.db_ssl_auth_type == '1-way':\n    218                                         db_ssl_cert_param=\" -Djavax.net.ssl.trustStore=%s -Djavax.net.ssl.trustStorePassword=%s \" %(self.javax_net_ssl_trustStore,self.javax_net_ssl_trustStorePassword)\n    219                                 else:\n    220                                         db_ssl_cert_param=\" -Djavax.net.ssl.keyStore=%s -Djavax.net.ssl.keyStorePassword=%s -Djavax.net.ssl.trustStore=%s -Djavax.net.ssl.trustStorePassword=%s \" %(self.javax_net_ssl_keyStore,self.javax_net_ssl_keyStorePassword,self.javax_net_ssl_trustStore,self.javax_net_ssl_trustStorePassword)\n    221                 else:\n    222                         db_ssl_param='?useSSL=false'","source":"_posts/Ranger-Admin安装MySQL初始化问题解决.md","raw":"title: Ranger Admin安装MySQL初始化问题解决\ndate: 2020-12-01 16:47:49\ntags:\n- 大数据\n- Ranger\ncategories:\n- 大数据\n- Ranger\n---\n\n告警信息及如何解决见我的另外一篇博客：http://zhang-jc.github.io/2020/11/27/MySQL-JDBC-%E8%BF%9E%E6%8E%A5%E5%BC%82%E5%B8%B8%EF%BC%9Ajavax-net-ssl-SSLException-closing-inbound-before-receiving-peer-s-close-notify/\n\n但是Ranger的安装配置install.properties中未启用ssl，配置信息如下：\n\n\t#SSL config\n\tdb_ssl_enabled=false\n\tdb_ssl_required=false\n\tdb_ssl_verifyServerCertificate=false\n\t#db_ssl_auth_type=1-way|2-way, where 1-way represents standard one way ssl authentication and 2-way represents mutual ssl authentication\n\tdb_ssl_auth_type=2-way\n\tjavax_net_ssl_keyStore=\n\tjavax_net_ssl_keyStorePassword=\n\tjavax_net_ssl_trustStore=\n\tjavax_net_ssl_trustStorePassword=\n\t\n跟踪源代码发现安装过程只处理了启用ssl的情况，但是新版本的mysql默认是需要启用ssl的。修改Ranger安装源代码（apache-ranger-1.2.0/security-admin/scripts/db_setup.py）增加下面代码中的221、222行，如下：\n\n    210         def get_jisql_cmd(self, user, password ,db_name):\n    211                 path = RANGER_ADMIN_HOME\n    212                 db_ssl_param=''\n    213                 db_ssl_cert_param=''\n    214                 if self.db_ssl_enabled == 'true':\n    215                         db_ssl_param=\"?useSSL=%s&requireSSL=%s&verifyServerCertificate=%s\" %(self.db_ssl_enabled,self.db_ssl_required,self.db_ssl_verifyServerCertificate)\n    216                         if self.db_ssl_verifyServerCertificate == 'true':\n    217                                 if self.db_ssl_auth_type == '1-way':\n    218                                         db_ssl_cert_param=\" -Djavax.net.ssl.trustStore=%s -Djavax.net.ssl.trustStorePassword=%s \" %(self.javax_net_ssl_trustStore,self.javax_net_ssl_trustStorePassword)\n    219                                 else:\n    220                                         db_ssl_cert_param=\" -Djavax.net.ssl.keyStore=%s -Djavax.net.ssl.keyStorePassword=%s -Djavax.net.ssl.trustStore=%s -Djavax.net.ssl.trustStorePassword=%s \" %(self.javax_net_ssl_keyStore,self.javax_net_ssl_keyStorePassword,self.javax_net_ssl_trustStore,self.javax_net_ssl_trustStorePassword)\n    221                 else:\n    222                         db_ssl_param='?useSSL=false'","slug":"Ranger-Admin安装MySQL初始化问题解决","published":1,"updated":"2021-07-19T16:28:00.076Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphpb009mitd308t9enft","content":"<p>告警信息及如何解决见我的另外一篇博客：<a href=\"http://zhang-jc.github.io/2020/11/27/MySQL-JDBC-%E8%BF%9E%E6%8E%A5%E5%BC%82%E5%B8%B8%EF%BC%9Ajavax-net-ssl-SSLException-closing-inbound-before-receiving-peer-s-close-notify/\">http://zhang-jc.github.io/2020/11/27/MySQL-JDBC-%E8%BF%9E%E6%8E%A5%E5%BC%82%E5%B8%B8%EF%BC%9Ajavax-net-ssl-SSLException-closing-inbound-before-receiving-peer-s-close-notify/</a></p>\n<p>但是Ranger的安装配置install.properties中未启用ssl，配置信息如下：</p>\n<pre><code>#SSL config\ndb_ssl_enabled=false\ndb_ssl_required=false\ndb_ssl_verifyServerCertificate=false\n#db_ssl_auth_type=1-way|2-way, where 1-way represents standard one way ssl authentication and 2-way represents mutual ssl authentication\ndb_ssl_auth_type=2-way\njavax_net_ssl_keyStore=\njavax_net_ssl_keyStorePassword=\njavax_net_ssl_trustStore=\njavax_net_ssl_trustStorePassword=\n</code></pre>\n<p>跟踪源代码发现安装过程只处理了启用ssl的情况，但是新版本的mysql默认是需要启用ssl的。修改Ranger安装源代码（apache-ranger-1.2.0/security-admin/scripts/db_setup.py）增加下面代码中的221、222行，如下：</p>\n<pre><code>210         def get_jisql_cmd(self, user, password ,db_name):\n211                 path = RANGER_ADMIN_HOME\n212                 db_ssl_param=&#39;&#39;\n213                 db_ssl_cert_param=&#39;&#39;\n214                 if self.db_ssl_enabled == &#39;true&#39;:\n215                         db_ssl_param=&quot;?useSSL=%s&amp;requireSSL=%s&amp;verifyServerCertificate=%s&quot; %(self.db_ssl_enabled,self.db_ssl_required,self.db_ssl_verifyServerCertificate)\n216                         if self.db_ssl_verifyServerCertificate == &#39;true&#39;:\n217                                 if self.db_ssl_auth_type == &#39;1-way&#39;:\n218                                         db_ssl_cert_param=&quot; -Djavax.net.ssl.trustStore=%s -Djavax.net.ssl.trustStorePassword=%s &quot; %(self.javax_net_ssl_trustStore,self.javax_net_ssl_trustStorePassword)\n219                                 else:\n220                                         db_ssl_cert_param=&quot; -Djavax.net.ssl.keyStore=%s -Djavax.net.ssl.keyStorePassword=%s -Djavax.net.ssl.trustStore=%s -Djavax.net.ssl.trustStorePassword=%s &quot; %(self.javax_net_ssl_keyStore,self.javax_net_ssl_keyStorePassword,self.javax_net_ssl_trustStore,self.javax_net_ssl_trustStorePassword)\n221                 else:\n222                         db_ssl_param=&#39;?useSSL=false&#39;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>告警信息及如何解决见我的另外一篇博客：<a href=\"http://zhang-jc.github.io/2020/11/27/MySQL-JDBC-%E8%BF%9E%E6%8E%A5%E5%BC%82%E5%B8%B8%EF%BC%9Ajavax-net-ssl-SSLException-closing-inbound-before-receiving-peer-s-close-notify/\">http://zhang-jc.github.io/2020/11/27/MySQL-JDBC-%E8%BF%9E%E6%8E%A5%E5%BC%82%E5%B8%B8%EF%BC%9Ajavax-net-ssl-SSLException-closing-inbound-before-receiving-peer-s-close-notify/</a></p>\n<p>但是Ranger的安装配置install.properties中未启用ssl，配置信息如下：</p>\n<pre><code>#SSL config\ndb_ssl_enabled=false\ndb_ssl_required=false\ndb_ssl_verifyServerCertificate=false\n#db_ssl_auth_type=1-way|2-way, where 1-way represents standard one way ssl authentication and 2-way represents mutual ssl authentication\ndb_ssl_auth_type=2-way\njavax_net_ssl_keyStore=\njavax_net_ssl_keyStorePassword=\njavax_net_ssl_trustStore=\njavax_net_ssl_trustStorePassword=\n</code></pre>\n<p>跟踪源代码发现安装过程只处理了启用ssl的情况，但是新版本的mysql默认是需要启用ssl的。修改Ranger安装源代码（apache-ranger-1.2.0/security-admin/scripts/db_setup.py）增加下面代码中的221、222行，如下：</p>\n<pre><code>210         def get_jisql_cmd(self, user, password ,db_name):\n211                 path = RANGER_ADMIN_HOME\n212                 db_ssl_param=&#39;&#39;\n213                 db_ssl_cert_param=&#39;&#39;\n214                 if self.db_ssl_enabled == &#39;true&#39;:\n215                         db_ssl_param=&quot;?useSSL=%s&amp;requireSSL=%s&amp;verifyServerCertificate=%s&quot; %(self.db_ssl_enabled,self.db_ssl_required,self.db_ssl_verifyServerCertificate)\n216                         if self.db_ssl_verifyServerCertificate == &#39;true&#39;:\n217                                 if self.db_ssl_auth_type == &#39;1-way&#39;:\n218                                         db_ssl_cert_param=&quot; -Djavax.net.ssl.trustStore=%s -Djavax.net.ssl.trustStorePassword=%s &quot; %(self.javax_net_ssl_trustStore,self.javax_net_ssl_trustStorePassword)\n219                                 else:\n220                                         db_ssl_cert_param=&quot; -Djavax.net.ssl.keyStore=%s -Djavax.net.ssl.keyStorePassword=%s -Djavax.net.ssl.trustStore=%s -Djavax.net.ssl.trustStorePassword=%s &quot; %(self.javax_net_ssl_keyStore,self.javax_net_ssl_keyStorePassword,self.javax_net_ssl_trustStore,self.javax_net_ssl_trustStorePassword)\n221                 else:\n222                         db_ssl_param=&#39;?useSSL=false&#39;\n</code></pre>\n"},{"title":"Ranger Hive Service连接测试失败问题解决","date":"2020-12-02T02:56:59.000Z","_content":"\n异常信息如下：\n\n\torg.apache.ranger.plugin.client.HadoopException: Unable to connect to Hive Thrift Server instance..\n\tUnable to connect to Hive Thrift Server instance..\n\tCould not open client transport with JDBC Uri: jdbc:hive2://192.168.72.212:10000: Could not establish connection to jdbc:hive2://192.168.72.212:10000: Required field 'client_protocol' is unset! Struct:TOpenSessionReq(client_protocol:null, configuration:{set:hiveconf:hive.server2.thrift.resultset.default.fetch.size=1000, use:database=default}).\n\tCould not establish connection to jdbc:hive2://192.168.72.212:10000: Required field 'client_protocol' is unset! Struct:TOpenSessionReq(client_protocol:null, configuration:{set:hiveconf:hive.server2.thrift.resultset.default.fetch.size=1000, use:database=default}).\n\tRequired field 'client_protocol' is unset! Struct:TOpenSessionReq(client_protocol:null, configuration:{set:hiveconf:hive.server2.thrift.resultset.default.fetch.size=1000, use:database=default}). \n\n百度后是因为Hive版本不匹配导致的。我的测试环境Hive版本是2.1.1，通过下面的命令发现确实版本不一致：\n\n\t# find . -name \"*hive*\"\n\t./ews/webapp/WEB-INF/classes/ranger-plugins/hive\n\t./ews/webapp/WEB-INF/classes/ranger-plugins/hive/ranger-hive-plugin-1.2.0.jar\n\t./ews/webapp/WEB-INF/classes/ranger-plugins/hive/hive-common-2.3.2.jar\n\t./ews/webapp/WEB-INF/classes/ranger-plugins/hive/hive-service-2.3.2.jar\n\t./ews/webapp/WEB-INF/classes/ranger-plugins/hive/hive-exec-2.3.2.jar\n\t./ews/webapp/WEB-INF/classes/ranger-plugins/hive/hive-metastore-2.3.2.jar\n\t./ews/webapp/WEB-INF/classes/ranger-plugins/hive/hive-jdbc-2.3.2.jar\n\t\n问题解决方法就显而易见了。","source":"_posts/Ranger-Hive-Service连接测试失败问题解决.md","raw":"title: Ranger Hive Service连接测试失败问题解决\ndate: 2020-12-02 10:56:59\ntags:\n- Ranger\n- Hive\n- 大数据\ncategories:\n- 大数据\n- Ranger\n---\n\n异常信息如下：\n\n\torg.apache.ranger.plugin.client.HadoopException: Unable to connect to Hive Thrift Server instance..\n\tUnable to connect to Hive Thrift Server instance..\n\tCould not open client transport with JDBC Uri: jdbc:hive2://192.168.72.212:10000: Could not establish connection to jdbc:hive2://192.168.72.212:10000: Required field 'client_protocol' is unset! Struct:TOpenSessionReq(client_protocol:null, configuration:{set:hiveconf:hive.server2.thrift.resultset.default.fetch.size=1000, use:database=default}).\n\tCould not establish connection to jdbc:hive2://192.168.72.212:10000: Required field 'client_protocol' is unset! Struct:TOpenSessionReq(client_protocol:null, configuration:{set:hiveconf:hive.server2.thrift.resultset.default.fetch.size=1000, use:database=default}).\n\tRequired field 'client_protocol' is unset! Struct:TOpenSessionReq(client_protocol:null, configuration:{set:hiveconf:hive.server2.thrift.resultset.default.fetch.size=1000, use:database=default}). \n\n百度后是因为Hive版本不匹配导致的。我的测试环境Hive版本是2.1.1，通过下面的命令发现确实版本不一致：\n\n\t# find . -name \"*hive*\"\n\t./ews/webapp/WEB-INF/classes/ranger-plugins/hive\n\t./ews/webapp/WEB-INF/classes/ranger-plugins/hive/ranger-hive-plugin-1.2.0.jar\n\t./ews/webapp/WEB-INF/classes/ranger-plugins/hive/hive-common-2.3.2.jar\n\t./ews/webapp/WEB-INF/classes/ranger-plugins/hive/hive-service-2.3.2.jar\n\t./ews/webapp/WEB-INF/classes/ranger-plugins/hive/hive-exec-2.3.2.jar\n\t./ews/webapp/WEB-INF/classes/ranger-plugins/hive/hive-metastore-2.3.2.jar\n\t./ews/webapp/WEB-INF/classes/ranger-plugins/hive/hive-jdbc-2.3.2.jar\n\t\n问题解决方法就显而易见了。","slug":"Ranger-Hive-Service连接测试失败问题解决","published":1,"updated":"2021-07-19T16:28:00.064Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphpd009qitd3f1q74v01","content":"<p>异常信息如下：</p>\n<pre><code>org.apache.ranger.plugin.client.HadoopException: Unable to connect to Hive Thrift Server instance..\nUnable to connect to Hive Thrift Server instance..\nCould not open client transport with JDBC Uri: jdbc:hive2://192.168.72.212:10000: Could not establish connection to jdbc:hive2://192.168.72.212:10000: Required field &#39;client_protocol&#39; is unset! Struct:TOpenSessionReq(client_protocol:null, configuration:&#123;set:hiveconf:hive.server2.thrift.resultset.default.fetch.size=1000, use:database=default&#125;).\nCould not establish connection to jdbc:hive2://192.168.72.212:10000: Required field &#39;client_protocol&#39; is unset! Struct:TOpenSessionReq(client_protocol:null, configuration:&#123;set:hiveconf:hive.server2.thrift.resultset.default.fetch.size=1000, use:database=default&#125;).\nRequired field &#39;client_protocol&#39; is unset! Struct:TOpenSessionReq(client_protocol:null, configuration:&#123;set:hiveconf:hive.server2.thrift.resultset.default.fetch.size=1000, use:database=default&#125;). \n</code></pre>\n<p>百度后是因为Hive版本不匹配导致的。我的测试环境Hive版本是2.1.1，通过下面的命令发现确实版本不一致：</p>\n<pre><code># find . -name &quot;*hive*&quot;\n./ews/webapp/WEB-INF/classes/ranger-plugins/hive\n./ews/webapp/WEB-INF/classes/ranger-plugins/hive/ranger-hive-plugin-1.2.0.jar\n./ews/webapp/WEB-INF/classes/ranger-plugins/hive/hive-common-2.3.2.jar\n./ews/webapp/WEB-INF/classes/ranger-plugins/hive/hive-service-2.3.2.jar\n./ews/webapp/WEB-INF/classes/ranger-plugins/hive/hive-exec-2.3.2.jar\n./ews/webapp/WEB-INF/classes/ranger-plugins/hive/hive-metastore-2.3.2.jar\n./ews/webapp/WEB-INF/classes/ranger-plugins/hive/hive-jdbc-2.3.2.jar\n</code></pre>\n<p>问题解决方法就显而易见了。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>异常信息如下：</p>\n<pre><code>org.apache.ranger.plugin.client.HadoopException: Unable to connect to Hive Thrift Server instance..\nUnable to connect to Hive Thrift Server instance..\nCould not open client transport with JDBC Uri: jdbc:hive2://192.168.72.212:10000: Could not establish connection to jdbc:hive2://192.168.72.212:10000: Required field &#39;client_protocol&#39; is unset! Struct:TOpenSessionReq(client_protocol:null, configuration:&#123;set:hiveconf:hive.server2.thrift.resultset.default.fetch.size=1000, use:database=default&#125;).\nCould not establish connection to jdbc:hive2://192.168.72.212:10000: Required field &#39;client_protocol&#39; is unset! Struct:TOpenSessionReq(client_protocol:null, configuration:&#123;set:hiveconf:hive.server2.thrift.resultset.default.fetch.size=1000, use:database=default&#125;).\nRequired field &#39;client_protocol&#39; is unset! Struct:TOpenSessionReq(client_protocol:null, configuration:&#123;set:hiveconf:hive.server2.thrift.resultset.default.fetch.size=1000, use:database=default&#125;). \n</code></pre>\n<p>百度后是因为Hive版本不匹配导致的。我的测试环境Hive版本是2.1.1，通过下面的命令发现确实版本不一致：</p>\n<pre><code># find . -name &quot;*hive*&quot;\n./ews/webapp/WEB-INF/classes/ranger-plugins/hive\n./ews/webapp/WEB-INF/classes/ranger-plugins/hive/ranger-hive-plugin-1.2.0.jar\n./ews/webapp/WEB-INF/classes/ranger-plugins/hive/hive-common-2.3.2.jar\n./ews/webapp/WEB-INF/classes/ranger-plugins/hive/hive-service-2.3.2.jar\n./ews/webapp/WEB-INF/classes/ranger-plugins/hive/hive-exec-2.3.2.jar\n./ews/webapp/WEB-INF/classes/ranger-plugins/hive/hive-metastore-2.3.2.jar\n./ews/webapp/WEB-INF/classes/ranger-plugins/hive/hive-jdbc-2.3.2.jar\n</code></pre>\n<p>问题解决方法就显而易见了。</p>\n"},{"title":"Redis 初探","date":"2016-05-31T12:57:20.000Z","_content":"\n### 安装\nRedis 下载地址及安装说明官方地址：<http://redis.io/download>。Redis 目前没有官方支持的 Windows 平台版本。\n\n<!-- more -->\n\n用以下命令下载、解压、编译 Redis：\n\n    $ wget http://download.redis.io/releases/redis-3.2.0.tar.gz\n    $ tar xzf redis-3.2.0.tar.gz\n    $ cd redis-3.2.0\n    $ make\n\n编译之后的二进制文件在 src 目录下。用以下命令启动 Redis：\n\n    $ src/redis-server\n\n可以用 Redis 内置的客户端与 Redis 交互：\n\n    $ src/redis-cli\n    redis> set foo bar\n    OK\n    redis> get foo\n    \"bar\"\n\n### Java 客户端\n\nRedis 分语言的客户端列表可以从官网链接查看：<http://redis.io/clients>。Jedis 是推荐的 Java 客户端，并且近 6 个月有更新，所以选用该客户端。Jedis 的 github 项目链接：<https://github.com/xetorthio/jedis>\n\nMave 依赖配置：\n\n    <dependency>\n        <groupId>redis.clients</groupId>\n        <artifactId>jedis</artifactId>\n        <version>2.8.0</version>\n        <type>jar</type>\n        <scope>compile</scope>\n    </dependency>\n\n使用：\n\n    Jedis jedis = new Jedis(\"localhost\");\n    jedis.set(\"foo\", \"bar\");\n    String value = jedis.get(\"foo\");\n","source":"_posts/Redis-初探.md","raw":"title: Redis 初探\ntags:\n  - Redis\ncategories:\n  - 工具箱\n  - Redis\ndate: 2016-05-31 20:57:20\n---\n\n### 安装\nRedis 下载地址及安装说明官方地址：<http://redis.io/download>。Redis 目前没有官方支持的 Windows 平台版本。\n\n<!-- more -->\n\n用以下命令下载、解压、编译 Redis：\n\n    $ wget http://download.redis.io/releases/redis-3.2.0.tar.gz\n    $ tar xzf redis-3.2.0.tar.gz\n    $ cd redis-3.2.0\n    $ make\n\n编译之后的二进制文件在 src 目录下。用以下命令启动 Redis：\n\n    $ src/redis-server\n\n可以用 Redis 内置的客户端与 Redis 交互：\n\n    $ src/redis-cli\n    redis> set foo bar\n    OK\n    redis> get foo\n    \"bar\"\n\n### Java 客户端\n\nRedis 分语言的客户端列表可以从官网链接查看：<http://redis.io/clients>。Jedis 是推荐的 Java 客户端，并且近 6 个月有更新，所以选用该客户端。Jedis 的 github 项目链接：<https://github.com/xetorthio/jedis>\n\nMave 依赖配置：\n\n    <dependency>\n        <groupId>redis.clients</groupId>\n        <artifactId>jedis</artifactId>\n        <version>2.8.0</version>\n        <type>jar</type>\n        <scope>compile</scope>\n    </dependency>\n\n使用：\n\n    Jedis jedis = new Jedis(\"localhost\");\n    jedis.set(\"foo\", \"bar\");\n    String value = jedis.get(\"foo\");\n","slug":"Redis-初探","published":1,"updated":"2021-07-19T16:28:00.288Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphpe009uitd3dadeea4d","content":"<h3 id=\"安装\"><a href=\"#安装\" class=\"headerlink\" title=\"安装\"></a>安装</h3><p>Redis 下载地址及安装说明官方地址：<a href=\"http://redis.io/download\">http://redis.io/download</a>。Redis 目前没有官方支持的 Windows 平台版本。</p>\n<span id=\"more\"></span>\n\n<p>用以下命令下载、解压、编译 Redis：</p>\n<pre><code>$ wget http://download.redis.io/releases/redis-3.2.0.tar.gz\n$ tar xzf redis-3.2.0.tar.gz\n$ cd redis-3.2.0\n$ make\n</code></pre>\n<p>编译之后的二进制文件在 src 目录下。用以下命令启动 Redis：</p>\n<pre><code>$ src/redis-server\n</code></pre>\n<p>可以用 Redis 内置的客户端与 Redis 交互：</p>\n<pre><code>$ src/redis-cli\nredis&gt; set foo bar\nOK\nredis&gt; get foo\n&quot;bar&quot;\n</code></pre>\n<h3 id=\"Java-客户端\"><a href=\"#Java-客户端\" class=\"headerlink\" title=\"Java 客户端\"></a>Java 客户端</h3><p>Redis 分语言的客户端列表可以从官网链接查看：<a href=\"http://redis.io/clients\">http://redis.io/clients</a>。Jedis 是推荐的 Java 客户端，并且近 6 个月有更新，所以选用该客户端。Jedis 的 github 项目链接：<a href=\"https://github.com/xetorthio/jedis\">https://github.com/xetorthio/jedis</a></p>\n<p>Mave 依赖配置：</p>\n<pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;redis.clients&lt;/groupId&gt;\n    &lt;artifactId&gt;jedis&lt;/artifactId&gt;\n    &lt;version&gt;2.8.0&lt;/version&gt;\n    &lt;type&gt;jar&lt;/type&gt;\n    &lt;scope&gt;compile&lt;/scope&gt;\n&lt;/dependency&gt;\n</code></pre>\n<p>使用：</p>\n<pre><code>Jedis jedis = new Jedis(&quot;localhost&quot;);\njedis.set(&quot;foo&quot;, &quot;bar&quot;);\nString value = jedis.get(&quot;foo&quot;);\n</code></pre>\n","site":{"data":{}},"excerpt":"<h3 id=\"安装\"><a href=\"#安装\" class=\"headerlink\" title=\"安装\"></a>安装</h3><p>Redis 下载地址及安装说明官方地址：<a href=\"http://redis.io/download\">http://redis.io/download</a>。Redis 目前没有官方支持的 Windows 平台版本。</p>","more":"<p>用以下命令下载、解压、编译 Redis：</p>\n<pre><code>$ wget http://download.redis.io/releases/redis-3.2.0.tar.gz\n$ tar xzf redis-3.2.0.tar.gz\n$ cd redis-3.2.0\n$ make\n</code></pre>\n<p>编译之后的二进制文件在 src 目录下。用以下命令启动 Redis：</p>\n<pre><code>$ src/redis-server\n</code></pre>\n<p>可以用 Redis 内置的客户端与 Redis 交互：</p>\n<pre><code>$ src/redis-cli\nredis&gt; set foo bar\nOK\nredis&gt; get foo\n&quot;bar&quot;\n</code></pre>\n<h3 id=\"Java-客户端\"><a href=\"#Java-客户端\" class=\"headerlink\" title=\"Java 客户端\"></a>Java 客户端</h3><p>Redis 分语言的客户端列表可以从官网链接查看：<a href=\"http://redis.io/clients\">http://redis.io/clients</a>。Jedis 是推荐的 Java 客户端，并且近 6 个月有更新，所以选用该客户端。Jedis 的 github 项目链接：<a href=\"https://github.com/xetorthio/jedis\">https://github.com/xetorthio/jedis</a></p>\n<p>Mave 依赖配置：</p>\n<pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;redis.clients&lt;/groupId&gt;\n    &lt;artifactId&gt;jedis&lt;/artifactId&gt;\n    &lt;version&gt;2.8.0&lt;/version&gt;\n    &lt;type&gt;jar&lt;/type&gt;\n    &lt;scope&gt;compile&lt;/scope&gt;\n&lt;/dependency&gt;\n</code></pre>\n<p>使用：</p>\n<pre><code>Jedis jedis = new Jedis(&quot;localhost&quot;);\njedis.set(&quot;foo&quot;, &quot;bar&quot;);\nString value = jedis.get(&quot;foo&quot;);\n</code></pre>"},{"title":"Rsyslog omhttp（HTTP 输出模块）","date":"2019-04-01T07:31:48.000Z","_content":"\n使用 Rsyslog 的 omhttp 模块可以将收集的日志数据以 HTTP 请求的方式输出。该模块支持单条/批量发送数据，支持 GZIP 压缩，支持 HTTPS。\n\n<!--more-->\n\nRsyslog 默认未包含 omhttp 模块，需要重新编译。编译过程参考我的博文：\n[CentOS 7.3 编译 Rsyslog 8.1903.0](http://zhang-jc.github.io/2019/04/03/CentOS-7-3-%E7%BC%96%E8%AF%91-Rsyslog-8-1903-0/)\n[CentOS 6.5 编译 Rsyslog 8.1903.0](http://zhang-jc.github.io/2019/04/03/CentOS-6-5-%E7%BC%96%E8%AF%91-Rsyslog-8-1903-0/)\n\nCentOS 6.5 系统下编译比较麻烦，可以下载我编译后的 so 文件：[CentOS 6.5 编译 omhttp.so 下载](http://zhang-jc.github.io/2019/04/06/CentOS-6-5-%E7%BC%96%E8%AF%91-omhttp-so-%E4%B8%8B%E8%BD%BD/)\n\n一个简单的配置示例如下：\n\n    module(load=\"omhttp\")\n    template(name=\"tpl1\" type=\"string\" string=\"{\\\"type\\\":\\\"syslog\\\", \\\"host\\\":\\\"%HOSTNAME%\\\"}\")\n    action(\n        type=\"omhttp\"\n        server=\"127.0.0.1\"\n        serverport=\"8080\"\n        restpath=\"events\"\n        template=\"tpl1\"\n        action.resumeRetryCount=\"3\"\n    )","source":"_posts/Rsyslog-omhttp（HTTP-输出模块）.md","raw":"title: Rsyslog omhttp（HTTP 输出模块）\ntags:\n  - Rsyslog\ncategories:\n  - 大数据\n  - Rsyslog\ndate: 2019-04-01 15:31:48\n---\n\n使用 Rsyslog 的 omhttp 模块可以将收集的日志数据以 HTTP 请求的方式输出。该模块支持单条/批量发送数据，支持 GZIP 压缩，支持 HTTPS。\n\n<!--more-->\n\nRsyslog 默认未包含 omhttp 模块，需要重新编译。编译过程参考我的博文：\n[CentOS 7.3 编译 Rsyslog 8.1903.0](http://zhang-jc.github.io/2019/04/03/CentOS-7-3-%E7%BC%96%E8%AF%91-Rsyslog-8-1903-0/)\n[CentOS 6.5 编译 Rsyslog 8.1903.0](http://zhang-jc.github.io/2019/04/03/CentOS-6-5-%E7%BC%96%E8%AF%91-Rsyslog-8-1903-0/)\n\nCentOS 6.5 系统下编译比较麻烦，可以下载我编译后的 so 文件：[CentOS 6.5 编译 omhttp.so 下载](http://zhang-jc.github.io/2019/04/06/CentOS-6-5-%E7%BC%96%E8%AF%91-omhttp-so-%E4%B8%8B%E8%BD%BD/)\n\n一个简单的配置示例如下：\n\n    module(load=\"omhttp\")\n    template(name=\"tpl1\" type=\"string\" string=\"{\\\"type\\\":\\\"syslog\\\", \\\"host\\\":\\\"%HOSTNAME%\\\"}\")\n    action(\n        type=\"omhttp\"\n        server=\"127.0.0.1\"\n        serverport=\"8080\"\n        restpath=\"events\"\n        template=\"tpl1\"\n        action.resumeRetryCount=\"3\"\n    )","slug":"Rsyslog-omhttp（HTTP-输出模块）","published":1,"updated":"2021-07-19T16:28:00.288Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphpj009yitd32yfpg5kv","content":"<p>使用 Rsyslog 的 omhttp 模块可以将收集的日志数据以 HTTP 请求的方式输出。该模块支持单条/批量发送数据，支持 GZIP 压缩，支持 HTTPS。</p>\n<span id=\"more\"></span>\n\n<p>Rsyslog 默认未包含 omhttp 模块，需要重新编译。编译过程参考我的博文：<br><a href=\"http://zhang-jc.github.io/2019/04/03/CentOS-7-3-%E7%BC%96%E8%AF%91-Rsyslog-8-1903-0/\">CentOS 7.3 编译 Rsyslog 8.1903.0</a><br><a href=\"http://zhang-jc.github.io/2019/04/03/CentOS-6-5-%E7%BC%96%E8%AF%91-Rsyslog-8-1903-0/\">CentOS 6.5 编译 Rsyslog 8.1903.0</a></p>\n<p>CentOS 6.5 系统下编译比较麻烦，可以下载我编译后的 so 文件：<a href=\"http://zhang-jc.github.io/2019/04/06/CentOS-6-5-%E7%BC%96%E8%AF%91-omhttp-so-%E4%B8%8B%E8%BD%BD/\">CentOS 6.5 编译 omhttp.so 下载</a></p>\n<p>一个简单的配置示例如下：</p>\n<pre><code>module(load=&quot;omhttp&quot;)\ntemplate(name=&quot;tpl1&quot; type=&quot;string&quot; string=&quot;&#123;\\&quot;type\\&quot;:\\&quot;syslog\\&quot;, \\&quot;host\\&quot;:\\&quot;%HOSTNAME%\\&quot;&#125;&quot;)\naction(\n    type=&quot;omhttp&quot;\n    server=&quot;127.0.0.1&quot;\n    serverport=&quot;8080&quot;\n    restpath=&quot;events&quot;\n    template=&quot;tpl1&quot;\n    action.resumeRetryCount=&quot;3&quot;\n)\n</code></pre>\n","site":{"data":{}},"excerpt":"<p>使用 Rsyslog 的 omhttp 模块可以将收集的日志数据以 HTTP 请求的方式输出。该模块支持单条/批量发送数据，支持 GZIP 压缩，支持 HTTPS。</p>","more":"<p>Rsyslog 默认未包含 omhttp 模块，需要重新编译。编译过程参考我的博文：<br><a href=\"http://zhang-jc.github.io/2019/04/03/CentOS-7-3-%E7%BC%96%E8%AF%91-Rsyslog-8-1903-0/\">CentOS 7.3 编译 Rsyslog 8.1903.0</a><br><a href=\"http://zhang-jc.github.io/2019/04/03/CentOS-6-5-%E7%BC%96%E8%AF%91-Rsyslog-8-1903-0/\">CentOS 6.5 编译 Rsyslog 8.1903.0</a></p>\n<p>CentOS 6.5 系统下编译比较麻烦，可以下载我编译后的 so 文件：<a href=\"http://zhang-jc.github.io/2019/04/06/CentOS-6-5-%E7%BC%96%E8%AF%91-omhttp-so-%E4%B8%8B%E8%BD%BD/\">CentOS 6.5 编译 omhttp.so 下载</a></p>\n<p>一个简单的配置示例如下：</p>\n<pre><code>module(load=&quot;omhttp&quot;)\ntemplate(name=&quot;tpl1&quot; type=&quot;string&quot; string=&quot;&#123;\\&quot;type\\&quot;:\\&quot;syslog\\&quot;, \\&quot;host\\&quot;:\\&quot;%HOSTNAME%\\&quot;&#125;&quot;)\naction(\n    type=&quot;omhttp&quot;\n    server=&quot;127.0.0.1&quot;\n    serverport=&quot;8080&quot;\n    restpath=&quot;events&quot;\n    template=&quot;tpl1&quot;\n    action.resumeRetryCount=&quot;3&quot;\n)\n</code></pre>"},{"title":"Shell $0","date":"2016-09-10T19:33:03.000Z","_content":"\n\n我们已经知道在 Shell 中 $0 表示 Shell 脚本的文件名，但在有脚本调用的情形中，子脚本中的 $0 会是什么值呢？我们通过下面的实例来看。\n\n<!-- more -->\n\n已测试系统列表：\n\n- Mac OS X EI Capitan 10.11.6\n- Ubuntu 16.04 LTS (GNU/Linux 4.4.0-28-generic x86_64)\n\n父脚本 a.sh，位置 test1/a.sh，内容如下：\n\n    !/usr/bin/env bash\n\n    echo \"$0\"\n    . ../test2/b.sh\n\n子脚本 b.sh，位置 test2/b.sh，内容如下：\n\n    #!/usr/bin/env bash\n\n    echo \"the sub script is: $0\"\n\n此时执行父脚本输出结果是：\n\n    $ sh a.sh\n    the main script is a.sh\n    the sub script is: a.sh\n\n如果父脚本内容如下：\n\n    #!/usr/bin/env bash\n\n    echo \"the main script is $0\"\n    ../test2/b.sh\n\n则输出结果为：\n\n    $ sh a.sh \n    the main script is a.sh\n    the sub script is: ../test2/b.sh\n\n可见，在父脚本中调用子脚本的不同，在子脚本中 $0 的值也不同。至于为什么会这样本人需要继续学习以找到答案。\n\n> 测试过程中注意给脚本赋可执行权限。","source":"_posts/Shell-dollar0.md","raw":"title: Shell $0\ntags:\n  - Shell\ncategories:\n  - 语言\n  - Shell\ndate: 2016-09-11 03:33:03\n---\n\n\n我们已经知道在 Shell 中 $0 表示 Shell 脚本的文件名，但在有脚本调用的情形中，子脚本中的 $0 会是什么值呢？我们通过下面的实例来看。\n\n<!-- more -->\n\n已测试系统列表：\n\n- Mac OS X EI Capitan 10.11.6\n- Ubuntu 16.04 LTS (GNU/Linux 4.4.0-28-generic x86_64)\n\n父脚本 a.sh，位置 test1/a.sh，内容如下：\n\n    !/usr/bin/env bash\n\n    echo \"$0\"\n    . ../test2/b.sh\n\n子脚本 b.sh，位置 test2/b.sh，内容如下：\n\n    #!/usr/bin/env bash\n\n    echo \"the sub script is: $0\"\n\n此时执行父脚本输出结果是：\n\n    $ sh a.sh\n    the main script is a.sh\n    the sub script is: a.sh\n\n如果父脚本内容如下：\n\n    #!/usr/bin/env bash\n\n    echo \"the main script is $0\"\n    ../test2/b.sh\n\n则输出结果为：\n\n    $ sh a.sh \n    the main script is a.sh\n    the sub script is: ../test2/b.sh\n\n可见，在父脚本中调用子脚本的不同，在子脚本中 $0 的值也不同。至于为什么会这样本人需要继续学习以找到答案。\n\n> 测试过程中注意给脚本赋可执行权限。","slug":"Shell-dollar0","published":1,"updated":"2021-07-19T16:28:00.288Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphpl00a2itd3ed0g550n","content":"<p>我们已经知道在 Shell 中 $0 表示 Shell 脚本的文件名，但在有脚本调用的情形中，子脚本中的 $0 会是什么值呢？我们通过下面的实例来看。</p>\n<span id=\"more\"></span>\n\n<p>已测试系统列表：</p>\n<ul>\n<li>Mac OS X EI Capitan 10.11.6</li>\n<li>Ubuntu 16.04 LTS (GNU/Linux 4.4.0-28-generic x86_64)</li>\n</ul>\n<p>父脚本 a.sh，位置 test1/a.sh，内容如下：</p>\n<pre><code>!/usr/bin/env bash\n\necho &quot;$0&quot;\n. ../test2/b.sh\n</code></pre>\n<p>子脚本 b.sh，位置 test2/b.sh，内容如下：</p>\n<pre><code>#!/usr/bin/env bash\n\necho &quot;the sub script is: $0&quot;\n</code></pre>\n<p>此时执行父脚本输出结果是：</p>\n<pre><code>$ sh a.sh\nthe main script is a.sh\nthe sub script is: a.sh\n</code></pre>\n<p>如果父脚本内容如下：</p>\n<pre><code>#!/usr/bin/env bash\n\necho &quot;the main script is $0&quot;\n../test2/b.sh\n</code></pre>\n<p>则输出结果为：</p>\n<pre><code>$ sh a.sh \nthe main script is a.sh\nthe sub script is: ../test2/b.sh\n</code></pre>\n<p>可见，在父脚本中调用子脚本的不同，在子脚本中 $0 的值也不同。至于为什么会这样本人需要继续学习以找到答案。</p>\n<blockquote>\n<p>测试过程中注意给脚本赋可执行权限。</p>\n</blockquote>\n","site":{"data":{}},"excerpt":"<p>我们已经知道在 Shell 中 $0 表示 Shell 脚本的文件名，但在有脚本调用的情形中，子脚本中的 $0 会是什么值呢？我们通过下面的实例来看。</p>","more":"<p>已测试系统列表：</p>\n<ul>\n<li>Mac OS X EI Capitan 10.11.6</li>\n<li>Ubuntu 16.04 LTS (GNU/Linux 4.4.0-28-generic x86_64)</li>\n</ul>\n<p>父脚本 a.sh，位置 test1/a.sh，内容如下：</p>\n<pre><code>!/usr/bin/env bash\n\necho &quot;$0&quot;\n. ../test2/b.sh\n</code></pre>\n<p>子脚本 b.sh，位置 test2/b.sh，内容如下：</p>\n<pre><code>#!/usr/bin/env bash\n\necho &quot;the sub script is: $0&quot;\n</code></pre>\n<p>此时执行父脚本输出结果是：</p>\n<pre><code>$ sh a.sh\nthe main script is a.sh\nthe sub script is: a.sh\n</code></pre>\n<p>如果父脚本内容如下：</p>\n<pre><code>#!/usr/bin/env bash\n\necho &quot;the main script is $0&quot;\n../test2/b.sh\n</code></pre>\n<p>则输出结果为：</p>\n<pre><code>$ sh a.sh \nthe main script is a.sh\nthe sub script is: ../test2/b.sh\n</code></pre>\n<p>可见，在父脚本中调用子脚本的不同，在子脚本中 $0 的值也不同。至于为什么会这样本人需要继续学习以找到答案。</p>\n<blockquote>\n<p>测试过程中注意给脚本赋可执行权限。</p>\n</blockquote>"},{"title":"Shell 字符串截取技巧","date":"2020-11-26T07:55:22.000Z","_content":"\n- \\# 号截取：删除左边字符，保留右边字符。\n- \\## 号截取：删除左边字符，保留右边字符。\n- %号截取：删除右边字符，保留左边字符\n- %% 号截取：删除右边字符，保留左边字符\n\n示例一：\n\n\t$ var='abcbd'\n\t$ echo ${var#*b}\n\tcbd\n\t$ echo ${var##*b}\n\td\n\t$ echo ${var%b*}\n\tabc\n\t$ echo ${var%%b*}\n\ta\n\t\n示例二：\n\n\t$ cat /etc/passwd\n\troot:x:0:0:root:/root:/bin/bash\n\tdaemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin\n\tbin:x:2:2:bin:/bin:/usr/sbin/nologin\n\tsys:x:3:3:sys:/dev:/usr/sbin/nologin\n\tsync:x:4:65534:sync:/bin:/bin/sync\n\t$ for line in `cat /etc/passwd`; do echo ${line%%:*}; done\n\troot\n\tdaemon\n\tbin\n\tsys\n\tsync\n\t","source":"_posts/Shell-字符串截取技巧.md","raw":"title: Shell 字符串截取技巧\ndate: 2020-11-26 15:55:22\ntags:\n- Shell\n- Linux\ncategories:\n- 语言\n- Shell\n---\n\n- \\# 号截取：删除左边字符，保留右边字符。\n- \\## 号截取：删除左边字符，保留右边字符。\n- %号截取：删除右边字符，保留左边字符\n- %% 号截取：删除右边字符，保留左边字符\n\n示例一：\n\n\t$ var='abcbd'\n\t$ echo ${var#*b}\n\tcbd\n\t$ echo ${var##*b}\n\td\n\t$ echo ${var%b*}\n\tabc\n\t$ echo ${var%%b*}\n\ta\n\t\n示例二：\n\n\t$ cat /etc/passwd\n\troot:x:0:0:root:/root:/bin/bash\n\tdaemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin\n\tbin:x:2:2:bin:/bin:/usr/sbin/nologin\n\tsys:x:3:3:sys:/dev:/usr/sbin/nologin\n\tsync:x:4:65534:sync:/bin:/bin/sync\n\t$ for line in `cat /etc/passwd`; do echo ${line%%:*}; done\n\troot\n\tdaemon\n\tbin\n\tsys\n\tsync\n\t","slug":"Shell-字符串截取技巧","published":1,"updated":"2021-07-19T16:27:59.964Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphpm00a6itd35xstgfga","content":"<ul>\n<li># 号截取：删除左边字符，保留右边字符。</li>\n<li>## 号截取：删除左边字符，保留右边字符。</li>\n<li>%号截取：删除右边字符，保留左边字符</li>\n<li>%% 号截取：删除右边字符，保留左边字符</li>\n</ul>\n<p>示例一：</p>\n<pre><code>$ var=&#39;abcbd&#39;\n$ echo $&#123;var#*b&#125;\ncbd\n$ echo $&#123;var##*b&#125;\nd\n$ echo $&#123;var%b*&#125;\nabc\n$ echo $&#123;var%%b*&#125;\na\n</code></pre>\n<p>示例二：</p>\n<pre><code>$ cat /etc/passwd\nroot:x:0:0:root:/root:/bin/bash\ndaemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin\nbin:x:2:2:bin:/bin:/usr/sbin/nologin\nsys:x:3:3:sys:/dev:/usr/sbin/nologin\nsync:x:4:65534:sync:/bin:/bin/sync\n$ for line in `cat /etc/passwd`; do echo $&#123;line%%:*&#125;; done\nroot\ndaemon\nbin\nsys\nsync\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<ul>\n<li># 号截取：删除左边字符，保留右边字符。</li>\n<li>## 号截取：删除左边字符，保留右边字符。</li>\n<li>%号截取：删除右边字符，保留左边字符</li>\n<li>%% 号截取：删除右边字符，保留左边字符</li>\n</ul>\n<p>示例一：</p>\n<pre><code>$ var=&#39;abcbd&#39;\n$ echo $&#123;var#*b&#125;\ncbd\n$ echo $&#123;var##*b&#125;\nd\n$ echo $&#123;var%b*&#125;\nabc\n$ echo $&#123;var%%b*&#125;\na\n</code></pre>\n<p>示例二：</p>\n<pre><code>$ cat /etc/passwd\nroot:x:0:0:root:/root:/bin/bash\ndaemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin\nbin:x:2:2:bin:/bin:/usr/sbin/nologin\nsys:x:3:3:sys:/dev:/usr/sbin/nologin\nsync:x:4:65534:sync:/bin:/bin/sync\n$ for line in `cat /etc/passwd`; do echo $&#123;line%%:*&#125;; done\nroot\ndaemon\nbin\nsys\nsync\n</code></pre>\n"},{"title":"Shell 文件判断条件","date":"2016-09-11T14:03:28.000Z","_content":"\n1. -e filename：如果 filename 存在，则为真。\n2. -d filename：如果 filename 为目录，则为真。\n\n<!-- more -->\n\n3. -f filename：如果 filename 为常规文件，则为真。\n4. -L filename：如果 filename 为符号链接，则为真。\n5. -r filename：如果 filename 可读，则为真。\n6. -w filename：如果 filename 可写，则为真。\n7. -x filename：如果 filename 可执行，则为真。\n8. -s filename：如果文件长度不为 0，则为真。\n9. -h filename：如果文件是软链接，则为真。\n10. filename1 -nt filename2：如果 filename1 比 filename2 新，则为真。\n11. filename1 -ot filename2：如果 filename1 比 filename2 旧，则为真。","source":"_posts/Shell-文件判断条件.md","raw":"title: Shell 文件判断条件\ntags:\n  - Shell\ncategories:\n  - 语言\n  - Shell\ndate: 2016-09-11 22:03:28\n---\n\n1. -e filename：如果 filename 存在，则为真。\n2. -d filename：如果 filename 为目录，则为真。\n\n<!-- more -->\n\n3. -f filename：如果 filename 为常规文件，则为真。\n4. -L filename：如果 filename 为符号链接，则为真。\n5. -r filename：如果 filename 可读，则为真。\n6. -w filename：如果 filename 可写，则为真。\n7. -x filename：如果 filename 可执行，则为真。\n8. -s filename：如果文件长度不为 0，则为真。\n9. -h filename：如果文件是软链接，则为真。\n10. filename1 -nt filename2：如果 filename1 比 filename2 新，则为真。\n11. filename1 -ot filename2：如果 filename1 比 filename2 旧，则为真。","slug":"Shell-文件判断条件","published":1,"updated":"2021-07-19T16:28:00.288Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphpo00aaitd3249c5t7p","content":"<ol>\n<li>-e filename：如果 filename 存在，则为真。</li>\n<li>-d filename：如果 filename 为目录，则为真。</li>\n</ol>\n<span id=\"more\"></span>\n\n<ol start=\"3\">\n<li>-f filename：如果 filename 为常规文件，则为真。</li>\n<li>-L filename：如果 filename 为符号链接，则为真。</li>\n<li>-r filename：如果 filename 可读，则为真。</li>\n<li>-w filename：如果 filename 可写，则为真。</li>\n<li>-x filename：如果 filename 可执行，则为真。</li>\n<li>-s filename：如果文件长度不为 0，则为真。</li>\n<li>-h filename：如果文件是软链接，则为真。</li>\n<li>filename1 -nt filename2：如果 filename1 比 filename2 新，则为真。</li>\n<li>filename1 -ot filename2：如果 filename1 比 filename2 旧，则为真。</li>\n</ol>\n","site":{"data":{}},"excerpt":"<ol>\n<li>-e filename：如果 filename 存在，则为真。</li>\n<li>-d filename：如果 filename 为目录，则为真。</li>\n</ol>","more":"<ol start=\"3\">\n<li>-f filename：如果 filename 为常规文件，则为真。</li>\n<li>-L filename：如果 filename 为符号链接，则为真。</li>\n<li>-r filename：如果 filename 可读，则为真。</li>\n<li>-w filename：如果 filename 可写，则为真。</li>\n<li>-x filename：如果 filename 可执行，则为真。</li>\n<li>-s filename：如果文件长度不为 0，则为真。</li>\n<li>-h filename：如果文件是软链接，则为真。</li>\n<li>filename1 -nt filename2：如果 filename1 比 filename2 新，则为真。</li>\n<li>filename1 -ot filename2：如果 filename1 比 filename2 旧，则为真。</li>\n</ol>"},{"title":"Shell 条件变量替换","date":"2016-09-11T11:18:58.000Z","_content":"Bash Shell 可以进行变量的条件替换，条件放在 {} 中，只有条件发生时才进行替换：\n\n<!-- more -->\n\n- ${v1:\\-$v2}\n\n当变量 v1 未定义或者值为空时，返回 v2 的值，否则返回 v1 的值。\n\n- ${v1:=$v2}\n\n若变量 v1 未定义或者值为空时，在返回 v2 的值的同时将 v2 的值给 v1。\n\n- ${v1:?message}\n\n若变量已赋值的话，正常替换。否则将消息message送到标准错误输出(若此替换出现在 Shell 程序中，那么该程序将终止运行)。\n\n- ${v1:+$v2}\n\n若变量已赋值的话，其值才用 v2 的值替换，否则不进行任何替换。\n\n- ${v1:offset}、${v1:offset:length}\n\n从变量中提取子串，这里 offset 和 length 可以是算术表达式。\n\n- ${&#35;v1}\n\n变量的字符个数 (变量的字符个数，并不是变量个数）。\n\n- ${v1&#35;pattern}、${v1&#35;&#35;pattern}\n\n去掉 v1 中与 pattern 相匹配的部分，条件是 v1 的开头与 pattern 相匹配。&#35; 与 &#35;&#35; 的区别在于一个是最短匹配模式，一个是最长匹配模式。\n\n- ${v1％pattern}、${v1％％pattern}\n\n于 7 类似，只是 从v1 的尾部与 pattern 相匹配，％ 与 ％％ 的区别与 &#35; 与 &#35;&#35; 一样。\n\n- ${v1/pattern/string}、${v1//pattern/string}\n\n进行变量内容的替换，把与 pattern 匹配的部分替换为 string 的内容，/ 与 // 的区别与上同。\n\n注意：上述条件变量替换中，除 2 外，其余均不影响变量本身的值。\n","source":"_posts/Shell-条件变量替换.md","raw":"title: Shell 条件变量替换\ntags:\n  - Shell\ncategories:\n  - 语言\n  - Shell\ndate: 2016-09-11 19:18:58\n---\nBash Shell 可以进行变量的条件替换，条件放在 {} 中，只有条件发生时才进行替换：\n\n<!-- more -->\n\n- ${v1:\\-$v2}\n\n当变量 v1 未定义或者值为空时，返回 v2 的值，否则返回 v1 的值。\n\n- ${v1:=$v2}\n\n若变量 v1 未定义或者值为空时，在返回 v2 的值的同时将 v2 的值给 v1。\n\n- ${v1:?message}\n\n若变量已赋值的话，正常替换。否则将消息message送到标准错误输出(若此替换出现在 Shell 程序中，那么该程序将终止运行)。\n\n- ${v1:+$v2}\n\n若变量已赋值的话，其值才用 v2 的值替换，否则不进行任何替换。\n\n- ${v1:offset}、${v1:offset:length}\n\n从变量中提取子串，这里 offset 和 length 可以是算术表达式。\n\n- ${&#35;v1}\n\n变量的字符个数 (变量的字符个数，并不是变量个数）。\n\n- ${v1&#35;pattern}、${v1&#35;&#35;pattern}\n\n去掉 v1 中与 pattern 相匹配的部分，条件是 v1 的开头与 pattern 相匹配。&#35; 与 &#35;&#35; 的区别在于一个是最短匹配模式，一个是最长匹配模式。\n\n- ${v1％pattern}、${v1％％pattern}\n\n于 7 类似，只是 从v1 的尾部与 pattern 相匹配，％ 与 ％％ 的区别与 &#35; 与 &#35;&#35; 一样。\n\n- ${v1/pattern/string}、${v1//pattern/string}\n\n进行变量内容的替换，把与 pattern 匹配的部分替换为 string 的内容，/ 与 // 的区别与上同。\n\n注意：上述条件变量替换中，除 2 外，其余均不影响变量本身的值。\n","slug":"Shell-条件变量替换","published":1,"updated":"2021-07-19T16:28:00.288Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphpr00aeitd3gasa1ljm","content":"<p>Bash Shell 可以进行变量的条件替换，条件放在 {} 中，只有条件发生时才进行替换：</p>\n<span id=\"more\"></span>\n\n<ul>\n<li>${v1:-$v2}</li>\n</ul>\n<p>当变量 v1 未定义或者值为空时，返回 v2 的值，否则返回 v1 的值。</p>\n<ul>\n<li>${v1:=$v2}</li>\n</ul>\n<p>若变量 v1 未定义或者值为空时，在返回 v2 的值的同时将 v2 的值给 v1。</p>\n<ul>\n<li>${v1:?message}</li>\n</ul>\n<p>若变量已赋值的话，正常替换。否则将消息message送到标准错误输出(若此替换出现在 Shell 程序中，那么该程序将终止运行)。</p>\n<ul>\n<li>${v1:+$v2}</li>\n</ul>\n<p>若变量已赋值的话，其值才用 v2 的值替换，否则不进行任何替换。</p>\n<ul>\n<li>${v1:offset}、${v1:offset:length}</li>\n</ul>\n<p>从变量中提取子串，这里 offset 和 length 可以是算术表达式。</p>\n<ul>\n<li>${&#35;v1}</li>\n</ul>\n<p>变量的字符个数 (变量的字符个数，并不是变量个数）。</p>\n<ul>\n<li>${v1&#35;pattern}、${v1&#35;&#35;pattern}</li>\n</ul>\n<p>去掉 v1 中与 pattern 相匹配的部分，条件是 v1 的开头与 pattern 相匹配。&#35; 与 &#35;&#35; 的区别在于一个是最短匹配模式，一个是最长匹配模式。</p>\n<ul>\n<li>${v1％pattern}、${v1％％pattern}</li>\n</ul>\n<p>于 7 类似，只是 从v1 的尾部与 pattern 相匹配，％ 与 ％％ 的区别与 &#35; 与 &#35;&#35; 一样。</p>\n<ul>\n<li>${v1/pattern/string}、${v1//pattern/string}</li>\n</ul>\n<p>进行变量内容的替换，把与 pattern 匹配的部分替换为 string 的内容，/ 与 // 的区别与上同。</p>\n<p>注意：上述条件变量替换中，除 2 外，其余均不影响变量本身的值。</p>\n","site":{"data":{}},"excerpt":"<p>Bash Shell 可以进行变量的条件替换，条件放在 {} 中，只有条件发生时才进行替换：</p>","more":"<ul>\n<li>${v1:-$v2}</li>\n</ul>\n<p>当变量 v1 未定义或者值为空时，返回 v2 的值，否则返回 v1 的值。</p>\n<ul>\n<li>${v1:=$v2}</li>\n</ul>\n<p>若变量 v1 未定义或者值为空时，在返回 v2 的值的同时将 v2 的值给 v1。</p>\n<ul>\n<li>${v1:?message}</li>\n</ul>\n<p>若变量已赋值的话，正常替换。否则将消息message送到标准错误输出(若此替换出现在 Shell 程序中，那么该程序将终止运行)。</p>\n<ul>\n<li>${v1:+$v2}</li>\n</ul>\n<p>若变量已赋值的话，其值才用 v2 的值替换，否则不进行任何替换。</p>\n<ul>\n<li>${v1:offset}、${v1:offset:length}</li>\n</ul>\n<p>从变量中提取子串，这里 offset 和 length 可以是算术表达式。</p>\n<ul>\n<li>${&#35;v1}</li>\n</ul>\n<p>变量的字符个数 (变量的字符个数，并不是变量个数）。</p>\n<ul>\n<li>${v1&#35;pattern}、${v1&#35;&#35;pattern}</li>\n</ul>\n<p>去掉 v1 中与 pattern 相匹配的部分，条件是 v1 的开头与 pattern 相匹配。&#35; 与 &#35;&#35; 的区别在于一个是最短匹配模式，一个是最长匹配模式。</p>\n<ul>\n<li>${v1％pattern}、${v1％％pattern}</li>\n</ul>\n<p>于 7 类似，只是 从v1 的尾部与 pattern 相匹配，％ 与 ％％ 的区别与 &#35; 与 &#35;&#35; 一样。</p>\n<ul>\n<li>${v1/pattern/string}、${v1//pattern/string}</li>\n</ul>\n<p>进行变量内容的替换，把与 pattern 匹配的部分替换为 string 的内容，/ 与 // 的区别与上同。</p>\n<p>注意：上述条件变量替换中，除 2 外，其余均不影响变量本身的值。</p>"},{"title":"Socket.IO Java 客户端","date":"2016-06-14T14:49:33.000Z","_content":"\nSocket.IO 是一个非常棒的项目；Java 是目前应用非常广的开发语言。两者的结合也是必然的。本篇翻译自 Socket.IO-client Java 项目的 github 主页。\n\n<!-- more -->\n\n[Socket.IO-client Java](https://github.com/socketio/socket.io-client-java) 是 Socket.IO v1.x 的 Java 客户端类库，这个类库是从 [JavaScript client](https://github.com/socketio/socket.io-client) 移植过来的。\n\n参见：\n\n- [Android chat demo](https://github.com/nkzawa/socket.io-android-chat)\n- [engine.io-client-java](https://github.com/socketio/engine.io-client-java)\n\n### 安装\n\n最新的包可以从 Maven 的中心仓库获取。你将需要安装[依赖](http://socketio.github.io/socket.io-client-java/dependencies.html)。\n\n> 注意：v0.6.1 之后，包名改为了 io.socket 。请确认更新了依赖配置。\n\n#### Maven\n\n在 pom.xml 中添加依赖：\n\n    <dependencies>\n      <dependency>\n        <groupId>io.socket</groupId>\n        <artifactId>socket.io-client</artifactId>\n        <version>0.7.0</version>\n      </dependency>\n    </dependencies>\n\n#### Gradle\n\n在 build.gradle 中，给 Android Studio 添加 gradle 依赖：\n\n    compile ('io.socket:socket.io-client:0.7.0') {\n      // excluding org.json which is provided by Android\n      exclude group: 'org.json', module: 'json'\n    }\n\n### 使用\nSocket.IO-client Java 几乎拥有跟原生 JS 客户端相同的 api 和特性。使用 *IO.socket* 初始化 *Socket* ：\n\n    socket = IO.socket(\"http://localhost\");\n    socket.on(Socket.EVENT_CONNECT, new Emitter.Listener() {\n\n      @Override\n      public void call(Object... args) {\n        socket.emit(\"foo\", \"hi\");\n        socket.disconnect();\n      }\n\n    }).on(\"event\", new Emitter.Listener() {\n\n      @Override\n      public void call(Object... args) {}\n\n    }).on(Socket.EVENT_DISCONNECT, new  Emitter.Listener() {\n\n      @Override\n      public void call(Object... args) {}\n\n    });\n    socket.connect();\n\n这个类库使用 [org.json](http://www.json.org/) 解析和构造 JSON 字符串：\n\n    // Sending an object\n    JSONObject obj = new JSONObject();\n    obj.put(\"hello\", \"server\");\n    obj.put(\"binary\", new byte[42]);\n    socket.emit(\"foo\", obj);\n\n    // Receiving an object\n    socket.on(\"foo\", new Emitter.Listener() {\n      @Override\n      public void call(Object... args) {\n        JSONObject obj = (JSONObject)args[0];\n      }\n    });\n\n像下面这样设置选项：\n\n    IO.Options opts = new IO.Options();\n    opts.forceNew = true;\n    opts.reconnection = false;\n\n    socket = IO.socket(\"http://localhost\", opts);\n\n可以用 *query* 选项设置查询参数。请注意：如果当查询参数改变时不想复用一个缓存的 socket 实例，应该使用 *forceNew* 选项，可能的使用场景是如果你的应用允许用户退出，并且一个新的用户再次登录：\n\n    IO.Options opts = new IO.Options();\n    opts.forceNew = true;\n    opts.query = \"auth_token=\" + authToken;\n    Socket socket = IO.socket(\"http://localhost\", opts);\n\n当服务器接收到一个消息时，可以用 *Ack* 获取一个回调：\n\n    socket.emit(\"foo\", \"woot\", new Ack() {\n      @Override\n      public void call(Object... args) {}\n    });\n\n反之亦然：\n\n    // ack from client to server\n    socket.on(\"foo\", new Emitter.Listener() {\n      @Override\n      public void call(Object... args) {\n        Ack ack = (Ack) args[args.length - 1];\n        ack.call();\n      }\n    });\n\nSSL (HTTPS, WSS) 设置：\n\n    // default settings for all sockets\n    IO.setDefaultSSLContext(mySSLContext);\n    IO.setDefaultHostnameVerifier(myHostnameVerifier);\n\n    // set as an option\n    opts = new IO.Options();\n    opts.sslContext = mySSLContext;\n    opts.hostnameVerifier = myHostnameVerifier;\n    socket = IO.socket(\"https://localhost\", opts);\n\n查阅 Javadoc 获取更多详情：<http://socketio.github.io/socket.io-client-java/apidocs/>\n\n### Transports 和 HTTP 头\n\n可以像下面这样访问 transports 和它们的 HTTP 头：\n\n    // Called upon transport creation.\n    socket.io().on(Manager.EVENT_TRANSPORT, new Emitter.listener() {\n      @Override\n      public void call(Object... args) {\n        Transport transport = (Transport)args[0];\n\n        transport.on(Transport.EVENT_REQUEST_HEADERS, new Emitter.Listener() {\n          @Override\n          public void call(Object... args) {\n            @SuppressWarnings(\"unchecked\")\n            Map<String, List<String>> headers = (Map<String, List<String>>)args[0];\n            // modify request headers\n            headers.put(\"Cookie\", Arrays.asList(\"foo=1;\"));\n          }\n        });\n\n        transport.on(Transport.EVENT_RESPONSE_HEADERS, new Emitter.Listener() {\n          @Override\n          public void call(Object... args) {\n            @SuppressWarnings(\"unchecked\")\n            Map<String, List<String>> headers = (Map<String, List<String>>)args[0];\n            // access response headers\n            String cookie = headers.get(\"Set-Cookie\").get(0);\n          }\n        });\n      }\n    });\n\n### 特性\n\n这个类库支持 JS 客户端支持的所有特性，包括 events、options、upgrading transport。完全支持 Android。\n\n### License\n\nMIT\n","source":"_posts/Socket-IO-Java-客户端.md","raw":"title: Socket.IO Java 客户端\ntags:\n  - Socket.IO\n  - Java\ncategories:\n  - 工具箱\n  - Socket.IO\ndate: 2016-06-14 22:49:33\n---\n\nSocket.IO 是一个非常棒的项目；Java 是目前应用非常广的开发语言。两者的结合也是必然的。本篇翻译自 Socket.IO-client Java 项目的 github 主页。\n\n<!-- more -->\n\n[Socket.IO-client Java](https://github.com/socketio/socket.io-client-java) 是 Socket.IO v1.x 的 Java 客户端类库，这个类库是从 [JavaScript client](https://github.com/socketio/socket.io-client) 移植过来的。\n\n参见：\n\n- [Android chat demo](https://github.com/nkzawa/socket.io-android-chat)\n- [engine.io-client-java](https://github.com/socketio/engine.io-client-java)\n\n### 安装\n\n最新的包可以从 Maven 的中心仓库获取。你将需要安装[依赖](http://socketio.github.io/socket.io-client-java/dependencies.html)。\n\n> 注意：v0.6.1 之后，包名改为了 io.socket 。请确认更新了依赖配置。\n\n#### Maven\n\n在 pom.xml 中添加依赖：\n\n    <dependencies>\n      <dependency>\n        <groupId>io.socket</groupId>\n        <artifactId>socket.io-client</artifactId>\n        <version>0.7.0</version>\n      </dependency>\n    </dependencies>\n\n#### Gradle\n\n在 build.gradle 中，给 Android Studio 添加 gradle 依赖：\n\n    compile ('io.socket:socket.io-client:0.7.0') {\n      // excluding org.json which is provided by Android\n      exclude group: 'org.json', module: 'json'\n    }\n\n### 使用\nSocket.IO-client Java 几乎拥有跟原生 JS 客户端相同的 api 和特性。使用 *IO.socket* 初始化 *Socket* ：\n\n    socket = IO.socket(\"http://localhost\");\n    socket.on(Socket.EVENT_CONNECT, new Emitter.Listener() {\n\n      @Override\n      public void call(Object... args) {\n        socket.emit(\"foo\", \"hi\");\n        socket.disconnect();\n      }\n\n    }).on(\"event\", new Emitter.Listener() {\n\n      @Override\n      public void call(Object... args) {}\n\n    }).on(Socket.EVENT_DISCONNECT, new  Emitter.Listener() {\n\n      @Override\n      public void call(Object... args) {}\n\n    });\n    socket.connect();\n\n这个类库使用 [org.json](http://www.json.org/) 解析和构造 JSON 字符串：\n\n    // Sending an object\n    JSONObject obj = new JSONObject();\n    obj.put(\"hello\", \"server\");\n    obj.put(\"binary\", new byte[42]);\n    socket.emit(\"foo\", obj);\n\n    // Receiving an object\n    socket.on(\"foo\", new Emitter.Listener() {\n      @Override\n      public void call(Object... args) {\n        JSONObject obj = (JSONObject)args[0];\n      }\n    });\n\n像下面这样设置选项：\n\n    IO.Options opts = new IO.Options();\n    opts.forceNew = true;\n    opts.reconnection = false;\n\n    socket = IO.socket(\"http://localhost\", opts);\n\n可以用 *query* 选项设置查询参数。请注意：如果当查询参数改变时不想复用一个缓存的 socket 实例，应该使用 *forceNew* 选项，可能的使用场景是如果你的应用允许用户退出，并且一个新的用户再次登录：\n\n    IO.Options opts = new IO.Options();\n    opts.forceNew = true;\n    opts.query = \"auth_token=\" + authToken;\n    Socket socket = IO.socket(\"http://localhost\", opts);\n\n当服务器接收到一个消息时，可以用 *Ack* 获取一个回调：\n\n    socket.emit(\"foo\", \"woot\", new Ack() {\n      @Override\n      public void call(Object... args) {}\n    });\n\n反之亦然：\n\n    // ack from client to server\n    socket.on(\"foo\", new Emitter.Listener() {\n      @Override\n      public void call(Object... args) {\n        Ack ack = (Ack) args[args.length - 1];\n        ack.call();\n      }\n    });\n\nSSL (HTTPS, WSS) 设置：\n\n    // default settings for all sockets\n    IO.setDefaultSSLContext(mySSLContext);\n    IO.setDefaultHostnameVerifier(myHostnameVerifier);\n\n    // set as an option\n    opts = new IO.Options();\n    opts.sslContext = mySSLContext;\n    opts.hostnameVerifier = myHostnameVerifier;\n    socket = IO.socket(\"https://localhost\", opts);\n\n查阅 Javadoc 获取更多详情：<http://socketio.github.io/socket.io-client-java/apidocs/>\n\n### Transports 和 HTTP 头\n\n可以像下面这样访问 transports 和它们的 HTTP 头：\n\n    // Called upon transport creation.\n    socket.io().on(Manager.EVENT_TRANSPORT, new Emitter.listener() {\n      @Override\n      public void call(Object... args) {\n        Transport transport = (Transport)args[0];\n\n        transport.on(Transport.EVENT_REQUEST_HEADERS, new Emitter.Listener() {\n          @Override\n          public void call(Object... args) {\n            @SuppressWarnings(\"unchecked\")\n            Map<String, List<String>> headers = (Map<String, List<String>>)args[0];\n            // modify request headers\n            headers.put(\"Cookie\", Arrays.asList(\"foo=1;\"));\n          }\n        });\n\n        transport.on(Transport.EVENT_RESPONSE_HEADERS, new Emitter.Listener() {\n          @Override\n          public void call(Object... args) {\n            @SuppressWarnings(\"unchecked\")\n            Map<String, List<String>> headers = (Map<String, List<String>>)args[0];\n            // access response headers\n            String cookie = headers.get(\"Set-Cookie\").get(0);\n          }\n        });\n      }\n    });\n\n### 特性\n\n这个类库支持 JS 客户端支持的所有特性，包括 events、options、upgrading transport。完全支持 Android。\n\n### License\n\nMIT\n","slug":"Socket-IO-Java-客户端","published":1,"updated":"2021-07-19T16:28:00.288Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphpt00aiitd34qsj8mo8","content":"<p>Socket.IO 是一个非常棒的项目；Java 是目前应用非常广的开发语言。两者的结合也是必然的。本篇翻译自 Socket.IO-client Java 项目的 github 主页。</p>\n<span id=\"more\"></span>\n\n<p><a href=\"https://github.com/socketio/socket.io-client-java\">Socket.IO-client Java</a> 是 Socket.IO v1.x 的 Java 客户端类库，这个类库是从 <a href=\"https://github.com/socketio/socket.io-client\">JavaScript client</a> 移植过来的。</p>\n<p>参见：</p>\n<ul>\n<li><a href=\"https://github.com/nkzawa/socket.io-android-chat\">Android chat demo</a></li>\n<li><a href=\"https://github.com/socketio/engine.io-client-java\">engine.io-client-java</a></li>\n</ul>\n<h3 id=\"安装\"><a href=\"#安装\" class=\"headerlink\" title=\"安装\"></a>安装</h3><p>最新的包可以从 Maven 的中心仓库获取。你将需要安装<a href=\"http://socketio.github.io/socket.io-client-java/dependencies.html\">依赖</a>。</p>\n<blockquote>\n<p>注意：v0.6.1 之后，包名改为了 io.socket 。请确认更新了依赖配置。</p>\n</blockquote>\n<h4 id=\"Maven\"><a href=\"#Maven\" class=\"headerlink\" title=\"Maven\"></a>Maven</h4><p>在 pom.xml 中添加依赖：</p>\n<pre><code>&lt;dependencies&gt;\n  &lt;dependency&gt;\n    &lt;groupId&gt;io.socket&lt;/groupId&gt;\n    &lt;artifactId&gt;socket.io-client&lt;/artifactId&gt;\n    &lt;version&gt;0.7.0&lt;/version&gt;\n  &lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre>\n<h4 id=\"Gradle\"><a href=\"#Gradle\" class=\"headerlink\" title=\"Gradle\"></a>Gradle</h4><p>在 build.gradle 中，给 Android Studio 添加 gradle 依赖：</p>\n<pre><code>compile (&#39;io.socket:socket.io-client:0.7.0&#39;) &#123;\n  // excluding org.json which is provided by Android\n  exclude group: &#39;org.json&#39;, module: &#39;json&#39;\n&#125;\n</code></pre>\n<h3 id=\"使用\"><a href=\"#使用\" class=\"headerlink\" title=\"使用\"></a>使用</h3><p>Socket.IO-client Java 几乎拥有跟原生 JS 客户端相同的 api 和特性。使用 <em>IO.socket</em> 初始化 <em>Socket</em> ：</p>\n<pre><code>socket = IO.socket(&quot;http://localhost&quot;);\nsocket.on(Socket.EVENT_CONNECT, new Emitter.Listener() &#123;\n\n  @Override\n  public void call(Object... args) &#123;\n    socket.emit(&quot;foo&quot;, &quot;hi&quot;);\n    socket.disconnect();\n  &#125;\n\n&#125;).on(&quot;event&quot;, new Emitter.Listener() &#123;\n\n  @Override\n  public void call(Object... args) &#123;&#125;\n\n&#125;).on(Socket.EVENT_DISCONNECT, new  Emitter.Listener() &#123;\n\n  @Override\n  public void call(Object... args) &#123;&#125;\n\n&#125;);\nsocket.connect();\n</code></pre>\n<p>这个类库使用 <a href=\"http://www.json.org/\">org.json</a> 解析和构造 JSON 字符串：</p>\n<pre><code>// Sending an object\nJSONObject obj = new JSONObject();\nobj.put(&quot;hello&quot;, &quot;server&quot;);\nobj.put(&quot;binary&quot;, new byte[42]);\nsocket.emit(&quot;foo&quot;, obj);\n\n// Receiving an object\nsocket.on(&quot;foo&quot;, new Emitter.Listener() &#123;\n  @Override\n  public void call(Object... args) &#123;\n    JSONObject obj = (JSONObject)args[0];\n  &#125;\n&#125;);\n</code></pre>\n<p>像下面这样设置选项：</p>\n<pre><code>IO.Options opts = new IO.Options();\nopts.forceNew = true;\nopts.reconnection = false;\n\nsocket = IO.socket(&quot;http://localhost&quot;, opts);\n</code></pre>\n<p>可以用 <em>query</em> 选项设置查询参数。请注意：如果当查询参数改变时不想复用一个缓存的 socket 实例，应该使用 <em>forceNew</em> 选项，可能的使用场景是如果你的应用允许用户退出，并且一个新的用户再次登录：</p>\n<pre><code>IO.Options opts = new IO.Options();\nopts.forceNew = true;\nopts.query = &quot;auth_token=&quot; + authToken;\nSocket socket = IO.socket(&quot;http://localhost&quot;, opts);\n</code></pre>\n<p>当服务器接收到一个消息时，可以用 <em>Ack</em> 获取一个回调：</p>\n<pre><code>socket.emit(&quot;foo&quot;, &quot;woot&quot;, new Ack() &#123;\n  @Override\n  public void call(Object... args) &#123;&#125;\n&#125;);\n</code></pre>\n<p>反之亦然：</p>\n<pre><code>// ack from client to server\nsocket.on(&quot;foo&quot;, new Emitter.Listener() &#123;\n  @Override\n  public void call(Object... args) &#123;\n    Ack ack = (Ack) args[args.length - 1];\n    ack.call();\n  &#125;\n&#125;);\n</code></pre>\n<p>SSL (HTTPS, WSS) 设置：</p>\n<pre><code>// default settings for all sockets\nIO.setDefaultSSLContext(mySSLContext);\nIO.setDefaultHostnameVerifier(myHostnameVerifier);\n\n// set as an option\nopts = new IO.Options();\nopts.sslContext = mySSLContext;\nopts.hostnameVerifier = myHostnameVerifier;\nsocket = IO.socket(&quot;https://localhost&quot;, opts);\n</code></pre>\n<p>查阅 Javadoc 获取更多详情：<a href=\"http://socketio.github.io/socket.io-client-java/apidocs/\">http://socketio.github.io/socket.io-client-java/apidocs/</a></p>\n<h3 id=\"Transports-和-HTTP-头\"><a href=\"#Transports-和-HTTP-头\" class=\"headerlink\" title=\"Transports 和 HTTP 头\"></a>Transports 和 HTTP 头</h3><p>可以像下面这样访问 transports 和它们的 HTTP 头：</p>\n<pre><code>// Called upon transport creation.\nsocket.io().on(Manager.EVENT_TRANSPORT, new Emitter.listener() &#123;\n  @Override\n  public void call(Object... args) &#123;\n    Transport transport = (Transport)args[0];\n\n    transport.on(Transport.EVENT_REQUEST_HEADERS, new Emitter.Listener() &#123;\n      @Override\n      public void call(Object... args) &#123;\n        @SuppressWarnings(&quot;unchecked&quot;)\n        Map&lt;String, List&lt;String&gt;&gt; headers = (Map&lt;String, List&lt;String&gt;&gt;)args[0];\n        // modify request headers\n        headers.put(&quot;Cookie&quot;, Arrays.asList(&quot;foo=1;&quot;));\n      &#125;\n    &#125;);\n\n    transport.on(Transport.EVENT_RESPONSE_HEADERS, new Emitter.Listener() &#123;\n      @Override\n      public void call(Object... args) &#123;\n        @SuppressWarnings(&quot;unchecked&quot;)\n        Map&lt;String, List&lt;String&gt;&gt; headers = (Map&lt;String, List&lt;String&gt;&gt;)args[0];\n        // access response headers\n        String cookie = headers.get(&quot;Set-Cookie&quot;).get(0);\n      &#125;\n    &#125;);\n  &#125;\n&#125;);\n</code></pre>\n<h3 id=\"特性\"><a href=\"#特性\" class=\"headerlink\" title=\"特性\"></a>特性</h3><p>这个类库支持 JS 客户端支持的所有特性，包括 events、options、upgrading transport。完全支持 Android。</p>\n<h3 id=\"License\"><a href=\"#License\" class=\"headerlink\" title=\"License\"></a>License</h3><p>MIT</p>\n","site":{"data":{}},"excerpt":"<p>Socket.IO 是一个非常棒的项目；Java 是目前应用非常广的开发语言。两者的结合也是必然的。本篇翻译自 Socket.IO-client Java 项目的 github 主页。</p>","more":"<p><a href=\"https://github.com/socketio/socket.io-client-java\">Socket.IO-client Java</a> 是 Socket.IO v1.x 的 Java 客户端类库，这个类库是从 <a href=\"https://github.com/socketio/socket.io-client\">JavaScript client</a> 移植过来的。</p>\n<p>参见：</p>\n<ul>\n<li><a href=\"https://github.com/nkzawa/socket.io-android-chat\">Android chat demo</a></li>\n<li><a href=\"https://github.com/socketio/engine.io-client-java\">engine.io-client-java</a></li>\n</ul>\n<h3 id=\"安装\"><a href=\"#安装\" class=\"headerlink\" title=\"安装\"></a>安装</h3><p>最新的包可以从 Maven 的中心仓库获取。你将需要安装<a href=\"http://socketio.github.io/socket.io-client-java/dependencies.html\">依赖</a>。</p>\n<blockquote>\n<p>注意：v0.6.1 之后，包名改为了 io.socket 。请确认更新了依赖配置。</p>\n</blockquote>\n<h4 id=\"Maven\"><a href=\"#Maven\" class=\"headerlink\" title=\"Maven\"></a>Maven</h4><p>在 pom.xml 中添加依赖：</p>\n<pre><code>&lt;dependencies&gt;\n  &lt;dependency&gt;\n    &lt;groupId&gt;io.socket&lt;/groupId&gt;\n    &lt;artifactId&gt;socket.io-client&lt;/artifactId&gt;\n    &lt;version&gt;0.7.0&lt;/version&gt;\n  &lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre>\n<h4 id=\"Gradle\"><a href=\"#Gradle\" class=\"headerlink\" title=\"Gradle\"></a>Gradle</h4><p>在 build.gradle 中，给 Android Studio 添加 gradle 依赖：</p>\n<pre><code>compile (&#39;io.socket:socket.io-client:0.7.0&#39;) &#123;\n  // excluding org.json which is provided by Android\n  exclude group: &#39;org.json&#39;, module: &#39;json&#39;\n&#125;\n</code></pre>\n<h3 id=\"使用\"><a href=\"#使用\" class=\"headerlink\" title=\"使用\"></a>使用</h3><p>Socket.IO-client Java 几乎拥有跟原生 JS 客户端相同的 api 和特性。使用 <em>IO.socket</em> 初始化 <em>Socket</em> ：</p>\n<pre><code>socket = IO.socket(&quot;http://localhost&quot;);\nsocket.on(Socket.EVENT_CONNECT, new Emitter.Listener() &#123;\n\n  @Override\n  public void call(Object... args) &#123;\n    socket.emit(&quot;foo&quot;, &quot;hi&quot;);\n    socket.disconnect();\n  &#125;\n\n&#125;).on(&quot;event&quot;, new Emitter.Listener() &#123;\n\n  @Override\n  public void call(Object... args) &#123;&#125;\n\n&#125;).on(Socket.EVENT_DISCONNECT, new  Emitter.Listener() &#123;\n\n  @Override\n  public void call(Object... args) &#123;&#125;\n\n&#125;);\nsocket.connect();\n</code></pre>\n<p>这个类库使用 <a href=\"http://www.json.org/\">org.json</a> 解析和构造 JSON 字符串：</p>\n<pre><code>// Sending an object\nJSONObject obj = new JSONObject();\nobj.put(&quot;hello&quot;, &quot;server&quot;);\nobj.put(&quot;binary&quot;, new byte[42]);\nsocket.emit(&quot;foo&quot;, obj);\n\n// Receiving an object\nsocket.on(&quot;foo&quot;, new Emitter.Listener() &#123;\n  @Override\n  public void call(Object... args) &#123;\n    JSONObject obj = (JSONObject)args[0];\n  &#125;\n&#125;);\n</code></pre>\n<p>像下面这样设置选项：</p>\n<pre><code>IO.Options opts = new IO.Options();\nopts.forceNew = true;\nopts.reconnection = false;\n\nsocket = IO.socket(&quot;http://localhost&quot;, opts);\n</code></pre>\n<p>可以用 <em>query</em> 选项设置查询参数。请注意：如果当查询参数改变时不想复用一个缓存的 socket 实例，应该使用 <em>forceNew</em> 选项，可能的使用场景是如果你的应用允许用户退出，并且一个新的用户再次登录：</p>\n<pre><code>IO.Options opts = new IO.Options();\nopts.forceNew = true;\nopts.query = &quot;auth_token=&quot; + authToken;\nSocket socket = IO.socket(&quot;http://localhost&quot;, opts);\n</code></pre>\n<p>当服务器接收到一个消息时，可以用 <em>Ack</em> 获取一个回调：</p>\n<pre><code>socket.emit(&quot;foo&quot;, &quot;woot&quot;, new Ack() &#123;\n  @Override\n  public void call(Object... args) &#123;&#125;\n&#125;);\n</code></pre>\n<p>反之亦然：</p>\n<pre><code>// ack from client to server\nsocket.on(&quot;foo&quot;, new Emitter.Listener() &#123;\n  @Override\n  public void call(Object... args) &#123;\n    Ack ack = (Ack) args[args.length - 1];\n    ack.call();\n  &#125;\n&#125;);\n</code></pre>\n<p>SSL (HTTPS, WSS) 设置：</p>\n<pre><code>// default settings for all sockets\nIO.setDefaultSSLContext(mySSLContext);\nIO.setDefaultHostnameVerifier(myHostnameVerifier);\n\n// set as an option\nopts = new IO.Options();\nopts.sslContext = mySSLContext;\nopts.hostnameVerifier = myHostnameVerifier;\nsocket = IO.socket(&quot;https://localhost&quot;, opts);\n</code></pre>\n<p>查阅 Javadoc 获取更多详情：<a href=\"http://socketio.github.io/socket.io-client-java/apidocs/\">http://socketio.github.io/socket.io-client-java/apidocs/</a></p>\n<h3 id=\"Transports-和-HTTP-头\"><a href=\"#Transports-和-HTTP-头\" class=\"headerlink\" title=\"Transports 和 HTTP 头\"></a>Transports 和 HTTP 头</h3><p>可以像下面这样访问 transports 和它们的 HTTP 头：</p>\n<pre><code>// Called upon transport creation.\nsocket.io().on(Manager.EVENT_TRANSPORT, new Emitter.listener() &#123;\n  @Override\n  public void call(Object... args) &#123;\n    Transport transport = (Transport)args[0];\n\n    transport.on(Transport.EVENT_REQUEST_HEADERS, new Emitter.Listener() &#123;\n      @Override\n      public void call(Object... args) &#123;\n        @SuppressWarnings(&quot;unchecked&quot;)\n        Map&lt;String, List&lt;String&gt;&gt; headers = (Map&lt;String, List&lt;String&gt;&gt;)args[0];\n        // modify request headers\n        headers.put(&quot;Cookie&quot;, Arrays.asList(&quot;foo=1;&quot;));\n      &#125;\n    &#125;);\n\n    transport.on(Transport.EVENT_RESPONSE_HEADERS, new Emitter.Listener() &#123;\n      @Override\n      public void call(Object... args) &#123;\n        @SuppressWarnings(&quot;unchecked&quot;)\n        Map&lt;String, List&lt;String&gt;&gt; headers = (Map&lt;String, List&lt;String&gt;&gt;)args[0];\n        // access response headers\n        String cookie = headers.get(&quot;Set-Cookie&quot;).get(0);\n      &#125;\n    &#125;);\n  &#125;\n&#125;);\n</code></pre>\n<h3 id=\"特性\"><a href=\"#特性\" class=\"headerlink\" title=\"特性\"></a>特性</h3><p>这个类库支持 JS 客户端支持的所有特性，包括 events、options、upgrading transport。完全支持 Android。</p>\n<h3 id=\"License\"><a href=\"#License\" class=\"headerlink\" title=\"License\"></a>License</h3><p>MIT</p>"},{"title":"Socket.IO 聊天应用实例","date":"2016-06-04T19:17:11.000Z","_content":"\n本篇翻译自 Socket.IO 官网的入门实例：<http://socket.io/get-started/chat/>\n\n在本篇指导中，我们将创建一个基本的聊天应用。这个应用几乎不要求事先具有 Node.JS 或 Socket.IO 的基础知识，因此对任意知识水平的用户它都是适合的。\n\n<!-- more -->\n\n### 引言\n\n使用流行的 Web 应用工具栈（如 LAMP（PHP））写一个聊天应用通常是非常困难的。这涉及到从服务器拉取变化的信息，保持时间戳的跟踪，并且这比应该需要的速度慢很多。\n\n通常是大多数实时聊天系统都是围绕 Sockets 设计解决方案，在客户端和服务端提供一个双向通信通道。\n\n这意味着服务端可以推送消息给客户端。其思想是，每当你写了一个聊天消息，服务端将获取它，并把它推送给所有其他连接的客户端。\n\n### Web 框架\n\n第一个目标是，设置一个提供提供表单和消息列表的简单 HTML 网页。我们将使用 Node.JS Web 框架 express 来达到目的。确保已经[安装了 Node.JS](https://nodejs.org/en/)。\n\n首先，创建清单文件 package.json 来描述我们的项目。建议你将这个文件放在专门的一个空目录下（如，chat-example）。\n\n    {\n      \"name\": \"socket-chat-example\",\n      \"version\": \"0.0.1\",\n      \"description\": \"my first socket.io app\",\n      \"dependencies\": {}\n    }\n\n现在，为了方便迁移需要的依赖，我们使用 npm install --save:\n\n    npm install --save express@4.10.2\n\n这样 Express 就安装好了。现在，我们创建应用的 index.js 文件：\n\n    var app = require('express')();\n    var http = require('http').Server(app);\n\n    app.get('/', function(req, res){\n      res.send('<h1>Hello world</h1>');\n    });\n\n    http.listen(3000, function(){\n      console.log('listening on *:3000');\n    });\n\n说明：\n\n1. Express 初始化 app 作为功能处理器，你可以将它传给 HTTP 服务器（如第 2 行看到的）。\n2. 定义一个路由处理器 / ，当我们访问网站主页时就会调用这个处理器。\n3. HTTP 服务器监听端口 3000。\n\n执行命令 node index.js 将看到下面的信息：![index](/uploads/20160604/index.png)\n\n在浏览器地址栏输入 http://localhost:3000 访问我们的 Web 应用：![index_page](/uploads/20160604/index_page.png)\n\n### HTML 服务\n\n目前为止，我们在 index.js 中调用 res.send，并且传给它一个 HTML 字符串。如果我们把整个应用的 HTML 都放到这里，代码会非常混乱。应该是，我们创建一个 index.html 文件并用它提供服务。\n\n让我们使用 sendFile 方法重构路由处理器：\n\n    app.get('/', function(req, res){\n      res.sendFile(__dirname + '/index.html');\n    });\n\n创建 index.html 文件，内容如下：\n\n    <!doctype html>\n    <html>\n      <head>\n        <title>Socket.IO chat</title>\n        <style>\n          * { margin: 0; padding: 0; box-sizing: border-box; }\n          body { font: 13px Helvetica, Arial; }\n          form { background: #000; padding: 3px; position: fixed; bottom: 0; width: 100%; }\n          form input { border: 0; padding: 10px; width: 90%; margin-right: .5%; }\n          form button { width: 9%; background: rgb(130, 224, 255); border: none; padding: 10px; }\n          #messages { list-style-type: none; margin: 0; padding: 0; }\n          #messages li { padding: 5px 10px; }\n          #messages li:nth-child(odd) { background: #eee; }\n        </style>\n      </head>\n      <body>\n        <ul id=\"messages\"></ul>\n        <form action=\"\">\n          <input id=\"m\" autocomplete=\"off\" /><button>Send</button>\n        </form>\n      </body>\n    </html>\n\n重启进程（输入 Control+C 并再次运行 node index），并刷新，页面展示如下：![index_page_2](/uploads/20160604/index_page_2.png)\n\n### 整合 Socket.IO\n\nSocket.IO 由两部分组成：\n\n- 整合（或挂载）了 Node.JS HTTP 服务器的服务器：socket.io\n- 在浏览器端加载的客户端库：socket.io-client\n\n开发时，socket.io 自动为我们提供这个客户端，像我们将看到的，因此现在我们仅需要安装一个模块：\n\n    npm install --save socket.io\n\n这个命令将安装这个模块并添加依赖到 package.json。现在修改 index.js：\n\n    var app = require('express')();\n    var http = require('http').Server(app);\n    var io = require('socket.io')(http);\n\n    app.get('/', function(req, res){\n      res.sendfile('index.html');\n    });\n\n    io.on('connection', function(socket){\n      console.log('a user connected');\n    });\n\n    http.listen(3000, function(){\n      console.log('listening on *:3000');\n    });\n\n注意，通过传递 http（HTTP 服务器）对象初始化了一个 socket.io 的新实例。然后为到来的 socket 监听 connection 事件，并且输入日志到控制台。\n\n现在，在 index.html 中 </body> 前添加下面的片段：\n\n    <script src=\"/socket.io/socket.io.js\"></script>\n    <script>\n      var socket = io();\n    </script>\n\n这将加载 socket.io-client，创建全局变量 io，并且连接。\n\n注意，当调用 io() 的时候没有制定任何 URL，因为它默认尝试连接提供页面服务的主机。\n\n现在重新加载服务器和网站将会看到控制台打印“a user connected”。尝试打开多个标签页，将看到多条消息：![console](/uploads/20160604/console.png)\n\n每个 socket 也监听一个特殊的 disconnect 事件：\n\n    io.on('connection', function(socket){\n      console.log('a user connected');\n      socket.on('disconnect', function(){\n        console.log('user disconnected');\n      });\n    });\n\n如果多次刷新一个标签页将看到下面的动作：![console2.png](/uploads/20160604/console2.png)\n\n### 发送事件\n\nSocket.IO 背后主要的思想是你可以发送和接收想要的任何事件，携带你想要的任何数据。任何可以编码为 JSON 的对象都可以做到，并且也支持二进制数据。\n\n让我们来实现当一个用户输入一条消息时，服务器作为一个 chat message 事件获取它。在 index.html 中的脚本现在看起来如下：\n\n    <script src=\"/socket.io/socket.io.js\"></script>\n    <script src=\"http://code.jquery.com/jquery-1.11.1.js\"></script>\n    <script>\n      var socket = io();\n      $('form').submit(function(){\n        socket.emit('chat message', $('#m').val());\n        $('#m').val('');\n        return false;\n      });\n    </script>\n\n在 index.js 中我们打印出 chat message 事件：\n\n    io.on('connection', function(socket){\n      socket.on('chat message', function(msg){\n        console.log('message: ' + msg);\n      });\n    });\n\n结果像下面的视频展示的：\n<p><video autoplay loop width=\"100%\"><source src=\"/uploads/20160604/video.mp4\"></video></p>\n\n### 广播\n\n下一个目标是我们从服务器发送事件给其他用户。\n\n为了发送事件给所有人，Socket.IO 给我们提供了 io.emit：\n\n    io.emit('some event', { for: 'everyone' });\n\n如果你想发送一个确定 socket 的消息给所有人，我们有 broadcast 标识：\n\n    io.on('connection', function(socket){\n      socket.broadcast.emit('hi');\n    });\n\n在这个例子中，为了简单，为了简单我们发送消息给所有人，包括发送者。\n\n    io.on('connection', function(socket){\n      socket.on('chat message', function(msg){\n        io.emit('chat message', msg);\n      });\n    });\n\n在客户端，当我们捕获到一个 chat message 事件，我们把它包含到页面中。客户端完整的代码如下：\n\n    <script>\n      var socket = io();\n      $('form').submit(function(){\n        socket.emit('chat message', $('#m').val());\n        $('#m').val('');\n        return false;\n      });\n      socket.on('chat message', function(msg){\n        $('#messages').append($('<li>').text(msg));\n      });\n    </script>\n\n我们的聊天应用就完成了，用了大概 20 行代码！看起来像这样：\n<p><video autoplay loop width=\"100%\"><source src=\"/uploads/20160604/video2.mp4\"></video></p>\n","source":"_posts/Socket-IO-聊天应用实例.md","raw":"title: Socket.IO 聊天应用实例\ntags:\n  - Socket.IO\n  - Node.js\n  - WebSocket\ncategories:\n  - 工具箱\n  - Socket.IO\ndate: 2016-06-05 03:17:11\n---\n\n本篇翻译自 Socket.IO 官网的入门实例：<http://socket.io/get-started/chat/>\n\n在本篇指导中，我们将创建一个基本的聊天应用。这个应用几乎不要求事先具有 Node.JS 或 Socket.IO 的基础知识，因此对任意知识水平的用户它都是适合的。\n\n<!-- more -->\n\n### 引言\n\n使用流行的 Web 应用工具栈（如 LAMP（PHP））写一个聊天应用通常是非常困难的。这涉及到从服务器拉取变化的信息，保持时间戳的跟踪，并且这比应该需要的速度慢很多。\n\n通常是大多数实时聊天系统都是围绕 Sockets 设计解决方案，在客户端和服务端提供一个双向通信通道。\n\n这意味着服务端可以推送消息给客户端。其思想是，每当你写了一个聊天消息，服务端将获取它，并把它推送给所有其他连接的客户端。\n\n### Web 框架\n\n第一个目标是，设置一个提供提供表单和消息列表的简单 HTML 网页。我们将使用 Node.JS Web 框架 express 来达到目的。确保已经[安装了 Node.JS](https://nodejs.org/en/)。\n\n首先，创建清单文件 package.json 来描述我们的项目。建议你将这个文件放在专门的一个空目录下（如，chat-example）。\n\n    {\n      \"name\": \"socket-chat-example\",\n      \"version\": \"0.0.1\",\n      \"description\": \"my first socket.io app\",\n      \"dependencies\": {}\n    }\n\n现在，为了方便迁移需要的依赖，我们使用 npm install --save:\n\n    npm install --save express@4.10.2\n\n这样 Express 就安装好了。现在，我们创建应用的 index.js 文件：\n\n    var app = require('express')();\n    var http = require('http').Server(app);\n\n    app.get('/', function(req, res){\n      res.send('<h1>Hello world</h1>');\n    });\n\n    http.listen(3000, function(){\n      console.log('listening on *:3000');\n    });\n\n说明：\n\n1. Express 初始化 app 作为功能处理器，你可以将它传给 HTTP 服务器（如第 2 行看到的）。\n2. 定义一个路由处理器 / ，当我们访问网站主页时就会调用这个处理器。\n3. HTTP 服务器监听端口 3000。\n\n执行命令 node index.js 将看到下面的信息：![index](/uploads/20160604/index.png)\n\n在浏览器地址栏输入 http://localhost:3000 访问我们的 Web 应用：![index_page](/uploads/20160604/index_page.png)\n\n### HTML 服务\n\n目前为止，我们在 index.js 中调用 res.send，并且传给它一个 HTML 字符串。如果我们把整个应用的 HTML 都放到这里，代码会非常混乱。应该是，我们创建一个 index.html 文件并用它提供服务。\n\n让我们使用 sendFile 方法重构路由处理器：\n\n    app.get('/', function(req, res){\n      res.sendFile(__dirname + '/index.html');\n    });\n\n创建 index.html 文件，内容如下：\n\n    <!doctype html>\n    <html>\n      <head>\n        <title>Socket.IO chat</title>\n        <style>\n          * { margin: 0; padding: 0; box-sizing: border-box; }\n          body { font: 13px Helvetica, Arial; }\n          form { background: #000; padding: 3px; position: fixed; bottom: 0; width: 100%; }\n          form input { border: 0; padding: 10px; width: 90%; margin-right: .5%; }\n          form button { width: 9%; background: rgb(130, 224, 255); border: none; padding: 10px; }\n          #messages { list-style-type: none; margin: 0; padding: 0; }\n          #messages li { padding: 5px 10px; }\n          #messages li:nth-child(odd) { background: #eee; }\n        </style>\n      </head>\n      <body>\n        <ul id=\"messages\"></ul>\n        <form action=\"\">\n          <input id=\"m\" autocomplete=\"off\" /><button>Send</button>\n        </form>\n      </body>\n    </html>\n\n重启进程（输入 Control+C 并再次运行 node index），并刷新，页面展示如下：![index_page_2](/uploads/20160604/index_page_2.png)\n\n### 整合 Socket.IO\n\nSocket.IO 由两部分组成：\n\n- 整合（或挂载）了 Node.JS HTTP 服务器的服务器：socket.io\n- 在浏览器端加载的客户端库：socket.io-client\n\n开发时，socket.io 自动为我们提供这个客户端，像我们将看到的，因此现在我们仅需要安装一个模块：\n\n    npm install --save socket.io\n\n这个命令将安装这个模块并添加依赖到 package.json。现在修改 index.js：\n\n    var app = require('express')();\n    var http = require('http').Server(app);\n    var io = require('socket.io')(http);\n\n    app.get('/', function(req, res){\n      res.sendfile('index.html');\n    });\n\n    io.on('connection', function(socket){\n      console.log('a user connected');\n    });\n\n    http.listen(3000, function(){\n      console.log('listening on *:3000');\n    });\n\n注意，通过传递 http（HTTP 服务器）对象初始化了一个 socket.io 的新实例。然后为到来的 socket 监听 connection 事件，并且输入日志到控制台。\n\n现在，在 index.html 中 </body> 前添加下面的片段：\n\n    <script src=\"/socket.io/socket.io.js\"></script>\n    <script>\n      var socket = io();\n    </script>\n\n这将加载 socket.io-client，创建全局变量 io，并且连接。\n\n注意，当调用 io() 的时候没有制定任何 URL，因为它默认尝试连接提供页面服务的主机。\n\n现在重新加载服务器和网站将会看到控制台打印“a user connected”。尝试打开多个标签页，将看到多条消息：![console](/uploads/20160604/console.png)\n\n每个 socket 也监听一个特殊的 disconnect 事件：\n\n    io.on('connection', function(socket){\n      console.log('a user connected');\n      socket.on('disconnect', function(){\n        console.log('user disconnected');\n      });\n    });\n\n如果多次刷新一个标签页将看到下面的动作：![console2.png](/uploads/20160604/console2.png)\n\n### 发送事件\n\nSocket.IO 背后主要的思想是你可以发送和接收想要的任何事件，携带你想要的任何数据。任何可以编码为 JSON 的对象都可以做到，并且也支持二进制数据。\n\n让我们来实现当一个用户输入一条消息时，服务器作为一个 chat message 事件获取它。在 index.html 中的脚本现在看起来如下：\n\n    <script src=\"/socket.io/socket.io.js\"></script>\n    <script src=\"http://code.jquery.com/jquery-1.11.1.js\"></script>\n    <script>\n      var socket = io();\n      $('form').submit(function(){\n        socket.emit('chat message', $('#m').val());\n        $('#m').val('');\n        return false;\n      });\n    </script>\n\n在 index.js 中我们打印出 chat message 事件：\n\n    io.on('connection', function(socket){\n      socket.on('chat message', function(msg){\n        console.log('message: ' + msg);\n      });\n    });\n\n结果像下面的视频展示的：\n<p><video autoplay loop width=\"100%\"><source src=\"/uploads/20160604/video.mp4\"></video></p>\n\n### 广播\n\n下一个目标是我们从服务器发送事件给其他用户。\n\n为了发送事件给所有人，Socket.IO 给我们提供了 io.emit：\n\n    io.emit('some event', { for: 'everyone' });\n\n如果你想发送一个确定 socket 的消息给所有人，我们有 broadcast 标识：\n\n    io.on('connection', function(socket){\n      socket.broadcast.emit('hi');\n    });\n\n在这个例子中，为了简单，为了简单我们发送消息给所有人，包括发送者。\n\n    io.on('connection', function(socket){\n      socket.on('chat message', function(msg){\n        io.emit('chat message', msg);\n      });\n    });\n\n在客户端，当我们捕获到一个 chat message 事件，我们把它包含到页面中。客户端完整的代码如下：\n\n    <script>\n      var socket = io();\n      $('form').submit(function(){\n        socket.emit('chat message', $('#m').val());\n        $('#m').val('');\n        return false;\n      });\n      socket.on('chat message', function(msg){\n        $('#messages').append($('<li>').text(msg));\n      });\n    </script>\n\n我们的聊天应用就完成了，用了大概 20 行代码！看起来像这样：\n<p><video autoplay loop width=\"100%\"><source src=\"/uploads/20160604/video2.mp4\"></video></p>\n","slug":"Socket-IO-聊天应用实例","published":1,"updated":"2021-07-19T16:28:00.288Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphpv00amitd3hmxh1a9h","content":"<p>本篇翻译自 Socket.IO 官网的入门实例：<a href=\"http://socket.io/get-started/chat/\">http://socket.io/get-started/chat/</a></p>\n<p>在本篇指导中，我们将创建一个基本的聊天应用。这个应用几乎不要求事先具有 Node.JS 或 Socket.IO 的基础知识，因此对任意知识水平的用户它都是适合的。</p>\n<span id=\"more\"></span>\n\n<h3 id=\"引言\"><a href=\"#引言\" class=\"headerlink\" title=\"引言\"></a>引言</h3><p>使用流行的 Web 应用工具栈（如 LAMP（PHP））写一个聊天应用通常是非常困难的。这涉及到从服务器拉取变化的信息，保持时间戳的跟踪，并且这比应该需要的速度慢很多。</p>\n<p>通常是大多数实时聊天系统都是围绕 Sockets 设计解决方案，在客户端和服务端提供一个双向通信通道。</p>\n<p>这意味着服务端可以推送消息给客户端。其思想是，每当你写了一个聊天消息，服务端将获取它，并把它推送给所有其他连接的客户端。</p>\n<h3 id=\"Web-框架\"><a href=\"#Web-框架\" class=\"headerlink\" title=\"Web 框架\"></a>Web 框架</h3><p>第一个目标是，设置一个提供提供表单和消息列表的简单 HTML 网页。我们将使用 Node.JS Web 框架 express 来达到目的。确保已经<a href=\"https://nodejs.org/en/\">安装了 Node.JS</a>。</p>\n<p>首先，创建清单文件 package.json 来描述我们的项目。建议你将这个文件放在专门的一个空目录下（如，chat-example）。</p>\n<pre><code>&#123;\n  &quot;name&quot;: &quot;socket-chat-example&quot;,\n  &quot;version&quot;: &quot;0.0.1&quot;,\n  &quot;description&quot;: &quot;my first socket.io app&quot;,\n  &quot;dependencies&quot;: &#123;&#125;\n&#125;\n</code></pre>\n<p>现在，为了方便迁移需要的依赖，我们使用 npm install –save:</p>\n<pre><code>npm install --save express@4.10.2\n</code></pre>\n<p>这样 Express 就安装好了。现在，我们创建应用的 index.js 文件：</p>\n<pre><code>var app = require(&#39;express&#39;)();\nvar http = require(&#39;http&#39;).Server(app);\n\napp.get(&#39;/&#39;, function(req, res)&#123;\n  res.send(&#39;&lt;h1&gt;Hello world&lt;/h1&gt;&#39;);\n&#125;);\n\nhttp.listen(3000, function()&#123;\n  console.log(&#39;listening on *:3000&#39;);\n&#125;);\n</code></pre>\n<p>说明：</p>\n<ol>\n<li>Express 初始化 app 作为功能处理器，你可以将它传给 HTTP 服务器（如第 2 行看到的）。</li>\n<li>定义一个路由处理器 / ，当我们访问网站主页时就会调用这个处理器。</li>\n<li>HTTP 服务器监听端口 3000。</li>\n</ol>\n<p>执行命令 node index.js 将看到下面的信息：<img src=\"/uploads/20160604/index.png\" alt=\"index\"></p>\n<p>在浏览器地址栏输入 <a href=\"http://localhost:3000/\">http://localhost:3000</a> 访问我们的 Web 应用：<img src=\"/uploads/20160604/index_page.png\" alt=\"index_page\"></p>\n<h3 id=\"HTML-服务\"><a href=\"#HTML-服务\" class=\"headerlink\" title=\"HTML 服务\"></a>HTML 服务</h3><p>目前为止，我们在 index.js 中调用 res.send，并且传给它一个 HTML 字符串。如果我们把整个应用的 HTML 都放到这里，代码会非常混乱。应该是，我们创建一个 index.html 文件并用它提供服务。</p>\n<p>让我们使用 sendFile 方法重构路由处理器：</p>\n<pre><code>app.get(&#39;/&#39;, function(req, res)&#123;\n  res.sendFile(__dirname + &#39;/index.html&#39;);\n&#125;);\n</code></pre>\n<p>创建 index.html 文件，内容如下：</p>\n<pre><code>&lt;!doctype html&gt;\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;title&gt;Socket.IO chat&lt;/title&gt;\n    &lt;style&gt;\n      * &#123; margin: 0; padding: 0; box-sizing: border-box; &#125;\n      body &#123; font: 13px Helvetica, Arial; &#125;\n      form &#123; background: #000; padding: 3px; position: fixed; bottom: 0; width: 100%; &#125;\n      form input &#123; border: 0; padding: 10px; width: 90%; margin-right: .5%; &#125;\n      form button &#123; width: 9%; background: rgb(130, 224, 255); border: none; padding: 10px; &#125;\n      #messages &#123; list-style-type: none; margin: 0; padding: 0; &#125;\n      #messages li &#123; padding: 5px 10px; &#125;\n      #messages li:nth-child(odd) &#123; background: #eee; &#125;\n    &lt;/style&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;ul id=&quot;messages&quot;&gt;&lt;/ul&gt;\n    &lt;form action=&quot;&quot;&gt;\n      &lt;input id=&quot;m&quot; autocomplete=&quot;off&quot; /&gt;&lt;button&gt;Send&lt;/button&gt;\n    &lt;/form&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre>\n<p>重启进程（输入 Control+C 并再次运行 node index），并刷新，页面展示如下：<img src=\"/uploads/20160604/index_page_2.png\" alt=\"index_page_2\"></p>\n<h3 id=\"整合-Socket-IO\"><a href=\"#整合-Socket-IO\" class=\"headerlink\" title=\"整合 Socket.IO\"></a>整合 Socket.IO</h3><p>Socket.IO 由两部分组成：</p>\n<ul>\n<li>整合（或挂载）了 Node.JS HTTP 服务器的服务器：socket.io</li>\n<li>在浏览器端加载的客户端库：socket.io-client</li>\n</ul>\n<p>开发时，socket.io 自动为我们提供这个客户端，像我们将看到的，因此现在我们仅需要安装一个模块：</p>\n<pre><code>npm install --save socket.io\n</code></pre>\n<p>这个命令将安装这个模块并添加依赖到 package.json。现在修改 index.js：</p>\n<pre><code>var app = require(&#39;express&#39;)();\nvar http = require(&#39;http&#39;).Server(app);\nvar io = require(&#39;socket.io&#39;)(http);\n\napp.get(&#39;/&#39;, function(req, res)&#123;\n  res.sendfile(&#39;index.html&#39;);\n&#125;);\n\nio.on(&#39;connection&#39;, function(socket)&#123;\n  console.log(&#39;a user connected&#39;);\n&#125;);\n\nhttp.listen(3000, function()&#123;\n  console.log(&#39;listening on *:3000&#39;);\n&#125;);\n</code></pre>\n<p>注意，通过传递 http（HTTP 服务器）对象初始化了一个 socket.io 的新实例。然后为到来的 socket 监听 connection 事件，并且输入日志到控制台。</p>\n<p>现在，在 index.html 中 </body> 前添加下面的片段：</p>\n<pre><code>&lt;script src=&quot;/socket.io/socket.io.js&quot;&gt;&lt;/script&gt;\n&lt;script&gt;\n  var socket = io();\n&lt;/script&gt;\n</code></pre>\n<p>这将加载 socket.io-client，创建全局变量 io，并且连接。</p>\n<p>注意，当调用 io() 的时候没有制定任何 URL，因为它默认尝试连接提供页面服务的主机。</p>\n<p>现在重新加载服务器和网站将会看到控制台打印“a user connected”。尝试打开多个标签页，将看到多条消息：<img src=\"/uploads/20160604/console.png\" alt=\"console\"></p>\n<p>每个 socket 也监听一个特殊的 disconnect 事件：</p>\n<pre><code>io.on(&#39;connection&#39;, function(socket)&#123;\n  console.log(&#39;a user connected&#39;);\n  socket.on(&#39;disconnect&#39;, function()&#123;\n    console.log(&#39;user disconnected&#39;);\n  &#125;);\n&#125;);\n</code></pre>\n<p>如果多次刷新一个标签页将看到下面的动作：<img src=\"/uploads/20160604/console2.png\" alt=\"console2.png\"></p>\n<h3 id=\"发送事件\"><a href=\"#发送事件\" class=\"headerlink\" title=\"发送事件\"></a>发送事件</h3><p>Socket.IO 背后主要的思想是你可以发送和接收想要的任何事件，携带你想要的任何数据。任何可以编码为 JSON 的对象都可以做到，并且也支持二进制数据。</p>\n<p>让我们来实现当一个用户输入一条消息时，服务器作为一个 chat message 事件获取它。在 index.html 中的脚本现在看起来如下：</p>\n<pre><code>&lt;script src=&quot;/socket.io/socket.io.js&quot;&gt;&lt;/script&gt;\n&lt;script src=&quot;http://code.jquery.com/jquery-1.11.1.js&quot;&gt;&lt;/script&gt;\n&lt;script&gt;\n  var socket = io();\n  $(&#39;form&#39;).submit(function()&#123;\n    socket.emit(&#39;chat message&#39;, $(&#39;#m&#39;).val());\n    $(&#39;#m&#39;).val(&#39;&#39;);\n    return false;\n  &#125;);\n&lt;/script&gt;\n</code></pre>\n<p>在 index.js 中我们打印出 chat message 事件：</p>\n<pre><code>io.on(&#39;connection&#39;, function(socket)&#123;\n  socket.on(&#39;chat message&#39;, function(msg)&#123;\n    console.log(&#39;message: &#39; + msg);\n  &#125;);\n&#125;);\n</code></pre>\n<p>结果像下面的视频展示的：</p>\n<p><video autoplay loop width=\"100%\"><source src=\"/uploads/20160604/video.mp4\"></video></p>\n\n<h3 id=\"广播\"><a href=\"#广播\" class=\"headerlink\" title=\"广播\"></a>广播</h3><p>下一个目标是我们从服务器发送事件给其他用户。</p>\n<p>为了发送事件给所有人，Socket.IO 给我们提供了 io.emit：</p>\n<pre><code>io.emit(&#39;some event&#39;, &#123; for: &#39;everyone&#39; &#125;);\n</code></pre>\n<p>如果你想发送一个确定 socket 的消息给所有人，我们有 broadcast 标识：</p>\n<pre><code>io.on(&#39;connection&#39;, function(socket)&#123;\n  socket.broadcast.emit(&#39;hi&#39;);\n&#125;);\n</code></pre>\n<p>在这个例子中，为了简单，为了简单我们发送消息给所有人，包括发送者。</p>\n<pre><code>io.on(&#39;connection&#39;, function(socket)&#123;\n  socket.on(&#39;chat message&#39;, function(msg)&#123;\n    io.emit(&#39;chat message&#39;, msg);\n  &#125;);\n&#125;);\n</code></pre>\n<p>在客户端，当我们捕获到一个 chat message 事件，我们把它包含到页面中。客户端完整的代码如下：</p>\n<pre><code>&lt;script&gt;\n  var socket = io();\n  $(&#39;form&#39;).submit(function()&#123;\n    socket.emit(&#39;chat message&#39;, $(&#39;#m&#39;).val());\n    $(&#39;#m&#39;).val(&#39;&#39;);\n    return false;\n  &#125;);\n  socket.on(&#39;chat message&#39;, function(msg)&#123;\n    $(&#39;#messages&#39;).append($(&#39;&lt;li&gt;&#39;).text(msg));\n  &#125;);\n&lt;/script&gt;\n</code></pre>\n<p>我们的聊天应用就完成了，用了大概 20 行代码！看起来像这样：</p>\n<p><video autoplay loop width=\"100%\"><source src=\"/uploads/20160604/video2.mp4\"></video></p>\n","site":{"data":{}},"excerpt":"<p>本篇翻译自 Socket.IO 官网的入门实例：<a href=\"http://socket.io/get-started/chat/\">http://socket.io/get-started/chat/</a></p>\n<p>在本篇指导中，我们将创建一个基本的聊天应用。这个应用几乎不要求事先具有 Node.JS 或 Socket.IO 的基础知识，因此对任意知识水平的用户它都是适合的。</p>","more":"<h3 id=\"引言\"><a href=\"#引言\" class=\"headerlink\" title=\"引言\"></a>引言</h3><p>使用流行的 Web 应用工具栈（如 LAMP（PHP））写一个聊天应用通常是非常困难的。这涉及到从服务器拉取变化的信息，保持时间戳的跟踪，并且这比应该需要的速度慢很多。</p>\n<p>通常是大多数实时聊天系统都是围绕 Sockets 设计解决方案，在客户端和服务端提供一个双向通信通道。</p>\n<p>这意味着服务端可以推送消息给客户端。其思想是，每当你写了一个聊天消息，服务端将获取它，并把它推送给所有其他连接的客户端。</p>\n<h3 id=\"Web-框架\"><a href=\"#Web-框架\" class=\"headerlink\" title=\"Web 框架\"></a>Web 框架</h3><p>第一个目标是，设置一个提供提供表单和消息列表的简单 HTML 网页。我们将使用 Node.JS Web 框架 express 来达到目的。确保已经<a href=\"https://nodejs.org/en/\">安装了 Node.JS</a>。</p>\n<p>首先，创建清单文件 package.json 来描述我们的项目。建议你将这个文件放在专门的一个空目录下（如，chat-example）。</p>\n<pre><code>&#123;\n  &quot;name&quot;: &quot;socket-chat-example&quot;,\n  &quot;version&quot;: &quot;0.0.1&quot;,\n  &quot;description&quot;: &quot;my first socket.io app&quot;,\n  &quot;dependencies&quot;: &#123;&#125;\n&#125;\n</code></pre>\n<p>现在，为了方便迁移需要的依赖，我们使用 npm install –save:</p>\n<pre><code>npm install --save express@4.10.2\n</code></pre>\n<p>这样 Express 就安装好了。现在，我们创建应用的 index.js 文件：</p>\n<pre><code>var app = require(&#39;express&#39;)();\nvar http = require(&#39;http&#39;).Server(app);\n\napp.get(&#39;/&#39;, function(req, res)&#123;\n  res.send(&#39;&lt;h1&gt;Hello world&lt;/h1&gt;&#39;);\n&#125;);\n\nhttp.listen(3000, function()&#123;\n  console.log(&#39;listening on *:3000&#39;);\n&#125;);\n</code></pre>\n<p>说明：</p>\n<ol>\n<li>Express 初始化 app 作为功能处理器，你可以将它传给 HTTP 服务器（如第 2 行看到的）。</li>\n<li>定义一个路由处理器 / ，当我们访问网站主页时就会调用这个处理器。</li>\n<li>HTTP 服务器监听端口 3000。</li>\n</ol>\n<p>执行命令 node index.js 将看到下面的信息：<img src=\"/uploads/20160604/index.png\" alt=\"index\"></p>\n<p>在浏览器地址栏输入 <a href=\"http://localhost:3000/\">http://localhost:3000</a> 访问我们的 Web 应用：<img src=\"/uploads/20160604/index_page.png\" alt=\"index_page\"></p>\n<h3 id=\"HTML-服务\"><a href=\"#HTML-服务\" class=\"headerlink\" title=\"HTML 服务\"></a>HTML 服务</h3><p>目前为止，我们在 index.js 中调用 res.send，并且传给它一个 HTML 字符串。如果我们把整个应用的 HTML 都放到这里，代码会非常混乱。应该是，我们创建一个 index.html 文件并用它提供服务。</p>\n<p>让我们使用 sendFile 方法重构路由处理器：</p>\n<pre><code>app.get(&#39;/&#39;, function(req, res)&#123;\n  res.sendFile(__dirname + &#39;/index.html&#39;);\n&#125;);\n</code></pre>\n<p>创建 index.html 文件，内容如下：</p>\n<pre><code>&lt;!doctype html&gt;\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;title&gt;Socket.IO chat&lt;/title&gt;\n    &lt;style&gt;\n      * &#123; margin: 0; padding: 0; box-sizing: border-box; &#125;\n      body &#123; font: 13px Helvetica, Arial; &#125;\n      form &#123; background: #000; padding: 3px; position: fixed; bottom: 0; width: 100%; &#125;\n      form input &#123; border: 0; padding: 10px; width: 90%; margin-right: .5%; &#125;\n      form button &#123; width: 9%; background: rgb(130, 224, 255); border: none; padding: 10px; &#125;\n      #messages &#123; list-style-type: none; margin: 0; padding: 0; &#125;\n      #messages li &#123; padding: 5px 10px; &#125;\n      #messages li:nth-child(odd) &#123; background: #eee; &#125;\n    &lt;/style&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;ul id=&quot;messages&quot;&gt;&lt;/ul&gt;\n    &lt;form action=&quot;&quot;&gt;\n      &lt;input id=&quot;m&quot; autocomplete=&quot;off&quot; /&gt;&lt;button&gt;Send&lt;/button&gt;\n    &lt;/form&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre>\n<p>重启进程（输入 Control+C 并再次运行 node index），并刷新，页面展示如下：<img src=\"/uploads/20160604/index_page_2.png\" alt=\"index_page_2\"></p>\n<h3 id=\"整合-Socket-IO\"><a href=\"#整合-Socket-IO\" class=\"headerlink\" title=\"整合 Socket.IO\"></a>整合 Socket.IO</h3><p>Socket.IO 由两部分组成：</p>\n<ul>\n<li>整合（或挂载）了 Node.JS HTTP 服务器的服务器：socket.io</li>\n<li>在浏览器端加载的客户端库：socket.io-client</li>\n</ul>\n<p>开发时，socket.io 自动为我们提供这个客户端，像我们将看到的，因此现在我们仅需要安装一个模块：</p>\n<pre><code>npm install --save socket.io\n</code></pre>\n<p>这个命令将安装这个模块并添加依赖到 package.json。现在修改 index.js：</p>\n<pre><code>var app = require(&#39;express&#39;)();\nvar http = require(&#39;http&#39;).Server(app);\nvar io = require(&#39;socket.io&#39;)(http);\n\napp.get(&#39;/&#39;, function(req, res)&#123;\n  res.sendfile(&#39;index.html&#39;);\n&#125;);\n\nio.on(&#39;connection&#39;, function(socket)&#123;\n  console.log(&#39;a user connected&#39;);\n&#125;);\n\nhttp.listen(3000, function()&#123;\n  console.log(&#39;listening on *:3000&#39;);\n&#125;);\n</code></pre>\n<p>注意，通过传递 http（HTTP 服务器）对象初始化了一个 socket.io 的新实例。然后为到来的 socket 监听 connection 事件，并且输入日志到控制台。</p>\n<p>现在，在 index.html 中 </body> 前添加下面的片段：</p>\n<pre><code>&lt;script src=&quot;/socket.io/socket.io.js&quot;&gt;&lt;/script&gt;\n&lt;script&gt;\n  var socket = io();\n&lt;/script&gt;\n</code></pre>\n<p>这将加载 socket.io-client，创建全局变量 io，并且连接。</p>\n<p>注意，当调用 io() 的时候没有制定任何 URL，因为它默认尝试连接提供页面服务的主机。</p>\n<p>现在重新加载服务器和网站将会看到控制台打印“a user connected”。尝试打开多个标签页，将看到多条消息：<img src=\"/uploads/20160604/console.png\" alt=\"console\"></p>\n<p>每个 socket 也监听一个特殊的 disconnect 事件：</p>\n<pre><code>io.on(&#39;connection&#39;, function(socket)&#123;\n  console.log(&#39;a user connected&#39;);\n  socket.on(&#39;disconnect&#39;, function()&#123;\n    console.log(&#39;user disconnected&#39;);\n  &#125;);\n&#125;);\n</code></pre>\n<p>如果多次刷新一个标签页将看到下面的动作：<img src=\"/uploads/20160604/console2.png\" alt=\"console2.png\"></p>\n<h3 id=\"发送事件\"><a href=\"#发送事件\" class=\"headerlink\" title=\"发送事件\"></a>发送事件</h3><p>Socket.IO 背后主要的思想是你可以发送和接收想要的任何事件，携带你想要的任何数据。任何可以编码为 JSON 的对象都可以做到，并且也支持二进制数据。</p>\n<p>让我们来实现当一个用户输入一条消息时，服务器作为一个 chat message 事件获取它。在 index.html 中的脚本现在看起来如下：</p>\n<pre><code>&lt;script src=&quot;/socket.io/socket.io.js&quot;&gt;&lt;/script&gt;\n&lt;script src=&quot;http://code.jquery.com/jquery-1.11.1.js&quot;&gt;&lt;/script&gt;\n&lt;script&gt;\n  var socket = io();\n  $(&#39;form&#39;).submit(function()&#123;\n    socket.emit(&#39;chat message&#39;, $(&#39;#m&#39;).val());\n    $(&#39;#m&#39;).val(&#39;&#39;);\n    return false;\n  &#125;);\n&lt;/script&gt;\n</code></pre>\n<p>在 index.js 中我们打印出 chat message 事件：</p>\n<pre><code>io.on(&#39;connection&#39;, function(socket)&#123;\n  socket.on(&#39;chat message&#39;, function(msg)&#123;\n    console.log(&#39;message: &#39; + msg);\n  &#125;);\n&#125;);\n</code></pre>\n<p>结果像下面的视频展示的：</p>\n<p><video autoplay loop width=\"100%\"><source src=\"/uploads/20160604/video.mp4\"></video></p>\n\n<h3 id=\"广播\"><a href=\"#广播\" class=\"headerlink\" title=\"广播\"></a>广播</h3><p>下一个目标是我们从服务器发送事件给其他用户。</p>\n<p>为了发送事件给所有人，Socket.IO 给我们提供了 io.emit：</p>\n<pre><code>io.emit(&#39;some event&#39;, &#123; for: &#39;everyone&#39; &#125;);\n</code></pre>\n<p>如果你想发送一个确定 socket 的消息给所有人，我们有 broadcast 标识：</p>\n<pre><code>io.on(&#39;connection&#39;, function(socket)&#123;\n  socket.broadcast.emit(&#39;hi&#39;);\n&#125;);\n</code></pre>\n<p>在这个例子中，为了简单，为了简单我们发送消息给所有人，包括发送者。</p>\n<pre><code>io.on(&#39;connection&#39;, function(socket)&#123;\n  socket.on(&#39;chat message&#39;, function(msg)&#123;\n    io.emit(&#39;chat message&#39;, msg);\n  &#125;);\n&#125;);\n</code></pre>\n<p>在客户端，当我们捕获到一个 chat message 事件，我们把它包含到页面中。客户端完整的代码如下：</p>\n<pre><code>&lt;script&gt;\n  var socket = io();\n  $(&#39;form&#39;).submit(function()&#123;\n    socket.emit(&#39;chat message&#39;, $(&#39;#m&#39;).val());\n    $(&#39;#m&#39;).val(&#39;&#39;);\n    return false;\n  &#125;);\n  socket.on(&#39;chat message&#39;, function(msg)&#123;\n    $(&#39;#messages&#39;).append($(&#39;&lt;li&gt;&#39;).text(msg));\n  &#125;);\n&lt;/script&gt;\n</code></pre>\n<p>我们的聊天应用就完成了，用了大概 20 行代码！看起来像这样：</p>\n<p><video autoplay loop width=\"100%\"><source src=\"/uploads/20160604/video2.mp4\"></video></p>"},{"title":"Socket.IO 负载均衡","date":"2016-10-16T10:35:34.000Z","_content":"\n\n### 架构\n\n![Socket.IO 负载均衡架构](/uploads/20161016/socketio.png)\n\n<!-- more -->\n\n### Nginx 配置\n\n为了负载均衡时连接保证始终连到一个节点上，使用 Nginx 的 ip_hash 实现 session sticky，让客户端始终连接到集群内一台节点上。\n\n在 Nginx 的 conf.d 目录下创建配置文件 socket_io.conf，内容如下：\n\n    upstream nodejs_websocket {\n      ip_hash;\n      server 192.168.1.100:3000;\n      server 192.168.1.101:3000;\n    }\n\n    server {\n      listen 80;\n      server_name 127.0.0.1 localhost;\n      access_log off;\n      add_header Content-Type \"text/html; charset=UTF-8\";\n\n      location / {\n        proxy_pass http://nodejs_websocket;\n\n        proxy_redirect off;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header Host $host;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection \"upgrade\";\n      }\n    }\n\n由于 Nginx 的反向代理机制和 Socket.IO 的自动重连机制，上述架构还具备高可用的特性，即当某个节点宕机时，原先连接到该节点上的客户端会自动重连至其它节点上。","source":"_posts/Socket-IO-负载均衡.md","raw":"title: Socket.IO 负载均衡\ntags:\n  - Socket.IO\n  - Nginx\ncategories:\n  - 工具箱\n  - Socket.IO\ndate: 2016-10-16 18:35:34\n---\n\n\n### 架构\n\n![Socket.IO 负载均衡架构](/uploads/20161016/socketio.png)\n\n<!-- more -->\n\n### Nginx 配置\n\n为了负载均衡时连接保证始终连到一个节点上，使用 Nginx 的 ip_hash 实现 session sticky，让客户端始终连接到集群内一台节点上。\n\n在 Nginx 的 conf.d 目录下创建配置文件 socket_io.conf，内容如下：\n\n    upstream nodejs_websocket {\n      ip_hash;\n      server 192.168.1.100:3000;\n      server 192.168.1.101:3000;\n    }\n\n    server {\n      listen 80;\n      server_name 127.0.0.1 localhost;\n      access_log off;\n      add_header Content-Type \"text/html; charset=UTF-8\";\n\n      location / {\n        proxy_pass http://nodejs_websocket;\n\n        proxy_redirect off;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header Host $host;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection \"upgrade\";\n      }\n    }\n\n由于 Nginx 的反向代理机制和 Socket.IO 的自动重连机制，上述架构还具备高可用的特性，即当某个节点宕机时，原先连接到该节点上的客户端会自动重连至其它节点上。","slug":"Socket-IO-负载均衡","published":1,"updated":"2021-07-19T16:28:00.292Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphpw00aqitd3f2i83yg8","content":"<h3 id=\"架构\"><a href=\"#架构\" class=\"headerlink\" title=\"架构\"></a>架构</h3><p><img src=\"/uploads/20161016/socketio.png\" alt=\"Socket.IO 负载均衡架构\"></p>\n<span id=\"more\"></span>\n\n<h3 id=\"Nginx-配置\"><a href=\"#Nginx-配置\" class=\"headerlink\" title=\"Nginx 配置\"></a>Nginx 配置</h3><p>为了负载均衡时连接保证始终连到一个节点上，使用 Nginx 的 ip_hash 实现 session sticky，让客户端始终连接到集群内一台节点上。</p>\n<p>在 Nginx 的 conf.d 目录下创建配置文件 socket_io.conf，内容如下：</p>\n<pre><code>upstream nodejs_websocket &#123;\n  ip_hash;\n  server 192.168.1.100:3000;\n  server 192.168.1.101:3000;\n&#125;\n\nserver &#123;\n  listen 80;\n  server_name 127.0.0.1 localhost;\n  access_log off;\n  add_header Content-Type &quot;text/html; charset=UTF-8&quot;;\n\n  location / &#123;\n    proxy_pass http://nodejs_websocket;\n\n    proxy_redirect off;\n    proxy_set_header X-Real-IP $remote_addr;\n    proxy_set_header Host $host;\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n\n    proxy_http_version 1.1;\n    proxy_set_header Upgrade $http_upgrade;\n    proxy_set_header Connection &quot;upgrade&quot;;\n  &#125;\n&#125;\n</code></pre>\n<p>由于 Nginx 的反向代理机制和 Socket.IO 的自动重连机制，上述架构还具备高可用的特性，即当某个节点宕机时，原先连接到该节点上的客户端会自动重连至其它节点上。</p>\n","site":{"data":{}},"excerpt":"<h3 id=\"架构\"><a href=\"#架构\" class=\"headerlink\" title=\"架构\"></a>架构</h3><p><img src=\"/uploads/20161016/socketio.png\" alt=\"Socket.IO 负载均衡架构\"></p>","more":"<h3 id=\"Nginx-配置\"><a href=\"#Nginx-配置\" class=\"headerlink\" title=\"Nginx 配置\"></a>Nginx 配置</h3><p>为了负载均衡时连接保证始终连到一个节点上，使用 Nginx 的 ip_hash 实现 session sticky，让客户端始终连接到集群内一台节点上。</p>\n<p>在 Nginx 的 conf.d 目录下创建配置文件 socket_io.conf，内容如下：</p>\n<pre><code>upstream nodejs_websocket &#123;\n  ip_hash;\n  server 192.168.1.100:3000;\n  server 192.168.1.101:3000;\n&#125;\n\nserver &#123;\n  listen 80;\n  server_name 127.0.0.1 localhost;\n  access_log off;\n  add_header Content-Type &quot;text/html; charset=UTF-8&quot;;\n\n  location / &#123;\n    proxy_pass http://nodejs_websocket;\n\n    proxy_redirect off;\n    proxy_set_header X-Real-IP $remote_addr;\n    proxy_set_header Host $host;\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n\n    proxy_http_version 1.1;\n    proxy_set_header Upgrade $http_upgrade;\n    proxy_set_header Connection &quot;upgrade&quot;;\n  &#125;\n&#125;\n</code></pre>\n<p>由于 Nginx 的反向代理机制和 Socket.IO 的自动重连机制，上述架构还具备高可用的特性，即当某个节点宕机时，原先连接到该节点上的客户端会自动重连至其它节点上。</p>"},{"title":"Sqoop 导入 MySQL 中含有回车换行符的数据","date":"2017-07-05T01:24:44.000Z","_content":"\nMySQL 中的数据如下图：![MySQL 中带回车换行符的数据](/uploads/20170704/mysql-data.png)\n\n<!-- more -->\n\n检查 HDFS 上的目标文件内容可以看出，回车换行符位置的数据被截断了，导致数据列错位。![HDFS 问题数据](/uploads/20170704/hdfs-data.png)\n\nSqoop 提供了配置参数，在导入时丢弃掉数据的分隔符（\\n，\\r，\\01）。\n\n    --hive-drop-import-delims : Drops \\n, \\r, and \\01 from string fields when importing to Hive. \n","source":"_posts/Sqoop-导入-MySQL-中含有回车换行符的数据.md","raw":"title: Sqoop 导入 MySQL 中含有回车换行符的数据\ntags:\n  - Sqoop\n  - Hive\ncategories:\n  - 大数据\n  - Sqoop\ndate: 2017-07-05 09:24:44\n---\n\nMySQL 中的数据如下图：![MySQL 中带回车换行符的数据](/uploads/20170704/mysql-data.png)\n\n<!-- more -->\n\n检查 HDFS 上的目标文件内容可以看出，回车换行符位置的数据被截断了，导致数据列错位。![HDFS 问题数据](/uploads/20170704/hdfs-data.png)\n\nSqoop 提供了配置参数，在导入时丢弃掉数据的分隔符（\\n，\\r，\\01）。\n\n    --hive-drop-import-delims : Drops \\n, \\r, and \\01 from string fields when importing to Hive. \n","slug":"Sqoop-导入-MySQL-中含有回车换行符的数据","published":1,"updated":"2021-07-19T16:28:00.292Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphpy00auitd31m93df49","content":"<p>MySQL 中的数据如下图：<img src=\"/uploads/20170704/mysql-data.png\" alt=\"MySQL 中带回车换行符的数据\"></p>\n<span id=\"more\"></span>\n\n<p>检查 HDFS 上的目标文件内容可以看出，回车换行符位置的数据被截断了，导致数据列错位。<img src=\"/uploads/20170704/hdfs-data.png\" alt=\"HDFS 问题数据\"></p>\n<p>Sqoop 提供了配置参数，在导入时丢弃掉数据的分隔符（\\n，\\r，\\01）。</p>\n<pre><code>--hive-drop-import-delims : Drops \\n, \\r, and \\01 from string fields when importing to Hive. \n</code></pre>\n","site":{"data":{}},"excerpt":"<p>MySQL 中的数据如下图：<img src=\"/uploads/20170704/mysql-data.png\" alt=\"MySQL 中带回车换行符的数据\"></p>","more":"<p>检查 HDFS 上的目标文件内容可以看出，回车换行符位置的数据被截断了，导致数据列错位。<img src=\"/uploads/20170704/hdfs-data.png\" alt=\"HDFS 问题数据\"></p>\n<p>Sqoop 提供了配置参数，在导入时丢弃掉数据的分隔符（\\n，\\r，\\01）。</p>\n<pre><code>--hive-drop-import-delims : Drops \\n, \\r, and \\01 from string fields when importing to Hive. \n</code></pre>"},{"title":"Sqoop 支持 ORC 文件格式","date":"2018-12-27T17:18:34.000Z","_content":"\n\n### ORC 介绍\n   ORC 文件格式是 Hive 0.11.0 版本引入的一种文件格式。ORC 的引入是为了解决其他 Hive 文件格式的局限性。使用 ORC 文件格式提升 Hive 读取、写入及处理数据的性能。\n\n<!-- more -->\n\n   与 RCFile 对比，ORC 文件格式有很多优点：\n   - 每个 Task 只输出一个文件，降低 NameNode 的负载。\n   - Hive 数据类型支持，包括：datetime、decimal 以及复杂数据类型（struct、list、map、union）。\n   - 文件中存储轻量级的索引：\n     - 跳过不通过谓语过滤的行组\n     - 跳转到指定的行\n   - 基于数据类型的块模式压缩：\n     - 整型数据列采用行程长度编码（run-length encoding）\n     - 字符串数据列采用词典编码（dictionary encoding）\n   - 使用独立的 RecordReader 并发读取相同的文件\n   - 无需扫描 markers 就可以分割文件的能力\n   - 绑定读写需要的内存量\n   - 使用 Protocol Buffer 存储元数据，允许添加、移除字段\n\n   Hive 官网介绍：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC\n\n### Sqoop 支持 ORC\n\n   通过 Sqoop-HCatalog 集成解决 Sqoop 不支持 ORC 的问题。\n\n#### HCatalog 背景\n\n    HCatalog 是 Hadoop 的一个 table 与存储管理的一个服务，用户可以更容易地使用不同的数据处理工具 Pig、MapReduce 和 Hive 读写数据。HCatalog 表的抽象呈现给用户一个 HDFS 分布式文件系统（HDFS）中的关系视图，用户不需要担心数据存储在哪里及数据的存储格式：RCFile 格式、text 文件、或者 SequenceFile。\n\n    HCatalog 支持读写任何实现了 Hive SerDe（serializer-deserializer）的文件格式。默认的，HCatalog 支持 RCFile、CSV、JSON 和 SequenceFile。要使用用户自定义格式，必须提供 InputFormat 和 OutputFormat 及 SerDe。\n\n    Sqoop 使用 HCatalog 抽象不同存储格式的能力来支持 RCFile（以及未来的文件类型）。\n\n#### 集成 HCatalog 后新增的参数\n\n    见 Sqoop 官方文档：http://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html#_sqoop_hcatalog_integration\n\n#### 定制 Sqoop 改造\n    修改 bin/sqoop 命令脚本：为 import 操作增加 Hive Table 分区数据删除逻辑，在执行导入前不需要另行清理数据，从而简化 Sqoop import 脚本的开发工作。\n\n#### Sqoop 导入 ORC 实例\n##### 第一步：创建 Hive 表\n    CREATE TABLE `dev.bims_device`(\n      `code` string,\n      `mac` string,\n      `wifi_mac` string,\n      `create_date` string,\n      `activate_date` string,\n      `state` string,\n      `area_id` bigint,\n      `city_id` bigint,\n      `sp_define_id` bigint)\n    PARTITIONED BY (\n      `dt` string)\n    stored as orc;\n##### 第二步：Sqoop import\n    sqoop import \\\n      --hcatalog-database dev \\\n      --hcatalog-table bims_device \\\n      --hcatalog-partition-keys dt \\\n      --hcatalog-partition-values ${dt} \\\n      --connect jdbc:mysql://${host}:${port}/bims \\\n      --username \"${userName}\" \\\n      --password \"${password}\" \\\n      --table bims_device \\\n      --columns \"code,mac,wifi_mac,create_date,activate_date,state,area_id,city_id,sp_define_id\"\n#### Sqoop 导出 ORC 实例\n##### 第一步：创建 MySQL 数据表\n    CREATE TABLE `test`.`bims_device`(\n      `code` varchar(50),\n      `mac` varchar(50),\n      `wifi_mac` varchar(50),\n      `create_date` varchar(50),\n      `activate_date` varchar(50),\n      `state` varchar(50),\n      `area_id` bigint,\n      `city_id` bigint,\n      `sp_define_id` bigint);\n##### 第二步：Sqoop export\n    sqoop export \\\n      --hcatalog-database dev \\\n      --hcatalog-table bims_device \\\n      --hcatalog-partition-keys dt \\\n      --hcatalog-partition-values ${dt} \\\n      --connect jdbc:mysql://${host}:${port}/test?useCompression=true \\\n      --username ${userName} \\\n      --password ${password} \\\n      --table bims_device\n### 注意事项\n     - MySQL 表字段名称必须与 Hive 仓库中的表字段名称一致\n     - Sqoop ORC 导出时，数据类型需要转换时如果存在脏数据会导致导出失败。例如，长度为 0 的 String 如果导出为 int 时。\n     - 导出时需要提前清理 MySQL 中之前导出的数据，避免重复运行时造成数据重复。\n","source":"_posts/Sqoop-支持-ORC-文件格式.md","raw":"title: Sqoop 支持 ORC 文件格式\ntags:\n  - Hive\n  - ORC\ncategories:\n  - 大数据\n  - Hive\ndate: 2018-12-28 01:18:34\n---\n\n\n### ORC 介绍\n   ORC 文件格式是 Hive 0.11.0 版本引入的一种文件格式。ORC 的引入是为了解决其他 Hive 文件格式的局限性。使用 ORC 文件格式提升 Hive 读取、写入及处理数据的性能。\n\n<!-- more -->\n\n   与 RCFile 对比，ORC 文件格式有很多优点：\n   - 每个 Task 只输出一个文件，降低 NameNode 的负载。\n   - Hive 数据类型支持，包括：datetime、decimal 以及复杂数据类型（struct、list、map、union）。\n   - 文件中存储轻量级的索引：\n     - 跳过不通过谓语过滤的行组\n     - 跳转到指定的行\n   - 基于数据类型的块模式压缩：\n     - 整型数据列采用行程长度编码（run-length encoding）\n     - 字符串数据列采用词典编码（dictionary encoding）\n   - 使用独立的 RecordReader 并发读取相同的文件\n   - 无需扫描 markers 就可以分割文件的能力\n   - 绑定读写需要的内存量\n   - 使用 Protocol Buffer 存储元数据，允许添加、移除字段\n\n   Hive 官网介绍：https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC\n\n### Sqoop 支持 ORC\n\n   通过 Sqoop-HCatalog 集成解决 Sqoop 不支持 ORC 的问题。\n\n#### HCatalog 背景\n\n    HCatalog 是 Hadoop 的一个 table 与存储管理的一个服务，用户可以更容易地使用不同的数据处理工具 Pig、MapReduce 和 Hive 读写数据。HCatalog 表的抽象呈现给用户一个 HDFS 分布式文件系统（HDFS）中的关系视图，用户不需要担心数据存储在哪里及数据的存储格式：RCFile 格式、text 文件、或者 SequenceFile。\n\n    HCatalog 支持读写任何实现了 Hive SerDe（serializer-deserializer）的文件格式。默认的，HCatalog 支持 RCFile、CSV、JSON 和 SequenceFile。要使用用户自定义格式，必须提供 InputFormat 和 OutputFormat 及 SerDe。\n\n    Sqoop 使用 HCatalog 抽象不同存储格式的能力来支持 RCFile（以及未来的文件类型）。\n\n#### 集成 HCatalog 后新增的参数\n\n    见 Sqoop 官方文档：http://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html#_sqoop_hcatalog_integration\n\n#### 定制 Sqoop 改造\n    修改 bin/sqoop 命令脚本：为 import 操作增加 Hive Table 分区数据删除逻辑，在执行导入前不需要另行清理数据，从而简化 Sqoop import 脚本的开发工作。\n\n#### Sqoop 导入 ORC 实例\n##### 第一步：创建 Hive 表\n    CREATE TABLE `dev.bims_device`(\n      `code` string,\n      `mac` string,\n      `wifi_mac` string,\n      `create_date` string,\n      `activate_date` string,\n      `state` string,\n      `area_id` bigint,\n      `city_id` bigint,\n      `sp_define_id` bigint)\n    PARTITIONED BY (\n      `dt` string)\n    stored as orc;\n##### 第二步：Sqoop import\n    sqoop import \\\n      --hcatalog-database dev \\\n      --hcatalog-table bims_device \\\n      --hcatalog-partition-keys dt \\\n      --hcatalog-partition-values ${dt} \\\n      --connect jdbc:mysql://${host}:${port}/bims \\\n      --username \"${userName}\" \\\n      --password \"${password}\" \\\n      --table bims_device \\\n      --columns \"code,mac,wifi_mac,create_date,activate_date,state,area_id,city_id,sp_define_id\"\n#### Sqoop 导出 ORC 实例\n##### 第一步：创建 MySQL 数据表\n    CREATE TABLE `test`.`bims_device`(\n      `code` varchar(50),\n      `mac` varchar(50),\n      `wifi_mac` varchar(50),\n      `create_date` varchar(50),\n      `activate_date` varchar(50),\n      `state` varchar(50),\n      `area_id` bigint,\n      `city_id` bigint,\n      `sp_define_id` bigint);\n##### 第二步：Sqoop export\n    sqoop export \\\n      --hcatalog-database dev \\\n      --hcatalog-table bims_device \\\n      --hcatalog-partition-keys dt \\\n      --hcatalog-partition-values ${dt} \\\n      --connect jdbc:mysql://${host}:${port}/test?useCompression=true \\\n      --username ${userName} \\\n      --password ${password} \\\n      --table bims_device\n### 注意事项\n     - MySQL 表字段名称必须与 Hive 仓库中的表字段名称一致\n     - Sqoop ORC 导出时，数据类型需要转换时如果存在脏数据会导致导出失败。例如，长度为 0 的 String 如果导出为 int 时。\n     - 导出时需要提前清理 MySQL 中之前导出的数据，避免重复运行时造成数据重复。\n","slug":"Sqoop-支持-ORC-文件格式","published":1,"updated":"2021-07-19T16:28:00.292Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphpz00axitd39tp849rp","content":"<h3 id=\"ORC-介绍\"><a href=\"#ORC-介绍\" class=\"headerlink\" title=\"ORC 介绍\"></a>ORC 介绍</h3><p>   ORC 文件格式是 Hive 0.11.0 版本引入的一种文件格式。ORC 的引入是为了解决其他 Hive 文件格式的局限性。使用 ORC 文件格式提升 Hive 读取、写入及处理数据的性能。</p>\n<span id=\"more\"></span>\n\n<p>   与 RCFile 对比，ORC 文件格式有很多优点：</p>\n<ul>\n<li>每个 Task 只输出一个文件，降低 NameNode 的负载。</li>\n<li>Hive 数据类型支持，包括：datetime、decimal 以及复杂数据类型（struct、list、map、union）。</li>\n<li>文件中存储轻量级的索引：<ul>\n<li>跳过不通过谓语过滤的行组</li>\n<li>跳转到指定的行</li>\n</ul>\n</li>\n<li>基于数据类型的块模式压缩：<ul>\n<li>整型数据列采用行程长度编码（run-length encoding）</li>\n<li>字符串数据列采用词典编码（dictionary encoding）</li>\n</ul>\n</li>\n<li>使用独立的 RecordReader 并发读取相同的文件</li>\n<li>无需扫描 markers 就可以分割文件的能力</li>\n<li>绑定读写需要的内存量</li>\n<li>使用 Protocol Buffer 存储元数据，允许添加、移除字段</li>\n</ul>\n<p>   Hive 官网介绍：<a href=\"https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC\">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC</a></p>\n<h3 id=\"Sqoop-支持-ORC\"><a href=\"#Sqoop-支持-ORC\" class=\"headerlink\" title=\"Sqoop 支持 ORC\"></a>Sqoop 支持 ORC</h3><p>   通过 Sqoop-HCatalog 集成解决 Sqoop 不支持 ORC 的问题。</p>\n<h4 id=\"HCatalog-背景\"><a href=\"#HCatalog-背景\" class=\"headerlink\" title=\"HCatalog 背景\"></a>HCatalog 背景</h4><pre><code>HCatalog 是 Hadoop 的一个 table 与存储管理的一个服务，用户可以更容易地使用不同的数据处理工具 Pig、MapReduce 和 Hive 读写数据。HCatalog 表的抽象呈现给用户一个 HDFS 分布式文件系统（HDFS）中的关系视图，用户不需要担心数据存储在哪里及数据的存储格式：RCFile 格式、text 文件、或者 SequenceFile。\n\nHCatalog 支持读写任何实现了 Hive SerDe（serializer-deserializer）的文件格式。默认的，HCatalog 支持 RCFile、CSV、JSON 和 SequenceFile。要使用用户自定义格式，必须提供 InputFormat 和 OutputFormat 及 SerDe。\n\nSqoop 使用 HCatalog 抽象不同存储格式的能力来支持 RCFile（以及未来的文件类型）。\n</code></pre>\n<h4 id=\"集成-HCatalog-后新增的参数\"><a href=\"#集成-HCatalog-后新增的参数\" class=\"headerlink\" title=\"集成 HCatalog 后新增的参数\"></a>集成 HCatalog 后新增的参数</h4><pre><code>见 Sqoop 官方文档：http://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html#_sqoop_hcatalog_integration\n</code></pre>\n<h4 id=\"定制-Sqoop-改造\"><a href=\"#定制-Sqoop-改造\" class=\"headerlink\" title=\"定制 Sqoop 改造\"></a>定制 Sqoop 改造</h4><pre><code>修改 bin/sqoop 命令脚本：为 import 操作增加 Hive Table 分区数据删除逻辑，在执行导入前不需要另行清理数据，从而简化 Sqoop import 脚本的开发工作。\n</code></pre>\n<h4 id=\"Sqoop-导入-ORC-实例\"><a href=\"#Sqoop-导入-ORC-实例\" class=\"headerlink\" title=\"Sqoop 导入 ORC 实例\"></a>Sqoop 导入 ORC 实例</h4><h5 id=\"第一步：创建-Hive-表\"><a href=\"#第一步：创建-Hive-表\" class=\"headerlink\" title=\"第一步：创建 Hive 表\"></a>第一步：创建 Hive 表</h5><pre><code>CREATE TABLE `dev.bims_device`(\n  `code` string,\n  `mac` string,\n  `wifi_mac` string,\n  `create_date` string,\n  `activate_date` string,\n  `state` string,\n  `area_id` bigint,\n  `city_id` bigint,\n  `sp_define_id` bigint)\nPARTITIONED BY (\n  `dt` string)\nstored as orc;\n</code></pre>\n<h5 id=\"第二步：Sqoop-import\"><a href=\"#第二步：Sqoop-import\" class=\"headerlink\" title=\"第二步：Sqoop import\"></a>第二步：Sqoop import</h5><pre><code>sqoop import \\\n  --hcatalog-database dev \\\n  --hcatalog-table bims_device \\\n  --hcatalog-partition-keys dt \\\n  --hcatalog-partition-values $&#123;dt&#125; \\\n  --connect jdbc:mysql://$&#123;host&#125;:$&#123;port&#125;/bims \\\n  --username &quot;$&#123;userName&#125;&quot; \\\n  --password &quot;$&#123;password&#125;&quot; \\\n  --table bims_device \\\n  --columns &quot;code,mac,wifi_mac,create_date,activate_date,state,area_id,city_id,sp_define_id&quot;\n</code></pre>\n<h4 id=\"Sqoop-导出-ORC-实例\"><a href=\"#Sqoop-导出-ORC-实例\" class=\"headerlink\" title=\"Sqoop 导出 ORC 实例\"></a>Sqoop 导出 ORC 实例</h4><h5 id=\"第一步：创建-MySQL-数据表\"><a href=\"#第一步：创建-MySQL-数据表\" class=\"headerlink\" title=\"第一步：创建 MySQL 数据表\"></a>第一步：创建 MySQL 数据表</h5><pre><code>CREATE TABLE `test`.`bims_device`(\n  `code` varchar(50),\n  `mac` varchar(50),\n  `wifi_mac` varchar(50),\n  `create_date` varchar(50),\n  `activate_date` varchar(50),\n  `state` varchar(50),\n  `area_id` bigint,\n  `city_id` bigint,\n  `sp_define_id` bigint);\n</code></pre>\n<h5 id=\"第二步：Sqoop-export\"><a href=\"#第二步：Sqoop-export\" class=\"headerlink\" title=\"第二步：Sqoop export\"></a>第二步：Sqoop export</h5><pre><code>sqoop export \\\n  --hcatalog-database dev \\\n  --hcatalog-table bims_device \\\n  --hcatalog-partition-keys dt \\\n  --hcatalog-partition-values $&#123;dt&#125; \\\n  --connect jdbc:mysql://$&#123;host&#125;:$&#123;port&#125;/test?useCompression=true \\\n  --username $&#123;userName&#125; \\\n  --password $&#123;password&#125; \\\n  --table bims_device\n</code></pre>\n<h3 id=\"注意事项\"><a href=\"#注意事项\" class=\"headerlink\" title=\"注意事项\"></a>注意事项</h3><pre><code> - MySQL 表字段名称必须与 Hive 仓库中的表字段名称一致\n - Sqoop ORC 导出时，数据类型需要转换时如果存在脏数据会导致导出失败。例如，长度为 0 的 String 如果导出为 int 时。\n - 导出时需要提前清理 MySQL 中之前导出的数据，避免重复运行时造成数据重复。\n</code></pre>\n","site":{"data":{}},"excerpt":"<h3 id=\"ORC-介绍\"><a href=\"#ORC-介绍\" class=\"headerlink\" title=\"ORC 介绍\"></a>ORC 介绍</h3><p>   ORC 文件格式是 Hive 0.11.0 版本引入的一种文件格式。ORC 的引入是为了解决其他 Hive 文件格式的局限性。使用 ORC 文件格式提升 Hive 读取、写入及处理数据的性能。</p>","more":"<p>   与 RCFile 对比，ORC 文件格式有很多优点：</p>\n<ul>\n<li>每个 Task 只输出一个文件，降低 NameNode 的负载。</li>\n<li>Hive 数据类型支持，包括：datetime、decimal 以及复杂数据类型（struct、list、map、union）。</li>\n<li>文件中存储轻量级的索引：<ul>\n<li>跳过不通过谓语过滤的行组</li>\n<li>跳转到指定的行</li>\n</ul>\n</li>\n<li>基于数据类型的块模式压缩：<ul>\n<li>整型数据列采用行程长度编码（run-length encoding）</li>\n<li>字符串数据列采用词典编码（dictionary encoding）</li>\n</ul>\n</li>\n<li>使用独立的 RecordReader 并发读取相同的文件</li>\n<li>无需扫描 markers 就可以分割文件的能力</li>\n<li>绑定读写需要的内存量</li>\n<li>使用 Protocol Buffer 存储元数据，允许添加、移除字段</li>\n</ul>\n<p>   Hive 官网介绍：<a href=\"https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC\">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC</a></p>\n<h3 id=\"Sqoop-支持-ORC\"><a href=\"#Sqoop-支持-ORC\" class=\"headerlink\" title=\"Sqoop 支持 ORC\"></a>Sqoop 支持 ORC</h3><p>   通过 Sqoop-HCatalog 集成解决 Sqoop 不支持 ORC 的问题。</p>\n<h4 id=\"HCatalog-背景\"><a href=\"#HCatalog-背景\" class=\"headerlink\" title=\"HCatalog 背景\"></a>HCatalog 背景</h4><pre><code>HCatalog 是 Hadoop 的一个 table 与存储管理的一个服务，用户可以更容易地使用不同的数据处理工具 Pig、MapReduce 和 Hive 读写数据。HCatalog 表的抽象呈现给用户一个 HDFS 分布式文件系统（HDFS）中的关系视图，用户不需要担心数据存储在哪里及数据的存储格式：RCFile 格式、text 文件、或者 SequenceFile。\n\nHCatalog 支持读写任何实现了 Hive SerDe（serializer-deserializer）的文件格式。默认的，HCatalog 支持 RCFile、CSV、JSON 和 SequenceFile。要使用用户自定义格式，必须提供 InputFormat 和 OutputFormat 及 SerDe。\n\nSqoop 使用 HCatalog 抽象不同存储格式的能力来支持 RCFile（以及未来的文件类型）。\n</code></pre>\n<h4 id=\"集成-HCatalog-后新增的参数\"><a href=\"#集成-HCatalog-后新增的参数\" class=\"headerlink\" title=\"集成 HCatalog 后新增的参数\"></a>集成 HCatalog 后新增的参数</h4><pre><code>见 Sqoop 官方文档：http://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html#_sqoop_hcatalog_integration\n</code></pre>\n<h4 id=\"定制-Sqoop-改造\"><a href=\"#定制-Sqoop-改造\" class=\"headerlink\" title=\"定制 Sqoop 改造\"></a>定制 Sqoop 改造</h4><pre><code>修改 bin/sqoop 命令脚本：为 import 操作增加 Hive Table 分区数据删除逻辑，在执行导入前不需要另行清理数据，从而简化 Sqoop import 脚本的开发工作。\n</code></pre>\n<h4 id=\"Sqoop-导入-ORC-实例\"><a href=\"#Sqoop-导入-ORC-实例\" class=\"headerlink\" title=\"Sqoop 导入 ORC 实例\"></a>Sqoop 导入 ORC 实例</h4><h5 id=\"第一步：创建-Hive-表\"><a href=\"#第一步：创建-Hive-表\" class=\"headerlink\" title=\"第一步：创建 Hive 表\"></a>第一步：创建 Hive 表</h5><pre><code>CREATE TABLE `dev.bims_device`(\n  `code` string,\n  `mac` string,\n  `wifi_mac` string,\n  `create_date` string,\n  `activate_date` string,\n  `state` string,\n  `area_id` bigint,\n  `city_id` bigint,\n  `sp_define_id` bigint)\nPARTITIONED BY (\n  `dt` string)\nstored as orc;\n</code></pre>\n<h5 id=\"第二步：Sqoop-import\"><a href=\"#第二步：Sqoop-import\" class=\"headerlink\" title=\"第二步：Sqoop import\"></a>第二步：Sqoop import</h5><pre><code>sqoop import \\\n  --hcatalog-database dev \\\n  --hcatalog-table bims_device \\\n  --hcatalog-partition-keys dt \\\n  --hcatalog-partition-values $&#123;dt&#125; \\\n  --connect jdbc:mysql://$&#123;host&#125;:$&#123;port&#125;/bims \\\n  --username &quot;$&#123;userName&#125;&quot; \\\n  --password &quot;$&#123;password&#125;&quot; \\\n  --table bims_device \\\n  --columns &quot;code,mac,wifi_mac,create_date,activate_date,state,area_id,city_id,sp_define_id&quot;\n</code></pre>\n<h4 id=\"Sqoop-导出-ORC-实例\"><a href=\"#Sqoop-导出-ORC-实例\" class=\"headerlink\" title=\"Sqoop 导出 ORC 实例\"></a>Sqoop 导出 ORC 实例</h4><h5 id=\"第一步：创建-MySQL-数据表\"><a href=\"#第一步：创建-MySQL-数据表\" class=\"headerlink\" title=\"第一步：创建 MySQL 数据表\"></a>第一步：创建 MySQL 数据表</h5><pre><code>CREATE TABLE `test`.`bims_device`(\n  `code` varchar(50),\n  `mac` varchar(50),\n  `wifi_mac` varchar(50),\n  `create_date` varchar(50),\n  `activate_date` varchar(50),\n  `state` varchar(50),\n  `area_id` bigint,\n  `city_id` bigint,\n  `sp_define_id` bigint);\n</code></pre>\n<h5 id=\"第二步：Sqoop-export\"><a href=\"#第二步：Sqoop-export\" class=\"headerlink\" title=\"第二步：Sqoop export\"></a>第二步：Sqoop export</h5><pre><code>sqoop export \\\n  --hcatalog-database dev \\\n  --hcatalog-table bims_device \\\n  --hcatalog-partition-keys dt \\\n  --hcatalog-partition-values $&#123;dt&#125; \\\n  --connect jdbc:mysql://$&#123;host&#125;:$&#123;port&#125;/test?useCompression=true \\\n  --username $&#123;userName&#125; \\\n  --password $&#123;password&#125; \\\n  --table bims_device\n</code></pre>\n<h3 id=\"注意事项\"><a href=\"#注意事项\" class=\"headerlink\" title=\"注意事项\"></a>注意事项</h3><pre><code> - MySQL 表字段名称必须与 Hive 仓库中的表字段名称一致\n - Sqoop ORC 导出时，数据类型需要转换时如果存在脏数据会导致导出失败。例如，长度为 0 的 String 如果导出为 int 时。\n - 导出时需要提前清理 MySQL 中之前导出的数据，避免重复运行时造成数据重复。\n</code></pre>"},{"title":"Sqoop 源码修改：增加落地 HDFS 文件数与 MapTask 数量一致性检查","date":"2018-12-30T09:04:29.000Z","_content":"\n本篇是对[记录一次 Sqoop 从 MySQL 导入数据到 Hive 问题的排查经过](http://zhang-jc.github.io/2018/12/11/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1-Sqoop-%E4%BB%8E-MySQL-%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E5%88%B0-Hive-%E9%97%AE%E9%A2%98%E7%9A%84%E6%8E%92%E6%9F%A5%E7%BB%8F%E8%BF%87/)的补充。\n\nSqoop 命令通过 bin 下面的脚本调用，调用如下：\n\n    exec ${HADOOP_COMMON_HOME}/bin/hadoop org.apache.sqoop.Sqoop \"$@\"\n\n<!-- more -->\n\norg.apache.sqoop.Sqoop 是 Sqoop 的入口类，在此主要是解析参数及初始化工具类，然后通过 org.apache.hadoop.util.ToolRunner 类调用对应的工具完成操作。Sqoop 的 Import 操作对应的是 org.apache.sqoop.tool.ImportTool 类。\n\n在 ImportTool 类的 return 代码前增加以下代码：\n\n    int numMappers = options.getNumMappers();\n\n    String hDbName = options.getHCatDatabaseName();\n    String hTableName = options.getHCatTableName();\n    String hPartKeys = options.getHCatalogPartitionKeys();\n    String hPartVals = options.getHCatalogPartitionValues();\n\n    if(isStringNotEmpty(hDbName) && isStringNotEmpty(hTableName) && isStringNotEmpty(hPartKeys) &&     isStringNotEmpty(hPartVals)) {\n      String[] partKeys = hPartKeys.split(\",\");\n      String[] partVals = hPartVals.split(\",\");\n\n      String partPathStr = \"\";\n      if(partKeys.length > 0 && partVals.length == partKeys.length) {\n        for(int i = 0; i < partKeys.length; i++) {\n          partPathStr += partKeys[i] + \"=\" + partVals[i] + \"/\";\n        }\n      }\n\n      String targetDir = \"/user/hive/warehouse/\" + hDbName + \".db/\" + hTableName + \"/\" + partPathStr;\n      targetDir = targetDir.toLowerCase();\n      LOG.info(\"---------targetDir=\" + targetDir);\n\n      try {\n        FileSystem fs = FileSystem.get(options.getConf());\n        RemoteIterator<LocatedFileStatus> rIter = fs.listFiles(new Path(targetDir), false);\n\n        int fileCount = 0;\n        while(rIter.hasNext()) {\n          fileCount++;\n          rIter.next();\n        }\n\n        LOG.info(\"---------------fileCount=\" + fileCount);\n\n        if(numMappers != fileCount) {\n          LOG.error(\"files number in hdfs not equals mapper task number !\");\n          return 2;\n        }\n      } catch (IOException e) {\n        LOG.error(\"count files number from hdfs error !\");\n        e.printStackTrace();\n        return 3;\n      }\n    }\n\n改动只针对 Sqoop 集成 HCatalog 方式导入 ORC 格式的情况。因为我们的数据仓库中都采用的是这种方式。\n\n> 优化：当 MySQL 中记录数特别少时，如少于 4 条记录，则默认 Sqoop 的 MapTask 数量为 4 但其实际执行时因为原始记录数不够则实际执行的 MapTask 数量会跟实际的记录数一致，此时 split 数量跟落地 HDFS 的文件数量一致。所以，可以根据 Sqoop 对应 MR 的实际 split 数量进行判断文件数量。\n","source":"_posts/Sqoop-源码修改：增加落地-HDFS-文件数与-MapTask-数量一致性检查.md","raw":"title: Sqoop 源码修改：增加落地 HDFS 文件数与 MapTask 数量一致性检查\ndate: 2018-12-30 17:04:29\ntags:\n- Sqoop\n- 大数据\ncategories:\n- 大数据\n- Sqoop\n---\n\n本篇是对[记录一次 Sqoop 从 MySQL 导入数据到 Hive 问题的排查经过](http://zhang-jc.github.io/2018/12/11/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1-Sqoop-%E4%BB%8E-MySQL-%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E5%88%B0-Hive-%E9%97%AE%E9%A2%98%E7%9A%84%E6%8E%92%E6%9F%A5%E7%BB%8F%E8%BF%87/)的补充。\n\nSqoop 命令通过 bin 下面的脚本调用，调用如下：\n\n    exec ${HADOOP_COMMON_HOME}/bin/hadoop org.apache.sqoop.Sqoop \"$@\"\n\n<!-- more -->\n\norg.apache.sqoop.Sqoop 是 Sqoop 的入口类，在此主要是解析参数及初始化工具类，然后通过 org.apache.hadoop.util.ToolRunner 类调用对应的工具完成操作。Sqoop 的 Import 操作对应的是 org.apache.sqoop.tool.ImportTool 类。\n\n在 ImportTool 类的 return 代码前增加以下代码：\n\n    int numMappers = options.getNumMappers();\n\n    String hDbName = options.getHCatDatabaseName();\n    String hTableName = options.getHCatTableName();\n    String hPartKeys = options.getHCatalogPartitionKeys();\n    String hPartVals = options.getHCatalogPartitionValues();\n\n    if(isStringNotEmpty(hDbName) && isStringNotEmpty(hTableName) && isStringNotEmpty(hPartKeys) &&     isStringNotEmpty(hPartVals)) {\n      String[] partKeys = hPartKeys.split(\",\");\n      String[] partVals = hPartVals.split(\",\");\n\n      String partPathStr = \"\";\n      if(partKeys.length > 0 && partVals.length == partKeys.length) {\n        for(int i = 0; i < partKeys.length; i++) {\n          partPathStr += partKeys[i] + \"=\" + partVals[i] + \"/\";\n        }\n      }\n\n      String targetDir = \"/user/hive/warehouse/\" + hDbName + \".db/\" + hTableName + \"/\" + partPathStr;\n      targetDir = targetDir.toLowerCase();\n      LOG.info(\"---------targetDir=\" + targetDir);\n\n      try {\n        FileSystem fs = FileSystem.get(options.getConf());\n        RemoteIterator<LocatedFileStatus> rIter = fs.listFiles(new Path(targetDir), false);\n\n        int fileCount = 0;\n        while(rIter.hasNext()) {\n          fileCount++;\n          rIter.next();\n        }\n\n        LOG.info(\"---------------fileCount=\" + fileCount);\n\n        if(numMappers != fileCount) {\n          LOG.error(\"files number in hdfs not equals mapper task number !\");\n          return 2;\n        }\n      } catch (IOException e) {\n        LOG.error(\"count files number from hdfs error !\");\n        e.printStackTrace();\n        return 3;\n      }\n    }\n\n改动只针对 Sqoop 集成 HCatalog 方式导入 ORC 格式的情况。因为我们的数据仓库中都采用的是这种方式。\n\n> 优化：当 MySQL 中记录数特别少时，如少于 4 条记录，则默认 Sqoop 的 MapTask 数量为 4 但其实际执行时因为原始记录数不够则实际执行的 MapTask 数量会跟实际的记录数一致，此时 split 数量跟落地 HDFS 的文件数量一致。所以，可以根据 Sqoop 对应 MR 的实际 split 数量进行判断文件数量。\n","slug":"Sqoop-源码修改：增加落地-HDFS-文件数与-MapTask-数量一致性检查","published":1,"updated":"2021-07-19T16:28:00.292Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphq200b2itd32go5eqzy","content":"<p>本篇是对<a href=\"http://zhang-jc.github.io/2018/12/11/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1-Sqoop-%E4%BB%8E-MySQL-%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E5%88%B0-Hive-%E9%97%AE%E9%A2%98%E7%9A%84%E6%8E%92%E6%9F%A5%E7%BB%8F%E8%BF%87/\">记录一次 Sqoop 从 MySQL 导入数据到 Hive 问题的排查经过</a>的补充。</p>\n<p>Sqoop 命令通过 bin 下面的脚本调用，调用如下：</p>\n<pre><code>exec $&#123;HADOOP_COMMON_HOME&#125;/bin/hadoop org.apache.sqoop.Sqoop &quot;$@&quot;\n</code></pre>\n<span id=\"more\"></span>\n\n<p>org.apache.sqoop.Sqoop 是 Sqoop 的入口类，在此主要是解析参数及初始化工具类，然后通过 org.apache.hadoop.util.ToolRunner 类调用对应的工具完成操作。Sqoop 的 Import 操作对应的是 org.apache.sqoop.tool.ImportTool 类。</p>\n<p>在 ImportTool 类的 return 代码前增加以下代码：</p>\n<pre><code>int numMappers = options.getNumMappers();\n\nString hDbName = options.getHCatDatabaseName();\nString hTableName = options.getHCatTableName();\nString hPartKeys = options.getHCatalogPartitionKeys();\nString hPartVals = options.getHCatalogPartitionValues();\n\nif(isStringNotEmpty(hDbName) &amp;&amp; isStringNotEmpty(hTableName) &amp;&amp; isStringNotEmpty(hPartKeys) &amp;&amp;     isStringNotEmpty(hPartVals)) &#123;\n  String[] partKeys = hPartKeys.split(&quot;,&quot;);\n  String[] partVals = hPartVals.split(&quot;,&quot;);\n\n  String partPathStr = &quot;&quot;;\n  if(partKeys.length &gt; 0 &amp;&amp; partVals.length == partKeys.length) &#123;\n    for(int i = 0; i &lt; partKeys.length; i++) &#123;\n      partPathStr += partKeys[i] + &quot;=&quot; + partVals[i] + &quot;/&quot;;\n    &#125;\n  &#125;\n\n  String targetDir = &quot;/user/hive/warehouse/&quot; + hDbName + &quot;.db/&quot; + hTableName + &quot;/&quot; + partPathStr;\n  targetDir = targetDir.toLowerCase();\n  LOG.info(&quot;---------targetDir=&quot; + targetDir);\n\n  try &#123;\n    FileSystem fs = FileSystem.get(options.getConf());\n    RemoteIterator&lt;LocatedFileStatus&gt; rIter = fs.listFiles(new Path(targetDir), false);\n\n    int fileCount = 0;\n    while(rIter.hasNext()) &#123;\n      fileCount++;\n      rIter.next();\n    &#125;\n\n    LOG.info(&quot;---------------fileCount=&quot; + fileCount);\n\n    if(numMappers != fileCount) &#123;\n      LOG.error(&quot;files number in hdfs not equals mapper task number !&quot;);\n      return 2;\n    &#125;\n  &#125; catch (IOException e) &#123;\n    LOG.error(&quot;count files number from hdfs error !&quot;);\n    e.printStackTrace();\n    return 3;\n  &#125;\n&#125;\n</code></pre>\n<p>改动只针对 Sqoop 集成 HCatalog 方式导入 ORC 格式的情况。因为我们的数据仓库中都采用的是这种方式。</p>\n<blockquote>\n<p>优化：当 MySQL 中记录数特别少时，如少于 4 条记录，则默认 Sqoop 的 MapTask 数量为 4 但其实际执行时因为原始记录数不够则实际执行的 MapTask 数量会跟实际的记录数一致，此时 split 数量跟落地 HDFS 的文件数量一致。所以，可以根据 Sqoop 对应 MR 的实际 split 数量进行判断文件数量。</p>\n</blockquote>\n","site":{"data":{}},"excerpt":"<p>本篇是对<a href=\"http://zhang-jc.github.io/2018/12/11/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1-Sqoop-%E4%BB%8E-MySQL-%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E5%88%B0-Hive-%E9%97%AE%E9%A2%98%E7%9A%84%E6%8E%92%E6%9F%A5%E7%BB%8F%E8%BF%87/\">记录一次 Sqoop 从 MySQL 导入数据到 Hive 问题的排查经过</a>的补充。</p>\n<p>Sqoop 命令通过 bin 下面的脚本调用，调用如下：</p>\n<pre><code>exec $&#123;HADOOP_COMMON_HOME&#125;/bin/hadoop org.apache.sqoop.Sqoop &quot;$@&quot;\n</code></pre>","more":"<p>org.apache.sqoop.Sqoop 是 Sqoop 的入口类，在此主要是解析参数及初始化工具类，然后通过 org.apache.hadoop.util.ToolRunner 类调用对应的工具完成操作。Sqoop 的 Import 操作对应的是 org.apache.sqoop.tool.ImportTool 类。</p>\n<p>在 ImportTool 类的 return 代码前增加以下代码：</p>\n<pre><code>int numMappers = options.getNumMappers();\n\nString hDbName = options.getHCatDatabaseName();\nString hTableName = options.getHCatTableName();\nString hPartKeys = options.getHCatalogPartitionKeys();\nString hPartVals = options.getHCatalogPartitionValues();\n\nif(isStringNotEmpty(hDbName) &amp;&amp; isStringNotEmpty(hTableName) &amp;&amp; isStringNotEmpty(hPartKeys) &amp;&amp;     isStringNotEmpty(hPartVals)) &#123;\n  String[] partKeys = hPartKeys.split(&quot;,&quot;);\n  String[] partVals = hPartVals.split(&quot;,&quot;);\n\n  String partPathStr = &quot;&quot;;\n  if(partKeys.length &gt; 0 &amp;&amp; partVals.length == partKeys.length) &#123;\n    for(int i = 0; i &lt; partKeys.length; i++) &#123;\n      partPathStr += partKeys[i] + &quot;=&quot; + partVals[i] + &quot;/&quot;;\n    &#125;\n  &#125;\n\n  String targetDir = &quot;/user/hive/warehouse/&quot; + hDbName + &quot;.db/&quot; + hTableName + &quot;/&quot; + partPathStr;\n  targetDir = targetDir.toLowerCase();\n  LOG.info(&quot;---------targetDir=&quot; + targetDir);\n\n  try &#123;\n    FileSystem fs = FileSystem.get(options.getConf());\n    RemoteIterator&lt;LocatedFileStatus&gt; rIter = fs.listFiles(new Path(targetDir), false);\n\n    int fileCount = 0;\n    while(rIter.hasNext()) &#123;\n      fileCount++;\n      rIter.next();\n    &#125;\n\n    LOG.info(&quot;---------------fileCount=&quot; + fileCount);\n\n    if(numMappers != fileCount) &#123;\n      LOG.error(&quot;files number in hdfs not equals mapper task number !&quot;);\n      return 2;\n    &#125;\n  &#125; catch (IOException e) &#123;\n    LOG.error(&quot;count files number from hdfs error !&quot;);\n    e.printStackTrace();\n    return 3;\n  &#125;\n&#125;\n</code></pre>\n<p>改动只针对 Sqoop 集成 HCatalog 方式导入 ORC 格式的情况。因为我们的数据仓库中都采用的是这种方式。</p>\n<blockquote>\n<p>优化：当 MySQL 中记录数特别少时，如少于 4 条记录，则默认 Sqoop 的 MapTask 数量为 4 但其实际执行时因为原始记录数不够则实际执行的 MapTask 数量会跟实际的记录数一致，此时 split 数量跟落地 HDFS 的文件数量一致。所以，可以根据 Sqoop 对应 MR 的实际 split 数量进行判断文件数量。</p>\n</blockquote>"},{"title":"ThinkPad E480 安装 Ubuntu 18.04 无线网卡驱动","date":"2019-10-23T04:02:00.000Z","_content":"遗憾的是虽然下面的方法可以解决，但是内核升级后需要重新安装。\n\n### 基本信息\n\n- Ubuntu 18.04\n- ThinkPad E480\n- 使用下面的命令查看 Linux 内核：\n\n    $ uname -r\n    5.0.0-32-generic\n\n- 使用以下命令查看网卡型号信息：\n\n    $ sudo lshw -C network\n      *-network UNCLAIMED\n           description: Network controller\n           product: RTL8821CE 802.11ac PCIe Wireless Network Adapter\n           vendor: Realtek Semiconductor Co., Ltd.\n           physical id: 0\n           bus info: pci@0000:05:00.0\n           version: 00\n           width: 64 bits\n           clock: 33MHz\n           capabilities: pm msi pciexpress cap_list\n           configuration: latency=0\n           resources: ioport:b000(size=256) memory:f2100000-f210ffff\n\n让人头疼的就是这款网卡 RTL8821CE！！这款网卡再 Linux 内核还未支持，只能自行编译安装。\n\n### 参考网址\n\n- https://blog.csdn.net/fljhm/article/details/79281655\n- https://unix.stackexchange.com/questions/379049/realtek-rtl8821ce-wifi-driver-problem-in-linux-mint-18-2\n\n### 驱动源码下载\n\n以下源代码是我修改过之后，可以成功编译。原始的代码编译过程中会有不少异常，可以参考上面的网址解决一部分，对于头文件找不到的按照提示将缺少的头文件拷贝到对应的位置即可。\n\n源码下载：[rtl8821ce.zip](/uploads/20191023/rtl8821ce.zip)","source":"_posts/ThinkPad-E480-安装-Ubuntu-18-04-无线网卡驱动.md","raw":"title: ThinkPad E480 安装 Ubuntu 18.04 无线网卡驱动\ndate: 2019-10-23 12:02:00\ntags:\n- Ubuntu\n- Linux\ncategories:\n- 操作系统\n- Ubuntu\n---\n遗憾的是虽然下面的方法可以解决，但是内核升级后需要重新安装。\n\n### 基本信息\n\n- Ubuntu 18.04\n- ThinkPad E480\n- 使用下面的命令查看 Linux 内核：\n\n    $ uname -r\n    5.0.0-32-generic\n\n- 使用以下命令查看网卡型号信息：\n\n    $ sudo lshw -C network\n      *-network UNCLAIMED\n           description: Network controller\n           product: RTL8821CE 802.11ac PCIe Wireless Network Adapter\n           vendor: Realtek Semiconductor Co., Ltd.\n           physical id: 0\n           bus info: pci@0000:05:00.0\n           version: 00\n           width: 64 bits\n           clock: 33MHz\n           capabilities: pm msi pciexpress cap_list\n           configuration: latency=0\n           resources: ioport:b000(size=256) memory:f2100000-f210ffff\n\n让人头疼的就是这款网卡 RTL8821CE！！这款网卡再 Linux 内核还未支持，只能自行编译安装。\n\n### 参考网址\n\n- https://blog.csdn.net/fljhm/article/details/79281655\n- https://unix.stackexchange.com/questions/379049/realtek-rtl8821ce-wifi-driver-problem-in-linux-mint-18-2\n\n### 驱动源码下载\n\n以下源代码是我修改过之后，可以成功编译。原始的代码编译过程中会有不少异常，可以参考上面的网址解决一部分，对于头文件找不到的按照提示将缺少的头文件拷贝到对应的位置即可。\n\n源码下载：[rtl8821ce.zip](/uploads/20191023/rtl8821ce.zip)","slug":"ThinkPad-E480-安装-Ubuntu-18-04-无线网卡驱动","published":1,"updated":"2021-07-19T16:28:00.328Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphq300b5itd32mw7evcj","content":"<p>遗憾的是虽然下面的方法可以解决，但是内核升级后需要重新安装。</p>\n<h3 id=\"基本信息\"><a href=\"#基本信息\" class=\"headerlink\" title=\"基本信息\"></a>基本信息</h3><ul>\n<li><p>Ubuntu 18.04</p>\n</li>\n<li><p>ThinkPad E480</p>\n</li>\n<li><p>使用下面的命令查看 Linux 内核：</p>\n<p>  $ uname -r<br>  5.0.0-32-generic</p>\n</li>\n<li><p>使用以下命令查看网卡型号信息：</p>\n<p>  $ sudo lshw -C network</p>\n<pre><code>*-network UNCLAIMED\n     description: Network controller\n     product: RTL8821CE 802.11ac PCIe Wireless Network Adapter\n     vendor: Realtek Semiconductor Co., Ltd.\n     physical id: 0\n     bus info: pci@0000:05:00.0\n     version: 00\n     width: 64 bits\n     clock: 33MHz\n     capabilities: pm msi pciexpress cap_list\n     configuration: latency=0\n     resources: ioport:b000(size=256) memory:f2100000-f210ffff\n</code></pre>\n</li>\n</ul>\n<p>让人头疼的就是这款网卡 RTL8821CE！！这款网卡再 Linux 内核还未支持，只能自行编译安装。</p>\n<h3 id=\"参考网址\"><a href=\"#参考网址\" class=\"headerlink\" title=\"参考网址\"></a>参考网址</h3><ul>\n<li><a href=\"https://blog.csdn.net/fljhm/article/details/79281655\">https://blog.csdn.net/fljhm/article/details/79281655</a></li>\n<li><a href=\"https://unix.stackexchange.com/questions/379049/realtek-rtl8821ce-wifi-driver-problem-in-linux-mint-18-2\">https://unix.stackexchange.com/questions/379049/realtek-rtl8821ce-wifi-driver-problem-in-linux-mint-18-2</a></li>\n</ul>\n<h3 id=\"驱动源码下载\"><a href=\"#驱动源码下载\" class=\"headerlink\" title=\"驱动源码下载\"></a>驱动源码下载</h3><p>以下源代码是我修改过之后，可以成功编译。原始的代码编译过程中会有不少异常，可以参考上面的网址解决一部分，对于头文件找不到的按照提示将缺少的头文件拷贝到对应的位置即可。</p>\n<p>源码下载：<a href=\"/uploads/20191023/rtl8821ce.zip\">rtl8821ce.zip</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>遗憾的是虽然下面的方法可以解决，但是内核升级后需要重新安装。</p>\n<h3 id=\"基本信息\"><a href=\"#基本信息\" class=\"headerlink\" title=\"基本信息\"></a>基本信息</h3><ul>\n<li><p>Ubuntu 18.04</p>\n</li>\n<li><p>ThinkPad E480</p>\n</li>\n<li><p>使用下面的命令查看 Linux 内核：</p>\n<p>  $ uname -r<br>  5.0.0-32-generic</p>\n</li>\n<li><p>使用以下命令查看网卡型号信息：</p>\n<p>  $ sudo lshw -C network</p>\n<pre><code>*-network UNCLAIMED\n     description: Network controller\n     product: RTL8821CE 802.11ac PCIe Wireless Network Adapter\n     vendor: Realtek Semiconductor Co., Ltd.\n     physical id: 0\n     bus info: pci@0000:05:00.0\n     version: 00\n     width: 64 bits\n     clock: 33MHz\n     capabilities: pm msi pciexpress cap_list\n     configuration: latency=0\n     resources: ioport:b000(size=256) memory:f2100000-f210ffff\n</code></pre>\n</li>\n</ul>\n<p>让人头疼的就是这款网卡 RTL8821CE！！这款网卡再 Linux 内核还未支持，只能自行编译安装。</p>\n<h3 id=\"参考网址\"><a href=\"#参考网址\" class=\"headerlink\" title=\"参考网址\"></a>参考网址</h3><ul>\n<li><a href=\"https://blog.csdn.net/fljhm/article/details/79281655\">https://blog.csdn.net/fljhm/article/details/79281655</a></li>\n<li><a href=\"https://unix.stackexchange.com/questions/379049/realtek-rtl8821ce-wifi-driver-problem-in-linux-mint-18-2\">https://unix.stackexchange.com/questions/379049/realtek-rtl8821ce-wifi-driver-problem-in-linux-mint-18-2</a></li>\n</ul>\n<h3 id=\"驱动源码下载\"><a href=\"#驱动源码下载\" class=\"headerlink\" title=\"驱动源码下载\"></a>驱动源码下载</h3><p>以下源代码是我修改过之后，可以成功编译。原始的代码编译过程中会有不少异常，可以参考上面的网址解决一部分，对于头文件找不到的按照提示将缺少的头文件拷贝到对应的位置即可。</p>\n<p>源码下载：<a href=\"/uploads/20191023/rtl8821ce.zip\">rtl8821ce.zip</a></p>\n"},{"title":"Ubuntu 16.04 安装 Lua","date":"2016-07-23T14:30:14.000Z","_content":"\n\n在 Linux 系统上使用以下命令编译安装 Lua：\n\n    curl -R -O http://www.lua.org/ftp/lua-5.3.3.tar.gz\n    tar zxf lua-5.3.3.tar.gz\n    cd lua-5.3.3\n    make linux test\n\n<!-- more -->\n\n#### 安装 make\n\n编译过程如果提示以下信息则需要先安装 make：\n\n    # make linux test\n    The program 'make' can be found in the following packages:\n     * make\n     * make-guile\n    Try: apt install <selected package>\n\n安装 make\n\n    # apt-get install make\n\n#### 安装 gcc\n\n编译过程提示以下信息则需要安装 gcc：\n\n    # make linux test\n    cd src && make linux\n    make[1]: Entering directory '/root/lua/lua-5.3.3/src'\n    make all SYSCFLAGS=\"-DLUA_USE_LINUX\" SYSLIBS=\"-Wl,-E -ldl -lreadline\"\n    make[2]: Entering directory '/root/lua/lua-5.3.3/src'\n    gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX    -c -o lapi.o lapi.c\n    make[2]: gcc: Command not found\n    <builtin>: recipe for target 'lapi.o' failed\n    make[2]: *** [lapi.o] Error 127\n    make[2]: Leaving directory '/root/lua/lua-5.3.3/src'\n    Makefile:110: recipe for target 'linux' failed\n    make[1]: *** [linux] Error 2\n    make[1]: Leaving directory '/root/lua/lua-5.3.3/src'\n    Makefile:55: recipe for target 'linux' failed\n    make: *** [linux] Error 2\n\n安装 gcc：\n\n    # apt-get install gcc\n\n安装 gcc 过程提示以下信息：\n\n    E: Failed to fetch http://cn.archive.ubuntu.com/ubuntu/pool/main/b/binutils/binutils_2.26.1-1ubuntu1~16.04_amd64.deb  404  Not Found [IP: 115.28.122.210 80]\n    E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n\n先更新安装源：\n\n    # apt-get update\n\n再次安装 gcc 成功。\n\n#### 安装 libreadline-dev\n\n编译过程提示以下信息则需要安装 libreadline-dev：\n\n    lua.c:80:31: fatal error: readline/readline.h: No such file or directory\n    compilation terminated.\n\n安装 libreadline-dev：\n\n    # apt-get install libreadline-dev\n\n再次编译成功。\n\n#### 检查安装\n\n用以下方法检查安装，如果安装成功会有版本信息提示：\n\n    # src/lua -v\n    Lua 5.3.3  Copyright (C) 1994-2016 Lua.org, PUC-Rio\n\n#### 编译安装\n\n    # make linux install","source":"_posts/Ubuntu-16-04-安装-Lua.md","raw":"title: Ubuntu 16.04 安装 Lua\ntags:\n  - Ubuntu\n  - Lua\ncategories:\n  - 语言\n  - Lua\ndate: 2016-07-23 22:30:14\n---\n\n\n在 Linux 系统上使用以下命令编译安装 Lua：\n\n    curl -R -O http://www.lua.org/ftp/lua-5.3.3.tar.gz\n    tar zxf lua-5.3.3.tar.gz\n    cd lua-5.3.3\n    make linux test\n\n<!-- more -->\n\n#### 安装 make\n\n编译过程如果提示以下信息则需要先安装 make：\n\n    # make linux test\n    The program 'make' can be found in the following packages:\n     * make\n     * make-guile\n    Try: apt install <selected package>\n\n安装 make\n\n    # apt-get install make\n\n#### 安装 gcc\n\n编译过程提示以下信息则需要安装 gcc：\n\n    # make linux test\n    cd src && make linux\n    make[1]: Entering directory '/root/lua/lua-5.3.3/src'\n    make all SYSCFLAGS=\"-DLUA_USE_LINUX\" SYSLIBS=\"-Wl,-E -ldl -lreadline\"\n    make[2]: Entering directory '/root/lua/lua-5.3.3/src'\n    gcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX    -c -o lapi.o lapi.c\n    make[2]: gcc: Command not found\n    <builtin>: recipe for target 'lapi.o' failed\n    make[2]: *** [lapi.o] Error 127\n    make[2]: Leaving directory '/root/lua/lua-5.3.3/src'\n    Makefile:110: recipe for target 'linux' failed\n    make[1]: *** [linux] Error 2\n    make[1]: Leaving directory '/root/lua/lua-5.3.3/src'\n    Makefile:55: recipe for target 'linux' failed\n    make: *** [linux] Error 2\n\n安装 gcc：\n\n    # apt-get install gcc\n\n安装 gcc 过程提示以下信息：\n\n    E: Failed to fetch http://cn.archive.ubuntu.com/ubuntu/pool/main/b/binutils/binutils_2.26.1-1ubuntu1~16.04_amd64.deb  404  Not Found [IP: 115.28.122.210 80]\n    E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n\n先更新安装源：\n\n    # apt-get update\n\n再次安装 gcc 成功。\n\n#### 安装 libreadline-dev\n\n编译过程提示以下信息则需要安装 libreadline-dev：\n\n    lua.c:80:31: fatal error: readline/readline.h: No such file or directory\n    compilation terminated.\n\n安装 libreadline-dev：\n\n    # apt-get install libreadline-dev\n\n再次编译成功。\n\n#### 检查安装\n\n用以下方法检查安装，如果安装成功会有版本信息提示：\n\n    # src/lua -v\n    Lua 5.3.3  Copyright (C) 1994-2016 Lua.org, PUC-Rio\n\n#### 编译安装\n\n    # make linux install","slug":"Ubuntu-16-04-安装-Lua","published":1,"updated":"2021-07-19T16:28:00.292Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphq500baitd32pbx58e1","content":"<p>在 Linux 系统上使用以下命令编译安装 Lua：</p>\n<pre><code>curl -R -O http://www.lua.org/ftp/lua-5.3.3.tar.gz\ntar zxf lua-5.3.3.tar.gz\ncd lua-5.3.3\nmake linux test\n</code></pre>\n<span id=\"more\"></span>\n\n<h4 id=\"安装-make\"><a href=\"#安装-make\" class=\"headerlink\" title=\"安装 make\"></a>安装 make</h4><p>编译过程如果提示以下信息则需要先安装 make：</p>\n<pre><code># make linux test\nThe program &#39;make&#39; can be found in the following packages:\n * make\n * make-guile\nTry: apt install &lt;selected package&gt;\n</code></pre>\n<p>安装 make</p>\n<pre><code># apt-get install make\n</code></pre>\n<h4 id=\"安装-gcc\"><a href=\"#安装-gcc\" class=\"headerlink\" title=\"安装 gcc\"></a>安装 gcc</h4><p>编译过程提示以下信息则需要安装 gcc：</p>\n<pre><code># make linux test\ncd src &amp;&amp; make linux\nmake[1]: Entering directory &#39;/root/lua/lua-5.3.3/src&#39;\nmake all SYSCFLAGS=&quot;-DLUA_USE_LINUX&quot; SYSLIBS=&quot;-Wl,-E -ldl -lreadline&quot;\nmake[2]: Entering directory &#39;/root/lua/lua-5.3.3/src&#39;\ngcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX    -c -o lapi.o lapi.c\nmake[2]: gcc: Command not found\n&lt;builtin&gt;: recipe for target &#39;lapi.o&#39; failed\nmake[2]: *** [lapi.o] Error 127\nmake[2]: Leaving directory &#39;/root/lua/lua-5.3.3/src&#39;\nMakefile:110: recipe for target &#39;linux&#39; failed\nmake[1]: *** [linux] Error 2\nmake[1]: Leaving directory &#39;/root/lua/lua-5.3.3/src&#39;\nMakefile:55: recipe for target &#39;linux&#39; failed\nmake: *** [linux] Error 2\n</code></pre>\n<p>安装 gcc：</p>\n<pre><code># apt-get install gcc\n</code></pre>\n<p>安装 gcc 过程提示以下信息：</p>\n<pre><code>E: Failed to fetch http://cn.archive.ubuntu.com/ubuntu/pool/main/b/binutils/binutils_2.26.1-1ubuntu1~16.04_amd64.deb  404  Not Found [IP: 115.28.122.210 80]\nE: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n</code></pre>\n<p>先更新安装源：</p>\n<pre><code># apt-get update\n</code></pre>\n<p>再次安装 gcc 成功。</p>\n<h4 id=\"安装-libreadline-dev\"><a href=\"#安装-libreadline-dev\" class=\"headerlink\" title=\"安装 libreadline-dev\"></a>安装 libreadline-dev</h4><p>编译过程提示以下信息则需要安装 libreadline-dev：</p>\n<pre><code>lua.c:80:31: fatal error: readline/readline.h: No such file or directory\ncompilation terminated.\n</code></pre>\n<p>安装 libreadline-dev：</p>\n<pre><code># apt-get install libreadline-dev\n</code></pre>\n<p>再次编译成功。</p>\n<h4 id=\"检查安装\"><a href=\"#检查安装\" class=\"headerlink\" title=\"检查安装\"></a>检查安装</h4><p>用以下方法检查安装，如果安装成功会有版本信息提示：</p>\n<pre><code># src/lua -v\nLua 5.3.3  Copyright (C) 1994-2016 Lua.org, PUC-Rio\n</code></pre>\n<h4 id=\"编译安装\"><a href=\"#编译安装\" class=\"headerlink\" title=\"编译安装\"></a>编译安装</h4><pre><code># make linux install\n</code></pre>\n","site":{"data":{}},"excerpt":"<p>在 Linux 系统上使用以下命令编译安装 Lua：</p>\n<pre><code>curl -R -O http://www.lua.org/ftp/lua-5.3.3.tar.gz\ntar zxf lua-5.3.3.tar.gz\ncd lua-5.3.3\nmake linux test\n</code></pre>","more":"<h4 id=\"安装-make\"><a href=\"#安装-make\" class=\"headerlink\" title=\"安装 make\"></a>安装 make</h4><p>编译过程如果提示以下信息则需要先安装 make：</p>\n<pre><code># make linux test\nThe program &#39;make&#39; can be found in the following packages:\n * make\n * make-guile\nTry: apt install &lt;selected package&gt;\n</code></pre>\n<p>安装 make</p>\n<pre><code># apt-get install make\n</code></pre>\n<h4 id=\"安装-gcc\"><a href=\"#安装-gcc\" class=\"headerlink\" title=\"安装 gcc\"></a>安装 gcc</h4><p>编译过程提示以下信息则需要安装 gcc：</p>\n<pre><code># make linux test\ncd src &amp;&amp; make linux\nmake[1]: Entering directory &#39;/root/lua/lua-5.3.3/src&#39;\nmake all SYSCFLAGS=&quot;-DLUA_USE_LINUX&quot; SYSLIBS=&quot;-Wl,-E -ldl -lreadline&quot;\nmake[2]: Entering directory &#39;/root/lua/lua-5.3.3/src&#39;\ngcc -std=gnu99 -O2 -Wall -Wextra -DLUA_COMPAT_5_2 -DLUA_USE_LINUX    -c -o lapi.o lapi.c\nmake[2]: gcc: Command not found\n&lt;builtin&gt;: recipe for target &#39;lapi.o&#39; failed\nmake[2]: *** [lapi.o] Error 127\nmake[2]: Leaving directory &#39;/root/lua/lua-5.3.3/src&#39;\nMakefile:110: recipe for target &#39;linux&#39; failed\nmake[1]: *** [linux] Error 2\nmake[1]: Leaving directory &#39;/root/lua/lua-5.3.3/src&#39;\nMakefile:55: recipe for target &#39;linux&#39; failed\nmake: *** [linux] Error 2\n</code></pre>\n<p>安装 gcc：</p>\n<pre><code># apt-get install gcc\n</code></pre>\n<p>安装 gcc 过程提示以下信息：</p>\n<pre><code>E: Failed to fetch http://cn.archive.ubuntu.com/ubuntu/pool/main/b/binutils/binutils_2.26.1-1ubuntu1~16.04_amd64.deb  404  Not Found [IP: 115.28.122.210 80]\nE: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n</code></pre>\n<p>先更新安装源：</p>\n<pre><code># apt-get update\n</code></pre>\n<p>再次安装 gcc 成功。</p>\n<h4 id=\"安装-libreadline-dev\"><a href=\"#安装-libreadline-dev\" class=\"headerlink\" title=\"安装 libreadline-dev\"></a>安装 libreadline-dev</h4><p>编译过程提示以下信息则需要安装 libreadline-dev：</p>\n<pre><code>lua.c:80:31: fatal error: readline/readline.h: No such file or directory\ncompilation terminated.\n</code></pre>\n<p>安装 libreadline-dev：</p>\n<pre><code># apt-get install libreadline-dev\n</code></pre>\n<p>再次编译成功。</p>\n<h4 id=\"检查安装\"><a href=\"#检查安装\" class=\"headerlink\" title=\"检查安装\"></a>检查安装</h4><p>用以下方法检查安装，如果安装成功会有版本信息提示：</p>\n<pre><code># src/lua -v\nLua 5.3.3  Copyright (C) 1994-2016 Lua.org, PUC-Rio\n</code></pre>\n<h4 id=\"编译安装\"><a href=\"#编译安装\" class=\"headerlink\" title=\"编译安装\"></a>编译安装</h4><pre><code># make linux install\n</code></pre>"},{"title":"Ubuntu 16.04 用 APT 安装 MySQL","date":"2017-06-11T10:39:03.000Z","_content":"\n### 安装 MySQL\n\n用以下命令安装 MySQL:\n\n    sudo apt-get install mysql-server\n    \n这个命令会安装 MySQL 服务器、客户端和公共文件。安装过程会出现两个要求输入的对话框：\n\n- 输入 MySQL root 用户的密码。\n- 指明是否创建 test 数据库。\n\n<!-- more -->\n\n### 启动/停止 MySQL\n\n安装之后，MySQL 服务器会自动启动。用下面的命令检查 MySQL 服务器状态：\n\n    $ sudo service mysql status\n    ● mysql.service - MySQL Community Server\n       Loaded: loaded (/lib/systemd/system/mysql.service; enabled; vendor preset: enabled)\n       Active: active (running) since 日 2017-06-11 17:05:11 CST; 14min ago\n     Main PID: 11970 (mysqld)\n       CGroup: /system.slice/mysql.service\n               └─11970 /usr/sbin/mysqld\n    \n    6月 11 17:05:09 frin systemd[1]: Starting MySQL Community Server...\n    6月 11 17:05:11 frin systemd[1]: Started MySQL Community Server.\n\n使用下面的命令停止 MySQL：\n\n    sudo service mysql stop\n    \n使用下面的命令启动 MySQL：\n\n    sudo service mysql start\n    \n### 连接/断开 MySQL 服务器：\n\n使用 root 用户连接 MySQL 服务器：\n\n    $ mysql -h 127.0.0.1 -u root -p\n    Enter password: \n    Welcome to the MySQL monitor.  Commands end with ; or \\g.\n    Your MySQL connection id is 4\n    Server version: 5.7.18-0ubuntu0.16.04.1 (Ubuntu)\n    \n    Copyright (c) 2000, 2017, Oracle and/or its affiliates. All rights reserved.\n    \n    Oracle is a registered trademark of Oracle Corporation and/or its\n    affiliates. Other names may be trademarks of their respective\n    owners.\n    \n    Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n    \n    mysql>\n    \n使用以下命令断开连接：\n\n    mysql> quit\n    Bye\n\n### 创建数据库\n\n创建 Hive 元数据使用的数据库 hive。使用以下命令：\n\n    mysql> CREATE DATABASE hive;\n    Query OK, 1 row affected (0.03 sec)\n    \n使用以下命令切换到 hive 数据库：\n\n    mysql> use hive;\n    Database changed\n\n使用以下命令创建用户 hive，并授权：\n\n    mysql> use mysql;\n    Database changed\n\n    mysql> CREATE USER 'hive'@'%' IDENTIFIED BY 'hive@mysql123';\n    Query OK, 0 rows affected (0.04 sec)\n    \n    mysql> GRANT SELECT,INSERT,UPDATE,DELETE,CREATE,DROP ON hive.* TO 'hive'@'%';\n    Query OK, 0 rows affected (0.01 sec)\n    \n    mysql> CREATE USER 'hive'@'localhost' IDENTIFIED BY 'hive@mysql123';\n    Query OK, 0 rows affected (0.12 sec)\n    \n    mysql> GRANT ALL ON hive.* TO 'hive'@'localhost';\n    Query OK, 0 rows affected (0.01 sec)\n    \n    mysql> FLUSH PRIVILEGES;\n    Query OK, 0 rows affected (0.00 sec)\n","source":"_posts/Ubuntu-16-04-用-APT-安装-MySQL.md","raw":"title: Ubuntu 16.04 用 APT 安装 MySQL\ntags:\n  - Ubuntu\n  - MySQL\ncategories:\n  - 数据库\n  - MySQL\ndate: 2017-06-11 18:39:03\n---\n\n### 安装 MySQL\n\n用以下命令安装 MySQL:\n\n    sudo apt-get install mysql-server\n    \n这个命令会安装 MySQL 服务器、客户端和公共文件。安装过程会出现两个要求输入的对话框：\n\n- 输入 MySQL root 用户的密码。\n- 指明是否创建 test 数据库。\n\n<!-- more -->\n\n### 启动/停止 MySQL\n\n安装之后，MySQL 服务器会自动启动。用下面的命令检查 MySQL 服务器状态：\n\n    $ sudo service mysql status\n    ● mysql.service - MySQL Community Server\n       Loaded: loaded (/lib/systemd/system/mysql.service; enabled; vendor preset: enabled)\n       Active: active (running) since 日 2017-06-11 17:05:11 CST; 14min ago\n     Main PID: 11970 (mysqld)\n       CGroup: /system.slice/mysql.service\n               └─11970 /usr/sbin/mysqld\n    \n    6月 11 17:05:09 frin systemd[1]: Starting MySQL Community Server...\n    6月 11 17:05:11 frin systemd[1]: Started MySQL Community Server.\n\n使用下面的命令停止 MySQL：\n\n    sudo service mysql stop\n    \n使用下面的命令启动 MySQL：\n\n    sudo service mysql start\n    \n### 连接/断开 MySQL 服务器：\n\n使用 root 用户连接 MySQL 服务器：\n\n    $ mysql -h 127.0.0.1 -u root -p\n    Enter password: \n    Welcome to the MySQL monitor.  Commands end with ; or \\g.\n    Your MySQL connection id is 4\n    Server version: 5.7.18-0ubuntu0.16.04.1 (Ubuntu)\n    \n    Copyright (c) 2000, 2017, Oracle and/or its affiliates. All rights reserved.\n    \n    Oracle is a registered trademark of Oracle Corporation and/or its\n    affiliates. Other names may be trademarks of their respective\n    owners.\n    \n    Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n    \n    mysql>\n    \n使用以下命令断开连接：\n\n    mysql> quit\n    Bye\n\n### 创建数据库\n\n创建 Hive 元数据使用的数据库 hive。使用以下命令：\n\n    mysql> CREATE DATABASE hive;\n    Query OK, 1 row affected (0.03 sec)\n    \n使用以下命令切换到 hive 数据库：\n\n    mysql> use hive;\n    Database changed\n\n使用以下命令创建用户 hive，并授权：\n\n    mysql> use mysql;\n    Database changed\n\n    mysql> CREATE USER 'hive'@'%' IDENTIFIED BY 'hive@mysql123';\n    Query OK, 0 rows affected (0.04 sec)\n    \n    mysql> GRANT SELECT,INSERT,UPDATE,DELETE,CREATE,DROP ON hive.* TO 'hive'@'%';\n    Query OK, 0 rows affected (0.01 sec)\n    \n    mysql> CREATE USER 'hive'@'localhost' IDENTIFIED BY 'hive@mysql123';\n    Query OK, 0 rows affected (0.12 sec)\n    \n    mysql> GRANT ALL ON hive.* TO 'hive'@'localhost';\n    Query OK, 0 rows affected (0.01 sec)\n    \n    mysql> FLUSH PRIVILEGES;\n    Query OK, 0 rows affected (0.00 sec)\n","slug":"Ubuntu-16-04-用-APT-安装-MySQL","published":1,"updated":"2021-07-19T16:28:00.292Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphq600bditd37w9ac3v5","content":"<h3 id=\"安装-MySQL\"><a href=\"#安装-MySQL\" class=\"headerlink\" title=\"安装 MySQL\"></a>安装 MySQL</h3><p>用以下命令安装 MySQL:</p>\n<pre><code>sudo apt-get install mysql-server\n</code></pre>\n<p>这个命令会安装 MySQL 服务器、客户端和公共文件。安装过程会出现两个要求输入的对话框：</p>\n<ul>\n<li>输入 MySQL root 用户的密码。</li>\n<li>指明是否创建 test 数据库。</li>\n</ul>\n<span id=\"more\"></span>\n\n<h3 id=\"启动-停止-MySQL\"><a href=\"#启动-停止-MySQL\" class=\"headerlink\" title=\"启动/停止 MySQL\"></a>启动/停止 MySQL</h3><p>安装之后，MySQL 服务器会自动启动。用下面的命令检查 MySQL 服务器状态：</p>\n<pre><code>$ sudo service mysql status\n● mysql.service - MySQL Community Server\n   Loaded: loaded (/lib/systemd/system/mysql.service; enabled; vendor preset: enabled)\n   Active: active (running) since 日 2017-06-11 17:05:11 CST; 14min ago\n Main PID: 11970 (mysqld)\n   CGroup: /system.slice/mysql.service\n           └─11970 /usr/sbin/mysqld\n\n6月 11 17:05:09 frin systemd[1]: Starting MySQL Community Server...\n6月 11 17:05:11 frin systemd[1]: Started MySQL Community Server.\n</code></pre>\n<p>使用下面的命令停止 MySQL：</p>\n<pre><code>sudo service mysql stop\n</code></pre>\n<p>使用下面的命令启动 MySQL：</p>\n<pre><code>sudo service mysql start\n</code></pre>\n<h3 id=\"连接-断开-MySQL-服务器：\"><a href=\"#连接-断开-MySQL-服务器：\" class=\"headerlink\" title=\"连接/断开 MySQL 服务器：\"></a>连接/断开 MySQL 服务器：</h3><p>使用 root 用户连接 MySQL 服务器：</p>\n<pre><code>$ mysql -h 127.0.0.1 -u root -p\nEnter password: \nWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 4\nServer version: 5.7.18-0ubuntu0.16.04.1 (Ubuntu)\n\nCopyright (c) 2000, 2017, Oracle and/or its affiliates. All rights reserved.\n\nOracle is a registered trademark of Oracle Corporation and/or its\naffiliates. Other names may be trademarks of their respective\nowners.\n\nType &#39;help;&#39; or &#39;\\h&#39; for help. Type &#39;\\c&#39; to clear the current input statement.\n\nmysql&gt;\n</code></pre>\n<p>使用以下命令断开连接：</p>\n<pre><code>mysql&gt; quit\nBye\n</code></pre>\n<h3 id=\"创建数据库\"><a href=\"#创建数据库\" class=\"headerlink\" title=\"创建数据库\"></a>创建数据库</h3><p>创建 Hive 元数据使用的数据库 hive。使用以下命令：</p>\n<pre><code>mysql&gt; CREATE DATABASE hive;\nQuery OK, 1 row affected (0.03 sec)\n</code></pre>\n<p>使用以下命令切换到 hive 数据库：</p>\n<pre><code>mysql&gt; use hive;\nDatabase changed\n</code></pre>\n<p>使用以下命令创建用户 hive，并授权：</p>\n<pre><code>mysql&gt; use mysql;\nDatabase changed\n\nmysql&gt; CREATE USER &#39;hive&#39;@&#39;%&#39; IDENTIFIED BY &#39;hive@mysql123&#39;;\nQuery OK, 0 rows affected (0.04 sec)\n\nmysql&gt; GRANT SELECT,INSERT,UPDATE,DELETE,CREATE,DROP ON hive.* TO &#39;hive&#39;@&#39;%&#39;;\nQuery OK, 0 rows affected (0.01 sec)\n\nmysql&gt; CREATE USER &#39;hive&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;hive@mysql123&#39;;\nQuery OK, 0 rows affected (0.12 sec)\n\nmysql&gt; GRANT ALL ON hive.* TO &#39;hive&#39;@&#39;localhost&#39;;\nQuery OK, 0 rows affected (0.01 sec)\n\nmysql&gt; FLUSH PRIVILEGES;\nQuery OK, 0 rows affected (0.00 sec)\n</code></pre>\n","site":{"data":{}},"excerpt":"<h3 id=\"安装-MySQL\"><a href=\"#安装-MySQL\" class=\"headerlink\" title=\"安装 MySQL\"></a>安装 MySQL</h3><p>用以下命令安装 MySQL:</p>\n<pre><code>sudo apt-get install mysql-server\n</code></pre>\n<p>这个命令会安装 MySQL 服务器、客户端和公共文件。安装过程会出现两个要求输入的对话框：</p>\n<ul>\n<li>输入 MySQL root 用户的密码。</li>\n<li>指明是否创建 test 数据库。</li>\n</ul>","more":"<h3 id=\"启动-停止-MySQL\"><a href=\"#启动-停止-MySQL\" class=\"headerlink\" title=\"启动/停止 MySQL\"></a>启动/停止 MySQL</h3><p>安装之后，MySQL 服务器会自动启动。用下面的命令检查 MySQL 服务器状态：</p>\n<pre><code>$ sudo service mysql status\n● mysql.service - MySQL Community Server\n   Loaded: loaded (/lib/systemd/system/mysql.service; enabled; vendor preset: enabled)\n   Active: active (running) since 日 2017-06-11 17:05:11 CST; 14min ago\n Main PID: 11970 (mysqld)\n   CGroup: /system.slice/mysql.service\n           └─11970 /usr/sbin/mysqld\n\n6月 11 17:05:09 frin systemd[1]: Starting MySQL Community Server...\n6月 11 17:05:11 frin systemd[1]: Started MySQL Community Server.\n</code></pre>\n<p>使用下面的命令停止 MySQL：</p>\n<pre><code>sudo service mysql stop\n</code></pre>\n<p>使用下面的命令启动 MySQL：</p>\n<pre><code>sudo service mysql start\n</code></pre>\n<h3 id=\"连接-断开-MySQL-服务器：\"><a href=\"#连接-断开-MySQL-服务器：\" class=\"headerlink\" title=\"连接/断开 MySQL 服务器：\"></a>连接/断开 MySQL 服务器：</h3><p>使用 root 用户连接 MySQL 服务器：</p>\n<pre><code>$ mysql -h 127.0.0.1 -u root -p\nEnter password: \nWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 4\nServer version: 5.7.18-0ubuntu0.16.04.1 (Ubuntu)\n\nCopyright (c) 2000, 2017, Oracle and/or its affiliates. All rights reserved.\n\nOracle is a registered trademark of Oracle Corporation and/or its\naffiliates. Other names may be trademarks of their respective\nowners.\n\nType &#39;help;&#39; or &#39;\\h&#39; for help. Type &#39;\\c&#39; to clear the current input statement.\n\nmysql&gt;\n</code></pre>\n<p>使用以下命令断开连接：</p>\n<pre><code>mysql&gt; quit\nBye\n</code></pre>\n<h3 id=\"创建数据库\"><a href=\"#创建数据库\" class=\"headerlink\" title=\"创建数据库\"></a>创建数据库</h3><p>创建 Hive 元数据使用的数据库 hive。使用以下命令：</p>\n<pre><code>mysql&gt; CREATE DATABASE hive;\nQuery OK, 1 row affected (0.03 sec)\n</code></pre>\n<p>使用以下命令切换到 hive 数据库：</p>\n<pre><code>mysql&gt; use hive;\nDatabase changed\n</code></pre>\n<p>使用以下命令创建用户 hive，并授权：</p>\n<pre><code>mysql&gt; use mysql;\nDatabase changed\n\nmysql&gt; CREATE USER &#39;hive&#39;@&#39;%&#39; IDENTIFIED BY &#39;hive@mysql123&#39;;\nQuery OK, 0 rows affected (0.04 sec)\n\nmysql&gt; GRANT SELECT,INSERT,UPDATE,DELETE,CREATE,DROP ON hive.* TO &#39;hive&#39;@&#39;%&#39;;\nQuery OK, 0 rows affected (0.01 sec)\n\nmysql&gt; CREATE USER &#39;hive&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;hive@mysql123&#39;;\nQuery OK, 0 rows affected (0.12 sec)\n\nmysql&gt; GRANT ALL ON hive.* TO &#39;hive&#39;@&#39;localhost&#39;;\nQuery OK, 0 rows affected (0.01 sec)\n\nmysql&gt; FLUSH PRIVILEGES;\nQuery OK, 0 rows affected (0.00 sec)\n</code></pre>"},{"title":"Ubuntu 18.04 安装 Emacs 26.2 问题解决","date":"2019-06-04T07:16:43.000Z","_content":"### no X development libraries were found\n\n    checking for X... no\n    checking for X... true\n    configure: error: You seem to be running X, but no X development libraries\n    were found.  You should install the relevant development files for X\n    and for the toolkit you want, such as Gtk+ or Motif.  Also make\n    sure you have development files for image handling, i.e.\n    tiff, gif, jpeg, png and xpm.\n    If you are sure you want Emacs compiled without X window support, pass\n      --without-x\n    to configure.\n\n安装 libgtk 开发包：\n\n    $ sudo apt-get install libgtk2.0-dev\n\n<!-- more -->\n\n### libXpm libjpeg libgif/libungif libtiff gnutls were not found\n\n    configure: error: The following required libraries were not found:\n        libXpm libjpeg libgif/libungif libtiff gnutls\n    Maybe some development libraries/packages are missing?\n    If you don't want to link with them give\n        --with-xpm=no --with-jpeg=no --with-gif=no --with-tiff=no --with-gnutls=no\n    as options to configure\n\n安装对应的包：\n\n    sudo apt-get install libxpm-dev\n    sudo apt-get install libjpeg62-dev\n    sudo apt-get install libgif-dev\n    sudo apt-get install libtiff5-dev\n    sudo apt-get install libgnutls28-dev\n","source":"_posts/Ubuntu-18-04-安装-Emacs-26-2-问题解决.md","raw":"title: Ubuntu 18.04 安装 Emacs 26.2 问题解决\ndate: 2019-06-04 15:16:43\ntags:\n- Ubuntu\n- Emacs\ncategories:\n- 开发工具\n- Emacs\n---\n### no X development libraries were found\n\n    checking for X... no\n    checking for X... true\n    configure: error: You seem to be running X, but no X development libraries\n    were found.  You should install the relevant development files for X\n    and for the toolkit you want, such as Gtk+ or Motif.  Also make\n    sure you have development files for image handling, i.e.\n    tiff, gif, jpeg, png and xpm.\n    If you are sure you want Emacs compiled without X window support, pass\n      --without-x\n    to configure.\n\n安装 libgtk 开发包：\n\n    $ sudo apt-get install libgtk2.0-dev\n\n<!-- more -->\n\n### libXpm libjpeg libgif/libungif libtiff gnutls were not found\n\n    configure: error: The following required libraries were not found:\n        libXpm libjpeg libgif/libungif libtiff gnutls\n    Maybe some development libraries/packages are missing?\n    If you don't want to link with them give\n        --with-xpm=no --with-jpeg=no --with-gif=no --with-tiff=no --with-gnutls=no\n    as options to configure\n\n安装对应的包：\n\n    sudo apt-get install libxpm-dev\n    sudo apt-get install libjpeg62-dev\n    sudo apt-get install libgif-dev\n    sudo apt-get install libtiff5-dev\n    sudo apt-get install libgnutls28-dev\n","slug":"Ubuntu-18-04-安装-Emacs-26-2-问题解决","published":1,"updated":"2021-07-19T16:28:00.292Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphq900biitd30zvw26fx","content":"<h3 id=\"no-X-development-libraries-were-found\"><a href=\"#no-X-development-libraries-were-found\" class=\"headerlink\" title=\"no X development libraries were found\"></a>no X development libraries were found</h3><pre><code>checking for X... no\nchecking for X... true\nconfigure: error: You seem to be running X, but no X development libraries\nwere found.  You should install the relevant development files for X\nand for the toolkit you want, such as Gtk+ or Motif.  Also make\nsure you have development files for image handling, i.e.\ntiff, gif, jpeg, png and xpm.\nIf you are sure you want Emacs compiled without X window support, pass\n  --without-x\nto configure.\n</code></pre>\n<p>安装 libgtk 开发包：</p>\n<pre><code>$ sudo apt-get install libgtk2.0-dev\n</code></pre>\n<span id=\"more\"></span>\n\n<h3 id=\"libXpm-libjpeg-libgif-libungif-libtiff-gnutls-were-not-found\"><a href=\"#libXpm-libjpeg-libgif-libungif-libtiff-gnutls-were-not-found\" class=\"headerlink\" title=\"libXpm libjpeg libgif/libungif libtiff gnutls were not found\"></a>libXpm libjpeg libgif/libungif libtiff gnutls were not found</h3><pre><code>configure: error: The following required libraries were not found:\n    libXpm libjpeg libgif/libungif libtiff gnutls\nMaybe some development libraries/packages are missing?\nIf you don&#39;t want to link with them give\n    --with-xpm=no --with-jpeg=no --with-gif=no --with-tiff=no --with-gnutls=no\nas options to configure\n</code></pre>\n<p>安装对应的包：</p>\n<pre><code>sudo apt-get install libxpm-dev\nsudo apt-get install libjpeg62-dev\nsudo apt-get install libgif-dev\nsudo apt-get install libtiff5-dev\nsudo apt-get install libgnutls28-dev\n</code></pre>\n","site":{"data":{}},"excerpt":"<h3 id=\"no-X-development-libraries-were-found\"><a href=\"#no-X-development-libraries-were-found\" class=\"headerlink\" title=\"no X development libraries were found\"></a>no X development libraries were found</h3><pre><code>checking for X... no\nchecking for X... true\nconfigure: error: You seem to be running X, but no X development libraries\nwere found.  You should install the relevant development files for X\nand for the toolkit you want, such as Gtk+ or Motif.  Also make\nsure you have development files for image handling, i.e.\ntiff, gif, jpeg, png and xpm.\nIf you are sure you want Emacs compiled without X window support, pass\n  --without-x\nto configure.\n</code></pre>\n<p>安装 libgtk 开发包：</p>\n<pre><code>$ sudo apt-get install libgtk2.0-dev\n</code></pre>","more":"<h3 id=\"libXpm-libjpeg-libgif-libungif-libtiff-gnutls-were-not-found\"><a href=\"#libXpm-libjpeg-libgif-libungif-libtiff-gnutls-were-not-found\" class=\"headerlink\" title=\"libXpm libjpeg libgif/libungif libtiff gnutls were not found\"></a>libXpm libjpeg libgif/libungif libtiff gnutls were not found</h3><pre><code>configure: error: The following required libraries were not found:\n    libXpm libjpeg libgif/libungif libtiff gnutls\nMaybe some development libraries/packages are missing?\nIf you don&#39;t want to link with them give\n    --with-xpm=no --with-jpeg=no --with-gif=no --with-tiff=no --with-gnutls=no\nas options to configure\n</code></pre>\n<p>安装对应的包：</p>\n<pre><code>sudo apt-get install libxpm-dev\nsudo apt-get install libjpeg62-dev\nsudo apt-get install libgif-dev\nsudo apt-get install libtiff5-dev\nsudo apt-get install libgnutls28-dev\n</code></pre>"},{"title":"Ubuntu 20.04安装Protocol Buffers 2.5.0","date":"2021-07-01T03:46:27.000Z","_content":"\n# 安装过程\n\nProtocol Buffers 2.5.0源码下载：https://github.com/protocolbuffers/protobuf/tree/v2.5.0。下载并解压。\n\n将autogen.sh文件中以下内容：\n\n```Shell\n  curl http://googletest.googlecode.com/files/gtest-1.5.0.tar.bz2 | tar jx\n  mv gtest-1.5.0 gtest\n```\n\n替换为：\n\n```Shell\n  wget https://github.com/google/googletest/archive/release-1.5.0.tar.gz\n  tar xzvf release-1.5.0.tar.gz\n  mv googletest-release-1.5.0 gtest\n```\n\n执行以下命令进行安装：\n\n```Shell\n./autogen.sh\n./configure\nmake\nmake check\nmake install\n```\n\n# 安装问题\n## 问题一\n### 问题现象\n\n执行 autogen.sh 时出现一下错误：\n\n```Shell\nconfigure.ac:29: error: possibly undefined macro: AC_PROG_LIBTOOL\n      If this token and others are legitimate, please use m4_pattern_allow.\n      See the Autoconf documentation.\nautoreconf: /usr/bin/autoconf failed with exit status: 1\n```\n\n### 问题解决\n\n安装 libtool：\n\n```Shell\nsudo apt-get install libtool\n```\n\n## 问题二\n### 问题现象\n\n安装完成后验证出现以下错误：\n\n```Shell\n$ protoc --version\nprotoc: error while loading shared libraries: libprotoc.so.8: cannot open shared object file: No such file or directory\n```\n\n### 问题解决\n\n- 使用find命令找到libprotoc.so.8文件位置。\n\n```Shell\n$ cd /usr/local/\n$ find . -name libprotoc.so.8\n./lib/libprotoc.so.8\n```\n\n- 创建/etc/ld.so.conf.d/libprotobuf.conf文件，并输入以下内容：\n\n```Text\n/usr/local/lib\n```\n\n- 执行命令：sudo ldconfig","source":"_posts/Ubuntu-20-04安装Protocol-Buffers-2-5-0.md","raw":"title: Ubuntu 20.04安装Protocol Buffers 2.5.0\ndate: 2021-07-01 11:46:27\ntags:\n- Ubuntu\n- Linux\n- Google\n- Protocol Buggers\ncategories:\n- 开发\n- 环境搭建\n---\n\n# 安装过程\n\nProtocol Buffers 2.5.0源码下载：https://github.com/protocolbuffers/protobuf/tree/v2.5.0。下载并解压。\n\n将autogen.sh文件中以下内容：\n\n```Shell\n  curl http://googletest.googlecode.com/files/gtest-1.5.0.tar.bz2 | tar jx\n  mv gtest-1.5.0 gtest\n```\n\n替换为：\n\n```Shell\n  wget https://github.com/google/googletest/archive/release-1.5.0.tar.gz\n  tar xzvf release-1.5.0.tar.gz\n  mv googletest-release-1.5.0 gtest\n```\n\n执行以下命令进行安装：\n\n```Shell\n./autogen.sh\n./configure\nmake\nmake check\nmake install\n```\n\n# 安装问题\n## 问题一\n### 问题现象\n\n执行 autogen.sh 时出现一下错误：\n\n```Shell\nconfigure.ac:29: error: possibly undefined macro: AC_PROG_LIBTOOL\n      If this token and others are legitimate, please use m4_pattern_allow.\n      See the Autoconf documentation.\nautoreconf: /usr/bin/autoconf failed with exit status: 1\n```\n\n### 问题解决\n\n安装 libtool：\n\n```Shell\nsudo apt-get install libtool\n```\n\n## 问题二\n### 问题现象\n\n安装完成后验证出现以下错误：\n\n```Shell\n$ protoc --version\nprotoc: error while loading shared libraries: libprotoc.so.8: cannot open shared object file: No such file or directory\n```\n\n### 问题解决\n\n- 使用find命令找到libprotoc.so.8文件位置。\n\n```Shell\n$ cd /usr/local/\n$ find . -name libprotoc.so.8\n./lib/libprotoc.so.8\n```\n\n- 创建/etc/ld.so.conf.d/libprotobuf.conf文件，并输入以下内容：\n\n```Text\n/usr/local/lib\n```\n\n- 执行命令：sudo ldconfig","slug":"Ubuntu-20-04安装Protocol-Buffers-2-5-0","published":1,"updated":"2021-07-19T16:28:00.188Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphqa00blitd3fuvr0e56","content":"<h1 id=\"安装过程\"><a href=\"#安装过程\" class=\"headerlink\" title=\"安装过程\"></a>安装过程</h1><p>Protocol Buffers 2.5.0源码下载：<a href=\"https://github.com/protocolbuffers/protobuf/tree/v2.5.0%E3%80%82%E4%B8%8B%E8%BD%BD%E5%B9%B6%E8%A7%A3%E5%8E%8B%E3%80%82\">https://github.com/protocolbuffers/protobuf/tree/v2.5.0。下载并解压。</a></p>\n<p>将autogen.sh文件中以下内容：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl http://googletest.googlecode.com/files/gtest-1.5.0.tar.bz2 | tar jx</span><br><span class=\"line\">mv gtest-1.5.0 gtest</span><br></pre></td></tr></table></figure>\n\n<p>替换为：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://github.com/google/googletest/archive/release-1.5.0.tar.gz</span><br><span class=\"line\">tar xzvf release-1.5.0.tar.gz</span><br><span class=\"line\">mv googletest-release-1.5.0 gtest</span><br></pre></td></tr></table></figure>\n\n<p>执行以下命令进行安装：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./autogen.sh</span><br><span class=\"line\">./configure</span><br><span class=\"line\">make</span><br><span class=\"line\">make check</span><br><span class=\"line\">make install</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"安装问题\"><a href=\"#安装问题\" class=\"headerlink\" title=\"安装问题\"></a>安装问题</h1><h2 id=\"问题一\"><a href=\"#问题一\" class=\"headerlink\" title=\"问题一\"></a>问题一</h2><h3 id=\"问题现象\"><a href=\"#问题现象\" class=\"headerlink\" title=\"问题现象\"></a>问题现象</h3><p>执行 autogen.sh 时出现一下错误：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">configure.ac:29: error: possibly undefined macro: AC_PROG_LIBTOOL</span><br><span class=\"line\">      If this token and others are legitimate, please use m4_pattern_allow.</span><br><span class=\"line\">      See the Autoconf documentation.</span><br><span class=\"line\">autoreconf: /usr/bin/autoconf failed with exit status: 1</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"问题解决\"><a href=\"#问题解决\" class=\"headerlink\" title=\"问题解决\"></a>问题解决</h3><p>安装 libtool：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get install libtool</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"问题二\"><a href=\"#问题二\" class=\"headerlink\" title=\"问题二\"></a>问题二</h2><h3 id=\"问题现象-1\"><a href=\"#问题现象-1\" class=\"headerlink\" title=\"问题现象\"></a>问题现象</h3><p>安装完成后验证出现以下错误：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">$</span><span class=\"bash\"> protoc --version</span></span><br><span class=\"line\">protoc: error while loading shared libraries: libprotoc.so.8: cannot open shared object file: No such file or directory</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"问题解决-1\"><a href=\"#问题解决-1\" class=\"headerlink\" title=\"问题解决\"></a>问题解决</h3><ul>\n<li>使用find命令找到libprotoc.so.8文件位置。</li>\n</ul>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">$</span><span class=\"bash\"> <span class=\"built_in\">cd</span> /usr/<span class=\"built_in\">local</span>/</span></span><br><span class=\"line\"><span class=\"meta\">$</span><span class=\"bash\"> find . -name libprotoc.so.8</span></span><br><span class=\"line\">./lib/libprotoc.so.8</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>创建/etc/ld.so.conf.d/libprotobuf.conf文件，并输入以下内容：</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/usr/local/lib</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>执行命令：sudo ldconfig</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"安装过程\"><a href=\"#安装过程\" class=\"headerlink\" title=\"安装过程\"></a>安装过程</h1><p>Protocol Buffers 2.5.0源码下载：<a href=\"https://github.com/protocolbuffers/protobuf/tree/v2.5.0%E3%80%82%E4%B8%8B%E8%BD%BD%E5%B9%B6%E8%A7%A3%E5%8E%8B%E3%80%82\">https://github.com/protocolbuffers/protobuf/tree/v2.5.0。下载并解压。</a></p>\n<p>将autogen.sh文件中以下内容：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl http://googletest.googlecode.com/files/gtest-1.5.0.tar.bz2 | tar jx</span><br><span class=\"line\">mv gtest-1.5.0 gtest</span><br></pre></td></tr></table></figure>\n\n<p>替换为：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://github.com/google/googletest/archive/release-1.5.0.tar.gz</span><br><span class=\"line\">tar xzvf release-1.5.0.tar.gz</span><br><span class=\"line\">mv googletest-release-1.5.0 gtest</span><br></pre></td></tr></table></figure>\n\n<p>执行以下命令进行安装：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./autogen.sh</span><br><span class=\"line\">./configure</span><br><span class=\"line\">make</span><br><span class=\"line\">make check</span><br><span class=\"line\">make install</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"安装问题\"><a href=\"#安装问题\" class=\"headerlink\" title=\"安装问题\"></a>安装问题</h1><h2 id=\"问题一\"><a href=\"#问题一\" class=\"headerlink\" title=\"问题一\"></a>问题一</h2><h3 id=\"问题现象\"><a href=\"#问题现象\" class=\"headerlink\" title=\"问题现象\"></a>问题现象</h3><p>执行 autogen.sh 时出现一下错误：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">configure.ac:29: error: possibly undefined macro: AC_PROG_LIBTOOL</span><br><span class=\"line\">      If this token and others are legitimate, please use m4_pattern_allow.</span><br><span class=\"line\">      See the Autoconf documentation.</span><br><span class=\"line\">autoreconf: /usr/bin/autoconf failed with exit status: 1</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"问题解决\"><a href=\"#问题解决\" class=\"headerlink\" title=\"问题解决\"></a>问题解决</h3><p>安装 libtool：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get install libtool</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"问题二\"><a href=\"#问题二\" class=\"headerlink\" title=\"问题二\"></a>问题二</h2><h3 id=\"问题现象-1\"><a href=\"#问题现象-1\" class=\"headerlink\" title=\"问题现象\"></a>问题现象</h3><p>安装完成后验证出现以下错误：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">$</span><span class=\"bash\"> protoc --version</span></span><br><span class=\"line\">protoc: error while loading shared libraries: libprotoc.so.8: cannot open shared object file: No such file or directory</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"问题解决-1\"><a href=\"#问题解决-1\" class=\"headerlink\" title=\"问题解决\"></a>问题解决</h3><ul>\n<li>使用find命令找到libprotoc.so.8文件位置。</li>\n</ul>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">$</span><span class=\"bash\"> <span class=\"built_in\">cd</span> /usr/<span class=\"built_in\">local</span>/</span></span><br><span class=\"line\"><span class=\"meta\">$</span><span class=\"bash\"> find . -name libprotoc.so.8</span></span><br><span class=\"line\">./lib/libprotoc.so.8</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>创建/etc/ld.so.conf.d/libprotobuf.conf文件，并输入以下内容：</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/usr/local/lib</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>执行命令：sudo ldconfig</li>\n</ul>\n"},{"title":"Ubuntu SSH 免密码登陆","-categories":["操作系统","Ubuntu"],"date":"2016-06-25T23:44:08.000Z","_content":"\n使用 Ubuntu 系统远程登陆时，一般使用用户名加密码登陆，但这种方式每次需要输入用户名和密码，比较繁琐。我们可以用更具安全性的 RSA 密钥认证方式避免频繁输入用户名和密码。\n\n<!-- more -->\n\n### 应用场景\n\n如果我们需要从一台机器上同时操作多台 Ubuntu 服务器，免密码登陆操作起来就方便多了。\n\n### 实现步骤\n#### 安装 SSH 服务\n\n在 Ubuntu 服务器上安装 SSH 服务，方法参见另外一篇博客：[Ubuntu 安装 SSH，并开启 root 远程登录](http://zhang-jc.github.io/2016/05/25/Ubuntu-%E5%AE%89%E8%A3%85-SSH%EF%BC%8C%E5%B9%B6%E5%BC%80%E5%90%AF-root-%E8%BF%9C%E7%A8%8B%E7%99%BB%E5%BD%95/)。\n\n#### Client 端生成公钥和密钥\n\n使用以下命令生成客户端公钥和密钥：\n\n    $ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\n\n生成的密钥对在目录 ~/.ssh/ 下，id_rsa.pub 是公钥，id_rsa 是密钥。\n\n#### 将 Client 端的公钥添加到 Server 端的公钥文件中\n\n首先检查 Server 端需要认证的用户（如 root）的 Home 目录下，隐藏目录 .ssh 中是否存在一个名为 authorized_keys 的文件，若不存在，使用命令：touch authorized_keys 创建一个空文件创建完成后，则可以执行如下步骤：\n\n1. 复制 Clinet 端公钥的内容\n2. 将复制的 Client 端公钥内容粘贴至 Server 端的 authorized_keys 文件中，保存文件。\n3. 更改 authorized_keys 文件的权限为 600，执行命令 chmod 600  authorized_keys\n4. 重启服务端 sshd 服务：service sshd restart\n\n#### 验证无密码登陆\n\n在客户端输入命令：ssh root@192.168.1.133（用户名和IP自行替换），看到如下结果则无密码登陆配置成功。\n![SSH 免密码登陆](/uploads/20160626/ssh.png)\n\n> 在将客户端公钥添加到服务端时，如果是从一台客户端登陆多台服务器，则可以先将公钥添加至客户端的 authorized_keys 文件，直接将客户端 authorized_keys 文件 scp 到服务器对应的目录下，然后重启服务器 sshd 服务。但这个过程中还是需要先输入用户名和密码登陆服务器重启 sshd 服务。是否有更便捷的方式有待学习。\n","source":"_posts/Ubuntu-SSH-免密码登陆.md","raw":"title: Ubuntu SSH 免密码登陆\ntags:\n  - Ubuntu\n'-categories':\n  - 操作系统\n  - Ubuntu\ndate: 2016-06-26 07:44:08\n---\n\n使用 Ubuntu 系统远程登陆时，一般使用用户名加密码登陆，但这种方式每次需要输入用户名和密码，比较繁琐。我们可以用更具安全性的 RSA 密钥认证方式避免频繁输入用户名和密码。\n\n<!-- more -->\n\n### 应用场景\n\n如果我们需要从一台机器上同时操作多台 Ubuntu 服务器，免密码登陆操作起来就方便多了。\n\n### 实现步骤\n#### 安装 SSH 服务\n\n在 Ubuntu 服务器上安装 SSH 服务，方法参见另外一篇博客：[Ubuntu 安装 SSH，并开启 root 远程登录](http://zhang-jc.github.io/2016/05/25/Ubuntu-%E5%AE%89%E8%A3%85-SSH%EF%BC%8C%E5%B9%B6%E5%BC%80%E5%90%AF-root-%E8%BF%9C%E7%A8%8B%E7%99%BB%E5%BD%95/)。\n\n#### Client 端生成公钥和密钥\n\n使用以下命令生成客户端公钥和密钥：\n\n    $ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\n\n生成的密钥对在目录 ~/.ssh/ 下，id_rsa.pub 是公钥，id_rsa 是密钥。\n\n#### 将 Client 端的公钥添加到 Server 端的公钥文件中\n\n首先检查 Server 端需要认证的用户（如 root）的 Home 目录下，隐藏目录 .ssh 中是否存在一个名为 authorized_keys 的文件，若不存在，使用命令：touch authorized_keys 创建一个空文件创建完成后，则可以执行如下步骤：\n\n1. 复制 Clinet 端公钥的内容\n2. 将复制的 Client 端公钥内容粘贴至 Server 端的 authorized_keys 文件中，保存文件。\n3. 更改 authorized_keys 文件的权限为 600，执行命令 chmod 600  authorized_keys\n4. 重启服务端 sshd 服务：service sshd restart\n\n#### 验证无密码登陆\n\n在客户端输入命令：ssh root@192.168.1.133（用户名和IP自行替换），看到如下结果则无密码登陆配置成功。\n![SSH 免密码登陆](/uploads/20160626/ssh.png)\n\n> 在将客户端公钥添加到服务端时，如果是从一台客户端登陆多台服务器，则可以先将公钥添加至客户端的 authorized_keys 文件，直接将客户端 authorized_keys 文件 scp 到服务器对应的目录下，然后重启服务器 sshd 服务。但这个过程中还是需要先输入用户名和密码登陆服务器重启 sshd 服务。是否有更便捷的方式有待学习。\n","slug":"Ubuntu-SSH-免密码登陆","published":1,"updated":"2021-07-19T16:28:00.292Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphqc00bqitd3evl67med","content":"<p>使用 Ubuntu 系统远程登陆时，一般使用用户名加密码登陆，但这种方式每次需要输入用户名和密码，比较繁琐。我们可以用更具安全性的 RSA 密钥认证方式避免频繁输入用户名和密码。</p>\n<span id=\"more\"></span>\n\n<h3 id=\"应用场景\"><a href=\"#应用场景\" class=\"headerlink\" title=\"应用场景\"></a>应用场景</h3><p>如果我们需要从一台机器上同时操作多台 Ubuntu 服务器，免密码登陆操作起来就方便多了。</p>\n<h3 id=\"实现步骤\"><a href=\"#实现步骤\" class=\"headerlink\" title=\"实现步骤\"></a>实现步骤</h3><h4 id=\"安装-SSH-服务\"><a href=\"#安装-SSH-服务\" class=\"headerlink\" title=\"安装 SSH 服务\"></a>安装 SSH 服务</h4><p>在 Ubuntu 服务器上安装 SSH 服务，方法参见另外一篇博客：<a href=\"http://zhang-jc.github.io/2016/05/25/Ubuntu-%E5%AE%89%E8%A3%85-SSH%EF%BC%8C%E5%B9%B6%E5%BC%80%E5%90%AF-root-%E8%BF%9C%E7%A8%8B%E7%99%BB%E5%BD%95/\">Ubuntu 安装 SSH，并开启 root 远程登录</a>。</p>\n<h4 id=\"Client-端生成公钥和密钥\"><a href=\"#Client-端生成公钥和密钥\" class=\"headerlink\" title=\"Client 端生成公钥和密钥\"></a>Client 端生成公钥和密钥</h4><p>使用以下命令生成客户端公钥和密钥：</p>\n<pre><code>$ ssh-keygen -t rsa -P &#39;&#39; -f ~/.ssh/id_rsa\n</code></pre>\n<p>生成的密钥对在目录 ~/.ssh/ 下，id_rsa.pub 是公钥，id_rsa 是密钥。</p>\n<h4 id=\"将-Client-端的公钥添加到-Server-端的公钥文件中\"><a href=\"#将-Client-端的公钥添加到-Server-端的公钥文件中\" class=\"headerlink\" title=\"将 Client 端的公钥添加到 Server 端的公钥文件中\"></a>将 Client 端的公钥添加到 Server 端的公钥文件中</h4><p>首先检查 Server 端需要认证的用户（如 root）的 Home 目录下，隐藏目录 .ssh 中是否存在一个名为 authorized_keys 的文件，若不存在，使用命令：touch authorized_keys 创建一个空文件创建完成后，则可以执行如下步骤：</p>\n<ol>\n<li>复制 Clinet 端公钥的内容</li>\n<li>将复制的 Client 端公钥内容粘贴至 Server 端的 authorized_keys 文件中，保存文件。</li>\n<li>更改 authorized_keys 文件的权限为 600，执行命令 chmod 600  authorized_keys</li>\n<li>重启服务端 sshd 服务：service sshd restart</li>\n</ol>\n<h4 id=\"验证无密码登陆\"><a href=\"#验证无密码登陆\" class=\"headerlink\" title=\"验证无密码登陆\"></a>验证无密码登陆</h4><p>在客户端输入命令：ssh <a href=\"mailto:&#114;&#111;&#x6f;&#x74;&#64;&#x31;&#x39;&#x32;&#x2e;&#x31;&#54;&#x38;&#46;&#49;&#46;&#49;&#x33;&#51;\">&#114;&#111;&#x6f;&#x74;&#64;&#x31;&#x39;&#x32;&#x2e;&#x31;&#54;&#x38;&#46;&#49;&#46;&#49;&#x33;&#51;</a>（用户名和IP自行替换），看到如下结果则无密码登陆配置成功。<br><img src=\"/uploads/20160626/ssh.png\" alt=\"SSH 免密码登陆\"></p>\n<blockquote>\n<p>在将客户端公钥添加到服务端时，如果是从一台客户端登陆多台服务器，则可以先将公钥添加至客户端的 authorized_keys 文件，直接将客户端 authorized_keys 文件 scp 到服务器对应的目录下，然后重启服务器 sshd 服务。但这个过程中还是需要先输入用户名和密码登陆服务器重启 sshd 服务。是否有更便捷的方式有待学习。</p>\n</blockquote>\n","site":{"data":{}},"excerpt":"<p>使用 Ubuntu 系统远程登陆时，一般使用用户名加密码登陆，但这种方式每次需要输入用户名和密码，比较繁琐。我们可以用更具安全性的 RSA 密钥认证方式避免频繁输入用户名和密码。</p>","more":"<h3 id=\"应用场景\"><a href=\"#应用场景\" class=\"headerlink\" title=\"应用场景\"></a>应用场景</h3><p>如果我们需要从一台机器上同时操作多台 Ubuntu 服务器，免密码登陆操作起来就方便多了。</p>\n<h3 id=\"实现步骤\"><a href=\"#实现步骤\" class=\"headerlink\" title=\"实现步骤\"></a>实现步骤</h3><h4 id=\"安装-SSH-服务\"><a href=\"#安装-SSH-服务\" class=\"headerlink\" title=\"安装 SSH 服务\"></a>安装 SSH 服务</h4><p>在 Ubuntu 服务器上安装 SSH 服务，方法参见另外一篇博客：<a href=\"http://zhang-jc.github.io/2016/05/25/Ubuntu-%E5%AE%89%E8%A3%85-SSH%EF%BC%8C%E5%B9%B6%E5%BC%80%E5%90%AF-root-%E8%BF%9C%E7%A8%8B%E7%99%BB%E5%BD%95/\">Ubuntu 安装 SSH，并开启 root 远程登录</a>。</p>\n<h4 id=\"Client-端生成公钥和密钥\"><a href=\"#Client-端生成公钥和密钥\" class=\"headerlink\" title=\"Client 端生成公钥和密钥\"></a>Client 端生成公钥和密钥</h4><p>使用以下命令生成客户端公钥和密钥：</p>\n<pre><code>$ ssh-keygen -t rsa -P &#39;&#39; -f ~/.ssh/id_rsa\n</code></pre>\n<p>生成的密钥对在目录 ~/.ssh/ 下，id_rsa.pub 是公钥，id_rsa 是密钥。</p>\n<h4 id=\"将-Client-端的公钥添加到-Server-端的公钥文件中\"><a href=\"#将-Client-端的公钥添加到-Server-端的公钥文件中\" class=\"headerlink\" title=\"将 Client 端的公钥添加到 Server 端的公钥文件中\"></a>将 Client 端的公钥添加到 Server 端的公钥文件中</h4><p>首先检查 Server 端需要认证的用户（如 root）的 Home 目录下，隐藏目录 .ssh 中是否存在一个名为 authorized_keys 的文件，若不存在，使用命令：touch authorized_keys 创建一个空文件创建完成后，则可以执行如下步骤：</p>\n<ol>\n<li>复制 Clinet 端公钥的内容</li>\n<li>将复制的 Client 端公钥内容粘贴至 Server 端的 authorized_keys 文件中，保存文件。</li>\n<li>更改 authorized_keys 文件的权限为 600，执行命令 chmod 600  authorized_keys</li>\n<li>重启服务端 sshd 服务：service sshd restart</li>\n</ol>\n<h4 id=\"验证无密码登陆\"><a href=\"#验证无密码登陆\" class=\"headerlink\" title=\"验证无密码登陆\"></a>验证无密码登陆</h4><p>在客户端输入命令：ssh <a href=\"mailto:&#114;&#111;&#x6f;&#x74;&#64;&#x31;&#x39;&#x32;&#x2e;&#x31;&#54;&#x38;&#46;&#49;&#46;&#49;&#x33;&#51;\">&#114;&#111;&#x6f;&#x74;&#64;&#x31;&#x39;&#x32;&#x2e;&#x31;&#54;&#x38;&#46;&#49;&#46;&#49;&#x33;&#51;</a>（用户名和IP自行替换），看到如下结果则无密码登陆配置成功。<br><img src=\"/uploads/20160626/ssh.png\" alt=\"SSH 免密码登陆\"></p>\n<blockquote>\n<p>在将客户端公钥添加到服务端时，如果是从一台客户端登陆多台服务器，则可以先将公钥添加至客户端的 authorized_keys 文件，直接将客户端 authorized_keys 文件 scp 到服务器对应的目录下，然后重启服务器 sshd 服务。但这个过程中还是需要先输入用户名和密码登陆服务器重启 sshd 服务。是否有更便捷的方式有待学习。</p>\n</blockquote>"},{"title":"Ubuntu 15.10 安装 MongoDB","date":"2016-04-16T03:47:22.000Z","_content":"\n本篇主要参考官方安装文档，在 Ubuntu 15.10 上安装并启动。中间遇到一些问题，已经找到解决方案。\n\n<!-- more -->\n\n### 概述\n\n从 3.2 版本开始，MongoDB 不再支持 32 位的平台。\n\n使用本教程从 **.deb** 包在 LTS Ubuntu Linux 系统上安装 MongoDB 社区版。虽然 Ubuntu 包含自己的 MongoDB 包，官方 MongoDB 社区版包通常是更新的。\n\n### 安装包\n\nMongoDB 在自己的仓库中提供了官方支持的安装包。这个仓库包含一下包：\n\n- mongodb-org：一个**元安装包**，可以自动安装下面的四个组件安装包。\n- mongodb-org-server：包含 [mongod](https://docs.mongodb.org/manual/reference/program/mongod/#bin.mongod) 守护进程和相关的配置及初始脚本。\n- mongodb-org-mongos：包含 [mongos](https://docs.mongodb.org/manual/reference/program/mongos/#bin.mongos) 守护进程。\n- mongodb-org-shell：包含 [mongo](https://docs.mongodb.org/manual/reference/program/mongo/#bin.mongo) shell。\n- mongodb-org-tools：包含以下 MongoDB 工具：[mongoimport](https://docs.mongodb.org/manual/reference/program/mongoimport/#bin.mongoimport)、[bsondump](https://docs.mongodb.org/manual/reference/program/bsondump/#bin.bsondump)、[mongodump](https://docs.mongodb.org/manual/reference/program/mongodump/#bin.mongodump)、[mongoexport](https://docs.mongodb.org/manual/reference/program/mongoexport/#bin.mongoexport)、[mongofiles](https://docs.mongodb.org/manual/reference/program/mongofiles/#bin.mongofiles)、[mongooplog](https://docs.mongodb.org/manual/reference/program/mongooplog/#bin.mongooplog)、[mongoperf](https://docs.mongodb.org/manual/reference/program/mongoperf/#bin.mongoperf)、[mongorestore](https://docs.mongodb.org/manual/reference/program/mongorestore/#bin.mongorestore)、[mongostat](https://docs.mongodb.org/manual/reference/program/mongostat/#bin.mongostat) 和 [mongotop](https://docs.mongodb.org/manual/reference/program/mongotop/#bin.mongotop)。\n\n这些包与 Ubuntu 提供的 **mongodb**、**mongodb-server** 和 **mongodb-clients** 包冲突。\n\n安装包提供的默认的 **/etc/mongod.conf** 配置文件默认设置 **bind_ip** 为 127.0.0.1。在初始化一个 [replica set](https://docs.mongodb.org/manual/reference/glossary/#term-replica-set) 之前根据环境的需要修改这个设置。\n\n### 初始化脚本\n\n**mongodb-org** 安装包包含许多[初始化脚本](https://docs.mongodb.org/manual/reference/glossary/#term-init-script)，包含初始化脚本 **/etc/init.d/mongod**。你可以用这些脚本停止、启动和重启守护进程。\n\n安装包使用 **/etc/mongod.conf** 文件结合初始化脚本配置 MongoDB。查看 [配置文件](https://docs.mongodb.org/manual/reference/configuration-options/) 参考获取配置文件中的设置参数。\n\n从 3.2.5 版本开始，不再有 [mongos](https://docs.mongodb.org/manual/reference/program/mongos/#bin.mongos) 初始脚本。[mongos](https://docs.mongodb.org/manual/reference/program/mongos/#bin.mongos) 进程只在 [sharding](https://docs.mongodb.org/manual/core/sharding/) 时用。你可以用 **mongod** 初始脚本生成自己的 [mongos](https://docs.mongodb.org/manual/reference/program/mongos/#bin.mongos) 初始化脚本在这样的环境使用。查看 [mongos](https://docs.mongodb.org/manual/reference/program/mongos/#bin.mongos) 参考获取详细配置。\n\n### 安装 MongoDB 社区版\n\nMongoDB 只为 64 位长期支持的 Ubuntu 发行版提供安装包。当前，这意味着 12.04 LTS (Precise Pangolin) 和 14.04 LTS (Trusty Tahr) 两个版本。同时b安装包在其他 Ubuntu 发行版本也可以运行，这不是一个维持的配置。\n\n#### 1、导入安装包管理系统使用的公钥\n\nUbuntu 包管理工具（例如 **dpkg** 和 **apt**) 通过要求发布者签名的 GPG 密钥确保安装包的一致和真实。输入下面的命令导入 [MongoDB GPG 公钥](https://www.mongodb.org/static/pgp/server-3.2.asc?_ga=1.177583341.957577678.1460369205)：\n\n\tsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv EA312927\n\n#### 2、为 MongoDB 创建一个文件列表\n\n使用适用你的 Ubuntu 版本的命令创建 **/etc/apt/sources.list.d/mongodb-org-3.2.list** 列表文件：\n\n  Ubuntu 12.04\n\n\techo \"deb http://repo.mongodb.org/apt/ubuntu precise/mongodb-org/3.2 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-3.2.list\n\n  Ubuntu 14.04\n\n\techo \"deb http://repo.mongodb.org/apt/ubuntu trusty/mongodb-org/3.2 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-3.2.list\n\n#### 3、重新加载本地安装包数据库\n\n输入以下命令重新加载本地安装包数据库：\n\n\tsudo apt-get update\n\n重新加载过程中可能会报以下错误信息，多试两次即可：\n\n\t错误 http://repo.mongodb.org trusty/mongodb-org/3.2/multiverse Translation-zh_CN                                                              \n\t  不能连接到 repo.mongodb.org：http：\n\t错误 http://repo.mongodb.org trusty/mongodb-org/3.2/multiverse Translation-zh\n\t  不能连接到 repo.mongodb.org：http：\n\t错误 http://repo.mongodb.org trusty/mongodb-org/3.2/multiverse Translation-en_US\n\t  不能连接到 repo.mongodb.org：http：\n\t错误 http://repo.mongodb.org trusty/mongodb-org/3.2/multiverse Translation-en\n\t  不能连接到 repo.mongodb.org：http：\n\t  \n如果报以下错误信息，可以将添加到文件列表中的信息由 HTTP 改为 HTTPS：\n\n  错误信息：\n\n\tW: 无法下载 http://repo.mongodb.org/apt/ubuntu/dists/trusty/mongodb-org/3.2/multiverse/binary-amd64/Packages  Hash 校验和不符\n\n  第 2 步创建文件列表命令改为：\n\n\techo \"deb http://repo.mongodb.org/apt/ubuntu trusty/mongodb-org/3.2 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-3.2.list\n\n再次执行更新命令成功。\n\n#### 4、安装 MongoDB\n\n可以安装 MongoDB 最新稳定版本或者指定的版本。\n\n#### 5、安装 MongoDB 最新稳定版本\n\n输入下面的命令：\n\n\tsudo apt-get install -y mongodb-org\n\t\n#### 6、安装特定版本的 MongoDB\n\n安装特定发行版，你必须分别单独用版本号指定每个组件，像下面示例：\n\n\tsudo apt-get install -y mongodb-org=3.2.5 mongodb-org-server=3.2.5 mongodb-org-shell=3.2.5 mongodb-org-mongos=3.2.5 mongodb-org-tools=3.2.5\n\n如果你只安装 **mongodb-org=3.2.5**，并且没有包含组件安装包，MongoDB 每个最新的版本将被安装，不管你指明了哪个版本。\n\n#### 7、固定一个特定版本的 MongoDB\n\n虽然你可以指定任何可以获取的 MongoDB 版本，当新版本可以获取时 **apt-get** 将更新安装包。为了防止无意的更新，固定安装包。为了固定当前安装的 MongoDB 版本，输入以下命令序列：\n\n\techo \"mongodb-org hold\" | sudo dpkg --set-selections\n\techo \"mongodb-org-server hold\" | sudo dpkg --set-selections\n\techo \"mongodb-org-shell hold\" | sudo dpkg --set-selections\n\techo \"mongodb-org-mongos hold\" | sudo dpkg --set-selections\n\techo \"mongodb-org-tools hold\" | sudo dpkg --set-selections\n\n### 运行 MongoDB 社区版\n\n默认的 MongoDB 实例在 **/var/lib/mongodb** 存储数据文件，并且在 **/var/log/mongodb** 存储日志文件，并且用 **mongodb** 用户用户帐号运行。你可以在 **/etc/mongod.conf** 中指定替换的日志和数据文件目录。查看 [systemLog.path](https://docs.mongodb.org/manual/reference/configuration-options/#systemLog.path) 和 [storage.dbPath](https://docs.mongodb.org/manual/reference/configuration-options/#storage.dbPath) 获取额外的信息。\n\n如果改变运行 MongoDB 进程的用户，你**必须**修改 **/var/lib/mongodb** 和 **/var/log/mongodb** 目录的访问控制权限给这个用户对这些目录的访问权限。\n\n#### 1、启动 MongoDB\n\n输入下面的命令启动 [mongod](https://docs.mongodb.org/manual/reference/program/mongod/#bin.mongod)：\n\n\tsudo service mongod start\n\n执行上面的命令会报错，错误信息如下：\n\n\tFailed to start mongod.service: Unit mongod.service failed to load: No such file or directory.\n\n原因是缺少 systemd 的 service 文件。不需要重新开始安装或者换用其他仓库。创建文件 /lib/systemd/system/mongod.service，并输入以下内容：\n\n\t[Unit]\n\tDescription=High-performance, schema-free document-oriented database\n\tAfter=network.target\n\t\n\t[Service]\n\tUser=mongodb\n\tExecStart=/usr/bin/mongod --quiet --config /etc/mongod.conf\n\t\n\t[Install]\n\tWantedBy=multi-user.target\n\n> 注意：/lib/systemd/system/mongod.service 和配置内容中的文件名可能需要对应的做修改，可能是 mongodb.service 和 mongodb.conf。参考网上的资料是 mongodb，而我的系统安装后都是 mongod。\n\n重新执行启动命令，提示以下信息，检查 mongo 进程不存在，说明未启动成功：\n\n\tWarning: mongod.service changed on disk. Run 'systemctl daemon-reload' to reload units.\n\n根据提示执行以下命令，并输入用户密码，再次执行启动命令，这次启动成功 :)\n\n\tsystemctl daemon-reload\n\n#### 2、确认 MongoDB 已经成功启动\n\n通过读取检查 **/var/log/mongodb/mongod.log** 日志文件中的下面一行记录确认 [mongod](https://docs.mongodb.org/manual/reference/program/mongod/#bin.mongod) 进程已经成功启动。\n\n\t[initandlisten] waiting for connections on port <port>\n\n**<port>** 是在 **/etc/mongod.conf** 文件中配置的端口号，默认是 27017。\n\n以下是我的安装过程的启动日志信息：\n\n![mongodb-start-log](/uploads/20160416/mongodb_start_log.png)\n\n#### 3、停止 MongoDB\n\n如果需要，你可以输入以下命令停止 [mongod](https://docs.mongodb.org/manual/reference/program/mongod/#bin.mongod) 进程：\n\n\tsudo service mongod stop\n\n#### 4、重启 MongoDB\n\n输入以下命令重启 [mongod](https://docs.mongodb.org/manual/reference/program/mongod/#bin.mongod) 进程：\n\n\tsudo service mongod restart\n\n### 卸载 MongoDB 社区版\n\n要从系统中完全移除 MongoDB，你必须移除 MongoDB 应用程序、配置文件和所有包含数据和日志的目录。下面的章节指导你通过必要的步骤完成卸载。\n\n> 警告：\n> 这个过程将完全*移除* MongoDB、配置和*所有*数据库。这个过程是不能恢复的，因此确认执行该过程前确保你的配置和数据应经备份。\n\n#### 1、停止 MongoDB\n\n输入以下命令停止 [mongod](https://docs.mongodb.org/manual/reference/program/mongod/#bin.mongod) 进程：\n\n\tsudo service mongod stop\n\n#### 2、移除安装包\n\n移除所有以前安装的 MongoDB 安装包。\n\n\tsudo apt-get purge mongodb-org*\n\n#### 3、移除数据目录\n\n移除 MongoDB 数据库和日志文件。\n\n\tsudo rm -r /var/log/mongodb\n\tsudo rm -r /var/lib/mongodb\n","source":"_posts/Ubuntu-安装-MongoDB.md","raw":"title: Ubuntu 15.10 安装 MongoDB\ntags:\n  - MongoDB\n  - Ubuntu\ncategories:\n  - 数据库\n  - MongoDB\ndate: 2016-04-16 11:47:22\n---\n\n本篇主要参考官方安装文档，在 Ubuntu 15.10 上安装并启动。中间遇到一些问题，已经找到解决方案。\n\n<!-- more -->\n\n### 概述\n\n从 3.2 版本开始，MongoDB 不再支持 32 位的平台。\n\n使用本教程从 **.deb** 包在 LTS Ubuntu Linux 系统上安装 MongoDB 社区版。虽然 Ubuntu 包含自己的 MongoDB 包，官方 MongoDB 社区版包通常是更新的。\n\n### 安装包\n\nMongoDB 在自己的仓库中提供了官方支持的安装包。这个仓库包含一下包：\n\n- mongodb-org：一个**元安装包**，可以自动安装下面的四个组件安装包。\n- mongodb-org-server：包含 [mongod](https://docs.mongodb.org/manual/reference/program/mongod/#bin.mongod) 守护进程和相关的配置及初始脚本。\n- mongodb-org-mongos：包含 [mongos](https://docs.mongodb.org/manual/reference/program/mongos/#bin.mongos) 守护进程。\n- mongodb-org-shell：包含 [mongo](https://docs.mongodb.org/manual/reference/program/mongo/#bin.mongo) shell。\n- mongodb-org-tools：包含以下 MongoDB 工具：[mongoimport](https://docs.mongodb.org/manual/reference/program/mongoimport/#bin.mongoimport)、[bsondump](https://docs.mongodb.org/manual/reference/program/bsondump/#bin.bsondump)、[mongodump](https://docs.mongodb.org/manual/reference/program/mongodump/#bin.mongodump)、[mongoexport](https://docs.mongodb.org/manual/reference/program/mongoexport/#bin.mongoexport)、[mongofiles](https://docs.mongodb.org/manual/reference/program/mongofiles/#bin.mongofiles)、[mongooplog](https://docs.mongodb.org/manual/reference/program/mongooplog/#bin.mongooplog)、[mongoperf](https://docs.mongodb.org/manual/reference/program/mongoperf/#bin.mongoperf)、[mongorestore](https://docs.mongodb.org/manual/reference/program/mongorestore/#bin.mongorestore)、[mongostat](https://docs.mongodb.org/manual/reference/program/mongostat/#bin.mongostat) 和 [mongotop](https://docs.mongodb.org/manual/reference/program/mongotop/#bin.mongotop)。\n\n这些包与 Ubuntu 提供的 **mongodb**、**mongodb-server** 和 **mongodb-clients** 包冲突。\n\n安装包提供的默认的 **/etc/mongod.conf** 配置文件默认设置 **bind_ip** 为 127.0.0.1。在初始化一个 [replica set](https://docs.mongodb.org/manual/reference/glossary/#term-replica-set) 之前根据环境的需要修改这个设置。\n\n### 初始化脚本\n\n**mongodb-org** 安装包包含许多[初始化脚本](https://docs.mongodb.org/manual/reference/glossary/#term-init-script)，包含初始化脚本 **/etc/init.d/mongod**。你可以用这些脚本停止、启动和重启守护进程。\n\n安装包使用 **/etc/mongod.conf** 文件结合初始化脚本配置 MongoDB。查看 [配置文件](https://docs.mongodb.org/manual/reference/configuration-options/) 参考获取配置文件中的设置参数。\n\n从 3.2.5 版本开始，不再有 [mongos](https://docs.mongodb.org/manual/reference/program/mongos/#bin.mongos) 初始脚本。[mongos](https://docs.mongodb.org/manual/reference/program/mongos/#bin.mongos) 进程只在 [sharding](https://docs.mongodb.org/manual/core/sharding/) 时用。你可以用 **mongod** 初始脚本生成自己的 [mongos](https://docs.mongodb.org/manual/reference/program/mongos/#bin.mongos) 初始化脚本在这样的环境使用。查看 [mongos](https://docs.mongodb.org/manual/reference/program/mongos/#bin.mongos) 参考获取详细配置。\n\n### 安装 MongoDB 社区版\n\nMongoDB 只为 64 位长期支持的 Ubuntu 发行版提供安装包。当前，这意味着 12.04 LTS (Precise Pangolin) 和 14.04 LTS (Trusty Tahr) 两个版本。同时b安装包在其他 Ubuntu 发行版本也可以运行，这不是一个维持的配置。\n\n#### 1、导入安装包管理系统使用的公钥\n\nUbuntu 包管理工具（例如 **dpkg** 和 **apt**) 通过要求发布者签名的 GPG 密钥确保安装包的一致和真实。输入下面的命令导入 [MongoDB GPG 公钥](https://www.mongodb.org/static/pgp/server-3.2.asc?_ga=1.177583341.957577678.1460369205)：\n\n\tsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv EA312927\n\n#### 2、为 MongoDB 创建一个文件列表\n\n使用适用你的 Ubuntu 版本的命令创建 **/etc/apt/sources.list.d/mongodb-org-3.2.list** 列表文件：\n\n  Ubuntu 12.04\n\n\techo \"deb http://repo.mongodb.org/apt/ubuntu precise/mongodb-org/3.2 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-3.2.list\n\n  Ubuntu 14.04\n\n\techo \"deb http://repo.mongodb.org/apt/ubuntu trusty/mongodb-org/3.2 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-3.2.list\n\n#### 3、重新加载本地安装包数据库\n\n输入以下命令重新加载本地安装包数据库：\n\n\tsudo apt-get update\n\n重新加载过程中可能会报以下错误信息，多试两次即可：\n\n\t错误 http://repo.mongodb.org trusty/mongodb-org/3.2/multiverse Translation-zh_CN                                                              \n\t  不能连接到 repo.mongodb.org：http：\n\t错误 http://repo.mongodb.org trusty/mongodb-org/3.2/multiverse Translation-zh\n\t  不能连接到 repo.mongodb.org：http：\n\t错误 http://repo.mongodb.org trusty/mongodb-org/3.2/multiverse Translation-en_US\n\t  不能连接到 repo.mongodb.org：http：\n\t错误 http://repo.mongodb.org trusty/mongodb-org/3.2/multiverse Translation-en\n\t  不能连接到 repo.mongodb.org：http：\n\t  \n如果报以下错误信息，可以将添加到文件列表中的信息由 HTTP 改为 HTTPS：\n\n  错误信息：\n\n\tW: 无法下载 http://repo.mongodb.org/apt/ubuntu/dists/trusty/mongodb-org/3.2/multiverse/binary-amd64/Packages  Hash 校验和不符\n\n  第 2 步创建文件列表命令改为：\n\n\techo \"deb http://repo.mongodb.org/apt/ubuntu trusty/mongodb-org/3.2 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-3.2.list\n\n再次执行更新命令成功。\n\n#### 4、安装 MongoDB\n\n可以安装 MongoDB 最新稳定版本或者指定的版本。\n\n#### 5、安装 MongoDB 最新稳定版本\n\n输入下面的命令：\n\n\tsudo apt-get install -y mongodb-org\n\t\n#### 6、安装特定版本的 MongoDB\n\n安装特定发行版，你必须分别单独用版本号指定每个组件，像下面示例：\n\n\tsudo apt-get install -y mongodb-org=3.2.5 mongodb-org-server=3.2.5 mongodb-org-shell=3.2.5 mongodb-org-mongos=3.2.5 mongodb-org-tools=3.2.5\n\n如果你只安装 **mongodb-org=3.2.5**，并且没有包含组件安装包，MongoDB 每个最新的版本将被安装，不管你指明了哪个版本。\n\n#### 7、固定一个特定版本的 MongoDB\n\n虽然你可以指定任何可以获取的 MongoDB 版本，当新版本可以获取时 **apt-get** 将更新安装包。为了防止无意的更新，固定安装包。为了固定当前安装的 MongoDB 版本，输入以下命令序列：\n\n\techo \"mongodb-org hold\" | sudo dpkg --set-selections\n\techo \"mongodb-org-server hold\" | sudo dpkg --set-selections\n\techo \"mongodb-org-shell hold\" | sudo dpkg --set-selections\n\techo \"mongodb-org-mongos hold\" | sudo dpkg --set-selections\n\techo \"mongodb-org-tools hold\" | sudo dpkg --set-selections\n\n### 运行 MongoDB 社区版\n\n默认的 MongoDB 实例在 **/var/lib/mongodb** 存储数据文件，并且在 **/var/log/mongodb** 存储日志文件，并且用 **mongodb** 用户用户帐号运行。你可以在 **/etc/mongod.conf** 中指定替换的日志和数据文件目录。查看 [systemLog.path](https://docs.mongodb.org/manual/reference/configuration-options/#systemLog.path) 和 [storage.dbPath](https://docs.mongodb.org/manual/reference/configuration-options/#storage.dbPath) 获取额外的信息。\n\n如果改变运行 MongoDB 进程的用户，你**必须**修改 **/var/lib/mongodb** 和 **/var/log/mongodb** 目录的访问控制权限给这个用户对这些目录的访问权限。\n\n#### 1、启动 MongoDB\n\n输入下面的命令启动 [mongod](https://docs.mongodb.org/manual/reference/program/mongod/#bin.mongod)：\n\n\tsudo service mongod start\n\n执行上面的命令会报错，错误信息如下：\n\n\tFailed to start mongod.service: Unit mongod.service failed to load: No such file or directory.\n\n原因是缺少 systemd 的 service 文件。不需要重新开始安装或者换用其他仓库。创建文件 /lib/systemd/system/mongod.service，并输入以下内容：\n\n\t[Unit]\n\tDescription=High-performance, schema-free document-oriented database\n\tAfter=network.target\n\t\n\t[Service]\n\tUser=mongodb\n\tExecStart=/usr/bin/mongod --quiet --config /etc/mongod.conf\n\t\n\t[Install]\n\tWantedBy=multi-user.target\n\n> 注意：/lib/systemd/system/mongod.service 和配置内容中的文件名可能需要对应的做修改，可能是 mongodb.service 和 mongodb.conf。参考网上的资料是 mongodb，而我的系统安装后都是 mongod。\n\n重新执行启动命令，提示以下信息，检查 mongo 进程不存在，说明未启动成功：\n\n\tWarning: mongod.service changed on disk. Run 'systemctl daemon-reload' to reload units.\n\n根据提示执行以下命令，并输入用户密码，再次执行启动命令，这次启动成功 :)\n\n\tsystemctl daemon-reload\n\n#### 2、确认 MongoDB 已经成功启动\n\n通过读取检查 **/var/log/mongodb/mongod.log** 日志文件中的下面一行记录确认 [mongod](https://docs.mongodb.org/manual/reference/program/mongod/#bin.mongod) 进程已经成功启动。\n\n\t[initandlisten] waiting for connections on port <port>\n\n**<port>** 是在 **/etc/mongod.conf** 文件中配置的端口号，默认是 27017。\n\n以下是我的安装过程的启动日志信息：\n\n![mongodb-start-log](/uploads/20160416/mongodb_start_log.png)\n\n#### 3、停止 MongoDB\n\n如果需要，你可以输入以下命令停止 [mongod](https://docs.mongodb.org/manual/reference/program/mongod/#bin.mongod) 进程：\n\n\tsudo service mongod stop\n\n#### 4、重启 MongoDB\n\n输入以下命令重启 [mongod](https://docs.mongodb.org/manual/reference/program/mongod/#bin.mongod) 进程：\n\n\tsudo service mongod restart\n\n### 卸载 MongoDB 社区版\n\n要从系统中完全移除 MongoDB，你必须移除 MongoDB 应用程序、配置文件和所有包含数据和日志的目录。下面的章节指导你通过必要的步骤完成卸载。\n\n> 警告：\n> 这个过程将完全*移除* MongoDB、配置和*所有*数据库。这个过程是不能恢复的，因此确认执行该过程前确保你的配置和数据应经备份。\n\n#### 1、停止 MongoDB\n\n输入以下命令停止 [mongod](https://docs.mongodb.org/manual/reference/program/mongod/#bin.mongod) 进程：\n\n\tsudo service mongod stop\n\n#### 2、移除安装包\n\n移除所有以前安装的 MongoDB 安装包。\n\n\tsudo apt-get purge mongodb-org*\n\n#### 3、移除数据目录\n\n移除 MongoDB 数据库和日志文件。\n\n\tsudo rm -r /var/log/mongodb\n\tsudo rm -r /var/lib/mongodb\n","slug":"Ubuntu-安装-MongoDB","published":1,"updated":"2021-07-19T16:28:00.292Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphqd00btitd3fyb804n6","content":"<p>本篇主要参考官方安装文档，在 Ubuntu 15.10 上安装并启动。中间遇到一些问题，已经找到解决方案。</p>\n<span id=\"more\"></span>\n\n<h3 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h3><p>从 3.2 版本开始，MongoDB 不再支持 32 位的平台。</p>\n<p>使用本教程从 <strong>.deb</strong> 包在 LTS Ubuntu Linux 系统上安装 MongoDB 社区版。虽然 Ubuntu 包含自己的 MongoDB 包，官方 MongoDB 社区版包通常是更新的。</p>\n<h3 id=\"安装包\"><a href=\"#安装包\" class=\"headerlink\" title=\"安装包\"></a>安装包</h3><p>MongoDB 在自己的仓库中提供了官方支持的安装包。这个仓库包含一下包：</p>\n<ul>\n<li>mongodb-org：一个<strong>元安装包</strong>，可以自动安装下面的四个组件安装包。</li>\n<li>mongodb-org-server：包含 <a href=\"https://docs.mongodb.org/manual/reference/program/mongod/#bin.mongod\">mongod</a> 守护进程和相关的配置及初始脚本。</li>\n<li>mongodb-org-mongos：包含 <a href=\"https://docs.mongodb.org/manual/reference/program/mongos/#bin.mongos\">mongos</a> 守护进程。</li>\n<li>mongodb-org-shell：包含 <a href=\"https://docs.mongodb.org/manual/reference/program/mongo/#bin.mongo\">mongo</a> shell。</li>\n<li>mongodb-org-tools：包含以下 MongoDB 工具：<a href=\"https://docs.mongodb.org/manual/reference/program/mongoimport/#bin.mongoimport\">mongoimport</a>、<a href=\"https://docs.mongodb.org/manual/reference/program/bsondump/#bin.bsondump\">bsondump</a>、<a href=\"https://docs.mongodb.org/manual/reference/program/mongodump/#bin.mongodump\">mongodump</a>、<a href=\"https://docs.mongodb.org/manual/reference/program/mongoexport/#bin.mongoexport\">mongoexport</a>、<a href=\"https://docs.mongodb.org/manual/reference/program/mongofiles/#bin.mongofiles\">mongofiles</a>、<a href=\"https://docs.mongodb.org/manual/reference/program/mongooplog/#bin.mongooplog\">mongooplog</a>、<a href=\"https://docs.mongodb.org/manual/reference/program/mongoperf/#bin.mongoperf\">mongoperf</a>、<a href=\"https://docs.mongodb.org/manual/reference/program/mongorestore/#bin.mongorestore\">mongorestore</a>、<a href=\"https://docs.mongodb.org/manual/reference/program/mongostat/#bin.mongostat\">mongostat</a> 和 <a href=\"https://docs.mongodb.org/manual/reference/program/mongotop/#bin.mongotop\">mongotop</a>。</li>\n</ul>\n<p>这些包与 Ubuntu 提供的 <strong>mongodb</strong>、<strong>mongodb-server</strong> 和 <strong>mongodb-clients</strong> 包冲突。</p>\n<p>安装包提供的默认的 <strong>/etc/mongod.conf</strong> 配置文件默认设置 <strong>bind_ip</strong> 为 127.0.0.1。在初始化一个 <a href=\"https://docs.mongodb.org/manual/reference/glossary/#term-replica-set\">replica set</a> 之前根据环境的需要修改这个设置。</p>\n<h3 id=\"初始化脚本\"><a href=\"#初始化脚本\" class=\"headerlink\" title=\"初始化脚本\"></a>初始化脚本</h3><p><strong>mongodb-org</strong> 安装包包含许多<a href=\"https://docs.mongodb.org/manual/reference/glossary/#term-init-script\">初始化脚本</a>，包含初始化脚本 <strong>/etc/init.d/mongod</strong>。你可以用这些脚本停止、启动和重启守护进程。</p>\n<p>安装包使用 <strong>/etc/mongod.conf</strong> 文件结合初始化脚本配置 MongoDB。查看 <a href=\"https://docs.mongodb.org/manual/reference/configuration-options/\">配置文件</a> 参考获取配置文件中的设置参数。</p>\n<p>从 3.2.5 版本开始，不再有 <a href=\"https://docs.mongodb.org/manual/reference/program/mongos/#bin.mongos\">mongos</a> 初始脚本。<a href=\"https://docs.mongodb.org/manual/reference/program/mongos/#bin.mongos\">mongos</a> 进程只在 <a href=\"https://docs.mongodb.org/manual/core/sharding/\">sharding</a> 时用。你可以用 <strong>mongod</strong> 初始脚本生成自己的 <a href=\"https://docs.mongodb.org/manual/reference/program/mongos/#bin.mongos\">mongos</a> 初始化脚本在这样的环境使用。查看 <a href=\"https://docs.mongodb.org/manual/reference/program/mongos/#bin.mongos\">mongos</a> 参考获取详细配置。</p>\n<h3 id=\"安装-MongoDB-社区版\"><a href=\"#安装-MongoDB-社区版\" class=\"headerlink\" title=\"安装 MongoDB 社区版\"></a>安装 MongoDB 社区版</h3><p>MongoDB 只为 64 位长期支持的 Ubuntu 发行版提供安装包。当前，这意味着 12.04 LTS (Precise Pangolin) 和 14.04 LTS (Trusty Tahr) 两个版本。同时b安装包在其他 Ubuntu 发行版本也可以运行，这不是一个维持的配置。</p>\n<h4 id=\"1、导入安装包管理系统使用的公钥\"><a href=\"#1、导入安装包管理系统使用的公钥\" class=\"headerlink\" title=\"1、导入安装包管理系统使用的公钥\"></a>1、导入安装包管理系统使用的公钥</h4><p>Ubuntu 包管理工具（例如 <strong>dpkg</strong> 和 <strong>apt</strong>) 通过要求发布者签名的 GPG 密钥确保安装包的一致和真实。输入下面的命令导入 <a href=\"https://www.mongodb.org/static/pgp/server-3.2.asc?_ga=1.177583341.957577678.1460369205\">MongoDB GPG 公钥</a>：</p>\n<pre><code>sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv EA312927\n</code></pre>\n<h4 id=\"2、为-MongoDB-创建一个文件列表\"><a href=\"#2、为-MongoDB-创建一个文件列表\" class=\"headerlink\" title=\"2、为 MongoDB 创建一个文件列表\"></a>2、为 MongoDB 创建一个文件列表</h4><p>使用适用你的 Ubuntu 版本的命令创建 <strong>/etc/apt/sources.list.d/mongodb-org-3.2.list</strong> 列表文件：</p>\n<p>  Ubuntu 12.04</p>\n<pre><code>echo &quot;deb http://repo.mongodb.org/apt/ubuntu precise/mongodb-org/3.2 multiverse&quot; | sudo tee /etc/apt/sources.list.d/mongodb-org-3.2.list\n</code></pre>\n<p>  Ubuntu 14.04</p>\n<pre><code>echo &quot;deb http://repo.mongodb.org/apt/ubuntu trusty/mongodb-org/3.2 multiverse&quot; | sudo tee /etc/apt/sources.list.d/mongodb-org-3.2.list\n</code></pre>\n<h4 id=\"3、重新加载本地安装包数据库\"><a href=\"#3、重新加载本地安装包数据库\" class=\"headerlink\" title=\"3、重新加载本地安装包数据库\"></a>3、重新加载本地安装包数据库</h4><p>输入以下命令重新加载本地安装包数据库：</p>\n<pre><code>sudo apt-get update\n</code></pre>\n<p>重新加载过程中可能会报以下错误信息，多试两次即可：</p>\n<pre><code>错误 http://repo.mongodb.org trusty/mongodb-org/3.2/multiverse Translation-zh_CN                                                              \n  不能连接到 repo.mongodb.org：http：\n错误 http://repo.mongodb.org trusty/mongodb-org/3.2/multiverse Translation-zh\n  不能连接到 repo.mongodb.org：http：\n错误 http://repo.mongodb.org trusty/mongodb-org/3.2/multiverse Translation-en_US\n  不能连接到 repo.mongodb.org：http：\n错误 http://repo.mongodb.org trusty/mongodb-org/3.2/multiverse Translation-en\n  不能连接到 repo.mongodb.org：http：\n  \n</code></pre>\n<p>如果报以下错误信息，可以将添加到文件列表中的信息由 HTTP 改为 HTTPS：</p>\n<p>  错误信息：</p>\n<pre><code>W: 无法下载 http://repo.mongodb.org/apt/ubuntu/dists/trusty/mongodb-org/3.2/multiverse/binary-amd64/Packages  Hash 校验和不符\n</code></pre>\n<p>  第 2 步创建文件列表命令改为：</p>\n<pre><code>echo &quot;deb http://repo.mongodb.org/apt/ubuntu trusty/mongodb-org/3.2 multiverse&quot; | sudo tee /etc/apt/sources.list.d/mongodb-org-3.2.list\n</code></pre>\n<p>再次执行更新命令成功。</p>\n<h4 id=\"4、安装-MongoDB\"><a href=\"#4、安装-MongoDB\" class=\"headerlink\" title=\"4、安装 MongoDB\"></a>4、安装 MongoDB</h4><p>可以安装 MongoDB 最新稳定版本或者指定的版本。</p>\n<h4 id=\"5、安装-MongoDB-最新稳定版本\"><a href=\"#5、安装-MongoDB-最新稳定版本\" class=\"headerlink\" title=\"5、安装 MongoDB 最新稳定版本\"></a>5、安装 MongoDB 最新稳定版本</h4><p>输入下面的命令：</p>\n<pre><code>sudo apt-get install -y mongodb-org\n</code></pre>\n<h4 id=\"6、安装特定版本的-MongoDB\"><a href=\"#6、安装特定版本的-MongoDB\" class=\"headerlink\" title=\"6、安装特定版本的 MongoDB\"></a>6、安装特定版本的 MongoDB</h4><p>安装特定发行版，你必须分别单独用版本号指定每个组件，像下面示例：</p>\n<pre><code>sudo apt-get install -y mongodb-org=3.2.5 mongodb-org-server=3.2.5 mongodb-org-shell=3.2.5 mongodb-org-mongos=3.2.5 mongodb-org-tools=3.2.5\n</code></pre>\n<p>如果你只安装 <strong>mongodb-org=3.2.5</strong>，并且没有包含组件安装包，MongoDB 每个最新的版本将被安装，不管你指明了哪个版本。</p>\n<h4 id=\"7、固定一个特定版本的-MongoDB\"><a href=\"#7、固定一个特定版本的-MongoDB\" class=\"headerlink\" title=\"7、固定一个特定版本的 MongoDB\"></a>7、固定一个特定版本的 MongoDB</h4><p>虽然你可以指定任何可以获取的 MongoDB 版本，当新版本可以获取时 <strong>apt-get</strong> 将更新安装包。为了防止无意的更新，固定安装包。为了固定当前安装的 MongoDB 版本，输入以下命令序列：</p>\n<pre><code>echo &quot;mongodb-org hold&quot; | sudo dpkg --set-selections\necho &quot;mongodb-org-server hold&quot; | sudo dpkg --set-selections\necho &quot;mongodb-org-shell hold&quot; | sudo dpkg --set-selections\necho &quot;mongodb-org-mongos hold&quot; | sudo dpkg --set-selections\necho &quot;mongodb-org-tools hold&quot; | sudo dpkg --set-selections\n</code></pre>\n<h3 id=\"运行-MongoDB-社区版\"><a href=\"#运行-MongoDB-社区版\" class=\"headerlink\" title=\"运行 MongoDB 社区版\"></a>运行 MongoDB 社区版</h3><p>默认的 MongoDB 实例在 <strong>/var/lib/mongodb</strong> 存储数据文件，并且在 <strong>/var/log/mongodb</strong> 存储日志文件，并且用 <strong>mongodb</strong> 用户用户帐号运行。你可以在 <strong>/etc/mongod.conf</strong> 中指定替换的日志和数据文件目录。查看 <a href=\"https://docs.mongodb.org/manual/reference/configuration-options/#systemLog.path\">systemLog.path</a> 和 <a href=\"https://docs.mongodb.org/manual/reference/configuration-options/#storage.dbPath\">storage.dbPath</a> 获取额外的信息。</p>\n<p>如果改变运行 MongoDB 进程的用户，你<strong>必须</strong>修改 <strong>/var/lib/mongodb</strong> 和 <strong>/var/log/mongodb</strong> 目录的访问控制权限给这个用户对这些目录的访问权限。</p>\n<h4 id=\"1、启动-MongoDB\"><a href=\"#1、启动-MongoDB\" class=\"headerlink\" title=\"1、启动 MongoDB\"></a>1、启动 MongoDB</h4><p>输入下面的命令启动 <a href=\"https://docs.mongodb.org/manual/reference/program/mongod/#bin.mongod\">mongod</a>：</p>\n<pre><code>sudo service mongod start\n</code></pre>\n<p>执行上面的命令会报错，错误信息如下：</p>\n<pre><code>Failed to start mongod.service: Unit mongod.service failed to load: No such file or directory.\n</code></pre>\n<p>原因是缺少 systemd 的 service 文件。不需要重新开始安装或者换用其他仓库。创建文件 /lib/systemd/system/mongod.service，并输入以下内容：</p>\n<pre><code>[Unit]\nDescription=High-performance, schema-free document-oriented database\nAfter=network.target\n\n[Service]\nUser=mongodb\nExecStart=/usr/bin/mongod --quiet --config /etc/mongod.conf\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>\n<blockquote>\n<p>注意：/lib/systemd/system/mongod.service 和配置内容中的文件名可能需要对应的做修改，可能是 mongodb.service 和 mongodb.conf。参考网上的资料是 mongodb，而我的系统安装后都是 mongod。</p>\n</blockquote>\n<p>重新执行启动命令，提示以下信息，检查 mongo 进程不存在，说明未启动成功：</p>\n<pre><code>Warning: mongod.service changed on disk. Run &#39;systemctl daemon-reload&#39; to reload units.\n</code></pre>\n<p>根据提示执行以下命令，并输入用户密码，再次执行启动命令，这次启动成功 :)</p>\n<pre><code>systemctl daemon-reload\n</code></pre>\n<h4 id=\"2、确认-MongoDB-已经成功启动\"><a href=\"#2、确认-MongoDB-已经成功启动\" class=\"headerlink\" title=\"2、确认 MongoDB 已经成功启动\"></a>2、确认 MongoDB 已经成功启动</h4><p>通过读取检查 <strong>/var/log/mongodb/mongod.log</strong> 日志文件中的下面一行记录确认 <a href=\"https://docs.mongodb.org/manual/reference/program/mongod/#bin.mongod\">mongod</a> 进程已经成功启动。</p>\n<pre><code>[initandlisten] waiting for connections on port &lt;port&gt;\n</code></pre>\n<p><strong><port></strong> 是在 <strong>/etc/mongod.conf</strong> 文件中配置的端口号，默认是 27017。</p>\n<p>以下是我的安装过程的启动日志信息：</p>\n<p><img src=\"/uploads/20160416/mongodb_start_log.png\" alt=\"mongodb-start-log\"></p>\n<h4 id=\"3、停止-MongoDB\"><a href=\"#3、停止-MongoDB\" class=\"headerlink\" title=\"3、停止 MongoDB\"></a>3、停止 MongoDB</h4><p>如果需要，你可以输入以下命令停止 <a href=\"https://docs.mongodb.org/manual/reference/program/mongod/#bin.mongod\">mongod</a> 进程：</p>\n<pre><code>sudo service mongod stop\n</code></pre>\n<h4 id=\"4、重启-MongoDB\"><a href=\"#4、重启-MongoDB\" class=\"headerlink\" title=\"4、重启 MongoDB\"></a>4、重启 MongoDB</h4><p>输入以下命令重启 <a href=\"https://docs.mongodb.org/manual/reference/program/mongod/#bin.mongod\">mongod</a> 进程：</p>\n<pre><code>sudo service mongod restart\n</code></pre>\n<h3 id=\"卸载-MongoDB-社区版\"><a href=\"#卸载-MongoDB-社区版\" class=\"headerlink\" title=\"卸载 MongoDB 社区版\"></a>卸载 MongoDB 社区版</h3><p>要从系统中完全移除 MongoDB，你必须移除 MongoDB 应用程序、配置文件和所有包含数据和日志的目录。下面的章节指导你通过必要的步骤完成卸载。</p>\n<blockquote>\n<p>警告：<br>这个过程将完全<em>移除</em> MongoDB、配置和<em>所有</em>数据库。这个过程是不能恢复的，因此确认执行该过程前确保你的配置和数据应经备份。</p>\n</blockquote>\n<h4 id=\"1、停止-MongoDB\"><a href=\"#1、停止-MongoDB\" class=\"headerlink\" title=\"1、停止 MongoDB\"></a>1、停止 MongoDB</h4><p>输入以下命令停止 <a href=\"https://docs.mongodb.org/manual/reference/program/mongod/#bin.mongod\">mongod</a> 进程：</p>\n<pre><code>sudo service mongod stop\n</code></pre>\n<h4 id=\"2、移除安装包\"><a href=\"#2、移除安装包\" class=\"headerlink\" title=\"2、移除安装包\"></a>2、移除安装包</h4><p>移除所有以前安装的 MongoDB 安装包。</p>\n<pre><code>sudo apt-get purge mongodb-org*\n</code></pre>\n<h4 id=\"3、移除数据目录\"><a href=\"#3、移除数据目录\" class=\"headerlink\" title=\"3、移除数据目录\"></a>3、移除数据目录</h4><p>移除 MongoDB 数据库和日志文件。</p>\n<pre><code>sudo rm -r /var/log/mongodb\nsudo rm -r /var/lib/mongodb\n</code></pre>\n","site":{"data":{}},"excerpt":"<p>本篇主要参考官方安装文档，在 Ubuntu 15.10 上安装并启动。中间遇到一些问题，已经找到解决方案。</p>","more":"<h3 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h3><p>从 3.2 版本开始，MongoDB 不再支持 32 位的平台。</p>\n<p>使用本教程从 <strong>.deb</strong> 包在 LTS Ubuntu Linux 系统上安装 MongoDB 社区版。虽然 Ubuntu 包含自己的 MongoDB 包，官方 MongoDB 社区版包通常是更新的。</p>\n<h3 id=\"安装包\"><a href=\"#安装包\" class=\"headerlink\" title=\"安装包\"></a>安装包</h3><p>MongoDB 在自己的仓库中提供了官方支持的安装包。这个仓库包含一下包：</p>\n<ul>\n<li>mongodb-org：一个<strong>元安装包</strong>，可以自动安装下面的四个组件安装包。</li>\n<li>mongodb-org-server：包含 <a href=\"https://docs.mongodb.org/manual/reference/program/mongod/#bin.mongod\">mongod</a> 守护进程和相关的配置及初始脚本。</li>\n<li>mongodb-org-mongos：包含 <a href=\"https://docs.mongodb.org/manual/reference/program/mongos/#bin.mongos\">mongos</a> 守护进程。</li>\n<li>mongodb-org-shell：包含 <a href=\"https://docs.mongodb.org/manual/reference/program/mongo/#bin.mongo\">mongo</a> shell。</li>\n<li>mongodb-org-tools：包含以下 MongoDB 工具：<a href=\"https://docs.mongodb.org/manual/reference/program/mongoimport/#bin.mongoimport\">mongoimport</a>、<a href=\"https://docs.mongodb.org/manual/reference/program/bsondump/#bin.bsondump\">bsondump</a>、<a href=\"https://docs.mongodb.org/manual/reference/program/mongodump/#bin.mongodump\">mongodump</a>、<a href=\"https://docs.mongodb.org/manual/reference/program/mongoexport/#bin.mongoexport\">mongoexport</a>、<a href=\"https://docs.mongodb.org/manual/reference/program/mongofiles/#bin.mongofiles\">mongofiles</a>、<a href=\"https://docs.mongodb.org/manual/reference/program/mongooplog/#bin.mongooplog\">mongooplog</a>、<a href=\"https://docs.mongodb.org/manual/reference/program/mongoperf/#bin.mongoperf\">mongoperf</a>、<a href=\"https://docs.mongodb.org/manual/reference/program/mongorestore/#bin.mongorestore\">mongorestore</a>、<a href=\"https://docs.mongodb.org/manual/reference/program/mongostat/#bin.mongostat\">mongostat</a> 和 <a href=\"https://docs.mongodb.org/manual/reference/program/mongotop/#bin.mongotop\">mongotop</a>。</li>\n</ul>\n<p>这些包与 Ubuntu 提供的 <strong>mongodb</strong>、<strong>mongodb-server</strong> 和 <strong>mongodb-clients</strong> 包冲突。</p>\n<p>安装包提供的默认的 <strong>/etc/mongod.conf</strong> 配置文件默认设置 <strong>bind_ip</strong> 为 127.0.0.1。在初始化一个 <a href=\"https://docs.mongodb.org/manual/reference/glossary/#term-replica-set\">replica set</a> 之前根据环境的需要修改这个设置。</p>\n<h3 id=\"初始化脚本\"><a href=\"#初始化脚本\" class=\"headerlink\" title=\"初始化脚本\"></a>初始化脚本</h3><p><strong>mongodb-org</strong> 安装包包含许多<a href=\"https://docs.mongodb.org/manual/reference/glossary/#term-init-script\">初始化脚本</a>，包含初始化脚本 <strong>/etc/init.d/mongod</strong>。你可以用这些脚本停止、启动和重启守护进程。</p>\n<p>安装包使用 <strong>/etc/mongod.conf</strong> 文件结合初始化脚本配置 MongoDB。查看 <a href=\"https://docs.mongodb.org/manual/reference/configuration-options/\">配置文件</a> 参考获取配置文件中的设置参数。</p>\n<p>从 3.2.5 版本开始，不再有 <a href=\"https://docs.mongodb.org/manual/reference/program/mongos/#bin.mongos\">mongos</a> 初始脚本。<a href=\"https://docs.mongodb.org/manual/reference/program/mongos/#bin.mongos\">mongos</a> 进程只在 <a href=\"https://docs.mongodb.org/manual/core/sharding/\">sharding</a> 时用。你可以用 <strong>mongod</strong> 初始脚本生成自己的 <a href=\"https://docs.mongodb.org/manual/reference/program/mongos/#bin.mongos\">mongos</a> 初始化脚本在这样的环境使用。查看 <a href=\"https://docs.mongodb.org/manual/reference/program/mongos/#bin.mongos\">mongos</a> 参考获取详细配置。</p>\n<h3 id=\"安装-MongoDB-社区版\"><a href=\"#安装-MongoDB-社区版\" class=\"headerlink\" title=\"安装 MongoDB 社区版\"></a>安装 MongoDB 社区版</h3><p>MongoDB 只为 64 位长期支持的 Ubuntu 发行版提供安装包。当前，这意味着 12.04 LTS (Precise Pangolin) 和 14.04 LTS (Trusty Tahr) 两个版本。同时b安装包在其他 Ubuntu 发行版本也可以运行，这不是一个维持的配置。</p>\n<h4 id=\"1、导入安装包管理系统使用的公钥\"><a href=\"#1、导入安装包管理系统使用的公钥\" class=\"headerlink\" title=\"1、导入安装包管理系统使用的公钥\"></a>1、导入安装包管理系统使用的公钥</h4><p>Ubuntu 包管理工具（例如 <strong>dpkg</strong> 和 <strong>apt</strong>) 通过要求发布者签名的 GPG 密钥确保安装包的一致和真实。输入下面的命令导入 <a href=\"https://www.mongodb.org/static/pgp/server-3.2.asc?_ga=1.177583341.957577678.1460369205\">MongoDB GPG 公钥</a>：</p>\n<pre><code>sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv EA312927\n</code></pre>\n<h4 id=\"2、为-MongoDB-创建一个文件列表\"><a href=\"#2、为-MongoDB-创建一个文件列表\" class=\"headerlink\" title=\"2、为 MongoDB 创建一个文件列表\"></a>2、为 MongoDB 创建一个文件列表</h4><p>使用适用你的 Ubuntu 版本的命令创建 <strong>/etc/apt/sources.list.d/mongodb-org-3.2.list</strong> 列表文件：</p>\n<p>  Ubuntu 12.04</p>\n<pre><code>echo &quot;deb http://repo.mongodb.org/apt/ubuntu precise/mongodb-org/3.2 multiverse&quot; | sudo tee /etc/apt/sources.list.d/mongodb-org-3.2.list\n</code></pre>\n<p>  Ubuntu 14.04</p>\n<pre><code>echo &quot;deb http://repo.mongodb.org/apt/ubuntu trusty/mongodb-org/3.2 multiverse&quot; | sudo tee /etc/apt/sources.list.d/mongodb-org-3.2.list\n</code></pre>\n<h4 id=\"3、重新加载本地安装包数据库\"><a href=\"#3、重新加载本地安装包数据库\" class=\"headerlink\" title=\"3、重新加载本地安装包数据库\"></a>3、重新加载本地安装包数据库</h4><p>输入以下命令重新加载本地安装包数据库：</p>\n<pre><code>sudo apt-get update\n</code></pre>\n<p>重新加载过程中可能会报以下错误信息，多试两次即可：</p>\n<pre><code>错误 http://repo.mongodb.org trusty/mongodb-org/3.2/multiverse Translation-zh_CN                                                              \n  不能连接到 repo.mongodb.org：http：\n错误 http://repo.mongodb.org trusty/mongodb-org/3.2/multiverse Translation-zh\n  不能连接到 repo.mongodb.org：http：\n错误 http://repo.mongodb.org trusty/mongodb-org/3.2/multiverse Translation-en_US\n  不能连接到 repo.mongodb.org：http：\n错误 http://repo.mongodb.org trusty/mongodb-org/3.2/multiverse Translation-en\n  不能连接到 repo.mongodb.org：http：\n  \n</code></pre>\n<p>如果报以下错误信息，可以将添加到文件列表中的信息由 HTTP 改为 HTTPS：</p>\n<p>  错误信息：</p>\n<pre><code>W: 无法下载 http://repo.mongodb.org/apt/ubuntu/dists/trusty/mongodb-org/3.2/multiverse/binary-amd64/Packages  Hash 校验和不符\n</code></pre>\n<p>  第 2 步创建文件列表命令改为：</p>\n<pre><code>echo &quot;deb http://repo.mongodb.org/apt/ubuntu trusty/mongodb-org/3.2 multiverse&quot; | sudo tee /etc/apt/sources.list.d/mongodb-org-3.2.list\n</code></pre>\n<p>再次执行更新命令成功。</p>\n<h4 id=\"4、安装-MongoDB\"><a href=\"#4、安装-MongoDB\" class=\"headerlink\" title=\"4、安装 MongoDB\"></a>4、安装 MongoDB</h4><p>可以安装 MongoDB 最新稳定版本或者指定的版本。</p>\n<h4 id=\"5、安装-MongoDB-最新稳定版本\"><a href=\"#5、安装-MongoDB-最新稳定版本\" class=\"headerlink\" title=\"5、安装 MongoDB 最新稳定版本\"></a>5、安装 MongoDB 最新稳定版本</h4><p>输入下面的命令：</p>\n<pre><code>sudo apt-get install -y mongodb-org\n</code></pre>\n<h4 id=\"6、安装特定版本的-MongoDB\"><a href=\"#6、安装特定版本的-MongoDB\" class=\"headerlink\" title=\"6、安装特定版本的 MongoDB\"></a>6、安装特定版本的 MongoDB</h4><p>安装特定发行版，你必须分别单独用版本号指定每个组件，像下面示例：</p>\n<pre><code>sudo apt-get install -y mongodb-org=3.2.5 mongodb-org-server=3.2.5 mongodb-org-shell=3.2.5 mongodb-org-mongos=3.2.5 mongodb-org-tools=3.2.5\n</code></pre>\n<p>如果你只安装 <strong>mongodb-org=3.2.5</strong>，并且没有包含组件安装包，MongoDB 每个最新的版本将被安装，不管你指明了哪个版本。</p>\n<h4 id=\"7、固定一个特定版本的-MongoDB\"><a href=\"#7、固定一个特定版本的-MongoDB\" class=\"headerlink\" title=\"7、固定一个特定版本的 MongoDB\"></a>7、固定一个特定版本的 MongoDB</h4><p>虽然你可以指定任何可以获取的 MongoDB 版本，当新版本可以获取时 <strong>apt-get</strong> 将更新安装包。为了防止无意的更新，固定安装包。为了固定当前安装的 MongoDB 版本，输入以下命令序列：</p>\n<pre><code>echo &quot;mongodb-org hold&quot; | sudo dpkg --set-selections\necho &quot;mongodb-org-server hold&quot; | sudo dpkg --set-selections\necho &quot;mongodb-org-shell hold&quot; | sudo dpkg --set-selections\necho &quot;mongodb-org-mongos hold&quot; | sudo dpkg --set-selections\necho &quot;mongodb-org-tools hold&quot; | sudo dpkg --set-selections\n</code></pre>\n<h3 id=\"运行-MongoDB-社区版\"><a href=\"#运行-MongoDB-社区版\" class=\"headerlink\" title=\"运行 MongoDB 社区版\"></a>运行 MongoDB 社区版</h3><p>默认的 MongoDB 实例在 <strong>/var/lib/mongodb</strong> 存储数据文件，并且在 <strong>/var/log/mongodb</strong> 存储日志文件，并且用 <strong>mongodb</strong> 用户用户帐号运行。你可以在 <strong>/etc/mongod.conf</strong> 中指定替换的日志和数据文件目录。查看 <a href=\"https://docs.mongodb.org/manual/reference/configuration-options/#systemLog.path\">systemLog.path</a> 和 <a href=\"https://docs.mongodb.org/manual/reference/configuration-options/#storage.dbPath\">storage.dbPath</a> 获取额外的信息。</p>\n<p>如果改变运行 MongoDB 进程的用户，你<strong>必须</strong>修改 <strong>/var/lib/mongodb</strong> 和 <strong>/var/log/mongodb</strong> 目录的访问控制权限给这个用户对这些目录的访问权限。</p>\n<h4 id=\"1、启动-MongoDB\"><a href=\"#1、启动-MongoDB\" class=\"headerlink\" title=\"1、启动 MongoDB\"></a>1、启动 MongoDB</h4><p>输入下面的命令启动 <a href=\"https://docs.mongodb.org/manual/reference/program/mongod/#bin.mongod\">mongod</a>：</p>\n<pre><code>sudo service mongod start\n</code></pre>\n<p>执行上面的命令会报错，错误信息如下：</p>\n<pre><code>Failed to start mongod.service: Unit mongod.service failed to load: No such file or directory.\n</code></pre>\n<p>原因是缺少 systemd 的 service 文件。不需要重新开始安装或者换用其他仓库。创建文件 /lib/systemd/system/mongod.service，并输入以下内容：</p>\n<pre><code>[Unit]\nDescription=High-performance, schema-free document-oriented database\nAfter=network.target\n\n[Service]\nUser=mongodb\nExecStart=/usr/bin/mongod --quiet --config /etc/mongod.conf\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>\n<blockquote>\n<p>注意：/lib/systemd/system/mongod.service 和配置内容中的文件名可能需要对应的做修改，可能是 mongodb.service 和 mongodb.conf。参考网上的资料是 mongodb，而我的系统安装后都是 mongod。</p>\n</blockquote>\n<p>重新执行启动命令，提示以下信息，检查 mongo 进程不存在，说明未启动成功：</p>\n<pre><code>Warning: mongod.service changed on disk. Run &#39;systemctl daemon-reload&#39; to reload units.\n</code></pre>\n<p>根据提示执行以下命令，并输入用户密码，再次执行启动命令，这次启动成功 :)</p>\n<pre><code>systemctl daemon-reload\n</code></pre>\n<h4 id=\"2、确认-MongoDB-已经成功启动\"><a href=\"#2、确认-MongoDB-已经成功启动\" class=\"headerlink\" title=\"2、确认 MongoDB 已经成功启动\"></a>2、确认 MongoDB 已经成功启动</h4><p>通过读取检查 <strong>/var/log/mongodb/mongod.log</strong> 日志文件中的下面一行记录确认 <a href=\"https://docs.mongodb.org/manual/reference/program/mongod/#bin.mongod\">mongod</a> 进程已经成功启动。</p>\n<pre><code>[initandlisten] waiting for connections on port &lt;port&gt;\n</code></pre>\n<p><strong><port></strong> 是在 <strong>/etc/mongod.conf</strong> 文件中配置的端口号，默认是 27017。</p>\n<p>以下是我的安装过程的启动日志信息：</p>\n<p><img src=\"/uploads/20160416/mongodb_start_log.png\" alt=\"mongodb-start-log\"></p>\n<h4 id=\"3、停止-MongoDB\"><a href=\"#3、停止-MongoDB\" class=\"headerlink\" title=\"3、停止 MongoDB\"></a>3、停止 MongoDB</h4><p>如果需要，你可以输入以下命令停止 <a href=\"https://docs.mongodb.org/manual/reference/program/mongod/#bin.mongod\">mongod</a> 进程：</p>\n<pre><code>sudo service mongod stop\n</code></pre>\n<h4 id=\"4、重启-MongoDB\"><a href=\"#4、重启-MongoDB\" class=\"headerlink\" title=\"4、重启 MongoDB\"></a>4、重启 MongoDB</h4><p>输入以下命令重启 <a href=\"https://docs.mongodb.org/manual/reference/program/mongod/#bin.mongod\">mongod</a> 进程：</p>\n<pre><code>sudo service mongod restart\n</code></pre>\n<h3 id=\"卸载-MongoDB-社区版\"><a href=\"#卸载-MongoDB-社区版\" class=\"headerlink\" title=\"卸载 MongoDB 社区版\"></a>卸载 MongoDB 社区版</h3><p>要从系统中完全移除 MongoDB，你必须移除 MongoDB 应用程序、配置文件和所有包含数据和日志的目录。下面的章节指导你通过必要的步骤完成卸载。</p>\n<blockquote>\n<p>警告：<br>这个过程将完全<em>移除</em> MongoDB、配置和<em>所有</em>数据库。这个过程是不能恢复的，因此确认执行该过程前确保你的配置和数据应经备份。</p>\n</blockquote>\n<h4 id=\"1、停止-MongoDB\"><a href=\"#1、停止-MongoDB\" class=\"headerlink\" title=\"1、停止 MongoDB\"></a>1、停止 MongoDB</h4><p>输入以下命令停止 <a href=\"https://docs.mongodb.org/manual/reference/program/mongod/#bin.mongod\">mongod</a> 进程：</p>\n<pre><code>sudo service mongod stop\n</code></pre>\n<h4 id=\"2、移除安装包\"><a href=\"#2、移除安装包\" class=\"headerlink\" title=\"2、移除安装包\"></a>2、移除安装包</h4><p>移除所有以前安装的 MongoDB 安装包。</p>\n<pre><code>sudo apt-get purge mongodb-org*\n</code></pre>\n<h4 id=\"3、移除数据目录\"><a href=\"#3、移除数据目录\" class=\"headerlink\" title=\"3、移除数据目录\"></a>3、移除数据目录</h4><p>移除 MongoDB 数据库和日志文件。</p>\n<pre><code>sudo rm -r /var/log/mongodb\nsudo rm -r /var/lib/mongodb\n</code></pre>"},{"title":"Ubuntu 安装 SSH，并开启 root 远程登录","date":"2016-05-24T21:18:28.000Z","_content":"\n#### 安装 SSH\n\n    apt-get install ssh\n\n其中包含了 openssh-client 和 openssh-server。\n\n<!-- more -->\n\n#### 开启 root 远程登录\n\n修改配置文件 /etc/ssh/sshd_config 中的 PermitRootLogin 参数值为 yes：  \n![PermitRootLogin](/uploads/20160525/ssh-PermitRootLogin.png)\n\n重启 SSH 服务：\n\n    service ssh restart\n\n#### PermitRootLogin 参数\n\n指定 root 是否可以用 [ssh](http://man.openbsd.org/ssh.1) 登录。参数值必须是“yes”、“prohibit-password”、“without-password”、“orced-commands-only”或者“no”。默认值是“prohibit-password”。\n\n如果这个选项设置为“prohibit-password”、“without-password”，通过密码和键盘交互的授权方式对 root 用户禁用。\n\n如果这个选项设置为“orced-commands-only”，root 用户只允许通过公钥授权登录，但只是指定了 *conmmand* 选项的时候（这可能对远程备份是有好处的，即使 root 不允许登录）。其他授权方式对 root 无效。\n\n如果这个选项设置为“no”，root 不允许登录。\n\n其他参数说明，请查阅[官方文档](http://man.openbsd.org/sshd_config)。\n","source":"_posts/Ubuntu-安装-SSH，并开启-root-远程登录.md","raw":"title: Ubuntu 安装 SSH，并开启 root 远程登录\ntags:\n  - Ubuntu\ncategories:\n  - 操作系统\n  - Ubuntu\ndate: 2016-05-25 05:18:28\n---\n\n#### 安装 SSH\n\n    apt-get install ssh\n\n其中包含了 openssh-client 和 openssh-server。\n\n<!-- more -->\n\n#### 开启 root 远程登录\n\n修改配置文件 /etc/ssh/sshd_config 中的 PermitRootLogin 参数值为 yes：  \n![PermitRootLogin](/uploads/20160525/ssh-PermitRootLogin.png)\n\n重启 SSH 服务：\n\n    service ssh restart\n\n#### PermitRootLogin 参数\n\n指定 root 是否可以用 [ssh](http://man.openbsd.org/ssh.1) 登录。参数值必须是“yes”、“prohibit-password”、“without-password”、“orced-commands-only”或者“no”。默认值是“prohibit-password”。\n\n如果这个选项设置为“prohibit-password”、“without-password”，通过密码和键盘交互的授权方式对 root 用户禁用。\n\n如果这个选项设置为“orced-commands-only”，root 用户只允许通过公钥授权登录，但只是指定了 *conmmand* 选项的时候（这可能对远程备份是有好处的，即使 root 不允许登录）。其他授权方式对 root 无效。\n\n如果这个选项设置为“no”，root 不允许登录。\n\n其他参数说明，请查阅[官方文档](http://man.openbsd.org/sshd_config)。\n","slug":"Ubuntu-安装-SSH，并开启-root-远程登录","published":1,"updated":"2021-07-19T16:28:00.292Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphqf00byitd3brmcejh4","content":"<h4 id=\"安装-SSH\"><a href=\"#安装-SSH\" class=\"headerlink\" title=\"安装 SSH\"></a>安装 SSH</h4><pre><code>apt-get install ssh\n</code></pre>\n<p>其中包含了 openssh-client 和 openssh-server。</p>\n<span id=\"more\"></span>\n\n<h4 id=\"开启-root-远程登录\"><a href=\"#开启-root-远程登录\" class=\"headerlink\" title=\"开启 root 远程登录\"></a>开启 root 远程登录</h4><p>修改配置文件 /etc/ssh/sshd_config 中的 PermitRootLogin 参数值为 yes：<br><img src=\"/uploads/20160525/ssh-PermitRootLogin.png\" alt=\"PermitRootLogin\"></p>\n<p>重启 SSH 服务：</p>\n<pre><code>service ssh restart\n</code></pre>\n<h4 id=\"PermitRootLogin-参数\"><a href=\"#PermitRootLogin-参数\" class=\"headerlink\" title=\"PermitRootLogin 参数\"></a>PermitRootLogin 参数</h4><p>指定 root 是否可以用 <a href=\"http://man.openbsd.org/ssh.1\">ssh</a> 登录。参数值必须是“yes”、“prohibit-password”、“without-password”、“orced-commands-only”或者“no”。默认值是“prohibit-password”。</p>\n<p>如果这个选项设置为“prohibit-password”、“without-password”，通过密码和键盘交互的授权方式对 root 用户禁用。</p>\n<p>如果这个选项设置为“orced-commands-only”，root 用户只允许通过公钥授权登录，但只是指定了 <em>conmmand</em> 选项的时候（这可能对远程备份是有好处的，即使 root 不允许登录）。其他授权方式对 root 无效。</p>\n<p>如果这个选项设置为“no”，root 不允许登录。</p>\n<p>其他参数说明，请查阅<a href=\"http://man.openbsd.org/sshd_config\">官方文档</a>。</p>\n","site":{"data":{}},"excerpt":"<h4 id=\"安装-SSH\"><a href=\"#安装-SSH\" class=\"headerlink\" title=\"安装 SSH\"></a>安装 SSH</h4><pre><code>apt-get install ssh\n</code></pre>\n<p>其中包含了 openssh-client 和 openssh-server。</p>","more":"<h4 id=\"开启-root-远程登录\"><a href=\"#开启-root-远程登录\" class=\"headerlink\" title=\"开启 root 远程登录\"></a>开启 root 远程登录</h4><p>修改配置文件 /etc/ssh/sshd_config 中的 PermitRootLogin 参数值为 yes：<br><img src=\"/uploads/20160525/ssh-PermitRootLogin.png\" alt=\"PermitRootLogin\"></p>\n<p>重启 SSH 服务：</p>\n<pre><code>service ssh restart\n</code></pre>\n<h4 id=\"PermitRootLogin-参数\"><a href=\"#PermitRootLogin-参数\" class=\"headerlink\" title=\"PermitRootLogin 参数\"></a>PermitRootLogin 参数</h4><p>指定 root 是否可以用 <a href=\"http://man.openbsd.org/ssh.1\">ssh</a> 登录。参数值必须是“yes”、“prohibit-password”、“without-password”、“orced-commands-only”或者“no”。默认值是“prohibit-password”。</p>\n<p>如果这个选项设置为“prohibit-password”、“without-password”，通过密码和键盘交互的授权方式对 root 用户禁用。</p>\n<p>如果这个选项设置为“orced-commands-only”，root 用户只允许通过公钥授权登录，但只是指定了 <em>conmmand</em> 选项的时候（这可能对远程备份是有好处的，即使 root 不允许登录）。其他授权方式对 root 无效。</p>\n<p>如果这个选项设置为“no”，root 不允许登录。</p>\n<p>其他参数说明，请查阅<a href=\"http://man.openbsd.org/sshd_config\">官方文档</a>。</p>"},{"title":"Ubuntu 常见安装源更新问题","date":"2016-04-16T12:19:29.000Z","_content":"\n\n以下两种情况都是因为无效安装源导致的，可以直接删除无效安装源解决。\n\n<!-- more -->\n\n有两个位置可以添加仓库。/etc/apt/sources.list 文件和所有在 /etc/apt/sources.list.d/ 目录下以 .list 结尾的文件。所以，只需要检查这个目录下的文件并移除不需要的即可。\n\n**1、Duplicate sources.list entry**\n\n\tW: Duplicate sources.list entry http://archive.ubuntukylin.com:10006/ubuntukylin/ trusty/main amd64 Packages (/var/lib/apt/lists/archive.ubuntukylin.com:10006_ubuntukylin_dists_trusty_main_binary-amd64_Packages)\n\tW: Duplicate sources.list entry http://archive.ubuntukylin.com:10006/ubuntukylin/ trusty/main i386 Packages (/var/lib/apt/lists/archive.ubuntukylin.com:10006_ubuntukylin_dists_trusty_main_binary-i386_Packages)\n\tW: 您可能需要运行 apt-get update 来解决这些问题\n\n通过以下命令查找目标文件：\n\n\t#cd /etc/apt/sources.list.d && find . -name '*.list*'|xargs grep \"archive.ubuntukylin.com:10006\"\n\t./wps-office.list.save:deb http://archive.ubuntukylin.com:10006/ubuntukylin trusty main\n\t./sogoupinyin.list:deb http://archive.ubuntukylin.com:10006/ubuntukylin trusty main\n\t./sogoupinyin.list.save:deb http://archive.ubuntukylin.com:10006/ubuntukylin trusty main\n\t./wps-office.list:deb http://archive.ubuntukylin.com:10006/ubuntukylin trusty main\n\n删除对应的文件：\n\n\t#sudo rm -f wps-office.list*\n\t#sudo rm -f sogoupinyin.list*\n\n再次执行安装源更新命令，问题解决：\n\n\tsudo apt-get update\n\n**2、404  Not Found**\n\n还有一种常见问题是找不到的错误，如下：\n\n\t错误 http://ppa.launchpad.net wily/main amd64 Packages\n\t  404  Not Found\n\t错误 http://ppa.launchpad.net wily/main i386 Packages\n\t  404  Not Found\n\t忽略 http://ppa.launchpad.net wily/main Translation-en_US\n\t忽略 http://ppa.launchpad.net wily/main Translation-zh_CN\n\t忽略 http://ppa.launchpad.net wily/main Translation-en\n\t忽略 http://ppa.launchpad.net wily/main Translation-zh\n\t下载 1,525 kB，耗时 43秒 (35.3 kB/s)\n\tW: 无法下载 http://ppa.launchpad.net/tldm217/tahutek.net/ubuntu/dists/wily/main/binary-amd64/Packages  404  Not Found\n\tW: 无法下载 http://ppa.launchpad.net/tldm217/tahutek.net/ubuntu/dists/wily/main/binary-i386/Packages  404  Not Found\n\tE: 部分索引文件下载失败。如果忽略它们，那将转而使用旧的索引文件。\n\n删除来源自 http://ppa.launchpad.net 的安装源列表，重新更新：\n\n\t#ls -l /etc/apt/sources.list.d/tldm217-ubuntu-tahutek_net-wily.list*\n\t-rw-r--r-- 1 root root 138  4月 16 02:03 /etc/apt/sources.list.d/tldm217-ubuntu-tahutek_net-wily.list\n\t-rw-r--r-- 1 root root 138  4月 16 02:03 /etc/apt/sources.list.d/tldm217-ubuntu-tahutek_net-wily.list.save\n\n\t#sudo rm -f /etc/apt/sources.list.d/tldm217-ubuntu-tahutek_net-wily.list*\n\n再次执行更新，问题解决。\n","source":"_posts/Ubuntu-常见安装源更新问题.md","raw":"title: Ubuntu 常见安装源更新问题\ntags:\n  - Ubuntu\ncategories:\n  - 操作系统\n  - Ubuntu\ndate: 2016-04-16 20:19:29\n---\n\n\n以下两种情况都是因为无效安装源导致的，可以直接删除无效安装源解决。\n\n<!-- more -->\n\n有两个位置可以添加仓库。/etc/apt/sources.list 文件和所有在 /etc/apt/sources.list.d/ 目录下以 .list 结尾的文件。所以，只需要检查这个目录下的文件并移除不需要的即可。\n\n**1、Duplicate sources.list entry**\n\n\tW: Duplicate sources.list entry http://archive.ubuntukylin.com:10006/ubuntukylin/ trusty/main amd64 Packages (/var/lib/apt/lists/archive.ubuntukylin.com:10006_ubuntukylin_dists_trusty_main_binary-amd64_Packages)\n\tW: Duplicate sources.list entry http://archive.ubuntukylin.com:10006/ubuntukylin/ trusty/main i386 Packages (/var/lib/apt/lists/archive.ubuntukylin.com:10006_ubuntukylin_dists_trusty_main_binary-i386_Packages)\n\tW: 您可能需要运行 apt-get update 来解决这些问题\n\n通过以下命令查找目标文件：\n\n\t#cd /etc/apt/sources.list.d && find . -name '*.list*'|xargs grep \"archive.ubuntukylin.com:10006\"\n\t./wps-office.list.save:deb http://archive.ubuntukylin.com:10006/ubuntukylin trusty main\n\t./sogoupinyin.list:deb http://archive.ubuntukylin.com:10006/ubuntukylin trusty main\n\t./sogoupinyin.list.save:deb http://archive.ubuntukylin.com:10006/ubuntukylin trusty main\n\t./wps-office.list:deb http://archive.ubuntukylin.com:10006/ubuntukylin trusty main\n\n删除对应的文件：\n\n\t#sudo rm -f wps-office.list*\n\t#sudo rm -f sogoupinyin.list*\n\n再次执行安装源更新命令，问题解决：\n\n\tsudo apt-get update\n\n**2、404  Not Found**\n\n还有一种常见问题是找不到的错误，如下：\n\n\t错误 http://ppa.launchpad.net wily/main amd64 Packages\n\t  404  Not Found\n\t错误 http://ppa.launchpad.net wily/main i386 Packages\n\t  404  Not Found\n\t忽略 http://ppa.launchpad.net wily/main Translation-en_US\n\t忽略 http://ppa.launchpad.net wily/main Translation-zh_CN\n\t忽略 http://ppa.launchpad.net wily/main Translation-en\n\t忽略 http://ppa.launchpad.net wily/main Translation-zh\n\t下载 1,525 kB，耗时 43秒 (35.3 kB/s)\n\tW: 无法下载 http://ppa.launchpad.net/tldm217/tahutek.net/ubuntu/dists/wily/main/binary-amd64/Packages  404  Not Found\n\tW: 无法下载 http://ppa.launchpad.net/tldm217/tahutek.net/ubuntu/dists/wily/main/binary-i386/Packages  404  Not Found\n\tE: 部分索引文件下载失败。如果忽略它们，那将转而使用旧的索引文件。\n\n删除来源自 http://ppa.launchpad.net 的安装源列表，重新更新：\n\n\t#ls -l /etc/apt/sources.list.d/tldm217-ubuntu-tahutek_net-wily.list*\n\t-rw-r--r-- 1 root root 138  4月 16 02:03 /etc/apt/sources.list.d/tldm217-ubuntu-tahutek_net-wily.list\n\t-rw-r--r-- 1 root root 138  4月 16 02:03 /etc/apt/sources.list.d/tldm217-ubuntu-tahutek_net-wily.list.save\n\n\t#sudo rm -f /etc/apt/sources.list.d/tldm217-ubuntu-tahutek_net-wily.list*\n\n再次执行更新，问题解决。\n","slug":"Ubuntu-常见安装源更新问题","published":1,"updated":"2021-07-19T16:28:00.296Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphqg00c1itd3fyw2gb25","content":"<p>以下两种情况都是因为无效安装源导致的，可以直接删除无效安装源解决。</p>\n<span id=\"more\"></span>\n\n<p>有两个位置可以添加仓库。/etc/apt/sources.list 文件和所有在 /etc/apt/sources.list.d/ 目录下以 .list 结尾的文件。所以，只需要检查这个目录下的文件并移除不需要的即可。</p>\n<p><strong>1、Duplicate sources.list entry</strong></p>\n<pre><code>W: Duplicate sources.list entry http://archive.ubuntukylin.com:10006/ubuntukylin/ trusty/main amd64 Packages (/var/lib/apt/lists/archive.ubuntukylin.com:10006_ubuntukylin_dists_trusty_main_binary-amd64_Packages)\nW: Duplicate sources.list entry http://archive.ubuntukylin.com:10006/ubuntukylin/ trusty/main i386 Packages (/var/lib/apt/lists/archive.ubuntukylin.com:10006_ubuntukylin_dists_trusty_main_binary-i386_Packages)\nW: 您可能需要运行 apt-get update 来解决这些问题\n</code></pre>\n<p>通过以下命令查找目标文件：</p>\n<pre><code>#cd /etc/apt/sources.list.d &amp;&amp; find . -name &#39;*.list*&#39;|xargs grep &quot;archive.ubuntukylin.com:10006&quot;\n./wps-office.list.save:deb http://archive.ubuntukylin.com:10006/ubuntukylin trusty main\n./sogoupinyin.list:deb http://archive.ubuntukylin.com:10006/ubuntukylin trusty main\n./sogoupinyin.list.save:deb http://archive.ubuntukylin.com:10006/ubuntukylin trusty main\n./wps-office.list:deb http://archive.ubuntukylin.com:10006/ubuntukylin trusty main\n</code></pre>\n<p>删除对应的文件：</p>\n<pre><code>#sudo rm -f wps-office.list*\n#sudo rm -f sogoupinyin.list*\n</code></pre>\n<p>再次执行安装源更新命令，问题解决：</p>\n<pre><code>sudo apt-get update\n</code></pre>\n<p><strong>2、404  Not Found</strong></p>\n<p>还有一种常见问题是找不到的错误，如下：</p>\n<pre><code>错误 http://ppa.launchpad.net wily/main amd64 Packages\n  404  Not Found\n错误 http://ppa.launchpad.net wily/main i386 Packages\n  404  Not Found\n忽略 http://ppa.launchpad.net wily/main Translation-en_US\n忽略 http://ppa.launchpad.net wily/main Translation-zh_CN\n忽略 http://ppa.launchpad.net wily/main Translation-en\n忽略 http://ppa.launchpad.net wily/main Translation-zh\n下载 1,525 kB，耗时 43秒 (35.3 kB/s)\nW: 无法下载 http://ppa.launchpad.net/tldm217/tahutek.net/ubuntu/dists/wily/main/binary-amd64/Packages  404  Not Found\nW: 无法下载 http://ppa.launchpad.net/tldm217/tahutek.net/ubuntu/dists/wily/main/binary-i386/Packages  404  Not Found\nE: 部分索引文件下载失败。如果忽略它们，那将转而使用旧的索引文件。\n</code></pre>\n<p>删除来源自 <a href=\"http://ppa.launchpad.net/\">http://ppa.launchpad.net</a> 的安装源列表，重新更新：</p>\n<pre><code>#ls -l /etc/apt/sources.list.d/tldm217-ubuntu-tahutek_net-wily.list*\n-rw-r--r-- 1 root root 138  4月 16 02:03 /etc/apt/sources.list.d/tldm217-ubuntu-tahutek_net-wily.list\n-rw-r--r-- 1 root root 138  4月 16 02:03 /etc/apt/sources.list.d/tldm217-ubuntu-tahutek_net-wily.list.save\n\n#sudo rm -f /etc/apt/sources.list.d/tldm217-ubuntu-tahutek_net-wily.list*\n</code></pre>\n<p>再次执行更新，问题解决。</p>\n","site":{"data":{}},"excerpt":"<p>以下两种情况都是因为无效安装源导致的，可以直接删除无效安装源解决。</p>","more":"<p>有两个位置可以添加仓库。/etc/apt/sources.list 文件和所有在 /etc/apt/sources.list.d/ 目录下以 .list 结尾的文件。所以，只需要检查这个目录下的文件并移除不需要的即可。</p>\n<p><strong>1、Duplicate sources.list entry</strong></p>\n<pre><code>W: Duplicate sources.list entry http://archive.ubuntukylin.com:10006/ubuntukylin/ trusty/main amd64 Packages (/var/lib/apt/lists/archive.ubuntukylin.com:10006_ubuntukylin_dists_trusty_main_binary-amd64_Packages)\nW: Duplicate sources.list entry http://archive.ubuntukylin.com:10006/ubuntukylin/ trusty/main i386 Packages (/var/lib/apt/lists/archive.ubuntukylin.com:10006_ubuntukylin_dists_trusty_main_binary-i386_Packages)\nW: 您可能需要运行 apt-get update 来解决这些问题\n</code></pre>\n<p>通过以下命令查找目标文件：</p>\n<pre><code>#cd /etc/apt/sources.list.d &amp;&amp; find . -name &#39;*.list*&#39;|xargs grep &quot;archive.ubuntukylin.com:10006&quot;\n./wps-office.list.save:deb http://archive.ubuntukylin.com:10006/ubuntukylin trusty main\n./sogoupinyin.list:deb http://archive.ubuntukylin.com:10006/ubuntukylin trusty main\n./sogoupinyin.list.save:deb http://archive.ubuntukylin.com:10006/ubuntukylin trusty main\n./wps-office.list:deb http://archive.ubuntukylin.com:10006/ubuntukylin trusty main\n</code></pre>\n<p>删除对应的文件：</p>\n<pre><code>#sudo rm -f wps-office.list*\n#sudo rm -f sogoupinyin.list*\n</code></pre>\n<p>再次执行安装源更新命令，问题解决：</p>\n<pre><code>sudo apt-get update\n</code></pre>\n<p><strong>2、404  Not Found</strong></p>\n<p>还有一种常见问题是找不到的错误，如下：</p>\n<pre><code>错误 http://ppa.launchpad.net wily/main amd64 Packages\n  404  Not Found\n错误 http://ppa.launchpad.net wily/main i386 Packages\n  404  Not Found\n忽略 http://ppa.launchpad.net wily/main Translation-en_US\n忽略 http://ppa.launchpad.net wily/main Translation-zh_CN\n忽略 http://ppa.launchpad.net wily/main Translation-en\n忽略 http://ppa.launchpad.net wily/main Translation-zh\n下载 1,525 kB，耗时 43秒 (35.3 kB/s)\nW: 无法下载 http://ppa.launchpad.net/tldm217/tahutek.net/ubuntu/dists/wily/main/binary-amd64/Packages  404  Not Found\nW: 无法下载 http://ppa.launchpad.net/tldm217/tahutek.net/ubuntu/dists/wily/main/binary-i386/Packages  404  Not Found\nE: 部分索引文件下载失败。如果忽略它们，那将转而使用旧的索引文件。\n</code></pre>\n<p>删除来源自 <a href=\"http://ppa.launchpad.net/\">http://ppa.launchpad.net</a> 的安装源列表，重新更新：</p>\n<pre><code>#ls -l /etc/apt/sources.list.d/tldm217-ubuntu-tahutek_net-wily.list*\n-rw-r--r-- 1 root root 138  4月 16 02:03 /etc/apt/sources.list.d/tldm217-ubuntu-tahutek_net-wily.list\n-rw-r--r-- 1 root root 138  4月 16 02:03 /etc/apt/sources.list.d/tldm217-ubuntu-tahutek_net-wily.list.save\n\n#sudo rm -f /etc/apt/sources.list.d/tldm217-ubuntu-tahutek_net-wily.list*\n</code></pre>\n<p>再次执行更新，问题解决。</p>"},{"title":"Ubuntu 终端 CTRL+S 被锁定后解锁快捷键","date":"2020-11-30T03:59:51.000Z","_content":"\nCTRL + S 表示暂停终端的作用，停止终端输入。使用快捷键 CTRL + Q 恢复，可以继续向终端输入。","source":"_posts/Ubuntu-终端-CTRL-S-被锁定后解锁快捷键.md","raw":"title: Ubuntu 终端 CTRL+S 被锁定后解锁快捷键\ndate: 2020-11-30 11:59:51\ntags:\n- Ubuntu\n- Linux\ncategories:\n- 操作系统\n- Linux\n---\n\nCTRL + S 表示暂停终端的作用，停止终端输入。使用快捷键 CTRL + Q 恢复，可以继续向终端输入。","slug":"Ubuntu-终端-CTRL-S-被锁定后解锁快捷键","published":1,"updated":"2021-07-19T16:27:59.892Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphqi00c6itd3dtskb4pb","content":"<p>CTRL + S 表示暂停终端的作用，停止终端输入。使用快捷键 CTRL + Q 恢复，可以继续向终端输入。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>CTRL + S 表示暂停终端的作用，停止终端输入。使用快捷键 CTRL + Q 恢复，可以继续向终端输入。</p>\n"},{"title":"VirtualBox启用无缝模式","date":"2015-11-22T02:05:33.000Z","_content":"VirtualBox是个不错的产品，免费而且轻量。平时学习、开发使用很方便。vbox安装ubuntu后不能启用无缝模式，外观不能全屏，操作也很不方便，如下图：\n\n<!-- more -->\n\n![vbox非无缝模式](/uploads/20151122/vbox1.png)\n\n要启用vbox无缝模式需要安装增强功能，如下图：\n\n![vbox增强功能](/uploads/20151122/vbox2.png)\n\n点击“安装增强功能”后，在客户机中会出现安装界面，根据提示安装即可。如下图：\n\n![vbox增强功能安装](/uploads/20151122/vbox3.png)\n\n如果提示没有光驱错误（如下图一），则需要在虚拟机设置中增加光驱，并且将增强功能的包添加进该光驱（如下图二）。增强功能包通常在vbox安装目录下，名为 **VBoxGuestAdditions.iso** 的文件。\n\n<center>图一：安装增强功能错误</center>\n![安装增强功能错误](/uploads/20151122/vbox4.png)\n\n<center>图二：虚拟机配置光驱</center>\n![虚拟机配置光驱](/uploads/20151122/vbox5.png)\n\n注意，配置光驱时需要先将虚拟机关机，配置完成后再开启。\n","source":"_posts/VirtualBox启用无缝模式.md","raw":"title: VirtualBox启用无缝模式\ndate: 2015-11-22 10:05:33\ntags:\n  - VirtualBox\n  - Ubuntu\ncategories:\n  - 操作系统\n  - 虚拟机\n---\nVirtualBox是个不错的产品，免费而且轻量。平时学习、开发使用很方便。vbox安装ubuntu后不能启用无缝模式，外观不能全屏，操作也很不方便，如下图：\n\n<!-- more -->\n\n![vbox非无缝模式](/uploads/20151122/vbox1.png)\n\n要启用vbox无缝模式需要安装增强功能，如下图：\n\n![vbox增强功能](/uploads/20151122/vbox2.png)\n\n点击“安装增强功能”后，在客户机中会出现安装界面，根据提示安装即可。如下图：\n\n![vbox增强功能安装](/uploads/20151122/vbox3.png)\n\n如果提示没有光驱错误（如下图一），则需要在虚拟机设置中增加光驱，并且将增强功能的包添加进该光驱（如下图二）。增强功能包通常在vbox安装目录下，名为 **VBoxGuestAdditions.iso** 的文件。\n\n<center>图一：安装增强功能错误</center>\n![安装增强功能错误](/uploads/20151122/vbox4.png)\n\n<center>图二：虚拟机配置光驱</center>\n![虚拟机配置光驱](/uploads/20151122/vbox5.png)\n\n注意，配置光驱时需要先将虚拟机关机，配置完成后再开启。\n","slug":"VirtualBox启用无缝模式","published":1,"updated":"2021-07-19T16:28:00.296Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphqj00c9itd34ptl7xgp","content":"<p>VirtualBox是个不错的产品，免费而且轻量。平时学习、开发使用很方便。vbox安装ubuntu后不能启用无缝模式，外观不能全屏，操作也很不方便，如下图：</p>\n<span id=\"more\"></span>\n\n<p><img src=\"/uploads/20151122/vbox1.png\" alt=\"vbox非无缝模式\"></p>\n<p>要启用vbox无缝模式需要安装增强功能，如下图：</p>\n<p><img src=\"/uploads/20151122/vbox2.png\" alt=\"vbox增强功能\"></p>\n<p>点击“安装增强功能”后，在客户机中会出现安装界面，根据提示安装即可。如下图：</p>\n<p><img src=\"/uploads/20151122/vbox3.png\" alt=\"vbox增强功能安装\"></p>\n<p>如果提示没有光驱错误（如下图一），则需要在虚拟机设置中增加光驱，并且将增强功能的包添加进该光驱（如下图二）。增强功能包通常在vbox安装目录下，名为 <strong>VBoxGuestAdditions.iso</strong> 的文件。</p>\n<center>图一：安装增强功能错误</center>\n![安装增强功能错误](/uploads/20151122/vbox4.png)\n\n<center>图二：虚拟机配置光驱</center>\n![虚拟机配置光驱](/uploads/20151122/vbox5.png)\n\n<p>注意，配置光驱时需要先将虚拟机关机，配置完成后再开启。</p>\n","site":{"data":{}},"excerpt":"<p>VirtualBox是个不错的产品，免费而且轻量。平时学习、开发使用很方便。vbox安装ubuntu后不能启用无缝模式，外观不能全屏，操作也很不方便，如下图：</p>","more":"<p><img src=\"/uploads/20151122/vbox1.png\" alt=\"vbox非无缝模式\"></p>\n<p>要启用vbox无缝模式需要安装增强功能，如下图：</p>\n<p><img src=\"/uploads/20151122/vbox2.png\" alt=\"vbox增强功能\"></p>\n<p>点击“安装增强功能”后，在客户机中会出现安装界面，根据提示安装即可。如下图：</p>\n<p><img src=\"/uploads/20151122/vbox3.png\" alt=\"vbox增强功能安装\"></p>\n<p>如果提示没有光驱错误（如下图一），则需要在虚拟机设置中增加光驱，并且将增强功能的包添加进该光驱（如下图二）。增强功能包通常在vbox安装目录下，名为 <strong>VBoxGuestAdditions.iso</strong> 的文件。</p>\n<center>图一：安装增强功能错误</center>\n![安装增强功能错误](/uploads/20151122/vbox4.png)\n\n<center>图二：虚拟机配置光驱</center>\n![虚拟机配置光驱](/uploads/20151122/vbox5.png)\n\n<p>注意，配置光驱时需要先将虚拟机关机，配置完成后再开启。</p>"},{"title":"VirtualBox快捷键","date":"2015-11-23T14:26:26.000Z","_content":"\nRight Ctrl + Home -- 显示控制菜单\nRight Ctrl + F    -- 切换到全屏模式\nRight Ctrl + L    -- 切换到无缝模式\nRight Ctrl + C    -- 切换到比例模式\n","source":"_posts/VirtualBox快捷键.md","raw":"title: VirtualBox快捷键\ndate: 2015-11-23 22:26:26\ntags:\n  - VirtualBox\n  - Ubuntu\ncategories:\n  - 操作系统\n  - 虚拟机\n---\n\nRight Ctrl + Home -- 显示控制菜单\nRight Ctrl + F    -- 切换到全屏模式\nRight Ctrl + L    -- 切换到无缝模式\nRight Ctrl + C    -- 切换到比例模式\n","slug":"VirtualBox快捷键","published":1,"updated":"2021-07-19T16:28:00.296Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphql00ceitd3gqo7aulb","content":"<p>Right Ctrl + Home – 显示控制菜单<br>Right Ctrl + F    – 切换到全屏模式<br>Right Ctrl + L    – 切换到无缝模式<br>Right Ctrl + C    – 切换到比例模式</p>\n","site":{"data":{}},"excerpt":"","more":"<p>Right Ctrl + Home – 显示控制菜单<br>Right Ctrl + F    – 切换到全屏模式<br>Right Ctrl + L    – 切换到无缝模式<br>Right Ctrl + C    – 切换到比例模式</p>\n"},{"title":"Visual Studio Code 修改 terminal 字体","date":"2019-10-17T04:05:27.000Z","_content":"默认打开中断后字体显示如下：![VS 字体设置前效果](/uploads/20191017/vs-code-terminal.png)\n\n打开设置，搜索配置项 terminal.integrated.fontFamily，修改配置为 monospace。修改后效果如下：![VS 字体设置后效果](/uploads/20191017/vs-code-terminal-font.png)","source":"_posts/Visual-Studio-Code-修改-terminal-字体.md","raw":"title: Visual Studio Code 修改 terminal 字体\ndate: 2019-10-17 12:05:27\ntags:\n- Visual Studio Code\ncategories:\n- 开发工具\n- Visual Studio Code\n---\n默认打开中断后字体显示如下：![VS 字体设置前效果](/uploads/20191017/vs-code-terminal.png)\n\n打开设置，搜索配置项 terminal.integrated.fontFamily，修改配置为 monospace。修改后效果如下：![VS 字体设置后效果](/uploads/20191017/vs-code-terminal-font.png)","slug":"Visual-Studio-Code-修改-terminal-字体","published":1,"updated":"2021-07-19T16:28:00.296Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphqm00chitd34ckh5jg7","content":"<p>默认打开中断后字体显示如下：<img src=\"/uploads/20191017/vs-code-terminal.png\" alt=\"VS 字体设置前效果\"></p>\n<p>打开设置，搜索配置项 terminal.integrated.fontFamily，修改配置为 monospace。修改后效果如下：<img src=\"/uploads/20191017/vs-code-terminal-font.png\" alt=\"VS 字体设置后效果\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p>默认打开中断后字体显示如下：<img src=\"/uploads/20191017/vs-code-terminal.png\" alt=\"VS 字体设置前效果\"></p>\n<p>打开设置，搜索配置项 terminal.integrated.fontFamily，修改配置为 monospace。修改后效果如下：<img src=\"/uploads/20191017/vs-code-terminal-font.png\" alt=\"VS 字体设置后效果\"></p>\n"},{"title":"Window7 搭建 Hadoop-2.7.3 源码阅读环境问题解决列表","date":"2017-02-11T10:47:51.000Z","_content":"\n#### 环境说明\n\n- Windows 7\n- java version \"1.7.0_80\"\n- Apache Maven 3.2.3\n- ProtocolBuffer 2.5.0\n- cmake version 3.7.2 win64 x64\n- Windows SDK 7.1\n\n构建过程参照源代码目录下 BUILDING.txt 说明文件中的“Building on Windows”中的内容；以及我之前的文章“[搭建 Hadoop 源代码阅读环境](http://zhang-jc.github.io/2016/08/29/%E6%90%AD%E5%BB%BA-Hadoop-%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E7%8E%AF%E5%A2%83/)”。\n\n<!-- more -->\n\n#### LINK : fatal error LNK1123: 转换到 COFF 期间失败\n\n错误信息：LINK : fatal error LNK1123: 转换到 COFF 期间失败: 文件无效或损坏 [C:\\hadoop-2.7.3\\hadoop-common-project\\hadoop-common\\src\\main\\winutils\\winutils.vcxproj]\n\n解决方法：安装 VS SP1 后重现编译。VS SP1 下载地址：<https://www.microsoft.com/en-us/download/details.aspx?id=23691>\n\n#### fatal error C1083: Cannot open include file: 'ammintrin.h'\n\n错误信息：C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\VC\\include\\intrin.h(26): fatal error C1083: Cannot open include file: 'ammintrin.h': No such file or directory [D:\\hadoop-2.7.3\\hadoop-common-project\\hadoop-common\\src\\main\\native\\native.vcxproj]\n\n解决方法：下载 ammintrin.h 文件，然后放置到目录 C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\VC\\include\\ 下，然后重新编译。下载地址：<http://wenku.baidu.com/link?url=a1MMc5tWY2SnczBiVLk0MiKFfnSpRWGRHKNIwS3vwCNCCAmTIT7fbyayzu1gJrjY4-0UCs7FLbXvMLItmLa-BPZOqHOQxy5cs--X_vSpYam>\n\n#### error : You are attempting to build a Win32 application from an x64 environment\n\n错误信息：C:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\Platforms\\Win32\\PlatformToolsets\\Windows7.1SDK\\Microsoft.Cpp.Win32.Windows7.1SDK.targets(20,5): error : You are attempting to build a Win32 application from an x64 environment. If using the Windows 7.1 SDK build environment, type setenv /x86 [D:\\hadoop-2.7.3\\hadoop-common-project\\hadoop-common\\src\\main\\winutils\\libwinutils.vcxproj]\n\n解决方法：这个错误是由于没有按照 BUILDING.txt 中的说明编译之前必须设置平台环境变量导致。编译前先用下面的命令设置平台环境变量：\n\n    set Platform=x64 (when building on a 64-bit system)\n    set Platform=Win32 (when building on a 32-bit system)\n\n#### CMake Error\n\n错误信息：[exec] CMake Error: Could not create named generator Visual Studio 10 Win64\n\n解决方法：最开始我安装的是 cmake 2.6.4，因为这个版本没有 win64 的版本，所以出错。卸载重新安装 cmake-3.7.2-win64-x64 版本，重新编译成功。cmake 各版本下载地址：<https://cmake.org/files/>\n\n> 网上也有类似错误是因为系统中装了 cygwin，且 cygwin 在环境变量 path 中的位置在 cmake 之前。如果是这种情况则将 cmake 目录路径在环境变量 path 中的位置移到 cygwin 前面就解决了。\n","source":"_posts/Window7-搭建-Hadoop-2-7-3-源码阅读环境问题解决列表.md","raw":"title: Window7 搭建 Hadoop-2.7.3 源码阅读环境问题解决列表\ntags:\n  - 大数据\n  - Hadoop\ncategories:\n  - 大数据\n  - Hadoop\ndate: 2017-02-11 18:47:51\n---\n\n#### 环境说明\n\n- Windows 7\n- java version \"1.7.0_80\"\n- Apache Maven 3.2.3\n- ProtocolBuffer 2.5.0\n- cmake version 3.7.2 win64 x64\n- Windows SDK 7.1\n\n构建过程参照源代码目录下 BUILDING.txt 说明文件中的“Building on Windows”中的内容；以及我之前的文章“[搭建 Hadoop 源代码阅读环境](http://zhang-jc.github.io/2016/08/29/%E6%90%AD%E5%BB%BA-Hadoop-%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E7%8E%AF%E5%A2%83/)”。\n\n<!-- more -->\n\n#### LINK : fatal error LNK1123: 转换到 COFF 期间失败\n\n错误信息：LINK : fatal error LNK1123: 转换到 COFF 期间失败: 文件无效或损坏 [C:\\hadoop-2.7.3\\hadoop-common-project\\hadoop-common\\src\\main\\winutils\\winutils.vcxproj]\n\n解决方法：安装 VS SP1 后重现编译。VS SP1 下载地址：<https://www.microsoft.com/en-us/download/details.aspx?id=23691>\n\n#### fatal error C1083: Cannot open include file: 'ammintrin.h'\n\n错误信息：C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\VC\\include\\intrin.h(26): fatal error C1083: Cannot open include file: 'ammintrin.h': No such file or directory [D:\\hadoop-2.7.3\\hadoop-common-project\\hadoop-common\\src\\main\\native\\native.vcxproj]\n\n解决方法：下载 ammintrin.h 文件，然后放置到目录 C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\VC\\include\\ 下，然后重新编译。下载地址：<http://wenku.baidu.com/link?url=a1MMc5tWY2SnczBiVLk0MiKFfnSpRWGRHKNIwS3vwCNCCAmTIT7fbyayzu1gJrjY4-0UCs7FLbXvMLItmLa-BPZOqHOQxy5cs--X_vSpYam>\n\n#### error : You are attempting to build a Win32 application from an x64 environment\n\n错误信息：C:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\Platforms\\Win32\\PlatformToolsets\\Windows7.1SDK\\Microsoft.Cpp.Win32.Windows7.1SDK.targets(20,5): error : You are attempting to build a Win32 application from an x64 environment. If using the Windows 7.1 SDK build environment, type setenv /x86 [D:\\hadoop-2.7.3\\hadoop-common-project\\hadoop-common\\src\\main\\winutils\\libwinutils.vcxproj]\n\n解决方法：这个错误是由于没有按照 BUILDING.txt 中的说明编译之前必须设置平台环境变量导致。编译前先用下面的命令设置平台环境变量：\n\n    set Platform=x64 (when building on a 64-bit system)\n    set Platform=Win32 (when building on a 32-bit system)\n\n#### CMake Error\n\n错误信息：[exec] CMake Error: Could not create named generator Visual Studio 10 Win64\n\n解决方法：最开始我安装的是 cmake 2.6.4，因为这个版本没有 win64 的版本，所以出错。卸载重新安装 cmake-3.7.2-win64-x64 版本，重新编译成功。cmake 各版本下载地址：<https://cmake.org/files/>\n\n> 网上也有类似错误是因为系统中装了 cygwin，且 cygwin 在环境变量 path 中的位置在 cmake 之前。如果是这种情况则将 cmake 目录路径在环境变量 path 中的位置移到 cygwin 前面就解决了。\n","slug":"Window7-搭建-Hadoop-2-7-3-源码阅读环境问题解决列表","published":1,"updated":"2021-07-19T16:28:00.296Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphqo00cmitd3bcfxbdjr","content":"<h4 id=\"环境说明\"><a href=\"#环境说明\" class=\"headerlink\" title=\"环境说明\"></a>环境说明</h4><ul>\n<li>Windows 7</li>\n<li>java version “1.7.0_80”</li>\n<li>Apache Maven 3.2.3</li>\n<li>ProtocolBuffer 2.5.0</li>\n<li>cmake version 3.7.2 win64 x64</li>\n<li>Windows SDK 7.1</li>\n</ul>\n<p>构建过程参照源代码目录下 BUILDING.txt 说明文件中的“Building on Windows”中的内容；以及我之前的文章“<a href=\"http://zhang-jc.github.io/2016/08/29/%E6%90%AD%E5%BB%BA-Hadoop-%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E7%8E%AF%E5%A2%83/\">搭建 Hadoop 源代码阅读环境</a>”。</p>\n<span id=\"more\"></span>\n\n<h4 id=\"LINK-fatal-error-LNK1123-转换到-COFF-期间失败\"><a href=\"#LINK-fatal-error-LNK1123-转换到-COFF-期间失败\" class=\"headerlink\" title=\"LINK : fatal error LNK1123: 转换到 COFF 期间失败\"></a>LINK : fatal error LNK1123: 转换到 COFF 期间失败</h4><p>错误信息：LINK : fatal error LNK1123: 转换到 COFF 期间失败: 文件无效或损坏 [C:\\hadoop-2.7.3\\hadoop-common-project\\hadoop-common\\src\\main\\winutils\\winutils.vcxproj]</p>\n<p>解决方法：安装 VS SP1 后重现编译。VS SP1 下载地址：<a href=\"https://www.microsoft.com/en-us/download/details.aspx?id=23691\">https://www.microsoft.com/en-us/download/details.aspx?id=23691</a></p>\n<h4 id=\"fatal-error-C1083-Cannot-open-include-file-‘ammintrin-h’\"><a href=\"#fatal-error-C1083-Cannot-open-include-file-‘ammintrin-h’\" class=\"headerlink\" title=\"fatal error C1083: Cannot open include file: ‘ammintrin.h’\"></a>fatal error C1083: Cannot open include file: ‘ammintrin.h’</h4><p>错误信息：C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\VC\\include\\intrin.h(26): fatal error C1083: Cannot open include file: ‘ammintrin.h’: No such file or directory [D:\\hadoop-2.7.3\\hadoop-common-project\\hadoop-common\\src\\main\\native\\native.vcxproj]</p>\n<p>解决方法：下载 ammintrin.h 文件，然后放置到目录 C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\VC\\include\\ 下，然后重新编译。下载地址：<a href=\"http://wenku.baidu.com/link?url=a1MMc5tWY2SnczBiVLk0MiKFfnSpRWGRHKNIwS3vwCNCCAmTIT7fbyayzu1gJrjY4-0UCs7FLbXvMLItmLa-BPZOqHOQxy5cs--X_vSpYam\">http://wenku.baidu.com/link?url=a1MMc5tWY2SnczBiVLk0MiKFfnSpRWGRHKNIwS3vwCNCCAmTIT7fbyayzu1gJrjY4-0UCs7FLbXvMLItmLa-BPZOqHOQxy5cs--X_vSpYam</a></p>\n<h4 id=\"error-You-are-attempting-to-build-a-Win32-application-from-an-x64-environment\"><a href=\"#error-You-are-attempting-to-build-a-Win32-application-from-an-x64-environment\" class=\"headerlink\" title=\"error : You are attempting to build a Win32 application from an x64 environment\"></a>error : You are attempting to build a Win32 application from an x64 environment</h4><p>错误信息：C:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\Platforms\\Win32\\PlatformToolsets\\Windows7.1SDK\\Microsoft.Cpp.Win32.Windows7.1SDK.targets(20,5): error : You are attempting to build a Win32 application from an x64 environment. If using the Windows 7.1 SDK build environment, type setenv /x86 [D:\\hadoop-2.7.3\\hadoop-common-project\\hadoop-common\\src\\main\\winutils\\libwinutils.vcxproj]</p>\n<p>解决方法：这个错误是由于没有按照 BUILDING.txt 中的说明编译之前必须设置平台环境变量导致。编译前先用下面的命令设置平台环境变量：</p>\n<pre><code>set Platform=x64 (when building on a 64-bit system)\nset Platform=Win32 (when building on a 32-bit system)\n</code></pre>\n<h4 id=\"CMake-Error\"><a href=\"#CMake-Error\" class=\"headerlink\" title=\"CMake Error\"></a>CMake Error</h4><p>错误信息：[exec] CMake Error: Could not create named generator Visual Studio 10 Win64</p>\n<p>解决方法：最开始我安装的是 cmake 2.6.4，因为这个版本没有 win64 的版本，所以出错。卸载重新安装 cmake-3.7.2-win64-x64 版本，重新编译成功。cmake 各版本下载地址：<a href=\"https://cmake.org/files/\">https://cmake.org/files/</a></p>\n<blockquote>\n<p>网上也有类似错误是因为系统中装了 cygwin，且 cygwin 在环境变量 path 中的位置在 cmake 之前。如果是这种情况则将 cmake 目录路径在环境变量 path 中的位置移到 cygwin 前面就解决了。</p>\n</blockquote>\n","site":{"data":{}},"excerpt":"<h4 id=\"环境说明\"><a href=\"#环境说明\" class=\"headerlink\" title=\"环境说明\"></a>环境说明</h4><ul>\n<li>Windows 7</li>\n<li>java version “1.7.0_80”</li>\n<li>Apache Maven 3.2.3</li>\n<li>ProtocolBuffer 2.5.0</li>\n<li>cmake version 3.7.2 win64 x64</li>\n<li>Windows SDK 7.1</li>\n</ul>\n<p>构建过程参照源代码目录下 BUILDING.txt 说明文件中的“Building on Windows”中的内容；以及我之前的文章“<a href=\"http://zhang-jc.github.io/2016/08/29/%E6%90%AD%E5%BB%BA-Hadoop-%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E7%8E%AF%E5%A2%83/\">搭建 Hadoop 源代码阅读环境</a>”。</p>","more":"<h4 id=\"LINK-fatal-error-LNK1123-转换到-COFF-期间失败\"><a href=\"#LINK-fatal-error-LNK1123-转换到-COFF-期间失败\" class=\"headerlink\" title=\"LINK : fatal error LNK1123: 转换到 COFF 期间失败\"></a>LINK : fatal error LNK1123: 转换到 COFF 期间失败</h4><p>错误信息：LINK : fatal error LNK1123: 转换到 COFF 期间失败: 文件无效或损坏 [C:\\hadoop-2.7.3\\hadoop-common-project\\hadoop-common\\src\\main\\winutils\\winutils.vcxproj]</p>\n<p>解决方法：安装 VS SP1 后重现编译。VS SP1 下载地址：<a href=\"https://www.microsoft.com/en-us/download/details.aspx?id=23691\">https://www.microsoft.com/en-us/download/details.aspx?id=23691</a></p>\n<h4 id=\"fatal-error-C1083-Cannot-open-include-file-‘ammintrin-h’\"><a href=\"#fatal-error-C1083-Cannot-open-include-file-‘ammintrin-h’\" class=\"headerlink\" title=\"fatal error C1083: Cannot open include file: ‘ammintrin.h’\"></a>fatal error C1083: Cannot open include file: ‘ammintrin.h’</h4><p>错误信息：C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\VC\\include\\intrin.h(26): fatal error C1083: Cannot open include file: ‘ammintrin.h’: No such file or directory [D:\\hadoop-2.7.3\\hadoop-common-project\\hadoop-common\\src\\main\\native\\native.vcxproj]</p>\n<p>解决方法：下载 ammintrin.h 文件，然后放置到目录 C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\VC\\include\\ 下，然后重新编译。下载地址：<a href=\"http://wenku.baidu.com/link?url=a1MMc5tWY2SnczBiVLk0MiKFfnSpRWGRHKNIwS3vwCNCCAmTIT7fbyayzu1gJrjY4-0UCs7FLbXvMLItmLa-BPZOqHOQxy5cs--X_vSpYam\">http://wenku.baidu.com/link?url=a1MMc5tWY2SnczBiVLk0MiKFfnSpRWGRHKNIwS3vwCNCCAmTIT7fbyayzu1gJrjY4-0UCs7FLbXvMLItmLa-BPZOqHOQxy5cs--X_vSpYam</a></p>\n<h4 id=\"error-You-are-attempting-to-build-a-Win32-application-from-an-x64-environment\"><a href=\"#error-You-are-attempting-to-build-a-Win32-application-from-an-x64-environment\" class=\"headerlink\" title=\"error : You are attempting to build a Win32 application from an x64 environment\"></a>error : You are attempting to build a Win32 application from an x64 environment</h4><p>错误信息：C:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\Platforms\\Win32\\PlatformToolsets\\Windows7.1SDK\\Microsoft.Cpp.Win32.Windows7.1SDK.targets(20,5): error : You are attempting to build a Win32 application from an x64 environment. If using the Windows 7.1 SDK build environment, type setenv /x86 [D:\\hadoop-2.7.3\\hadoop-common-project\\hadoop-common\\src\\main\\winutils\\libwinutils.vcxproj]</p>\n<p>解决方法：这个错误是由于没有按照 BUILDING.txt 中的说明编译之前必须设置平台环境变量导致。编译前先用下面的命令设置平台环境变量：</p>\n<pre><code>set Platform=x64 (when building on a 64-bit system)\nset Platform=Win32 (when building on a 32-bit system)\n</code></pre>\n<h4 id=\"CMake-Error\"><a href=\"#CMake-Error\" class=\"headerlink\" title=\"CMake Error\"></a>CMake Error</h4><p>错误信息：[exec] CMake Error: Could not create named generator Visual Studio 10 Win64</p>\n<p>解决方法：最开始我安装的是 cmake 2.6.4，因为这个版本没有 win64 的版本，所以出错。卸载重新安装 cmake-3.7.2-win64-x64 版本，重新编译成功。cmake 各版本下载地址：<a href=\"https://cmake.org/files/\">https://cmake.org/files/</a></p>\n<blockquote>\n<p>网上也有类似错误是因为系统中装了 cygwin，且 cygwin 在环境变量 path 中的位置在 cmake 之前。如果是这种情况则将 cmake 目录路径在环境变量 path 中的位置移到 cygwin 前面就解决了。</p>\n</blockquote>"},{"title":"Windows7 Emacs 设置及中文乱码解决","date":"2017-01-08T11:40:06.000Z","_content":"\n### 环境说明\n\n- Windows7\n- GNU Emacs 25.1.1\n- 安装路径：D:/apps/emacs/\n\n<!-- more -->\n\n### 配置 Emacs\n\n在 Windows7 下安装完 Emacs 后，默认情况下 Emacs 不会在一启动的时候就生成 .emacs 配置文件和 .emacs.d 目录。为了生成配置文件需要做如下操作：\n\n- 启动 Emacs\n- 在 Options 菜单中随便更改一下设置，如选中“Highlight Matching Parentheses”\n- 点 Save Options 保存\n\n生成的 .emacs 目录还是在 C:\\Users\\<username>\\AppData\\Roaming 下。打开 C:\\Users\\<username>\\AppData\\Roaming\\.emacs 配置文件，添加如下配置：\n\n    (load-file \"D:/apps/emacs/.emacs\")\n\nEmacs在启动的时候会加载 C:\\Users\\<username>\\AppData\\Roaming\\.emacs 这个配置文件，而该文件又加载 D:/apps/emacs/.emacs 配置文件。这样不用进入 C:\\Users\\<username>\\AppData\\Roaming\\.emacs 这个冗的路径了。\n\n使用以下配置修改 HOME PATH 配置项：\n\n    (setenv \"HOME\" \"D:/emacs-23.2\")\n    (setenv \"PATH\" \"D:/emacs-23.2\")\n    ;;set the default file path\n    (setq default-directory \"~/\")\n    (setq load-path (cons \"~/.emacs.d/elisp\" load-path))\n\n### 解决中文乱码\n\n在 .emacs 文件中加入如下配置：\n\n    ;; 编码设置 begin\n    (set-language-environment 'Chinese-GB)\n    \n    ;; default-buffer-file-coding-system变量在emacs23.2之后已被废弃，使用buffer-file-coding-system代替\n    (set-default buffer-file-coding-system 'utf-8-unix)\n    (set-default-coding-systems 'utf-8-unix)\n    (setq-default pathname-coding-system 'euc-cn)\n    (setq file-name-coding-system 'euc-cn)\n    \n    ;; 另外建议按下面的先后顺序来设置中文编码识别方式。\n    ;; 重要提示:写在最后一行的，实际上最优先使用; 最前面一行，反而放到最后才识别。\n    ;; utf-16le-with-signature 相当于 Windows 下的 Unicode 编码，这里也可写成\n    ;; utf-16 (utf-16 实际上还细分为 utf-16le, utf-16be, utf-16le-with-signature等多种)\n    (prefer-coding-system 'cp950)\n    (prefer-coding-system 'gb2312)\n    (prefer-coding-system 'cp936)\n    (prefer-coding-system 'gb18030)\n    \n    ;(prefer-coding-system 'utf-16le-with-signature)\n    (prefer-coding-system 'utf-16)\n    \n    ;; 新建文件使用utf-8-unix方式\n    ;; 如果不写下面两句，只写\n    ;; (prefer-coding-system 'utf-8)\n    ;; 这一句的话，新建文件以utf-8编码，行末结束符平台相关\n    (prefer-coding-system 'utf-8-dos)\n    (prefer-coding-system 'utf-8-unix)\n    ;; 编码设置 end","source":"_posts/Windows7-Emacs-设置及中文乱码解决.md","raw":"title: Windows7 Emacs 设置及中文乱码解决\ntags:\n  - Emacs\ncategories:\n  - 开发工具\n  - Emacs\ndate: 2017-01-08 19:40:06\n---\n\n### 环境说明\n\n- Windows7\n- GNU Emacs 25.1.1\n- 安装路径：D:/apps/emacs/\n\n<!-- more -->\n\n### 配置 Emacs\n\n在 Windows7 下安装完 Emacs 后，默认情况下 Emacs 不会在一启动的时候就生成 .emacs 配置文件和 .emacs.d 目录。为了生成配置文件需要做如下操作：\n\n- 启动 Emacs\n- 在 Options 菜单中随便更改一下设置，如选中“Highlight Matching Parentheses”\n- 点 Save Options 保存\n\n生成的 .emacs 目录还是在 C:\\Users\\<username>\\AppData\\Roaming 下。打开 C:\\Users\\<username>\\AppData\\Roaming\\.emacs 配置文件，添加如下配置：\n\n    (load-file \"D:/apps/emacs/.emacs\")\n\nEmacs在启动的时候会加载 C:\\Users\\<username>\\AppData\\Roaming\\.emacs 这个配置文件，而该文件又加载 D:/apps/emacs/.emacs 配置文件。这样不用进入 C:\\Users\\<username>\\AppData\\Roaming\\.emacs 这个冗的路径了。\n\n使用以下配置修改 HOME PATH 配置项：\n\n    (setenv \"HOME\" \"D:/emacs-23.2\")\n    (setenv \"PATH\" \"D:/emacs-23.2\")\n    ;;set the default file path\n    (setq default-directory \"~/\")\n    (setq load-path (cons \"~/.emacs.d/elisp\" load-path))\n\n### 解决中文乱码\n\n在 .emacs 文件中加入如下配置：\n\n    ;; 编码设置 begin\n    (set-language-environment 'Chinese-GB)\n    \n    ;; default-buffer-file-coding-system变量在emacs23.2之后已被废弃，使用buffer-file-coding-system代替\n    (set-default buffer-file-coding-system 'utf-8-unix)\n    (set-default-coding-systems 'utf-8-unix)\n    (setq-default pathname-coding-system 'euc-cn)\n    (setq file-name-coding-system 'euc-cn)\n    \n    ;; 另外建议按下面的先后顺序来设置中文编码识别方式。\n    ;; 重要提示:写在最后一行的，实际上最优先使用; 最前面一行，反而放到最后才识别。\n    ;; utf-16le-with-signature 相当于 Windows 下的 Unicode 编码，这里也可写成\n    ;; utf-16 (utf-16 实际上还细分为 utf-16le, utf-16be, utf-16le-with-signature等多种)\n    (prefer-coding-system 'cp950)\n    (prefer-coding-system 'gb2312)\n    (prefer-coding-system 'cp936)\n    (prefer-coding-system 'gb18030)\n    \n    ;(prefer-coding-system 'utf-16le-with-signature)\n    (prefer-coding-system 'utf-16)\n    \n    ;; 新建文件使用utf-8-unix方式\n    ;; 如果不写下面两句，只写\n    ;; (prefer-coding-system 'utf-8)\n    ;; 这一句的话，新建文件以utf-8编码，行末结束符平台相关\n    (prefer-coding-system 'utf-8-dos)\n    (prefer-coding-system 'utf-8-unix)\n    ;; 编码设置 end","slug":"Windows7-Emacs-设置及中文乱码解决","published":1,"updated":"2021-07-19T16:28:00.296Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphqp00cpitd349po09w5","content":"<h3 id=\"环境说明\"><a href=\"#环境说明\" class=\"headerlink\" title=\"环境说明\"></a>环境说明</h3><ul>\n<li>Windows7</li>\n<li>GNU Emacs 25.1.1</li>\n<li>安装路径：D:/apps/emacs/</li>\n</ul>\n<span id=\"more\"></span>\n\n<h3 id=\"配置-Emacs\"><a href=\"#配置-Emacs\" class=\"headerlink\" title=\"配置 Emacs\"></a>配置 Emacs</h3><p>在 Windows7 下安装完 Emacs 后，默认情况下 Emacs 不会在一启动的时候就生成 .emacs 配置文件和 .emacs.d 目录。为了生成配置文件需要做如下操作：</p>\n<ul>\n<li>启动 Emacs</li>\n<li>在 Options 菜单中随便更改一下设置，如选中“Highlight Matching Parentheses”</li>\n<li>点 Save Options 保存</li>\n</ul>\n<p>生成的 .emacs 目录还是在 C:\\Users&lt;username&gt;\\AppData\\Roaming 下。打开 C:\\Users&lt;username&gt;\\AppData\\Roaming.emacs 配置文件，添加如下配置：</p>\n<pre><code>(load-file &quot;D:/apps/emacs/.emacs&quot;)\n</code></pre>\n<p>Emacs在启动的时候会加载 C:\\Users&lt;username&gt;\\AppData\\Roaming.emacs 这个配置文件，而该文件又加载 D:/apps/emacs/.emacs 配置文件。这样不用进入 C:\\Users&lt;username&gt;\\AppData\\Roaming.emacs 这个冗的路径了。</p>\n<p>使用以下配置修改 HOME PATH 配置项：</p>\n<pre><code>(setenv &quot;HOME&quot; &quot;D:/emacs-23.2&quot;)\n(setenv &quot;PATH&quot; &quot;D:/emacs-23.2&quot;)\n;;set the default file path\n(setq default-directory &quot;~/&quot;)\n(setq load-path (cons &quot;~/.emacs.d/elisp&quot; load-path))\n</code></pre>\n<h3 id=\"解决中文乱码\"><a href=\"#解决中文乱码\" class=\"headerlink\" title=\"解决中文乱码\"></a>解决中文乱码</h3><p>在 .emacs 文件中加入如下配置：</p>\n<pre><code>;; 编码设置 begin\n(set-language-environment &#39;Chinese-GB)\n\n;; default-buffer-file-coding-system变量在emacs23.2之后已被废弃，使用buffer-file-coding-system代替\n(set-default buffer-file-coding-system &#39;utf-8-unix)\n(set-default-coding-systems &#39;utf-8-unix)\n(setq-default pathname-coding-system &#39;euc-cn)\n(setq file-name-coding-system &#39;euc-cn)\n\n;; 另外建议按下面的先后顺序来设置中文编码识别方式。\n;; 重要提示:写在最后一行的，实际上最优先使用; 最前面一行，反而放到最后才识别。\n;; utf-16le-with-signature 相当于 Windows 下的 Unicode 编码，这里也可写成\n;; utf-16 (utf-16 实际上还细分为 utf-16le, utf-16be, utf-16le-with-signature等多种)\n(prefer-coding-system &#39;cp950)\n(prefer-coding-system &#39;gb2312)\n(prefer-coding-system &#39;cp936)\n(prefer-coding-system &#39;gb18030)\n\n;(prefer-coding-system &#39;utf-16le-with-signature)\n(prefer-coding-system &#39;utf-16)\n\n;; 新建文件使用utf-8-unix方式\n;; 如果不写下面两句，只写\n;; (prefer-coding-system &#39;utf-8)\n;; 这一句的话，新建文件以utf-8编码，行末结束符平台相关\n(prefer-coding-system &#39;utf-8-dos)\n(prefer-coding-system &#39;utf-8-unix)\n;; 编码设置 end\n</code></pre>\n","site":{"data":{}},"excerpt":"<h3 id=\"环境说明\"><a href=\"#环境说明\" class=\"headerlink\" title=\"环境说明\"></a>环境说明</h3><ul>\n<li>Windows7</li>\n<li>GNU Emacs 25.1.1</li>\n<li>安装路径：D:/apps/emacs/</li>\n</ul>","more":"<h3 id=\"配置-Emacs\"><a href=\"#配置-Emacs\" class=\"headerlink\" title=\"配置 Emacs\"></a>配置 Emacs</h3><p>在 Windows7 下安装完 Emacs 后，默认情况下 Emacs 不会在一启动的时候就生成 .emacs 配置文件和 .emacs.d 目录。为了生成配置文件需要做如下操作：</p>\n<ul>\n<li>启动 Emacs</li>\n<li>在 Options 菜单中随便更改一下设置，如选中“Highlight Matching Parentheses”</li>\n<li>点 Save Options 保存</li>\n</ul>\n<p>生成的 .emacs 目录还是在 C:\\Users&lt;username&gt;\\AppData\\Roaming 下。打开 C:\\Users&lt;username&gt;\\AppData\\Roaming.emacs 配置文件，添加如下配置：</p>\n<pre><code>(load-file &quot;D:/apps/emacs/.emacs&quot;)\n</code></pre>\n<p>Emacs在启动的时候会加载 C:\\Users&lt;username&gt;\\AppData\\Roaming.emacs 这个配置文件，而该文件又加载 D:/apps/emacs/.emacs 配置文件。这样不用进入 C:\\Users&lt;username&gt;\\AppData\\Roaming.emacs 这个冗的路径了。</p>\n<p>使用以下配置修改 HOME PATH 配置项：</p>\n<pre><code>(setenv &quot;HOME&quot; &quot;D:/emacs-23.2&quot;)\n(setenv &quot;PATH&quot; &quot;D:/emacs-23.2&quot;)\n;;set the default file path\n(setq default-directory &quot;~/&quot;)\n(setq load-path (cons &quot;~/.emacs.d/elisp&quot; load-path))\n</code></pre>\n<h3 id=\"解决中文乱码\"><a href=\"#解决中文乱码\" class=\"headerlink\" title=\"解决中文乱码\"></a>解决中文乱码</h3><p>在 .emacs 文件中加入如下配置：</p>\n<pre><code>;; 编码设置 begin\n(set-language-environment &#39;Chinese-GB)\n\n;; default-buffer-file-coding-system变量在emacs23.2之后已被废弃，使用buffer-file-coding-system代替\n(set-default buffer-file-coding-system &#39;utf-8-unix)\n(set-default-coding-systems &#39;utf-8-unix)\n(setq-default pathname-coding-system &#39;euc-cn)\n(setq file-name-coding-system &#39;euc-cn)\n\n;; 另外建议按下面的先后顺序来设置中文编码识别方式。\n;; 重要提示:写在最后一行的，实际上最优先使用; 最前面一行，反而放到最后才识别。\n;; utf-16le-with-signature 相当于 Windows 下的 Unicode 编码，这里也可写成\n;; utf-16 (utf-16 实际上还细分为 utf-16le, utf-16be, utf-16le-with-signature等多种)\n(prefer-coding-system &#39;cp950)\n(prefer-coding-system &#39;gb2312)\n(prefer-coding-system &#39;cp936)\n(prefer-coding-system &#39;gb18030)\n\n;(prefer-coding-system &#39;utf-16le-with-signature)\n(prefer-coding-system &#39;utf-16)\n\n;; 新建文件使用utf-8-unix方式\n;; 如果不写下面两句，只写\n;; (prefer-coding-system &#39;utf-8)\n;; 这一句的话，新建文件以utf-8编码，行末结束符平台相关\n(prefer-coding-system &#39;utf-8-dos)\n(prefer-coding-system &#39;utf-8-unix)\n;; 编码设置 end\n</code></pre>"},{"title":"Windows7 上构建并安装 Hadoop 2.7.3","date":"2017-02-11T14:14:12.000Z","_content":"\n### 构建 Hadoop Windows 版本\n\n不要尝试从 Cygwin 中运行安装。Cygwin 既不是必须的也不被支持的。\n\n#### 选择 Java 版本并设置 JAVA_HOME\n\nHadoop 开发者已经测试了 Oracle JDK 1.7 和 1.6，并且已知可以正常工作的版本。\n\n确保设置了 JAVA_HOME，并且不包含任何空字符。\n\n<!-- more -->\n\n#### 获取 Hadoop 源代码\n\n下载 Hadoop 2.7.3 源代码。下载地址：<http://hadoop.apache.org/releases.html>\n\n#### 安装依赖并配置构建环境\n\n请阅读博文“[Window7 搭建 Hadoop-2.7.3 源码阅读环境问题解决列表](http://zhang-jc.github.io/2017/02/11/Window7-%E6%90%AD%E5%BB%BA-Hadoop-2-7-3-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E7%8E%AF%E5%A2%83%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E5%88%97%E8%A1%A8/)”。我环境及依赖列表如下：\n\n- Windows 7\n- java version \"1.7.0_80\"\n- Apache Maven 3.2.3\n- ProtocolBuffer 2.5.0\n- cmake version 3.7.2 win64 x64\n- Windows SDK 7.1\n\n#### 构建并拷贝打包文件\n\n在源代码根目录下运行下面的命令构建二进制发布代码。\n\n    mvn package -Pdist,native-win -DskipTests -Dtar\n\n注意这个命令像 BUILDING.txt 文档中要求的必须从 Windows SDK command prompt 窗口运行。成功构建后会在 hadoop-dist\\target\\ 目录下生成一个 hadoop.tar.gz 二进制包。\n\nHadoop 版本号会出现在包文件名中。构建不同的版本则报名也会不一样。\n\n#### 安装\n\n选择一个安装的目标目录。用 c:\\deploy 作为示例。解压 tar.gz 文件（hadoop-2.7.3.tar.gz）到 c:\\deploy 下。这将生成一个如下结构的目录。如果安装一个多节点的集群，那么在每台节点上重复该步骤。\n\n    C:\\deploy>dir\n     Volume in drive C has no label.\n     Volume Serial Number is 9D1F-7BAC\n\n     Directory of C:\\deploy\n\n    01/18/2014  08:11 AM    <DIR>          .\n    01/18/2014  08:11 AM    <DIR>          ..\n    01/18/2014  08:28 AM    <DIR>          bin\n    01/18/2014  08:28 AM    <DIR>          etc\n    01/18/2014  08:28 AM    <DIR>          include\n    01/18/2014  08:28 AM    <DIR>          libexec\n    01/18/2014  08:28 AM    <DIR>          sbin\n    01/18/2014  08:28 AM    <DIR>          share\n                   0 File(s)              0 bytes\n\n### 启动一个单节点（伪分布式）集群\n#### HDFS 配置示例\n\n在可以启动 Hadoop 守护进程之前，需要编辑几个配置文件。配置文件模板可以在 c:\\deploy\\etc\\hadoop 下找到，假设你的安装目录是 c:\\deploy。\n\n首先编辑文件 hadoop-env.cmd，在文件末尾添加下面的内容：\n\n    set HADOOP_PREFIX=c:\\deploy\n    set HADOOP_CONF_DIR=%HADOOP_PREFIX%\\etc\\hadoop\n    set YARN_CONF_DIR=%HADOOP_CONF_DIR%\n    set PATH=%PATH%;%HADOOP_PREFIX%\\bin\n\n编辑或创建文件 core-site.xml，并确保文件有下面的配置：\n\n    <configuration>\n      <property>\n        <name>fs.default.name</name>\n        <value>hdfs://0.0.0.0:19000</value>\n      </property>\n    </configuration>\n\n编辑或创建文件 hdfs-site.xml，并添加下面的配置：\n\n    <configuration>\n      <property>\n        <name>dfs.replication</name>\n        <value>1</value>\n      </property>\n    </configuration>\n\n最后，编辑或创建文件 slaves，并确保有下面的配置：\n\n    localhost\n\n按照默认配置 HDFS 元数据及数据文件放在当前磁盘的 \\tmp 目录下。在上面的示例中这个目录是 c:\\tmp。作为第一次测试安装可以保留默认配置。\n\n#### YARN 配置示例\n\n编辑或创建 %HADOOP_PREFIX%\\etc\\hadoop 下的文件 mapred-site.xml，并添加下面的配置，用你的 Windows 用户名替换 %USERNAME%。\n\n    <configuration>\n\n       <property>\n         <name>mapreduce.job.user.name</name>\n         <value>%USERNAME%</value>\n       </property>\n\n       <property>\n         <name>mapreduce.framework.name</name>\n         <value>yarn</value>\n       </property>\n\n      <property>\n        <name>yarn.apps.stagingDir</name>\n        <value>/user/%USERNAME%/staging</value>\n      </property>\n\n      <property>\n        <name>mapreduce.jobtracker.address</name>\n        <value>local</value>\n      </property>\n\n    </configuration>\n\n最后，编辑或创建文件 yarn-site.xml，并添加下面的配置：\n\n    <configuration>\n      <property>\n        <name>yarn.server.resourcemanager.address</name>\n        <value>0.0.0.0:8020</value>\n      </property>\n\n      <property>\n        <name>yarn.server.resourcemanager.application.expiry.interval</name>\n        <value>60000</value>\n      </property>\n\n      <property>\n        <name>yarn.server.nodemanager.address</name>\n        <value>0.0.0.0:45454</value>\n      </property>\n\n      <property>\n        <name>yarn.nodemanager.aux-services</name>\n        <value>mapreduce_shuffle</value>\n      </property>\n\n      <property>\n        <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>\n        <value>org.apache.hadoop.mapred.ShuffleHandler</value>\n      </property>\n\n      <property>\n        <name>yarn.server.nodemanager.remote-app-log-dir</name>\n        <value>/app-logs</value>\n      </property>\n\n      <property>\n        <name>yarn.nodemanager.log-dirs</name>\n        <value>/dep/logs/userlogs</value>\n      </property>\n\n      <property>\n        <name>yarn.server.mapreduce-appmanager.attempt-listener.bindAddress</name>\n        <value>0.0.0.0</value>\n      </property>\n\n      <property>\n        <name>yarn.server.mapreduce-appmanager.client-service.bindAddress</name>\n        <value>0.0.0.0</value>\n      </property>\n\n      <property>\n        <name>yarn.log-aggregation-enable</name>\n        <value>true</value>\n      </property>\n\n      <property>\n        <name>yarn.log-aggregation.retain-seconds</name>\n        <value>-1</value>\n      </property>\n\n      <property>\n        <name>yarn.application.classpath</name>\n        <value>%HADOOP_CONF_DIR%,%HADOOP_COMMON_HOME%/share/hadoop/common/*,%HADOOP_COMMON_HOME%/share/hadoop/common/lib/*,%HADOOP_HDFS_HOME%/share/hadoop/hdfs/*,%HADOOP_HDFS_HOME%/share/hadoop/hdfs/lib/*,%HADOOP_MAPRED_HOME%/share/hadoop/mapreduce/*,%HADOOP_MAPRED_HOME%/share/hadoop/mapreduce/lib/*,%HADOOP_YARN_HOME%/share/hadoop/yarn/*,%HADOOP_YARN_HOME%/share/hadoop/yarn/lib/*</value>\n      </property>\n    </configuration>\n\n#### 初始化环境变量\n\n运行 c:\\deploy\\etc\\hadoop\\hadoop-env.cmd 设置启动脚本及守护进程使用的环境变量。\n\n#### 格式化文件系统\n\n用下面的命令格式化文件系统：\n\n    %HADOOP_PREFIX%\\bin\\hdfs namenode -format\n\n这个命令将打印文件系统的参数。查找下面的两个字符串确保格式化名称执行成功。\n\n    14/01/18 08:36:23 INFO namenode.FSImage: Saving image file \\tmp\\hadoop-username\\dfs\\name\\current\\fsimage.ckpt_0000000000000000000 using no compression\n    14/01/18 08:36:23 INFO namenode.FSImage: Image file \\tmp\\hadoop-username\\dfs\\name\\current\\fsimage.ckpt_0000000000000000000 of size 200 bytes saved in 0 seconds.\n\n#### 启动 HDFS 守护进程\n\n运行下面的命令在本机启动 NameNode 和 DataNode。\n\n    %HADOOP_PREFIX%\\sbin\\start-dfs.cmd\n\n为了验证 HDFS 守护进程已经运行，试着拷贝一个文件到 HDFS。\n\n    C:\\deploy>%HADOOP_PREFIX%\\bin\\hdfs dfs -put myfile.txt /\n\n    C:\\deploy>%HADOOP_PREFIX%\\bin\\hdfs dfs -ls /\n    Found 1 items\n    drwxr-xr-x   - username supergroup          4640 2014-01-18 08:40 /myfile.txt\n\n    C:\\deploy>\n\n#### 启动 YARN 守护进程并运行一个 YARN 任务\n\n最后，启动 YARN 守护进程。\n\n    %HADOOP_PREFIX%\\sbin\\start-yarn.cmd\n\n集群应该已经启动并运行了。为了验证，我们可以在刚拷贝到 HDFS 上的文件上运行一个 wordcount 的示例任务。\n\n    %HADOOP_PREFIX%\\bin\\yarn jar %HADOOP_PREFIX%\\share\\hadoop\\mapreduce\\hadoop-mapreduce-examples-2.5.0.jar wordcount /myfile.txt /out\n\n### 远程调试\n\n现在可以在自己的个人电脑上跟踪 Hadoop 源代码，并远程调试本机上的 Hadoop。具体做法阅读我的另外一篇博文“[调试 Hadoop 源代码](http://zhang-jc.github.io/2016/09/11/%E8%B0%83%E8%AF%95-Hadoop-%E6%BA%90%E4%BB%A3%E7%A0%81/)”。\n","source":"_posts/Windows7-上构建并安装-Hadoop-2-7-3.md","raw":"title: Windows7 上构建并安装 Hadoop 2.7.3\ntags:\n  - 大数据\n  - Hadoop\ncategories:\n  - 大数据\n  - Hadoop\ndate: 2017-02-11 22:14:12\n---\n\n### 构建 Hadoop Windows 版本\n\n不要尝试从 Cygwin 中运行安装。Cygwin 既不是必须的也不被支持的。\n\n#### 选择 Java 版本并设置 JAVA_HOME\n\nHadoop 开发者已经测试了 Oracle JDK 1.7 和 1.6，并且已知可以正常工作的版本。\n\n确保设置了 JAVA_HOME，并且不包含任何空字符。\n\n<!-- more -->\n\n#### 获取 Hadoop 源代码\n\n下载 Hadoop 2.7.3 源代码。下载地址：<http://hadoop.apache.org/releases.html>\n\n#### 安装依赖并配置构建环境\n\n请阅读博文“[Window7 搭建 Hadoop-2.7.3 源码阅读环境问题解决列表](http://zhang-jc.github.io/2017/02/11/Window7-%E6%90%AD%E5%BB%BA-Hadoop-2-7-3-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E7%8E%AF%E5%A2%83%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E5%88%97%E8%A1%A8/)”。我环境及依赖列表如下：\n\n- Windows 7\n- java version \"1.7.0_80\"\n- Apache Maven 3.2.3\n- ProtocolBuffer 2.5.0\n- cmake version 3.7.2 win64 x64\n- Windows SDK 7.1\n\n#### 构建并拷贝打包文件\n\n在源代码根目录下运行下面的命令构建二进制发布代码。\n\n    mvn package -Pdist,native-win -DskipTests -Dtar\n\n注意这个命令像 BUILDING.txt 文档中要求的必须从 Windows SDK command prompt 窗口运行。成功构建后会在 hadoop-dist\\target\\ 目录下生成一个 hadoop.tar.gz 二进制包。\n\nHadoop 版本号会出现在包文件名中。构建不同的版本则报名也会不一样。\n\n#### 安装\n\n选择一个安装的目标目录。用 c:\\deploy 作为示例。解压 tar.gz 文件（hadoop-2.7.3.tar.gz）到 c:\\deploy 下。这将生成一个如下结构的目录。如果安装一个多节点的集群，那么在每台节点上重复该步骤。\n\n    C:\\deploy>dir\n     Volume in drive C has no label.\n     Volume Serial Number is 9D1F-7BAC\n\n     Directory of C:\\deploy\n\n    01/18/2014  08:11 AM    <DIR>          .\n    01/18/2014  08:11 AM    <DIR>          ..\n    01/18/2014  08:28 AM    <DIR>          bin\n    01/18/2014  08:28 AM    <DIR>          etc\n    01/18/2014  08:28 AM    <DIR>          include\n    01/18/2014  08:28 AM    <DIR>          libexec\n    01/18/2014  08:28 AM    <DIR>          sbin\n    01/18/2014  08:28 AM    <DIR>          share\n                   0 File(s)              0 bytes\n\n### 启动一个单节点（伪分布式）集群\n#### HDFS 配置示例\n\n在可以启动 Hadoop 守护进程之前，需要编辑几个配置文件。配置文件模板可以在 c:\\deploy\\etc\\hadoop 下找到，假设你的安装目录是 c:\\deploy。\n\n首先编辑文件 hadoop-env.cmd，在文件末尾添加下面的内容：\n\n    set HADOOP_PREFIX=c:\\deploy\n    set HADOOP_CONF_DIR=%HADOOP_PREFIX%\\etc\\hadoop\n    set YARN_CONF_DIR=%HADOOP_CONF_DIR%\n    set PATH=%PATH%;%HADOOP_PREFIX%\\bin\n\n编辑或创建文件 core-site.xml，并确保文件有下面的配置：\n\n    <configuration>\n      <property>\n        <name>fs.default.name</name>\n        <value>hdfs://0.0.0.0:19000</value>\n      </property>\n    </configuration>\n\n编辑或创建文件 hdfs-site.xml，并添加下面的配置：\n\n    <configuration>\n      <property>\n        <name>dfs.replication</name>\n        <value>1</value>\n      </property>\n    </configuration>\n\n最后，编辑或创建文件 slaves，并确保有下面的配置：\n\n    localhost\n\n按照默认配置 HDFS 元数据及数据文件放在当前磁盘的 \\tmp 目录下。在上面的示例中这个目录是 c:\\tmp。作为第一次测试安装可以保留默认配置。\n\n#### YARN 配置示例\n\n编辑或创建 %HADOOP_PREFIX%\\etc\\hadoop 下的文件 mapred-site.xml，并添加下面的配置，用你的 Windows 用户名替换 %USERNAME%。\n\n    <configuration>\n\n       <property>\n         <name>mapreduce.job.user.name</name>\n         <value>%USERNAME%</value>\n       </property>\n\n       <property>\n         <name>mapreduce.framework.name</name>\n         <value>yarn</value>\n       </property>\n\n      <property>\n        <name>yarn.apps.stagingDir</name>\n        <value>/user/%USERNAME%/staging</value>\n      </property>\n\n      <property>\n        <name>mapreduce.jobtracker.address</name>\n        <value>local</value>\n      </property>\n\n    </configuration>\n\n最后，编辑或创建文件 yarn-site.xml，并添加下面的配置：\n\n    <configuration>\n      <property>\n        <name>yarn.server.resourcemanager.address</name>\n        <value>0.0.0.0:8020</value>\n      </property>\n\n      <property>\n        <name>yarn.server.resourcemanager.application.expiry.interval</name>\n        <value>60000</value>\n      </property>\n\n      <property>\n        <name>yarn.server.nodemanager.address</name>\n        <value>0.0.0.0:45454</value>\n      </property>\n\n      <property>\n        <name>yarn.nodemanager.aux-services</name>\n        <value>mapreduce_shuffle</value>\n      </property>\n\n      <property>\n        <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>\n        <value>org.apache.hadoop.mapred.ShuffleHandler</value>\n      </property>\n\n      <property>\n        <name>yarn.server.nodemanager.remote-app-log-dir</name>\n        <value>/app-logs</value>\n      </property>\n\n      <property>\n        <name>yarn.nodemanager.log-dirs</name>\n        <value>/dep/logs/userlogs</value>\n      </property>\n\n      <property>\n        <name>yarn.server.mapreduce-appmanager.attempt-listener.bindAddress</name>\n        <value>0.0.0.0</value>\n      </property>\n\n      <property>\n        <name>yarn.server.mapreduce-appmanager.client-service.bindAddress</name>\n        <value>0.0.0.0</value>\n      </property>\n\n      <property>\n        <name>yarn.log-aggregation-enable</name>\n        <value>true</value>\n      </property>\n\n      <property>\n        <name>yarn.log-aggregation.retain-seconds</name>\n        <value>-1</value>\n      </property>\n\n      <property>\n        <name>yarn.application.classpath</name>\n        <value>%HADOOP_CONF_DIR%,%HADOOP_COMMON_HOME%/share/hadoop/common/*,%HADOOP_COMMON_HOME%/share/hadoop/common/lib/*,%HADOOP_HDFS_HOME%/share/hadoop/hdfs/*,%HADOOP_HDFS_HOME%/share/hadoop/hdfs/lib/*,%HADOOP_MAPRED_HOME%/share/hadoop/mapreduce/*,%HADOOP_MAPRED_HOME%/share/hadoop/mapreduce/lib/*,%HADOOP_YARN_HOME%/share/hadoop/yarn/*,%HADOOP_YARN_HOME%/share/hadoop/yarn/lib/*</value>\n      </property>\n    </configuration>\n\n#### 初始化环境变量\n\n运行 c:\\deploy\\etc\\hadoop\\hadoop-env.cmd 设置启动脚本及守护进程使用的环境变量。\n\n#### 格式化文件系统\n\n用下面的命令格式化文件系统：\n\n    %HADOOP_PREFIX%\\bin\\hdfs namenode -format\n\n这个命令将打印文件系统的参数。查找下面的两个字符串确保格式化名称执行成功。\n\n    14/01/18 08:36:23 INFO namenode.FSImage: Saving image file \\tmp\\hadoop-username\\dfs\\name\\current\\fsimage.ckpt_0000000000000000000 using no compression\n    14/01/18 08:36:23 INFO namenode.FSImage: Image file \\tmp\\hadoop-username\\dfs\\name\\current\\fsimage.ckpt_0000000000000000000 of size 200 bytes saved in 0 seconds.\n\n#### 启动 HDFS 守护进程\n\n运行下面的命令在本机启动 NameNode 和 DataNode。\n\n    %HADOOP_PREFIX%\\sbin\\start-dfs.cmd\n\n为了验证 HDFS 守护进程已经运行，试着拷贝一个文件到 HDFS。\n\n    C:\\deploy>%HADOOP_PREFIX%\\bin\\hdfs dfs -put myfile.txt /\n\n    C:\\deploy>%HADOOP_PREFIX%\\bin\\hdfs dfs -ls /\n    Found 1 items\n    drwxr-xr-x   - username supergroup          4640 2014-01-18 08:40 /myfile.txt\n\n    C:\\deploy>\n\n#### 启动 YARN 守护进程并运行一个 YARN 任务\n\n最后，启动 YARN 守护进程。\n\n    %HADOOP_PREFIX%\\sbin\\start-yarn.cmd\n\n集群应该已经启动并运行了。为了验证，我们可以在刚拷贝到 HDFS 上的文件上运行一个 wordcount 的示例任务。\n\n    %HADOOP_PREFIX%\\bin\\yarn jar %HADOOP_PREFIX%\\share\\hadoop\\mapreduce\\hadoop-mapreduce-examples-2.5.0.jar wordcount /myfile.txt /out\n\n### 远程调试\n\n现在可以在自己的个人电脑上跟踪 Hadoop 源代码，并远程调试本机上的 Hadoop。具体做法阅读我的另外一篇博文“[调试 Hadoop 源代码](http://zhang-jc.github.io/2016/09/11/%E8%B0%83%E8%AF%95-Hadoop-%E6%BA%90%E4%BB%A3%E7%A0%81/)”。\n","slug":"Windows7-上构建并安装-Hadoop-2-7-3","published":1,"updated":"2021-07-19T16:28:00.296Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphqq00cuitd395x22jl5","content":"<h3 id=\"构建-Hadoop-Windows-版本\"><a href=\"#构建-Hadoop-Windows-版本\" class=\"headerlink\" title=\"构建 Hadoop Windows 版本\"></a>构建 Hadoop Windows 版本</h3><p>不要尝试从 Cygwin 中运行安装。Cygwin 既不是必须的也不被支持的。</p>\n<h4 id=\"选择-Java-版本并设置-JAVA-HOME\"><a href=\"#选择-Java-版本并设置-JAVA-HOME\" class=\"headerlink\" title=\"选择 Java 版本并设置 JAVA_HOME\"></a>选择 Java 版本并设置 JAVA_HOME</h4><p>Hadoop 开发者已经测试了 Oracle JDK 1.7 和 1.6，并且已知可以正常工作的版本。</p>\n<p>确保设置了 JAVA_HOME，并且不包含任何空字符。</p>\n<span id=\"more\"></span>\n\n<h4 id=\"获取-Hadoop-源代码\"><a href=\"#获取-Hadoop-源代码\" class=\"headerlink\" title=\"获取 Hadoop 源代码\"></a>获取 Hadoop 源代码</h4><p>下载 Hadoop 2.7.3 源代码。下载地址：<a href=\"http://hadoop.apache.org/releases.html\">http://hadoop.apache.org/releases.html</a></p>\n<h4 id=\"安装依赖并配置构建环境\"><a href=\"#安装依赖并配置构建环境\" class=\"headerlink\" title=\"安装依赖并配置构建环境\"></a>安装依赖并配置构建环境</h4><p>请阅读博文“<a href=\"http://zhang-jc.github.io/2017/02/11/Window7-%E6%90%AD%E5%BB%BA-Hadoop-2-7-3-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E7%8E%AF%E5%A2%83%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E5%88%97%E8%A1%A8/\">Window7 搭建 Hadoop-2.7.3 源码阅读环境问题解决列表</a>”。我环境及依赖列表如下：</p>\n<ul>\n<li>Windows 7</li>\n<li>java version “1.7.0_80”</li>\n<li>Apache Maven 3.2.3</li>\n<li>ProtocolBuffer 2.5.0</li>\n<li>cmake version 3.7.2 win64 x64</li>\n<li>Windows SDK 7.1</li>\n</ul>\n<h4 id=\"构建并拷贝打包文件\"><a href=\"#构建并拷贝打包文件\" class=\"headerlink\" title=\"构建并拷贝打包文件\"></a>构建并拷贝打包文件</h4><p>在源代码根目录下运行下面的命令构建二进制发布代码。</p>\n<pre><code>mvn package -Pdist,native-win -DskipTests -Dtar\n</code></pre>\n<p>注意这个命令像 BUILDING.txt 文档中要求的必须从 Windows SDK command prompt 窗口运行。成功构建后会在 hadoop-dist\\target\\ 目录下生成一个 hadoop.tar.gz 二进制包。</p>\n<p>Hadoop 版本号会出现在包文件名中。构建不同的版本则报名也会不一样。</p>\n<h4 id=\"安装\"><a href=\"#安装\" class=\"headerlink\" title=\"安装\"></a>安装</h4><p>选择一个安装的目标目录。用 c:\\deploy 作为示例。解压 tar.gz 文件（hadoop-2.7.3.tar.gz）到 c:\\deploy 下。这将生成一个如下结构的目录。如果安装一个多节点的集群，那么在每台节点上重复该步骤。</p>\n<pre><code>C:\\deploy&gt;dir\n Volume in drive C has no label.\n Volume Serial Number is 9D1F-7BAC\n\n Directory of C:\\deploy\n\n01/18/2014  08:11 AM    &lt;DIR&gt;          .\n01/18/2014  08:11 AM    &lt;DIR&gt;          ..\n01/18/2014  08:28 AM    &lt;DIR&gt;          bin\n01/18/2014  08:28 AM    &lt;DIR&gt;          etc\n01/18/2014  08:28 AM    &lt;DIR&gt;          include\n01/18/2014  08:28 AM    &lt;DIR&gt;          libexec\n01/18/2014  08:28 AM    &lt;DIR&gt;          sbin\n01/18/2014  08:28 AM    &lt;DIR&gt;          share\n               0 File(s)              0 bytes\n</code></pre>\n<h3 id=\"启动一个单节点（伪分布式）集群\"><a href=\"#启动一个单节点（伪分布式）集群\" class=\"headerlink\" title=\"启动一个单节点（伪分布式）集群\"></a>启动一个单节点（伪分布式）集群</h3><h4 id=\"HDFS-配置示例\"><a href=\"#HDFS-配置示例\" class=\"headerlink\" title=\"HDFS 配置示例\"></a>HDFS 配置示例</h4><p>在可以启动 Hadoop 守护进程之前，需要编辑几个配置文件。配置文件模板可以在 c:\\deploy\\etc\\hadoop 下找到，假设你的安装目录是 c:\\deploy。</p>\n<p>首先编辑文件 hadoop-env.cmd，在文件末尾添加下面的内容：</p>\n<pre><code>set HADOOP_PREFIX=c:\\deploy\nset HADOOP_CONF_DIR=%HADOOP_PREFIX%\\etc\\hadoop\nset YARN_CONF_DIR=%HADOOP_CONF_DIR%\nset PATH=%PATH%;%HADOOP_PREFIX%\\bin\n</code></pre>\n<p>编辑或创建文件 core-site.xml，并确保文件有下面的配置：</p>\n<pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;fs.default.name&lt;/name&gt;\n    &lt;value&gt;hdfs://0.0.0.0:19000&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<p>编辑或创建文件 hdfs-site.xml，并添加下面的配置：</p>\n<pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.replication&lt;/name&gt;\n    &lt;value&gt;1&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<p>最后，编辑或创建文件 slaves，并确保有下面的配置：</p>\n<pre><code>localhost\n</code></pre>\n<p>按照默认配置 HDFS 元数据及数据文件放在当前磁盘的 \\tmp 目录下。在上面的示例中这个目录是 c:\\tmp。作为第一次测试安装可以保留默认配置。</p>\n<h4 id=\"YARN-配置示例\"><a href=\"#YARN-配置示例\" class=\"headerlink\" title=\"YARN 配置示例\"></a>YARN 配置示例</h4><p>编辑或创建 %HADOOP_PREFIX%\\etc\\hadoop 下的文件 mapred-site.xml，并添加下面的配置，用你的 Windows 用户名替换 %USERNAME%。</p>\n<pre><code>&lt;configuration&gt;\n\n   &lt;property&gt;\n     &lt;name&gt;mapreduce.job.user.name&lt;/name&gt;\n     &lt;value&gt;%USERNAME%&lt;/value&gt;\n   &lt;/property&gt;\n\n   &lt;property&gt;\n     &lt;name&gt;mapreduce.framework.name&lt;/name&gt;\n     &lt;value&gt;yarn&lt;/value&gt;\n   &lt;/property&gt;\n\n  &lt;property&gt;\n    &lt;name&gt;yarn.apps.stagingDir&lt;/name&gt;\n    &lt;value&gt;/user/%USERNAME%/staging&lt;/value&gt;\n  &lt;/property&gt;\n\n  &lt;property&gt;\n    &lt;name&gt;mapreduce.jobtracker.address&lt;/name&gt;\n    &lt;value&gt;local&lt;/value&gt;\n  &lt;/property&gt;\n\n&lt;/configuration&gt;\n</code></pre>\n<p>最后，编辑或创建文件 yarn-site.xml，并添加下面的配置：</p>\n<pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.server.resourcemanager.address&lt;/name&gt;\n    &lt;value&gt;0.0.0.0:8020&lt;/value&gt;\n  &lt;/property&gt;\n\n  &lt;property&gt;\n    &lt;name&gt;yarn.server.resourcemanager.application.expiry.interval&lt;/name&gt;\n    &lt;value&gt;60000&lt;/value&gt;\n  &lt;/property&gt;\n\n  &lt;property&gt;\n    &lt;name&gt;yarn.server.nodemanager.address&lt;/name&gt;\n    &lt;value&gt;0.0.0.0:45454&lt;/value&gt;\n  &lt;/property&gt;\n\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n  &lt;/property&gt;\n\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;\n    &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;\n  &lt;/property&gt;\n\n  &lt;property&gt;\n    &lt;name&gt;yarn.server.nodemanager.remote-app-log-dir&lt;/name&gt;\n    &lt;value&gt;/app-logs&lt;/value&gt;\n  &lt;/property&gt;\n\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.log-dirs&lt;/name&gt;\n    &lt;value&gt;/dep/logs/userlogs&lt;/value&gt;\n  &lt;/property&gt;\n\n  &lt;property&gt;\n    &lt;name&gt;yarn.server.mapreduce-appmanager.attempt-listener.bindAddress&lt;/name&gt;\n    &lt;value&gt;0.0.0.0&lt;/value&gt;\n  &lt;/property&gt;\n\n  &lt;property&gt;\n    &lt;name&gt;yarn.server.mapreduce-appmanager.client-service.bindAddress&lt;/name&gt;\n    &lt;value&gt;0.0.0.0&lt;/value&gt;\n  &lt;/property&gt;\n\n  &lt;property&gt;\n    &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n\n  &lt;property&gt;\n    &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;\n    &lt;value&gt;-1&lt;/value&gt;\n  &lt;/property&gt;\n\n  &lt;property&gt;\n    &lt;name&gt;yarn.application.classpath&lt;/name&gt;\n    &lt;value&gt;%HADOOP_CONF_DIR%,%HADOOP_COMMON_HOME%/share/hadoop/common/*,%HADOOP_COMMON_HOME%/share/hadoop/common/lib/*,%HADOOP_HDFS_HOME%/share/hadoop/hdfs/*,%HADOOP_HDFS_HOME%/share/hadoop/hdfs/lib/*,%HADOOP_MAPRED_HOME%/share/hadoop/mapreduce/*,%HADOOP_MAPRED_HOME%/share/hadoop/mapreduce/lib/*,%HADOOP_YARN_HOME%/share/hadoop/yarn/*,%HADOOP_YARN_HOME%/share/hadoop/yarn/lib/*&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<h4 id=\"初始化环境变量\"><a href=\"#初始化环境变量\" class=\"headerlink\" title=\"初始化环境变量\"></a>初始化环境变量</h4><p>运行 c:\\deploy\\etc\\hadoop\\hadoop-env.cmd 设置启动脚本及守护进程使用的环境变量。</p>\n<h4 id=\"格式化文件系统\"><a href=\"#格式化文件系统\" class=\"headerlink\" title=\"格式化文件系统\"></a>格式化文件系统</h4><p>用下面的命令格式化文件系统：</p>\n<pre><code>%HADOOP_PREFIX%\\bin\\hdfs namenode -format\n</code></pre>\n<p>这个命令将打印文件系统的参数。查找下面的两个字符串确保格式化名称执行成功。</p>\n<pre><code>14/01/18 08:36:23 INFO namenode.FSImage: Saving image file \\tmp\\hadoop-username\\dfs\\name\\current\\fsimage.ckpt_0000000000000000000 using no compression\n14/01/18 08:36:23 INFO namenode.FSImage: Image file \\tmp\\hadoop-username\\dfs\\name\\current\\fsimage.ckpt_0000000000000000000 of size 200 bytes saved in 0 seconds.\n</code></pre>\n<h4 id=\"启动-HDFS-守护进程\"><a href=\"#启动-HDFS-守护进程\" class=\"headerlink\" title=\"启动 HDFS 守护进程\"></a>启动 HDFS 守护进程</h4><p>运行下面的命令在本机启动 NameNode 和 DataNode。</p>\n<pre><code>%HADOOP_PREFIX%\\sbin\\start-dfs.cmd\n</code></pre>\n<p>为了验证 HDFS 守护进程已经运行，试着拷贝一个文件到 HDFS。</p>\n<pre><code>C:\\deploy&gt;%HADOOP_PREFIX%\\bin\\hdfs dfs -put myfile.txt /\n\nC:\\deploy&gt;%HADOOP_PREFIX%\\bin\\hdfs dfs -ls /\nFound 1 items\ndrwxr-xr-x   - username supergroup          4640 2014-01-18 08:40 /myfile.txt\n\nC:\\deploy&gt;\n</code></pre>\n<h4 id=\"启动-YARN-守护进程并运行一个-YARN-任务\"><a href=\"#启动-YARN-守护进程并运行一个-YARN-任务\" class=\"headerlink\" title=\"启动 YARN 守护进程并运行一个 YARN 任务\"></a>启动 YARN 守护进程并运行一个 YARN 任务</h4><p>最后，启动 YARN 守护进程。</p>\n<pre><code>%HADOOP_PREFIX%\\sbin\\start-yarn.cmd\n</code></pre>\n<p>集群应该已经启动并运行了。为了验证，我们可以在刚拷贝到 HDFS 上的文件上运行一个 wordcount 的示例任务。</p>\n<pre><code>%HADOOP_PREFIX%\\bin\\yarn jar %HADOOP_PREFIX%\\share\\hadoop\\mapreduce\\hadoop-mapreduce-examples-2.5.0.jar wordcount /myfile.txt /out\n</code></pre>\n<h3 id=\"远程调试\"><a href=\"#远程调试\" class=\"headerlink\" title=\"远程调试\"></a>远程调试</h3><p>现在可以在自己的个人电脑上跟踪 Hadoop 源代码，并远程调试本机上的 Hadoop。具体做法阅读我的另外一篇博文“<a href=\"http://zhang-jc.github.io/2016/09/11/%E8%B0%83%E8%AF%95-Hadoop-%E6%BA%90%E4%BB%A3%E7%A0%81/\">调试 Hadoop 源代码</a>”。</p>\n","site":{"data":{}},"excerpt":"<h3 id=\"构建-Hadoop-Windows-版本\"><a href=\"#构建-Hadoop-Windows-版本\" class=\"headerlink\" title=\"构建 Hadoop Windows 版本\"></a>构建 Hadoop Windows 版本</h3><p>不要尝试从 Cygwin 中运行安装。Cygwin 既不是必须的也不被支持的。</p>\n<h4 id=\"选择-Java-版本并设置-JAVA-HOME\"><a href=\"#选择-Java-版本并设置-JAVA-HOME\" class=\"headerlink\" title=\"选择 Java 版本并设置 JAVA_HOME\"></a>选择 Java 版本并设置 JAVA_HOME</h4><p>Hadoop 开发者已经测试了 Oracle JDK 1.7 和 1.6，并且已知可以正常工作的版本。</p>\n<p>确保设置了 JAVA_HOME，并且不包含任何空字符。</p>","more":"<h4 id=\"获取-Hadoop-源代码\"><a href=\"#获取-Hadoop-源代码\" class=\"headerlink\" title=\"获取 Hadoop 源代码\"></a>获取 Hadoop 源代码</h4><p>下载 Hadoop 2.7.3 源代码。下载地址：<a href=\"http://hadoop.apache.org/releases.html\">http://hadoop.apache.org/releases.html</a></p>\n<h4 id=\"安装依赖并配置构建环境\"><a href=\"#安装依赖并配置构建环境\" class=\"headerlink\" title=\"安装依赖并配置构建环境\"></a>安装依赖并配置构建环境</h4><p>请阅读博文“<a href=\"http://zhang-jc.github.io/2017/02/11/Window7-%E6%90%AD%E5%BB%BA-Hadoop-2-7-3-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E7%8E%AF%E5%A2%83%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E5%88%97%E8%A1%A8/\">Window7 搭建 Hadoop-2.7.3 源码阅读环境问题解决列表</a>”。我环境及依赖列表如下：</p>\n<ul>\n<li>Windows 7</li>\n<li>java version “1.7.0_80”</li>\n<li>Apache Maven 3.2.3</li>\n<li>ProtocolBuffer 2.5.0</li>\n<li>cmake version 3.7.2 win64 x64</li>\n<li>Windows SDK 7.1</li>\n</ul>\n<h4 id=\"构建并拷贝打包文件\"><a href=\"#构建并拷贝打包文件\" class=\"headerlink\" title=\"构建并拷贝打包文件\"></a>构建并拷贝打包文件</h4><p>在源代码根目录下运行下面的命令构建二进制发布代码。</p>\n<pre><code>mvn package -Pdist,native-win -DskipTests -Dtar\n</code></pre>\n<p>注意这个命令像 BUILDING.txt 文档中要求的必须从 Windows SDK command prompt 窗口运行。成功构建后会在 hadoop-dist\\target\\ 目录下生成一个 hadoop.tar.gz 二进制包。</p>\n<p>Hadoop 版本号会出现在包文件名中。构建不同的版本则报名也会不一样。</p>\n<h4 id=\"安装\"><a href=\"#安装\" class=\"headerlink\" title=\"安装\"></a>安装</h4><p>选择一个安装的目标目录。用 c:\\deploy 作为示例。解压 tar.gz 文件（hadoop-2.7.3.tar.gz）到 c:\\deploy 下。这将生成一个如下结构的目录。如果安装一个多节点的集群，那么在每台节点上重复该步骤。</p>\n<pre><code>C:\\deploy&gt;dir\n Volume in drive C has no label.\n Volume Serial Number is 9D1F-7BAC\n\n Directory of C:\\deploy\n\n01/18/2014  08:11 AM    &lt;DIR&gt;          .\n01/18/2014  08:11 AM    &lt;DIR&gt;          ..\n01/18/2014  08:28 AM    &lt;DIR&gt;          bin\n01/18/2014  08:28 AM    &lt;DIR&gt;          etc\n01/18/2014  08:28 AM    &lt;DIR&gt;          include\n01/18/2014  08:28 AM    &lt;DIR&gt;          libexec\n01/18/2014  08:28 AM    &lt;DIR&gt;          sbin\n01/18/2014  08:28 AM    &lt;DIR&gt;          share\n               0 File(s)              0 bytes\n</code></pre>\n<h3 id=\"启动一个单节点（伪分布式）集群\"><a href=\"#启动一个单节点（伪分布式）集群\" class=\"headerlink\" title=\"启动一个单节点（伪分布式）集群\"></a>启动一个单节点（伪分布式）集群</h3><h4 id=\"HDFS-配置示例\"><a href=\"#HDFS-配置示例\" class=\"headerlink\" title=\"HDFS 配置示例\"></a>HDFS 配置示例</h4><p>在可以启动 Hadoop 守护进程之前，需要编辑几个配置文件。配置文件模板可以在 c:\\deploy\\etc\\hadoop 下找到，假设你的安装目录是 c:\\deploy。</p>\n<p>首先编辑文件 hadoop-env.cmd，在文件末尾添加下面的内容：</p>\n<pre><code>set HADOOP_PREFIX=c:\\deploy\nset HADOOP_CONF_DIR=%HADOOP_PREFIX%\\etc\\hadoop\nset YARN_CONF_DIR=%HADOOP_CONF_DIR%\nset PATH=%PATH%;%HADOOP_PREFIX%\\bin\n</code></pre>\n<p>编辑或创建文件 core-site.xml，并确保文件有下面的配置：</p>\n<pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;fs.default.name&lt;/name&gt;\n    &lt;value&gt;hdfs://0.0.0.0:19000&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<p>编辑或创建文件 hdfs-site.xml，并添加下面的配置：</p>\n<pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.replication&lt;/name&gt;\n    &lt;value&gt;1&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<p>最后，编辑或创建文件 slaves，并确保有下面的配置：</p>\n<pre><code>localhost\n</code></pre>\n<p>按照默认配置 HDFS 元数据及数据文件放在当前磁盘的 \\tmp 目录下。在上面的示例中这个目录是 c:\\tmp。作为第一次测试安装可以保留默认配置。</p>\n<h4 id=\"YARN-配置示例\"><a href=\"#YARN-配置示例\" class=\"headerlink\" title=\"YARN 配置示例\"></a>YARN 配置示例</h4><p>编辑或创建 %HADOOP_PREFIX%\\etc\\hadoop 下的文件 mapred-site.xml，并添加下面的配置，用你的 Windows 用户名替换 %USERNAME%。</p>\n<pre><code>&lt;configuration&gt;\n\n   &lt;property&gt;\n     &lt;name&gt;mapreduce.job.user.name&lt;/name&gt;\n     &lt;value&gt;%USERNAME%&lt;/value&gt;\n   &lt;/property&gt;\n\n   &lt;property&gt;\n     &lt;name&gt;mapreduce.framework.name&lt;/name&gt;\n     &lt;value&gt;yarn&lt;/value&gt;\n   &lt;/property&gt;\n\n  &lt;property&gt;\n    &lt;name&gt;yarn.apps.stagingDir&lt;/name&gt;\n    &lt;value&gt;/user/%USERNAME%/staging&lt;/value&gt;\n  &lt;/property&gt;\n\n  &lt;property&gt;\n    &lt;name&gt;mapreduce.jobtracker.address&lt;/name&gt;\n    &lt;value&gt;local&lt;/value&gt;\n  &lt;/property&gt;\n\n&lt;/configuration&gt;\n</code></pre>\n<p>最后，编辑或创建文件 yarn-site.xml，并添加下面的配置：</p>\n<pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.server.resourcemanager.address&lt;/name&gt;\n    &lt;value&gt;0.0.0.0:8020&lt;/value&gt;\n  &lt;/property&gt;\n\n  &lt;property&gt;\n    &lt;name&gt;yarn.server.resourcemanager.application.expiry.interval&lt;/name&gt;\n    &lt;value&gt;60000&lt;/value&gt;\n  &lt;/property&gt;\n\n  &lt;property&gt;\n    &lt;name&gt;yarn.server.nodemanager.address&lt;/name&gt;\n    &lt;value&gt;0.0.0.0:45454&lt;/value&gt;\n  &lt;/property&gt;\n\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n  &lt;/property&gt;\n\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;\n    &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;\n  &lt;/property&gt;\n\n  &lt;property&gt;\n    &lt;name&gt;yarn.server.nodemanager.remote-app-log-dir&lt;/name&gt;\n    &lt;value&gt;/app-logs&lt;/value&gt;\n  &lt;/property&gt;\n\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.log-dirs&lt;/name&gt;\n    &lt;value&gt;/dep/logs/userlogs&lt;/value&gt;\n  &lt;/property&gt;\n\n  &lt;property&gt;\n    &lt;name&gt;yarn.server.mapreduce-appmanager.attempt-listener.bindAddress&lt;/name&gt;\n    &lt;value&gt;0.0.0.0&lt;/value&gt;\n  &lt;/property&gt;\n\n  &lt;property&gt;\n    &lt;name&gt;yarn.server.mapreduce-appmanager.client-service.bindAddress&lt;/name&gt;\n    &lt;value&gt;0.0.0.0&lt;/value&gt;\n  &lt;/property&gt;\n\n  &lt;property&gt;\n    &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt;\n  &lt;/property&gt;\n\n  &lt;property&gt;\n    &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;\n    &lt;value&gt;-1&lt;/value&gt;\n  &lt;/property&gt;\n\n  &lt;property&gt;\n    &lt;name&gt;yarn.application.classpath&lt;/name&gt;\n    &lt;value&gt;%HADOOP_CONF_DIR%,%HADOOP_COMMON_HOME%/share/hadoop/common/*,%HADOOP_COMMON_HOME%/share/hadoop/common/lib/*,%HADOOP_HDFS_HOME%/share/hadoop/hdfs/*,%HADOOP_HDFS_HOME%/share/hadoop/hdfs/lib/*,%HADOOP_MAPRED_HOME%/share/hadoop/mapreduce/*,%HADOOP_MAPRED_HOME%/share/hadoop/mapreduce/lib/*,%HADOOP_YARN_HOME%/share/hadoop/yarn/*,%HADOOP_YARN_HOME%/share/hadoop/yarn/lib/*&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>\n<h4 id=\"初始化环境变量\"><a href=\"#初始化环境变量\" class=\"headerlink\" title=\"初始化环境变量\"></a>初始化环境变量</h4><p>运行 c:\\deploy\\etc\\hadoop\\hadoop-env.cmd 设置启动脚本及守护进程使用的环境变量。</p>\n<h4 id=\"格式化文件系统\"><a href=\"#格式化文件系统\" class=\"headerlink\" title=\"格式化文件系统\"></a>格式化文件系统</h4><p>用下面的命令格式化文件系统：</p>\n<pre><code>%HADOOP_PREFIX%\\bin\\hdfs namenode -format\n</code></pre>\n<p>这个命令将打印文件系统的参数。查找下面的两个字符串确保格式化名称执行成功。</p>\n<pre><code>14/01/18 08:36:23 INFO namenode.FSImage: Saving image file \\tmp\\hadoop-username\\dfs\\name\\current\\fsimage.ckpt_0000000000000000000 using no compression\n14/01/18 08:36:23 INFO namenode.FSImage: Image file \\tmp\\hadoop-username\\dfs\\name\\current\\fsimage.ckpt_0000000000000000000 of size 200 bytes saved in 0 seconds.\n</code></pre>\n<h4 id=\"启动-HDFS-守护进程\"><a href=\"#启动-HDFS-守护进程\" class=\"headerlink\" title=\"启动 HDFS 守护进程\"></a>启动 HDFS 守护进程</h4><p>运行下面的命令在本机启动 NameNode 和 DataNode。</p>\n<pre><code>%HADOOP_PREFIX%\\sbin\\start-dfs.cmd\n</code></pre>\n<p>为了验证 HDFS 守护进程已经运行，试着拷贝一个文件到 HDFS。</p>\n<pre><code>C:\\deploy&gt;%HADOOP_PREFIX%\\bin\\hdfs dfs -put myfile.txt /\n\nC:\\deploy&gt;%HADOOP_PREFIX%\\bin\\hdfs dfs -ls /\nFound 1 items\ndrwxr-xr-x   - username supergroup          4640 2014-01-18 08:40 /myfile.txt\n\nC:\\deploy&gt;\n</code></pre>\n<h4 id=\"启动-YARN-守护进程并运行一个-YARN-任务\"><a href=\"#启动-YARN-守护进程并运行一个-YARN-任务\" class=\"headerlink\" title=\"启动 YARN 守护进程并运行一个 YARN 任务\"></a>启动 YARN 守护进程并运行一个 YARN 任务</h4><p>最后，启动 YARN 守护进程。</p>\n<pre><code>%HADOOP_PREFIX%\\sbin\\start-yarn.cmd\n</code></pre>\n<p>集群应该已经启动并运行了。为了验证，我们可以在刚拷贝到 HDFS 上的文件上运行一个 wordcount 的示例任务。</p>\n<pre><code>%HADOOP_PREFIX%\\bin\\yarn jar %HADOOP_PREFIX%\\share\\hadoop\\mapreduce\\hadoop-mapreduce-examples-2.5.0.jar wordcount /myfile.txt /out\n</code></pre>\n<h3 id=\"远程调试\"><a href=\"#远程调试\" class=\"headerlink\" title=\"远程调试\"></a>远程调试</h3><p>现在可以在自己的个人电脑上跟踪 Hadoop 源代码，并远程调试本机上的 Hadoop。具体做法阅读我的另外一篇博文“<a href=\"http://zhang-jc.github.io/2016/09/11/%E8%B0%83%E8%AF%95-Hadoop-%E6%BA%90%E4%BB%A3%E7%A0%81/\">调试 Hadoop 源代码</a>”。</p>"},{"title":"Zookeeper 单机操作","date":"2017-03-28T16:02:02.000Z","_content":"\n### 下载\n\n从 Apache 下载镜像站点下载当前稳定[发布版](http://zookeeper.apache.org/releases.html)。\n\n### 单机操作\n\n安装单机模式的 Zookeeper 服务器是简单明了的。服务器包含在一个单独的 JAR 文件中，因此安装包含创建配置。\n\n<!-- more -->\n\n下载稳定的 ZooKeeper 发布版后，解压并切换到包的根目录。\n\n启动 ZooKeeper 前需要一个配置文件。下面是示例配置文件，创建 conf/zoo.cfg：\n\n    # The number of milliseconds of each tick\n    tickTime=2000\n    # The number of ticks that the initial \n    # synchronization phase can take\n    initLimit=10\n    # The number of ticks that can pass between \n    # sending a request and getting an acknowledgement\n    syncLimit=5\n    # the directory where the snapshot is stored.\n    # do not use /tmp for storage, /tmp here is just \n    # example sakes.\n    dataDir=/data/zookeeper\n    # the port at which the clients will connect\n    clientPort=2181\n\n这个配置文件可以叫任意名字，在此我们叫它 conf/zoo.cfg。创建目录 /data/zookeeper，并修改 dataDir 为该目录。\n\n**tickTime**\n\nZookeeper 使用的单位为毫秒的基本时间。用来做心跳，会话超时的最小时间是两次 tickTime。\n\n**dataDir**\n\n目录位置用来保存内存数据库快照及数据库更新的事务日志，除非另有规定。\n\n**clientPort**\n\n监听客户端连接的端口号。\n\n创建配置文件后就可以启动  ZooKeeper 了：\n\n    bin/zkServer.sh start\n\nZooKeeper 用 log4j 记录日志消息 -- 可以在程序员指导手册中的 [Logging](http://zookeeper.apache.org/doc/trunk/zookeeperProgrammers.html#Logging) 一节获取更多详细信息。可以从控制台（默认）并且 / 或者看到日志信息，这依赖 log4j 的配置。\n\n这里列出的步骤是运行单机模式 ZooKeeper 的。单机模式没有副本，因此，如果 ZooKeeper 进程失败，则服务就会宕掉。这对于大多数开发场景是没问题的，运行副本模式的 ZooKeeper，请参见[运行副本模式的 Zookeeper](http://zookeeper.apache.org/doc/trunk/zookeeperStarted.html#sc_RunningReplicatedZooKeeper)。\n\n### 管理 ZooKeeper 存储\n\n对于长期运行的生产系统，ZooKeeper 存储必须从外部进行管理（dataDir 和 logs）。参见在 [maintenance](http://zookeeper.apache.org/doc/trunk/zookeeperAdmin.html#sc_maintenance) 中的章节获取更多信息。\n\n### 连接到 ZooKeeper\n\n    bin/zkCli.sh -server 127.0.0.1:2181\n\n这使你可以执行简单的、像文件一样的操作。\n\n当建立了连接，应该可以看到像下面的一些信息：\n\n    Connecting to localhost:2181\n    log4j:WARN No appenders could be found for logger (org.apache.zookeeper.ZooKeeper).\n    log4j:WARN Please initialize the log4j system properly.\n    Welcome to ZooKeeper!\n    JLine support is enabled\n    [zkshell: 0]\n\n在 shell 中，输入 help 获取可以从客户端执行的命令列表，像下面：\n\n    help\n    ZooKeeper -server host:port cmd args\n            connect host:port\n            get path [watch]\n            ls path [watch]\n            set path data [version]\n            rmr path\n            delquota [-n|-b] path\n            quit \n            printwatches on|off\n            create [-s] [-e] path data acl\n            stat path [watch]\n            close \n            ls2 path [watch]\n            history \n            listquota path\n            setAcl path acl\n            getAcl path\n            sync path\n            redo cmdno\n            addauth scheme auth\n            delete path [version]\n            setquota -n|-b val path\n\n从这里，可以尝试几个简单的命令来体验简单的命令行接口。首先，从命令列表开始，像 ls：\n\n    [zk: 127.0.0.1:2181(CONNECTED) 1] ls /\n    [zookeeper]\n\n接下来，通过运行 create /zk_test my_data 创建一个新的 znode。这会创建一个新的 znode 并与这个节点关联字符串“my_data”：\n\n    [zk: 127.0.0.1:2181(CONNECTED) 2] create /zk_test my_data\n    Created /zk_test\n\n输入 ls / 命令查看目录结构：\n\n    [zk: 127.0.0.1:2181(CONNECTED) 3] ls /\n    [zookeeper, zk_test]\n\n现在 zk_test 目录被创建了。\n\n接下来，通过 get 命令确认数据已经与 znode 关联，像：\n\n    [zk: 127.0.0.1:2181(CONNECTED) 4] get /zk_test\n    my_data\n    cZxid = 0x2\n    ctime = Tue Mar 28 23:37:16 CST 2017\n    mZxid = 0x2\n    mtime = Tue Mar 28 23:37:16 CST 2017\n    pZxid = 0x2\n    cversion = 0\n    dataVersion = 0\n    aclVersion = 0\n    ephemeralOwner = 0x0\n    dataLength = 7\n    numChildren = 0\n\n可以输入 set 命令修改与 zk_test 关联的数据，像：\n\n    [zk: 127.0.0.1:2181(CONNECTED) 5] set /zk_test junk\n    cZxid = 0x2\n    ctime = Tue Mar 28 23:37:16 CST 2017\n    mZxid = 0x3\n    mtime = Tue Mar 28 23:46:12 CST 2017\n    pZxid = 0x2\n    cversion = 0\n    dataVersion = 1\n    aclVersion = 0\n    ephemeralOwner = 0x0\n    dataLength = 4\n    numChildren = 0\n    [zk: 127.0.0.1:2181(CONNECTED) 6] get /zk_test\n    junk\n    cZxid = 0x2\n    ctime = Tue Mar 28 23:37:16 CST 2017\n    mZxid = 0x3\n    mtime = Tue Mar 28 23:46:12 CST 2017\n    pZxid = 0x2\n    cversion = 0\n    dataVersion = 1\n    aclVersion = 0\n    ephemeralOwner = 0x0\n    dataLength = 4\n    numChildren = 0\n\n最后，输入以下命令删除节点：\n\n    [zk: 127.0.0.1:2181(CONNECTED) 7] delete /zk_test\n    [zk: 127.0.0.1:2181(CONNECTED) 8] ls /\n    [zookeeper]","source":"_posts/Zookeeper-单机操作.md","raw":"title: Zookeeper 单机操作\ntags:\n  - Zookeeper\ncategories:\n  - 大数据\n  - Zookeeper\ndate: 2017-03-29 00:02:02\n---\n\n### 下载\n\n从 Apache 下载镜像站点下载当前稳定[发布版](http://zookeeper.apache.org/releases.html)。\n\n### 单机操作\n\n安装单机模式的 Zookeeper 服务器是简单明了的。服务器包含在一个单独的 JAR 文件中，因此安装包含创建配置。\n\n<!-- more -->\n\n下载稳定的 ZooKeeper 发布版后，解压并切换到包的根目录。\n\n启动 ZooKeeper 前需要一个配置文件。下面是示例配置文件，创建 conf/zoo.cfg：\n\n    # The number of milliseconds of each tick\n    tickTime=2000\n    # The number of ticks that the initial \n    # synchronization phase can take\n    initLimit=10\n    # The number of ticks that can pass between \n    # sending a request and getting an acknowledgement\n    syncLimit=5\n    # the directory where the snapshot is stored.\n    # do not use /tmp for storage, /tmp here is just \n    # example sakes.\n    dataDir=/data/zookeeper\n    # the port at which the clients will connect\n    clientPort=2181\n\n这个配置文件可以叫任意名字，在此我们叫它 conf/zoo.cfg。创建目录 /data/zookeeper，并修改 dataDir 为该目录。\n\n**tickTime**\n\nZookeeper 使用的单位为毫秒的基本时间。用来做心跳，会话超时的最小时间是两次 tickTime。\n\n**dataDir**\n\n目录位置用来保存内存数据库快照及数据库更新的事务日志，除非另有规定。\n\n**clientPort**\n\n监听客户端连接的端口号。\n\n创建配置文件后就可以启动  ZooKeeper 了：\n\n    bin/zkServer.sh start\n\nZooKeeper 用 log4j 记录日志消息 -- 可以在程序员指导手册中的 [Logging](http://zookeeper.apache.org/doc/trunk/zookeeperProgrammers.html#Logging) 一节获取更多详细信息。可以从控制台（默认）并且 / 或者看到日志信息，这依赖 log4j 的配置。\n\n这里列出的步骤是运行单机模式 ZooKeeper 的。单机模式没有副本，因此，如果 ZooKeeper 进程失败，则服务就会宕掉。这对于大多数开发场景是没问题的，运行副本模式的 ZooKeeper，请参见[运行副本模式的 Zookeeper](http://zookeeper.apache.org/doc/trunk/zookeeperStarted.html#sc_RunningReplicatedZooKeeper)。\n\n### 管理 ZooKeeper 存储\n\n对于长期运行的生产系统，ZooKeeper 存储必须从外部进行管理（dataDir 和 logs）。参见在 [maintenance](http://zookeeper.apache.org/doc/trunk/zookeeperAdmin.html#sc_maintenance) 中的章节获取更多信息。\n\n### 连接到 ZooKeeper\n\n    bin/zkCli.sh -server 127.0.0.1:2181\n\n这使你可以执行简单的、像文件一样的操作。\n\n当建立了连接，应该可以看到像下面的一些信息：\n\n    Connecting to localhost:2181\n    log4j:WARN No appenders could be found for logger (org.apache.zookeeper.ZooKeeper).\n    log4j:WARN Please initialize the log4j system properly.\n    Welcome to ZooKeeper!\n    JLine support is enabled\n    [zkshell: 0]\n\n在 shell 中，输入 help 获取可以从客户端执行的命令列表，像下面：\n\n    help\n    ZooKeeper -server host:port cmd args\n            connect host:port\n            get path [watch]\n            ls path [watch]\n            set path data [version]\n            rmr path\n            delquota [-n|-b] path\n            quit \n            printwatches on|off\n            create [-s] [-e] path data acl\n            stat path [watch]\n            close \n            ls2 path [watch]\n            history \n            listquota path\n            setAcl path acl\n            getAcl path\n            sync path\n            redo cmdno\n            addauth scheme auth\n            delete path [version]\n            setquota -n|-b val path\n\n从这里，可以尝试几个简单的命令来体验简单的命令行接口。首先，从命令列表开始，像 ls：\n\n    [zk: 127.0.0.1:2181(CONNECTED) 1] ls /\n    [zookeeper]\n\n接下来，通过运行 create /zk_test my_data 创建一个新的 znode。这会创建一个新的 znode 并与这个节点关联字符串“my_data”：\n\n    [zk: 127.0.0.1:2181(CONNECTED) 2] create /zk_test my_data\n    Created /zk_test\n\n输入 ls / 命令查看目录结构：\n\n    [zk: 127.0.0.1:2181(CONNECTED) 3] ls /\n    [zookeeper, zk_test]\n\n现在 zk_test 目录被创建了。\n\n接下来，通过 get 命令确认数据已经与 znode 关联，像：\n\n    [zk: 127.0.0.1:2181(CONNECTED) 4] get /zk_test\n    my_data\n    cZxid = 0x2\n    ctime = Tue Mar 28 23:37:16 CST 2017\n    mZxid = 0x2\n    mtime = Tue Mar 28 23:37:16 CST 2017\n    pZxid = 0x2\n    cversion = 0\n    dataVersion = 0\n    aclVersion = 0\n    ephemeralOwner = 0x0\n    dataLength = 7\n    numChildren = 0\n\n可以输入 set 命令修改与 zk_test 关联的数据，像：\n\n    [zk: 127.0.0.1:2181(CONNECTED) 5] set /zk_test junk\n    cZxid = 0x2\n    ctime = Tue Mar 28 23:37:16 CST 2017\n    mZxid = 0x3\n    mtime = Tue Mar 28 23:46:12 CST 2017\n    pZxid = 0x2\n    cversion = 0\n    dataVersion = 1\n    aclVersion = 0\n    ephemeralOwner = 0x0\n    dataLength = 4\n    numChildren = 0\n    [zk: 127.0.0.1:2181(CONNECTED) 6] get /zk_test\n    junk\n    cZxid = 0x2\n    ctime = Tue Mar 28 23:37:16 CST 2017\n    mZxid = 0x3\n    mtime = Tue Mar 28 23:46:12 CST 2017\n    pZxid = 0x2\n    cversion = 0\n    dataVersion = 1\n    aclVersion = 0\n    ephemeralOwner = 0x0\n    dataLength = 4\n    numChildren = 0\n\n最后，输入以下命令删除节点：\n\n    [zk: 127.0.0.1:2181(CONNECTED) 7] delete /zk_test\n    [zk: 127.0.0.1:2181(CONNECTED) 8] ls /\n    [zookeeper]","slug":"Zookeeper-单机操作","published":1,"updated":"2021-07-19T16:28:00.296Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphqs00cxitd39uop4cqi","content":"<h3 id=\"下载\"><a href=\"#下载\" class=\"headerlink\" title=\"下载\"></a>下载</h3><p>从 Apache 下载镜像站点下载当前稳定<a href=\"http://zookeeper.apache.org/releases.html\">发布版</a>。</p>\n<h3 id=\"单机操作\"><a href=\"#单机操作\" class=\"headerlink\" title=\"单机操作\"></a>单机操作</h3><p>安装单机模式的 Zookeeper 服务器是简单明了的。服务器包含在一个单独的 JAR 文件中，因此安装包含创建配置。</p>\n<span id=\"more\"></span>\n\n<p>下载稳定的 ZooKeeper 发布版后，解压并切换到包的根目录。</p>\n<p>启动 ZooKeeper 前需要一个配置文件。下面是示例配置文件，创建 conf/zoo.cfg：</p>\n<pre><code># The number of milliseconds of each tick\ntickTime=2000\n# The number of ticks that the initial \n# synchronization phase can take\ninitLimit=10\n# The number of ticks that can pass between \n# sending a request and getting an acknowledgement\nsyncLimit=5\n# the directory where the snapshot is stored.\n# do not use /tmp for storage, /tmp here is just \n# example sakes.\ndataDir=/data/zookeeper\n# the port at which the clients will connect\nclientPort=2181\n</code></pre>\n<p>这个配置文件可以叫任意名字，在此我们叫它 conf/zoo.cfg。创建目录 /data/zookeeper，并修改 dataDir 为该目录。</p>\n<p><strong>tickTime</strong></p>\n<p>Zookeeper 使用的单位为毫秒的基本时间。用来做心跳，会话超时的最小时间是两次 tickTime。</p>\n<p><strong>dataDir</strong></p>\n<p>目录位置用来保存内存数据库快照及数据库更新的事务日志，除非另有规定。</p>\n<p><strong>clientPort</strong></p>\n<p>监听客户端连接的端口号。</p>\n<p>创建配置文件后就可以启动  ZooKeeper 了：</p>\n<pre><code>bin/zkServer.sh start\n</code></pre>\n<p>ZooKeeper 用 log4j 记录日志消息 – 可以在程序员指导手册中的 <a href=\"http://zookeeper.apache.org/doc/trunk/zookeeperProgrammers.html#Logging\">Logging</a> 一节获取更多详细信息。可以从控制台（默认）并且 / 或者看到日志信息，这依赖 log4j 的配置。</p>\n<p>这里列出的步骤是运行单机模式 ZooKeeper 的。单机模式没有副本，因此，如果 ZooKeeper 进程失败，则服务就会宕掉。这对于大多数开发场景是没问题的，运行副本模式的 ZooKeeper，请参见<a href=\"http://zookeeper.apache.org/doc/trunk/zookeeperStarted.html#sc_RunningReplicatedZooKeeper\">运行副本模式的 Zookeeper</a>。</p>\n<h3 id=\"管理-ZooKeeper-存储\"><a href=\"#管理-ZooKeeper-存储\" class=\"headerlink\" title=\"管理 ZooKeeper 存储\"></a>管理 ZooKeeper 存储</h3><p>对于长期运行的生产系统，ZooKeeper 存储必须从外部进行管理（dataDir 和 logs）。参见在 <a href=\"http://zookeeper.apache.org/doc/trunk/zookeeperAdmin.html#sc_maintenance\">maintenance</a> 中的章节获取更多信息。</p>\n<h3 id=\"连接到-ZooKeeper\"><a href=\"#连接到-ZooKeeper\" class=\"headerlink\" title=\"连接到 ZooKeeper\"></a>连接到 ZooKeeper</h3><pre><code>bin/zkCli.sh -server 127.0.0.1:2181\n</code></pre>\n<p>这使你可以执行简单的、像文件一样的操作。</p>\n<p>当建立了连接，应该可以看到像下面的一些信息：</p>\n<pre><code>Connecting to localhost:2181\nlog4j:WARN No appenders could be found for logger (org.apache.zookeeper.ZooKeeper).\nlog4j:WARN Please initialize the log4j system properly.\nWelcome to ZooKeeper!\nJLine support is enabled\n[zkshell: 0]\n</code></pre>\n<p>在 shell 中，输入 help 获取可以从客户端执行的命令列表，像下面：</p>\n<pre><code>help\nZooKeeper -server host:port cmd args\n        connect host:port\n        get path [watch]\n        ls path [watch]\n        set path data [version]\n        rmr path\n        delquota [-n|-b] path\n        quit \n        printwatches on|off\n        create [-s] [-e] path data acl\n        stat path [watch]\n        close \n        ls2 path [watch]\n        history \n        listquota path\n        setAcl path acl\n        getAcl path\n        sync path\n        redo cmdno\n        addauth scheme auth\n        delete path [version]\n        setquota -n|-b val path\n</code></pre>\n<p>从这里，可以尝试几个简单的命令来体验简单的命令行接口。首先，从命令列表开始，像 ls：</p>\n<pre><code>[zk: 127.0.0.1:2181(CONNECTED) 1] ls /\n[zookeeper]\n</code></pre>\n<p>接下来，通过运行 create /zk_test my_data 创建一个新的 znode。这会创建一个新的 znode 并与这个节点关联字符串“my_data”：</p>\n<pre><code>[zk: 127.0.0.1:2181(CONNECTED) 2] create /zk_test my_data\nCreated /zk_test\n</code></pre>\n<p>输入 ls / 命令查看目录结构：</p>\n<pre><code>[zk: 127.0.0.1:2181(CONNECTED) 3] ls /\n[zookeeper, zk_test]\n</code></pre>\n<p>现在 zk_test 目录被创建了。</p>\n<p>接下来，通过 get 命令确认数据已经与 znode 关联，像：</p>\n<pre><code>[zk: 127.0.0.1:2181(CONNECTED) 4] get /zk_test\nmy_data\ncZxid = 0x2\nctime = Tue Mar 28 23:37:16 CST 2017\nmZxid = 0x2\nmtime = Tue Mar 28 23:37:16 CST 2017\npZxid = 0x2\ncversion = 0\ndataVersion = 0\naclVersion = 0\nephemeralOwner = 0x0\ndataLength = 7\nnumChildren = 0\n</code></pre>\n<p>可以输入 set 命令修改与 zk_test 关联的数据，像：</p>\n<pre><code>[zk: 127.0.0.1:2181(CONNECTED) 5] set /zk_test junk\ncZxid = 0x2\nctime = Tue Mar 28 23:37:16 CST 2017\nmZxid = 0x3\nmtime = Tue Mar 28 23:46:12 CST 2017\npZxid = 0x2\ncversion = 0\ndataVersion = 1\naclVersion = 0\nephemeralOwner = 0x0\ndataLength = 4\nnumChildren = 0\n[zk: 127.0.0.1:2181(CONNECTED) 6] get /zk_test\njunk\ncZxid = 0x2\nctime = Tue Mar 28 23:37:16 CST 2017\nmZxid = 0x3\nmtime = Tue Mar 28 23:46:12 CST 2017\npZxid = 0x2\ncversion = 0\ndataVersion = 1\naclVersion = 0\nephemeralOwner = 0x0\ndataLength = 4\nnumChildren = 0\n</code></pre>\n<p>最后，输入以下命令删除节点：</p>\n<pre><code>[zk: 127.0.0.1:2181(CONNECTED) 7] delete /zk_test\n[zk: 127.0.0.1:2181(CONNECTED) 8] ls /\n[zookeeper]\n</code></pre>\n","site":{"data":{}},"excerpt":"<h3 id=\"下载\"><a href=\"#下载\" class=\"headerlink\" title=\"下载\"></a>下载</h3><p>从 Apache 下载镜像站点下载当前稳定<a href=\"http://zookeeper.apache.org/releases.html\">发布版</a>。</p>\n<h3 id=\"单机操作\"><a href=\"#单机操作\" class=\"headerlink\" title=\"单机操作\"></a>单机操作</h3><p>安装单机模式的 Zookeeper 服务器是简单明了的。服务器包含在一个单独的 JAR 文件中，因此安装包含创建配置。</p>","more":"<p>下载稳定的 ZooKeeper 发布版后，解压并切换到包的根目录。</p>\n<p>启动 ZooKeeper 前需要一个配置文件。下面是示例配置文件，创建 conf/zoo.cfg：</p>\n<pre><code># The number of milliseconds of each tick\ntickTime=2000\n# The number of ticks that the initial \n# synchronization phase can take\ninitLimit=10\n# The number of ticks that can pass between \n# sending a request and getting an acknowledgement\nsyncLimit=5\n# the directory where the snapshot is stored.\n# do not use /tmp for storage, /tmp here is just \n# example sakes.\ndataDir=/data/zookeeper\n# the port at which the clients will connect\nclientPort=2181\n</code></pre>\n<p>这个配置文件可以叫任意名字，在此我们叫它 conf/zoo.cfg。创建目录 /data/zookeeper，并修改 dataDir 为该目录。</p>\n<p><strong>tickTime</strong></p>\n<p>Zookeeper 使用的单位为毫秒的基本时间。用来做心跳，会话超时的最小时间是两次 tickTime。</p>\n<p><strong>dataDir</strong></p>\n<p>目录位置用来保存内存数据库快照及数据库更新的事务日志，除非另有规定。</p>\n<p><strong>clientPort</strong></p>\n<p>监听客户端连接的端口号。</p>\n<p>创建配置文件后就可以启动  ZooKeeper 了：</p>\n<pre><code>bin/zkServer.sh start\n</code></pre>\n<p>ZooKeeper 用 log4j 记录日志消息 – 可以在程序员指导手册中的 <a href=\"http://zookeeper.apache.org/doc/trunk/zookeeperProgrammers.html#Logging\">Logging</a> 一节获取更多详细信息。可以从控制台（默认）并且 / 或者看到日志信息，这依赖 log4j 的配置。</p>\n<p>这里列出的步骤是运行单机模式 ZooKeeper 的。单机模式没有副本，因此，如果 ZooKeeper 进程失败，则服务就会宕掉。这对于大多数开发场景是没问题的，运行副本模式的 ZooKeeper，请参见<a href=\"http://zookeeper.apache.org/doc/trunk/zookeeperStarted.html#sc_RunningReplicatedZooKeeper\">运行副本模式的 Zookeeper</a>。</p>\n<h3 id=\"管理-ZooKeeper-存储\"><a href=\"#管理-ZooKeeper-存储\" class=\"headerlink\" title=\"管理 ZooKeeper 存储\"></a>管理 ZooKeeper 存储</h3><p>对于长期运行的生产系统，ZooKeeper 存储必须从外部进行管理（dataDir 和 logs）。参见在 <a href=\"http://zookeeper.apache.org/doc/trunk/zookeeperAdmin.html#sc_maintenance\">maintenance</a> 中的章节获取更多信息。</p>\n<h3 id=\"连接到-ZooKeeper\"><a href=\"#连接到-ZooKeeper\" class=\"headerlink\" title=\"连接到 ZooKeeper\"></a>连接到 ZooKeeper</h3><pre><code>bin/zkCli.sh -server 127.0.0.1:2181\n</code></pre>\n<p>这使你可以执行简单的、像文件一样的操作。</p>\n<p>当建立了连接，应该可以看到像下面的一些信息：</p>\n<pre><code>Connecting to localhost:2181\nlog4j:WARN No appenders could be found for logger (org.apache.zookeeper.ZooKeeper).\nlog4j:WARN Please initialize the log4j system properly.\nWelcome to ZooKeeper!\nJLine support is enabled\n[zkshell: 0]\n</code></pre>\n<p>在 shell 中，输入 help 获取可以从客户端执行的命令列表，像下面：</p>\n<pre><code>help\nZooKeeper -server host:port cmd args\n        connect host:port\n        get path [watch]\n        ls path [watch]\n        set path data [version]\n        rmr path\n        delquota [-n|-b] path\n        quit \n        printwatches on|off\n        create [-s] [-e] path data acl\n        stat path [watch]\n        close \n        ls2 path [watch]\n        history \n        listquota path\n        setAcl path acl\n        getAcl path\n        sync path\n        redo cmdno\n        addauth scheme auth\n        delete path [version]\n        setquota -n|-b val path\n</code></pre>\n<p>从这里，可以尝试几个简单的命令来体验简单的命令行接口。首先，从命令列表开始，像 ls：</p>\n<pre><code>[zk: 127.0.0.1:2181(CONNECTED) 1] ls /\n[zookeeper]\n</code></pre>\n<p>接下来，通过运行 create /zk_test my_data 创建一个新的 znode。这会创建一个新的 znode 并与这个节点关联字符串“my_data”：</p>\n<pre><code>[zk: 127.0.0.1:2181(CONNECTED) 2] create /zk_test my_data\nCreated /zk_test\n</code></pre>\n<p>输入 ls / 命令查看目录结构：</p>\n<pre><code>[zk: 127.0.0.1:2181(CONNECTED) 3] ls /\n[zookeeper, zk_test]\n</code></pre>\n<p>现在 zk_test 目录被创建了。</p>\n<p>接下来，通过 get 命令确认数据已经与 znode 关联，像：</p>\n<pre><code>[zk: 127.0.0.1:2181(CONNECTED) 4] get /zk_test\nmy_data\ncZxid = 0x2\nctime = Tue Mar 28 23:37:16 CST 2017\nmZxid = 0x2\nmtime = Tue Mar 28 23:37:16 CST 2017\npZxid = 0x2\ncversion = 0\ndataVersion = 0\naclVersion = 0\nephemeralOwner = 0x0\ndataLength = 7\nnumChildren = 0\n</code></pre>\n<p>可以输入 set 命令修改与 zk_test 关联的数据，像：</p>\n<pre><code>[zk: 127.0.0.1:2181(CONNECTED) 5] set /zk_test junk\ncZxid = 0x2\nctime = Tue Mar 28 23:37:16 CST 2017\nmZxid = 0x3\nmtime = Tue Mar 28 23:46:12 CST 2017\npZxid = 0x2\ncversion = 0\ndataVersion = 1\naclVersion = 0\nephemeralOwner = 0x0\ndataLength = 4\nnumChildren = 0\n[zk: 127.0.0.1:2181(CONNECTED) 6] get /zk_test\njunk\ncZxid = 0x2\nctime = Tue Mar 28 23:37:16 CST 2017\nmZxid = 0x3\nmtime = Tue Mar 28 23:46:12 CST 2017\npZxid = 0x2\ncversion = 0\ndataVersion = 1\naclVersion = 0\nephemeralOwner = 0x0\ndataLength = 4\nnumChildren = 0\n</code></pre>\n<p>最后，输入以下命令删除节点：</p>\n<pre><code>[zk: 127.0.0.1:2181(CONNECTED) 7] delete /zk_test\n[zk: 127.0.0.1:2181(CONNECTED) 8] ls /\n[zookeeper]\n</code></pre>"},{"title":"Zookeeper 集群搭建","date":"2017-04-01T07:05:04.000Z","_content":"\n### hosts 配置\n\n在 /etc/hosts 配置文件中添加如下内容：\n\n    10.142.165.40 frin-zookeeper1\n    10.142.165.41 frin-zookeeper2\n    10.142.165.44 frin-zookeeper3\n\n<!-- more -->\n\n### 创建用户及目录\n\n- 创建 zookeeper 用户，用来启动 zookeeper 进程。\n- 创建 /frin/zookeeper 目录作为 zookeeper 的 home 目录。修改该目录的属主与组为 zookeeper。\n\n### 下载 zookeeper 并解压\n\n当前最新稳定版本是 zookeeper-3.4.10.tar.gz，下载地址：http://www.apache.org/dyn/closer.cgi/zookeeper/。下载并解压到 /frin/zookeeper 目录下。\n\n创建软连接：\n\n    ln -s /frin/zookeeper/zookeeper-3.4.10 /usr/local/zookeeper-3.4.10\n\n### Java heap 设置\n\n设置Java heap 大小，避免内存与磁盘空间的交换，能够大大提升ZK的性能，设置合理的heap大小则能有效避免此类空间交换的触发。在正式发布上线之前，建议是针对使用场景进行一些压力测试，确保正常运行后内存的使用不会触发此类交换。通常在一个物理内存为4G的机器上，最多设置-Xmx为3G。\n\n因为存储数据量很小，此处使用默认值。\n\n### zookeeper 配置\n\n    cd conf\n    cp zoo_sample.cfg zoo.cfg\n\n在 zoo.cfg 中添加如下配置：\n\n    tickTime=2000\n    initLimit=10\n    syncLimit=5\n    dataDir=/data/zookeeper\n    clientPort=2181\n    server.1=frin-zookeeper1:2888:3888\n    server.2=frin-zookeeper2:2888:3888\n    server.3=frin-zookeeper3:2888:3888\n\n创建数据目录：/data/zookeeper\n\n### 创建 myid 文件\n\nmyid文件中只有一个数字，即一个Server ID。例如，server.1 的myid文件内容就是“1”。注意，请确保每个server的myid文件中id数字不同，并且和server.id=host:port:port中的id一致。另外，id的范围是1~255。\n\n分别在三台机器上的 /data/zookeeper 目录下创建 myid 文件，并设置相应的值。\n\n### 启动 zookeeper\n\n    bin/zkServer.sh start\n\n### 测试\n\n    bin/zkCli.sh -server frin-zookeeper1:2181","source":"_posts/Zookeeper-集群搭建.md","raw":"title: Zookeeper 集群搭建\ntags:\n  - Zookeeper\ncategories:\n  - 大数据\n  - Zookeeper\ndate: 2017-04-01 15:05:04\n---\n\n### hosts 配置\n\n在 /etc/hosts 配置文件中添加如下内容：\n\n    10.142.165.40 frin-zookeeper1\n    10.142.165.41 frin-zookeeper2\n    10.142.165.44 frin-zookeeper3\n\n<!-- more -->\n\n### 创建用户及目录\n\n- 创建 zookeeper 用户，用来启动 zookeeper 进程。\n- 创建 /frin/zookeeper 目录作为 zookeeper 的 home 目录。修改该目录的属主与组为 zookeeper。\n\n### 下载 zookeeper 并解压\n\n当前最新稳定版本是 zookeeper-3.4.10.tar.gz，下载地址：http://www.apache.org/dyn/closer.cgi/zookeeper/。下载并解压到 /frin/zookeeper 目录下。\n\n创建软连接：\n\n    ln -s /frin/zookeeper/zookeeper-3.4.10 /usr/local/zookeeper-3.4.10\n\n### Java heap 设置\n\n设置Java heap 大小，避免内存与磁盘空间的交换，能够大大提升ZK的性能，设置合理的heap大小则能有效避免此类空间交换的触发。在正式发布上线之前，建议是针对使用场景进行一些压力测试，确保正常运行后内存的使用不会触发此类交换。通常在一个物理内存为4G的机器上，最多设置-Xmx为3G。\n\n因为存储数据量很小，此处使用默认值。\n\n### zookeeper 配置\n\n    cd conf\n    cp zoo_sample.cfg zoo.cfg\n\n在 zoo.cfg 中添加如下配置：\n\n    tickTime=2000\n    initLimit=10\n    syncLimit=5\n    dataDir=/data/zookeeper\n    clientPort=2181\n    server.1=frin-zookeeper1:2888:3888\n    server.2=frin-zookeeper2:2888:3888\n    server.3=frin-zookeeper3:2888:3888\n\n创建数据目录：/data/zookeeper\n\n### 创建 myid 文件\n\nmyid文件中只有一个数字，即一个Server ID。例如，server.1 的myid文件内容就是“1”。注意，请确保每个server的myid文件中id数字不同，并且和server.id=host:port:port中的id一致。另外，id的范围是1~255。\n\n分别在三台机器上的 /data/zookeeper 目录下创建 myid 文件，并设置相应的值。\n\n### 启动 zookeeper\n\n    bin/zkServer.sh start\n\n### 测试\n\n    bin/zkCli.sh -server frin-zookeeper1:2181","slug":"Zookeeper-集群搭建","published":1,"updated":"2021-07-19T16:28:00.296Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphqt00d2itd31ci8dfpp","content":"<h3 id=\"hosts-配置\"><a href=\"#hosts-配置\" class=\"headerlink\" title=\"hosts 配置\"></a>hosts 配置</h3><p>在 /etc/hosts 配置文件中添加如下内容：</p>\n<pre><code>10.142.165.40 frin-zookeeper1\n10.142.165.41 frin-zookeeper2\n10.142.165.44 frin-zookeeper3\n</code></pre>\n<span id=\"more\"></span>\n\n<h3 id=\"创建用户及目录\"><a href=\"#创建用户及目录\" class=\"headerlink\" title=\"创建用户及目录\"></a>创建用户及目录</h3><ul>\n<li>创建 zookeeper 用户，用来启动 zookeeper 进程。</li>\n<li>创建 /frin/zookeeper 目录作为 zookeeper 的 home 目录。修改该目录的属主与组为 zookeeper。</li>\n</ul>\n<h3 id=\"下载-zookeeper-并解压\"><a href=\"#下载-zookeeper-并解压\" class=\"headerlink\" title=\"下载 zookeeper 并解压\"></a>下载 zookeeper 并解压</h3><p>当前最新稳定版本是 zookeeper-3.4.10.tar.gz，下载地址：<a href=\"http://www.apache.org/dyn/closer.cgi/zookeeper/%E3%80%82%E4%B8%8B%E8%BD%BD%E5%B9%B6%E8%A7%A3%E5%8E%8B%E5%88%B0\">http://www.apache.org/dyn/closer.cgi/zookeeper/。下载并解压到</a> /frin/zookeeper 目录下。</p>\n<p>创建软连接：</p>\n<pre><code>ln -s /frin/zookeeper/zookeeper-3.4.10 /usr/local/zookeeper-3.4.10\n</code></pre>\n<h3 id=\"Java-heap-设置\"><a href=\"#Java-heap-设置\" class=\"headerlink\" title=\"Java heap 设置\"></a>Java heap 设置</h3><p>设置Java heap 大小，避免内存与磁盘空间的交换，能够大大提升ZK的性能，设置合理的heap大小则能有效避免此类空间交换的触发。在正式发布上线之前，建议是针对使用场景进行一些压力测试，确保正常运行后内存的使用不会触发此类交换。通常在一个物理内存为4G的机器上，最多设置-Xmx为3G。</p>\n<p>因为存储数据量很小，此处使用默认值。</p>\n<h3 id=\"zookeeper-配置\"><a href=\"#zookeeper-配置\" class=\"headerlink\" title=\"zookeeper 配置\"></a>zookeeper 配置</h3><pre><code>cd conf\ncp zoo_sample.cfg zoo.cfg\n</code></pre>\n<p>在 zoo.cfg 中添加如下配置：</p>\n<pre><code>tickTime=2000\ninitLimit=10\nsyncLimit=5\ndataDir=/data/zookeeper\nclientPort=2181\nserver.1=frin-zookeeper1:2888:3888\nserver.2=frin-zookeeper2:2888:3888\nserver.3=frin-zookeeper3:2888:3888\n</code></pre>\n<p>创建数据目录：/data/zookeeper</p>\n<h3 id=\"创建-myid-文件\"><a href=\"#创建-myid-文件\" class=\"headerlink\" title=\"创建 myid 文件\"></a>创建 myid 文件</h3><p>myid文件中只有一个数字，即一个Server ID。例如，server.1 的myid文件内容就是“1”。注意，请确保每个server的myid文件中id数字不同，并且和server.id=host:port:port中的id一致。另外，id的范围是1~255。</p>\n<p>分别在三台机器上的 /data/zookeeper 目录下创建 myid 文件，并设置相应的值。</p>\n<h3 id=\"启动-zookeeper\"><a href=\"#启动-zookeeper\" class=\"headerlink\" title=\"启动 zookeeper\"></a>启动 zookeeper</h3><pre><code>bin/zkServer.sh start\n</code></pre>\n<h3 id=\"测试\"><a href=\"#测试\" class=\"headerlink\" title=\"测试\"></a>测试</h3><pre><code>bin/zkCli.sh -server frin-zookeeper1:2181\n</code></pre>\n","site":{"data":{}},"excerpt":"<h3 id=\"hosts-配置\"><a href=\"#hosts-配置\" class=\"headerlink\" title=\"hosts 配置\"></a>hosts 配置</h3><p>在 /etc/hosts 配置文件中添加如下内容：</p>\n<pre><code>10.142.165.40 frin-zookeeper1\n10.142.165.41 frin-zookeeper2\n10.142.165.44 frin-zookeeper3\n</code></pre>","more":"<h3 id=\"创建用户及目录\"><a href=\"#创建用户及目录\" class=\"headerlink\" title=\"创建用户及目录\"></a>创建用户及目录</h3><ul>\n<li>创建 zookeeper 用户，用来启动 zookeeper 进程。</li>\n<li>创建 /frin/zookeeper 目录作为 zookeeper 的 home 目录。修改该目录的属主与组为 zookeeper。</li>\n</ul>\n<h3 id=\"下载-zookeeper-并解压\"><a href=\"#下载-zookeeper-并解压\" class=\"headerlink\" title=\"下载 zookeeper 并解压\"></a>下载 zookeeper 并解压</h3><p>当前最新稳定版本是 zookeeper-3.4.10.tar.gz，下载地址：<a href=\"http://www.apache.org/dyn/closer.cgi/zookeeper/%E3%80%82%E4%B8%8B%E8%BD%BD%E5%B9%B6%E8%A7%A3%E5%8E%8B%E5%88%B0\">http://www.apache.org/dyn/closer.cgi/zookeeper/。下载并解压到</a> /frin/zookeeper 目录下。</p>\n<p>创建软连接：</p>\n<pre><code>ln -s /frin/zookeeper/zookeeper-3.4.10 /usr/local/zookeeper-3.4.10\n</code></pre>\n<h3 id=\"Java-heap-设置\"><a href=\"#Java-heap-设置\" class=\"headerlink\" title=\"Java heap 设置\"></a>Java heap 设置</h3><p>设置Java heap 大小，避免内存与磁盘空间的交换，能够大大提升ZK的性能，设置合理的heap大小则能有效避免此类空间交换的触发。在正式发布上线之前，建议是针对使用场景进行一些压力测试，确保正常运行后内存的使用不会触发此类交换。通常在一个物理内存为4G的机器上，最多设置-Xmx为3G。</p>\n<p>因为存储数据量很小，此处使用默认值。</p>\n<h3 id=\"zookeeper-配置\"><a href=\"#zookeeper-配置\" class=\"headerlink\" title=\"zookeeper 配置\"></a>zookeeper 配置</h3><pre><code>cd conf\ncp zoo_sample.cfg zoo.cfg\n</code></pre>\n<p>在 zoo.cfg 中添加如下配置：</p>\n<pre><code>tickTime=2000\ninitLimit=10\nsyncLimit=5\ndataDir=/data/zookeeper\nclientPort=2181\nserver.1=frin-zookeeper1:2888:3888\nserver.2=frin-zookeeper2:2888:3888\nserver.3=frin-zookeeper3:2888:3888\n</code></pre>\n<p>创建数据目录：/data/zookeeper</p>\n<h3 id=\"创建-myid-文件\"><a href=\"#创建-myid-文件\" class=\"headerlink\" title=\"创建 myid 文件\"></a>创建 myid 文件</h3><p>myid文件中只有一个数字，即一个Server ID。例如，server.1 的myid文件内容就是“1”。注意，请确保每个server的myid文件中id数字不同，并且和server.id=host:port:port中的id一致。另外，id的范围是1~255。</p>\n<p>分别在三台机器上的 /data/zookeeper 目录下创建 myid 文件，并设置相应的值。</p>\n<h3 id=\"启动-zookeeper\"><a href=\"#启动-zookeeper\" class=\"headerlink\" title=\"启动 zookeeper\"></a>启动 zookeeper</h3><pre><code>bin/zkServer.sh start\n</code></pre>\n<h3 id=\"测试\"><a href=\"#测试\" class=\"headerlink\" title=\"测试\"></a>测试</h3><pre><code>bin/zkCli.sh -server frin-zookeeper1:2181\n</code></pre>"},{"title":"addShutdownHook","date":"2015-11-16T14:19:28.000Z","_content":"### 方法介绍\n**addShutdownHook** 是 Runtime 类中的一个方法，方法声明：\n\n\tpublic void addShutdownHook(Thread hook)\n\n<!-- more -->\n\nJDK 帮助手册中的解释如下：\n\n注册一个新的虚拟机关闭钩子。\n\nJava 虚拟机在响应两种事件时关闭：\n\n- 当最后的非守护线程退出，或者调用 exit （等同于 System.exit ）方法时，程序正常退出。\n- 响应用户中断而终止虚拟机，例如键入 ^C；或者响应系统事件终止虚拟机，例如用户注销或者系统关闭。\n\n关闭钩子只是一个已初始化但尚未启动的线程。当虚拟机开启关闭序列时，它会以不确定的顺序启动全部注册的关闭钩子，并让它们并发运行。当所有的钩子已经运行完，如果已启用退出终结方法，那么虚拟机接着会运行所有未调用的终结方法。最后，虚拟机会停止。注意，在关闭序列执行期间守护线程会继续运行，如果通过调用 exit 方法启动关闭非守护线程也会继续运行。\n\n一旦关闭序列开始执行，只能通过调用 halt 方法停止它，该方法可强制终止虚拟机。\n\n一旦关闭序列开始执行就不能注册一个新的关闭钩子或者取消注册已经注册的钩子。尝试这些操作会导致抛出 IllegalStateException 。\n\n关闭钩子运行在虚拟机生命周期中的一个微妙的时间，因此应保护性地对其进行编码。特别是关闭钩子应该编写为线程安全的，并且尽可能避免死锁。关闭钩子还应该不盲目地依靠某些服务，这些服务可能已注册了自己的关闭钩子，所以其本身可能正处于关闭进程中。例如，试图使用其他基于线程的服务（如 AWT 事件指派线程）可能导致死锁。\n\n关闭钩子应该快速地完成它们的工作。当程序调用 exit 的时候，期望的结果是虚拟机及时关闭并且退出。当由于用户注销或者系统关闭引起虚拟机终止时，底层操作系统可能只允许在固定的时间内关闭并退出。因此，在关闭钩子中尝试任何用户交互或执行长时间的计算是不可取的。就像在其他线程中一样，通过调用线程的 ThreadGroup 对象的 uncaughtException 方法，未被捕获的异常可在关闭钩子中处理。此方法的默认实现是将该异常的堆栈跟踪信息打印至 System.err 并终止线程；它不会导致虚拟机退出或停止。\n\n仅在很少的情况下，虚拟机可能会中止，也就是没有完全关闭就停止运行。这种情况会发生在虚拟机被从外部终止时，比如在 Unix 上使用 SIGKILL 信号或在 Microsoft Windows 上调用 TerminateProcess 。如果由于例如内部数据结构损坏或试图访问不存在的内存而导致本地方法执行错误，虚拟机也可能会中止。如果虚拟机中止，则不能保证所有的关闭钩子都被运行。\n### 参数\nhook - 已初始化但尚未启动的线程对象\n### 抛出异常\nIllegalArgumentException - 如果指定的钩子已注册，或者可以确定钩子正在运行或者已运行完毕\n\nIllegalStateException - 如果虚拟机已经处于关闭过程中\n\nSecurityException - 如果安全管理器存在并且拒绝 RuntimePermission(”shutdownHooks“)\n### Since：\n1.3\n","source":"_posts/addShutdownHook.md","raw":"title: addShutdownHook\ndate: 2015-11-16 22:19:28\ntags:\n  - Java\ncategories:\n  - 语言\n  - Java\n---\n### 方法介绍\n**addShutdownHook** 是 Runtime 类中的一个方法，方法声明：\n\n\tpublic void addShutdownHook(Thread hook)\n\n<!-- more -->\n\nJDK 帮助手册中的解释如下：\n\n注册一个新的虚拟机关闭钩子。\n\nJava 虚拟机在响应两种事件时关闭：\n\n- 当最后的非守护线程退出，或者调用 exit （等同于 System.exit ）方法时，程序正常退出。\n- 响应用户中断而终止虚拟机，例如键入 ^C；或者响应系统事件终止虚拟机，例如用户注销或者系统关闭。\n\n关闭钩子只是一个已初始化但尚未启动的线程。当虚拟机开启关闭序列时，它会以不确定的顺序启动全部注册的关闭钩子，并让它们并发运行。当所有的钩子已经运行完，如果已启用退出终结方法，那么虚拟机接着会运行所有未调用的终结方法。最后，虚拟机会停止。注意，在关闭序列执行期间守护线程会继续运行，如果通过调用 exit 方法启动关闭非守护线程也会继续运行。\n\n一旦关闭序列开始执行，只能通过调用 halt 方法停止它，该方法可强制终止虚拟机。\n\n一旦关闭序列开始执行就不能注册一个新的关闭钩子或者取消注册已经注册的钩子。尝试这些操作会导致抛出 IllegalStateException 。\n\n关闭钩子运行在虚拟机生命周期中的一个微妙的时间，因此应保护性地对其进行编码。特别是关闭钩子应该编写为线程安全的，并且尽可能避免死锁。关闭钩子还应该不盲目地依靠某些服务，这些服务可能已注册了自己的关闭钩子，所以其本身可能正处于关闭进程中。例如，试图使用其他基于线程的服务（如 AWT 事件指派线程）可能导致死锁。\n\n关闭钩子应该快速地完成它们的工作。当程序调用 exit 的时候，期望的结果是虚拟机及时关闭并且退出。当由于用户注销或者系统关闭引起虚拟机终止时，底层操作系统可能只允许在固定的时间内关闭并退出。因此，在关闭钩子中尝试任何用户交互或执行长时间的计算是不可取的。就像在其他线程中一样，通过调用线程的 ThreadGroup 对象的 uncaughtException 方法，未被捕获的异常可在关闭钩子中处理。此方法的默认实现是将该异常的堆栈跟踪信息打印至 System.err 并终止线程；它不会导致虚拟机退出或停止。\n\n仅在很少的情况下，虚拟机可能会中止，也就是没有完全关闭就停止运行。这种情况会发生在虚拟机被从外部终止时，比如在 Unix 上使用 SIGKILL 信号或在 Microsoft Windows 上调用 TerminateProcess 。如果由于例如内部数据结构损坏或试图访问不存在的内存而导致本地方法执行错误，虚拟机也可能会中止。如果虚拟机中止，则不能保证所有的关闭钩子都被运行。\n### 参数\nhook - 已初始化但尚未启动的线程对象\n### 抛出异常\nIllegalArgumentException - 如果指定的钩子已注册，或者可以确定钩子正在运行或者已运行完毕\n\nIllegalStateException - 如果虚拟机已经处于关闭过程中\n\nSecurityException - 如果安全管理器存在并且拒绝 RuntimePermission(”shutdownHooks“)\n### Since：\n1.3\n","slug":"addShutdownHook","published":1,"updated":"2021-07-19T16:28:00.296Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphqu00d5itd35bjqdute","content":"<h3 id=\"方法介绍\"><a href=\"#方法介绍\" class=\"headerlink\" title=\"方法介绍\"></a>方法介绍</h3><p><strong>addShutdownHook</strong> 是 Runtime 类中的一个方法，方法声明：</p>\n<pre><code>public void addShutdownHook(Thread hook)\n</code></pre>\n<span id=\"more\"></span>\n\n<p>JDK 帮助手册中的解释如下：</p>\n<p>注册一个新的虚拟机关闭钩子。</p>\n<p>Java 虚拟机在响应两种事件时关闭：</p>\n<ul>\n<li>当最后的非守护线程退出，或者调用 exit （等同于 System.exit ）方法时，程序正常退出。</li>\n<li>响应用户中断而终止虚拟机，例如键入 ^C；或者响应系统事件终止虚拟机，例如用户注销或者系统关闭。</li>\n</ul>\n<p>关闭钩子只是一个已初始化但尚未启动的线程。当虚拟机开启关闭序列时，它会以不确定的顺序启动全部注册的关闭钩子，并让它们并发运行。当所有的钩子已经运行完，如果已启用退出终结方法，那么虚拟机接着会运行所有未调用的终结方法。最后，虚拟机会停止。注意，在关闭序列执行期间守护线程会继续运行，如果通过调用 exit 方法启动关闭非守护线程也会继续运行。</p>\n<p>一旦关闭序列开始执行，只能通过调用 halt 方法停止它，该方法可强制终止虚拟机。</p>\n<p>一旦关闭序列开始执行就不能注册一个新的关闭钩子或者取消注册已经注册的钩子。尝试这些操作会导致抛出 IllegalStateException 。</p>\n<p>关闭钩子运行在虚拟机生命周期中的一个微妙的时间，因此应保护性地对其进行编码。特别是关闭钩子应该编写为线程安全的，并且尽可能避免死锁。关闭钩子还应该不盲目地依靠某些服务，这些服务可能已注册了自己的关闭钩子，所以其本身可能正处于关闭进程中。例如，试图使用其他基于线程的服务（如 AWT 事件指派线程）可能导致死锁。</p>\n<p>关闭钩子应该快速地完成它们的工作。当程序调用 exit 的时候，期望的结果是虚拟机及时关闭并且退出。当由于用户注销或者系统关闭引起虚拟机终止时，底层操作系统可能只允许在固定的时间内关闭并退出。因此，在关闭钩子中尝试任何用户交互或执行长时间的计算是不可取的。就像在其他线程中一样，通过调用线程的 ThreadGroup 对象的 uncaughtException 方法，未被捕获的异常可在关闭钩子中处理。此方法的默认实现是将该异常的堆栈跟踪信息打印至 System.err 并终止线程；它不会导致虚拟机退出或停止。</p>\n<p>仅在很少的情况下，虚拟机可能会中止，也就是没有完全关闭就停止运行。这种情况会发生在虚拟机被从外部终止时，比如在 Unix 上使用 SIGKILL 信号或在 Microsoft Windows 上调用 TerminateProcess 。如果由于例如内部数据结构损坏或试图访问不存在的内存而导致本地方法执行错误，虚拟机也可能会中止。如果虚拟机中止，则不能保证所有的关闭钩子都被运行。</p>\n<h3 id=\"参数\"><a href=\"#参数\" class=\"headerlink\" title=\"参数\"></a>参数</h3><p>hook - 已初始化但尚未启动的线程对象</p>\n<h3 id=\"抛出异常\"><a href=\"#抛出异常\" class=\"headerlink\" title=\"抛出异常\"></a>抛出异常</h3><p>IllegalArgumentException - 如果指定的钩子已注册，或者可以确定钩子正在运行或者已运行完毕</p>\n<p>IllegalStateException - 如果虚拟机已经处于关闭过程中</p>\n<p>SecurityException - 如果安全管理器存在并且拒绝 RuntimePermission(”shutdownHooks“)</p>\n<h3 id=\"Since：\"><a href=\"#Since：\" class=\"headerlink\" title=\"Since：\"></a>Since：</h3><p>1.3</p>\n","site":{"data":{}},"excerpt":"<h3 id=\"方法介绍\"><a href=\"#方法介绍\" class=\"headerlink\" title=\"方法介绍\"></a>方法介绍</h3><p><strong>addShutdownHook</strong> 是 Runtime 类中的一个方法，方法声明：</p>\n<pre><code>public void addShutdownHook(Thread hook)\n</code></pre>","more":"<p>JDK 帮助手册中的解释如下：</p>\n<p>注册一个新的虚拟机关闭钩子。</p>\n<p>Java 虚拟机在响应两种事件时关闭：</p>\n<ul>\n<li>当最后的非守护线程退出，或者调用 exit （等同于 System.exit ）方法时，程序正常退出。</li>\n<li>响应用户中断而终止虚拟机，例如键入 ^C；或者响应系统事件终止虚拟机，例如用户注销或者系统关闭。</li>\n</ul>\n<p>关闭钩子只是一个已初始化但尚未启动的线程。当虚拟机开启关闭序列时，它会以不确定的顺序启动全部注册的关闭钩子，并让它们并发运行。当所有的钩子已经运行完，如果已启用退出终结方法，那么虚拟机接着会运行所有未调用的终结方法。最后，虚拟机会停止。注意，在关闭序列执行期间守护线程会继续运行，如果通过调用 exit 方法启动关闭非守护线程也会继续运行。</p>\n<p>一旦关闭序列开始执行，只能通过调用 halt 方法停止它，该方法可强制终止虚拟机。</p>\n<p>一旦关闭序列开始执行就不能注册一个新的关闭钩子或者取消注册已经注册的钩子。尝试这些操作会导致抛出 IllegalStateException 。</p>\n<p>关闭钩子运行在虚拟机生命周期中的一个微妙的时间，因此应保护性地对其进行编码。特别是关闭钩子应该编写为线程安全的，并且尽可能避免死锁。关闭钩子还应该不盲目地依靠某些服务，这些服务可能已注册了自己的关闭钩子，所以其本身可能正处于关闭进程中。例如，试图使用其他基于线程的服务（如 AWT 事件指派线程）可能导致死锁。</p>\n<p>关闭钩子应该快速地完成它们的工作。当程序调用 exit 的时候，期望的结果是虚拟机及时关闭并且退出。当由于用户注销或者系统关闭引起虚拟机终止时，底层操作系统可能只允许在固定的时间内关闭并退出。因此，在关闭钩子中尝试任何用户交互或执行长时间的计算是不可取的。就像在其他线程中一样，通过调用线程的 ThreadGroup 对象的 uncaughtException 方法，未被捕获的异常可在关闭钩子中处理。此方法的默认实现是将该异常的堆栈跟踪信息打印至 System.err 并终止线程；它不会导致虚拟机退出或停止。</p>\n<p>仅在很少的情况下，虚拟机可能会中止，也就是没有完全关闭就停止运行。这种情况会发生在虚拟机被从外部终止时，比如在 Unix 上使用 SIGKILL 信号或在 Microsoft Windows 上调用 TerminateProcess 。如果由于例如内部数据结构损坏或试图访问不存在的内存而导致本地方法执行错误，虚拟机也可能会中止。如果虚拟机中止，则不能保证所有的关闭钩子都被运行。</p>\n<h3 id=\"参数\"><a href=\"#参数\" class=\"headerlink\" title=\"参数\"></a>参数</h3><p>hook - 已初始化但尚未启动的线程对象</p>\n<h3 id=\"抛出异常\"><a href=\"#抛出异常\" class=\"headerlink\" title=\"抛出异常\"></a>抛出异常</h3><p>IllegalArgumentException - 如果指定的钩子已注册，或者可以确定钩子正在运行或者已运行完毕</p>\n<p>IllegalStateException - 如果虚拟机已经处于关闭过程中</p>\n<p>SecurityException - 如果安全管理器存在并且拒绝 RuntimePermission(”shutdownHooks“)</p>\n<h3 id=\"Since：\"><a href=\"#Since：\" class=\"headerlink\" title=\"Since：\"></a>Since：</h3><p>1.3</p>"},{"title":"dos命令","date":"2015-12-09T01:48:49.000Z","_content":"\n### 基础命令\n**1. dir**\n\n- 无参数： 查看当前所在目录的文件和文件夹。\n- /s： 查看当前目录已及其所有子目录的文件和文件夹。\n- /a： 查看包括隐藏文件的所有文件。\n- /ah： 只显示出隐含文件。\n- /w： 以紧凑方式（一行显示3个文件）显示文件和文件夹。\n- /p： 以分页方式（显示一页之后会自动暂停）显示。\n\n<!-- more -->\n\n**2. cd**\n\n- cd 目录名： 进入特定的目录。\n- cd \\： 退回到根目录。\n- cd ..： 退回到上一级目录。\n\n**3. md**\n\nmd 目录名： 建立特定的文件夹。\n\n**4. rd**\n\nrd 目录名： 删除特定的文件夹。\n\n**5. cls**\n\n清除屏幕。\n\n**6. copy**\n\ncopy 路径\\文件名 路径\\文件名： 把一个文件拷贝到另一个地方。\n\n**7. move**\n\nmove 路径\\文件名 路径\\文件名： 把一个文件移动到另一个地方。\n\n**8. del**\n\ndel不能删除文件夹。\n\n- del 文件名： 删除一个文件。\n- del *.*： 删除当前文件夹下所有文件。\n\n**9. deltree**\n\n删除文件夹和它下面的所有子文件夹还有文件。\n\n**10. format**\n\nformat x: x代表盘符，格式化一个分区。在dos下是用fat文件系统格式化的，在windows2000安装的时候会问你要不要转换为ntfs。\n\n**11. type**\n\ntype 文本文件名： 显示出文本文件的内容。\n\n**12. ren**\n\nren 旧文件名 新文件名： 改文件名。\n\n### 关于网络的常用命令\n\n**1. ping**\n\n- ping 主机ip或名字： 向目标主机发送4个icmp数据包，测试对方主机是否收到并响应，一般常用于做普通网络是否通畅的测试。但是ping不同不代表网络不通，有可能是目标主机装有防火墙并且阻止了icmp响应。\n- ping -t： 不停的发送数据包。当然都很小，不能称作攻击。有些人自己写了一些类似于ping命令的程序，不停的发送很大的数据包，以阻塞目标主机的网络连接。\n\n**2. net**\n\n建议是用 net /? 获取具体帮助信息。实在是有很多参数，参数下面还有参数。常用：net view \\\\主机 来看共享，net start/stop 服务来启动和停止服务。\n\n**3. netstat**\n\nnetstat 主机： 查看主机当前的 tcp/ip 连接状态，如端口的状态。\n\n**4. nbtstat**\n\n查看主机使用的 NetBIOS name。使用 nbtstat /? 查看帮助。\n\n**5. tracert**\n\ntracert 主机： 查看从你自己到目标主机到底经过了那些路径。如： tracert www.baidu.com 然后等待。。。 就会看到你经过的一个个路由节点，一般大一点的路由器，如电信的主干路由，除了ip以外，都有英文标示的。\n\n**6. pathping**\n\npathping 主机： 类似tracert，但可以显示一些tracert不能显示出来的信息。\n\n**7. ftp**\n\n字符方式的 ftp 。\n\n**8. telnet**\n\n字符方式的远程登录程序，是网络人员极其爱用的远程登录程序。一般可以用来测试主机端口是否可用：\ntelnet 主机IP 端口号\n\n**9. ipconfig**\n\n非常有用的网络配置、排错命令。\n\n- 不加参数： 显示当前机器的网络接口状态。\n- /all： 显示详细的信息。\n- /release： 释放当前ip。\n- /renew： 重新申请ip。\n- /flushdns： 刷新dns缓存。\n- /registerdns： 重新在dns服务器上注册自己。\n\n**10. arp**\n\n操作当前的arp缓存。\n\n- arp -a： 显示arp缓存。\n- arp -d： 删除一条缓存纪录。\n- arp -s： 添加一条缓存纪录。\n\n**11. nslookup**\n\n排除dns错误的利器。是一个交互的工具。使用之前请先努力弄清楚dns的作用以及dns的工作原理。\n\n**12. route**\n\n一般用来查看路由表或者添加静态路由：\n\n- route print： 打印路由\n- route add： 添加路由\n\n添加路由参考以下示例：\n\n\troute ADD 157.0.0.0 MASK 255.0.0.0  157.55.80.1\n\n如果想每次开机保留该路由，则添加 -p 参数：\n\n\troute ADD 157.0.0.0 MASK 255.0.0.0  157.55.80.1 -p\n","source":"_posts/dos命令.md","raw":"title: dos命令\ntags:\n  - DOS\ncategories:\n  - 操作系统\n  - DOS\ndate: 2015-12-09 09:48:49\n---\n\n### 基础命令\n**1. dir**\n\n- 无参数： 查看当前所在目录的文件和文件夹。\n- /s： 查看当前目录已及其所有子目录的文件和文件夹。\n- /a： 查看包括隐藏文件的所有文件。\n- /ah： 只显示出隐含文件。\n- /w： 以紧凑方式（一行显示3个文件）显示文件和文件夹。\n- /p： 以分页方式（显示一页之后会自动暂停）显示。\n\n<!-- more -->\n\n**2. cd**\n\n- cd 目录名： 进入特定的目录。\n- cd \\： 退回到根目录。\n- cd ..： 退回到上一级目录。\n\n**3. md**\n\nmd 目录名： 建立特定的文件夹。\n\n**4. rd**\n\nrd 目录名： 删除特定的文件夹。\n\n**5. cls**\n\n清除屏幕。\n\n**6. copy**\n\ncopy 路径\\文件名 路径\\文件名： 把一个文件拷贝到另一个地方。\n\n**7. move**\n\nmove 路径\\文件名 路径\\文件名： 把一个文件移动到另一个地方。\n\n**8. del**\n\ndel不能删除文件夹。\n\n- del 文件名： 删除一个文件。\n- del *.*： 删除当前文件夹下所有文件。\n\n**9. deltree**\n\n删除文件夹和它下面的所有子文件夹还有文件。\n\n**10. format**\n\nformat x: x代表盘符，格式化一个分区。在dos下是用fat文件系统格式化的，在windows2000安装的时候会问你要不要转换为ntfs。\n\n**11. type**\n\ntype 文本文件名： 显示出文本文件的内容。\n\n**12. ren**\n\nren 旧文件名 新文件名： 改文件名。\n\n### 关于网络的常用命令\n\n**1. ping**\n\n- ping 主机ip或名字： 向目标主机发送4个icmp数据包，测试对方主机是否收到并响应，一般常用于做普通网络是否通畅的测试。但是ping不同不代表网络不通，有可能是目标主机装有防火墙并且阻止了icmp响应。\n- ping -t： 不停的发送数据包。当然都很小，不能称作攻击。有些人自己写了一些类似于ping命令的程序，不停的发送很大的数据包，以阻塞目标主机的网络连接。\n\n**2. net**\n\n建议是用 net /? 获取具体帮助信息。实在是有很多参数，参数下面还有参数。常用：net view \\\\主机 来看共享，net start/stop 服务来启动和停止服务。\n\n**3. netstat**\n\nnetstat 主机： 查看主机当前的 tcp/ip 连接状态，如端口的状态。\n\n**4. nbtstat**\n\n查看主机使用的 NetBIOS name。使用 nbtstat /? 查看帮助。\n\n**5. tracert**\n\ntracert 主机： 查看从你自己到目标主机到底经过了那些路径。如： tracert www.baidu.com 然后等待。。。 就会看到你经过的一个个路由节点，一般大一点的路由器，如电信的主干路由，除了ip以外，都有英文标示的。\n\n**6. pathping**\n\npathping 主机： 类似tracert，但可以显示一些tracert不能显示出来的信息。\n\n**7. ftp**\n\n字符方式的 ftp 。\n\n**8. telnet**\n\n字符方式的远程登录程序，是网络人员极其爱用的远程登录程序。一般可以用来测试主机端口是否可用：\ntelnet 主机IP 端口号\n\n**9. ipconfig**\n\n非常有用的网络配置、排错命令。\n\n- 不加参数： 显示当前机器的网络接口状态。\n- /all： 显示详细的信息。\n- /release： 释放当前ip。\n- /renew： 重新申请ip。\n- /flushdns： 刷新dns缓存。\n- /registerdns： 重新在dns服务器上注册自己。\n\n**10. arp**\n\n操作当前的arp缓存。\n\n- arp -a： 显示arp缓存。\n- arp -d： 删除一条缓存纪录。\n- arp -s： 添加一条缓存纪录。\n\n**11. nslookup**\n\n排除dns错误的利器。是一个交互的工具。使用之前请先努力弄清楚dns的作用以及dns的工作原理。\n\n**12. route**\n\n一般用来查看路由表或者添加静态路由：\n\n- route print： 打印路由\n- route add： 添加路由\n\n添加路由参考以下示例：\n\n\troute ADD 157.0.0.0 MASK 255.0.0.0  157.55.80.1\n\n如果想每次开机保留该路由，则添加 -p 参数：\n\n\troute ADD 157.0.0.0 MASK 255.0.0.0  157.55.80.1 -p\n","slug":"dos命令","published":1,"updated":"2021-07-19T16:28:00.300Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphqw00daitd3fngrfz5c","content":"<h3 id=\"基础命令\"><a href=\"#基础命令\" class=\"headerlink\" title=\"基础命令\"></a>基础命令</h3><p><strong>1. dir</strong></p>\n<ul>\n<li>无参数： 查看当前所在目录的文件和文件夹。</li>\n<li>/s： 查看当前目录已及其所有子目录的文件和文件夹。</li>\n<li>/a： 查看包括隐藏文件的所有文件。</li>\n<li>/ah： 只显示出隐含文件。</li>\n<li>/w： 以紧凑方式（一行显示3个文件）显示文件和文件夹。</li>\n<li>/p： 以分页方式（显示一页之后会自动暂停）显示。</li>\n</ul>\n<span id=\"more\"></span>\n\n<p><strong>2. cd</strong></p>\n<ul>\n<li>cd 目录名： 进入特定的目录。</li>\n<li>cd \\： 退回到根目录。</li>\n<li>cd ..： 退回到上一级目录。</li>\n</ul>\n<p><strong>3. md</strong></p>\n<p>md 目录名： 建立特定的文件夹。</p>\n<p><strong>4. rd</strong></p>\n<p>rd 目录名： 删除特定的文件夹。</p>\n<p><strong>5. cls</strong></p>\n<p>清除屏幕。</p>\n<p><strong>6. copy</strong></p>\n<p>copy 路径\\文件名 路径\\文件名： 把一个文件拷贝到另一个地方。</p>\n<p><strong>7. move</strong></p>\n<p>move 路径\\文件名 路径\\文件名： 把一个文件移动到另一个地方。</p>\n<p><strong>8. del</strong></p>\n<p>del不能删除文件夹。</p>\n<ul>\n<li>del 文件名： 删除一个文件。</li>\n<li>del *.*： 删除当前文件夹下所有文件。</li>\n</ul>\n<p><strong>9. deltree</strong></p>\n<p>删除文件夹和它下面的所有子文件夹还有文件。</p>\n<p><strong>10. format</strong></p>\n<p>format x: x代表盘符，格式化一个分区。在dos下是用fat文件系统格式化的，在windows2000安装的时候会问你要不要转换为ntfs。</p>\n<p><strong>11. type</strong></p>\n<p>type 文本文件名： 显示出文本文件的内容。</p>\n<p><strong>12. ren</strong></p>\n<p>ren 旧文件名 新文件名： 改文件名。</p>\n<h3 id=\"关于网络的常用命令\"><a href=\"#关于网络的常用命令\" class=\"headerlink\" title=\"关于网络的常用命令\"></a>关于网络的常用命令</h3><p><strong>1. ping</strong></p>\n<ul>\n<li>ping 主机ip或名字： 向目标主机发送4个icmp数据包，测试对方主机是否收到并响应，一般常用于做普通网络是否通畅的测试。但是ping不同不代表网络不通，有可能是目标主机装有防火墙并且阻止了icmp响应。</li>\n<li>ping -t： 不停的发送数据包。当然都很小，不能称作攻击。有些人自己写了一些类似于ping命令的程序，不停的发送很大的数据包，以阻塞目标主机的网络连接。</li>\n</ul>\n<p><strong>2. net</strong></p>\n<p>建议是用 net /? 获取具体帮助信息。实在是有很多参数，参数下面还有参数。常用：net view \\主机 来看共享，net start/stop 服务来启动和停止服务。</p>\n<p><strong>3. netstat</strong></p>\n<p>netstat 主机： 查看主机当前的 tcp/ip 连接状态，如端口的状态。</p>\n<p><strong>4. nbtstat</strong></p>\n<p>查看主机使用的 NetBIOS name。使用 nbtstat /? 查看帮助。</p>\n<p><strong>5. tracert</strong></p>\n<p>tracert 主机： 查看从你自己到目标主机到底经过了那些路径。如： tracert <a href=\"http://www.baidu.com/\">www.baidu.com</a> 然后等待。。。 就会看到你经过的一个个路由节点，一般大一点的路由器，如电信的主干路由，除了ip以外，都有英文标示的。</p>\n<p><strong>6. pathping</strong></p>\n<p>pathping 主机： 类似tracert，但可以显示一些tracert不能显示出来的信息。</p>\n<p><strong>7. ftp</strong></p>\n<p>字符方式的 ftp 。</p>\n<p><strong>8. telnet</strong></p>\n<p>字符方式的远程登录程序，是网络人员极其爱用的远程登录程序。一般可以用来测试主机端口是否可用：<br>telnet 主机IP 端口号</p>\n<p><strong>9. ipconfig</strong></p>\n<p>非常有用的网络配置、排错命令。</p>\n<ul>\n<li>不加参数： 显示当前机器的网络接口状态。</li>\n<li>/all： 显示详细的信息。</li>\n<li>/release： 释放当前ip。</li>\n<li>/renew： 重新申请ip。</li>\n<li>/flushdns： 刷新dns缓存。</li>\n<li>/registerdns： 重新在dns服务器上注册自己。</li>\n</ul>\n<p><strong>10. arp</strong></p>\n<p>操作当前的arp缓存。</p>\n<ul>\n<li>arp -a： 显示arp缓存。</li>\n<li>arp -d： 删除一条缓存纪录。</li>\n<li>arp -s： 添加一条缓存纪录。</li>\n</ul>\n<p><strong>11. nslookup</strong></p>\n<p>排除dns错误的利器。是一个交互的工具。使用之前请先努力弄清楚dns的作用以及dns的工作原理。</p>\n<p><strong>12. route</strong></p>\n<p>一般用来查看路由表或者添加静态路由：</p>\n<ul>\n<li>route print： 打印路由</li>\n<li>route add： 添加路由</li>\n</ul>\n<p>添加路由参考以下示例：</p>\n<pre><code>route ADD 157.0.0.0 MASK 255.0.0.0  157.55.80.1\n</code></pre>\n<p>如果想每次开机保留该路由，则添加 -p 参数：</p>\n<pre><code>route ADD 157.0.0.0 MASK 255.0.0.0  157.55.80.1 -p\n</code></pre>\n","site":{"data":{}},"excerpt":"<h3 id=\"基础命令\"><a href=\"#基础命令\" class=\"headerlink\" title=\"基础命令\"></a>基础命令</h3><p><strong>1. dir</strong></p>\n<ul>\n<li>无参数： 查看当前所在目录的文件和文件夹。</li>\n<li>/s： 查看当前目录已及其所有子目录的文件和文件夹。</li>\n<li>/a： 查看包括隐藏文件的所有文件。</li>\n<li>/ah： 只显示出隐含文件。</li>\n<li>/w： 以紧凑方式（一行显示3个文件）显示文件和文件夹。</li>\n<li>/p： 以分页方式（显示一页之后会自动暂停）显示。</li>\n</ul>","more":"<p><strong>2. cd</strong></p>\n<ul>\n<li>cd 目录名： 进入特定的目录。</li>\n<li>cd \\： 退回到根目录。</li>\n<li>cd ..： 退回到上一级目录。</li>\n</ul>\n<p><strong>3. md</strong></p>\n<p>md 目录名： 建立特定的文件夹。</p>\n<p><strong>4. rd</strong></p>\n<p>rd 目录名： 删除特定的文件夹。</p>\n<p><strong>5. cls</strong></p>\n<p>清除屏幕。</p>\n<p><strong>6. copy</strong></p>\n<p>copy 路径\\文件名 路径\\文件名： 把一个文件拷贝到另一个地方。</p>\n<p><strong>7. move</strong></p>\n<p>move 路径\\文件名 路径\\文件名： 把一个文件移动到另一个地方。</p>\n<p><strong>8. del</strong></p>\n<p>del不能删除文件夹。</p>\n<ul>\n<li>del 文件名： 删除一个文件。</li>\n<li>del *.*： 删除当前文件夹下所有文件。</li>\n</ul>\n<p><strong>9. deltree</strong></p>\n<p>删除文件夹和它下面的所有子文件夹还有文件。</p>\n<p><strong>10. format</strong></p>\n<p>format x: x代表盘符，格式化一个分区。在dos下是用fat文件系统格式化的，在windows2000安装的时候会问你要不要转换为ntfs。</p>\n<p><strong>11. type</strong></p>\n<p>type 文本文件名： 显示出文本文件的内容。</p>\n<p><strong>12. ren</strong></p>\n<p>ren 旧文件名 新文件名： 改文件名。</p>\n<h3 id=\"关于网络的常用命令\"><a href=\"#关于网络的常用命令\" class=\"headerlink\" title=\"关于网络的常用命令\"></a>关于网络的常用命令</h3><p><strong>1. ping</strong></p>\n<ul>\n<li>ping 主机ip或名字： 向目标主机发送4个icmp数据包，测试对方主机是否收到并响应，一般常用于做普通网络是否通畅的测试。但是ping不同不代表网络不通，有可能是目标主机装有防火墙并且阻止了icmp响应。</li>\n<li>ping -t： 不停的发送数据包。当然都很小，不能称作攻击。有些人自己写了一些类似于ping命令的程序，不停的发送很大的数据包，以阻塞目标主机的网络连接。</li>\n</ul>\n<p><strong>2. net</strong></p>\n<p>建议是用 net /? 获取具体帮助信息。实在是有很多参数，参数下面还有参数。常用：net view \\主机 来看共享，net start/stop 服务来启动和停止服务。</p>\n<p><strong>3. netstat</strong></p>\n<p>netstat 主机： 查看主机当前的 tcp/ip 连接状态，如端口的状态。</p>\n<p><strong>4. nbtstat</strong></p>\n<p>查看主机使用的 NetBIOS name。使用 nbtstat /? 查看帮助。</p>\n<p><strong>5. tracert</strong></p>\n<p>tracert 主机： 查看从你自己到目标主机到底经过了那些路径。如： tracert <a href=\"http://www.baidu.com/\">www.baidu.com</a> 然后等待。。。 就会看到你经过的一个个路由节点，一般大一点的路由器，如电信的主干路由，除了ip以外，都有英文标示的。</p>\n<p><strong>6. pathping</strong></p>\n<p>pathping 主机： 类似tracert，但可以显示一些tracert不能显示出来的信息。</p>\n<p><strong>7. ftp</strong></p>\n<p>字符方式的 ftp 。</p>\n<p><strong>8. telnet</strong></p>\n<p>字符方式的远程登录程序，是网络人员极其爱用的远程登录程序。一般可以用来测试主机端口是否可用：<br>telnet 主机IP 端口号</p>\n<p><strong>9. ipconfig</strong></p>\n<p>非常有用的网络配置、排错命令。</p>\n<ul>\n<li>不加参数： 显示当前机器的网络接口状态。</li>\n<li>/all： 显示详细的信息。</li>\n<li>/release： 释放当前ip。</li>\n<li>/renew： 重新申请ip。</li>\n<li>/flushdns： 刷新dns缓存。</li>\n<li>/registerdns： 重新在dns服务器上注册自己。</li>\n</ul>\n<p><strong>10. arp</strong></p>\n<p>操作当前的arp缓存。</p>\n<ul>\n<li>arp -a： 显示arp缓存。</li>\n<li>arp -d： 删除一条缓存纪录。</li>\n<li>arp -s： 添加一条缓存纪录。</li>\n</ul>\n<p><strong>11. nslookup</strong></p>\n<p>排除dns错误的利器。是一个交互的工具。使用之前请先努力弄清楚dns的作用以及dns的工作原理。</p>\n<p><strong>12. route</strong></p>\n<p>一般用来查看路由表或者添加静态路由：</p>\n<ul>\n<li>route print： 打印路由</li>\n<li>route add： 添加路由</li>\n</ul>\n<p>添加路由参考以下示例：</p>\n<pre><code>route ADD 157.0.0.0 MASK 255.0.0.0  157.55.80.1\n</code></pre>\n<p>如果想每次开机保留该路由，则添加 -p 参数：</p>\n<pre><code>route ADD 157.0.0.0 MASK 255.0.0.0  157.55.80.1 -p\n</code></pre>"},{"title":"hadoop 2.7.3 集群安装","date":"2017-03-03T02:31:39.000Z","_content":"\n### 服务器信息\n\n#### 服务器列表\n\n- 10.142.166.81：NameNode\n- 10.142.166.119：ResourceManager / MapReduce JobHistory Server\n- 10.142.165.40 / 10.142.165.41 / 10.142.165.44：DataNode / NodeManager\n\n#### 服务器配置信息\n\n- OS：CentOS 6.6\n- 4 cpu\n- 8G Mem\n- 磁盘目录：\n  - /frin：80G\n  - /data：1T\n\n<!-- more -->\n\n### 准备工作\n\n#### 创建 hadoop 用户\n\n在所有服务器上创建 hadoop 用户，整个集群使用该用户进行管理。创建用户命令：\n\n    useradd hadoop\n\n#### SSH 免密钥\n\n配置服务器 10.142.166.81 的 hadoop 用户到其他服务器的 SSH 免密钥登陆。\n\n#### 目录权限\n\n修改所有服务器的 /data 目录的属主及组为 hadoop。\n\n在所有服务器上创建 /frin/hadoop 目录，并修改属主及组为 hadoop。命令如下：\n\n    cd /frin\n    mkdir hadoop\n    chown hadoop hadoop\n    chgrp hadoop hadoop\n\n### 安装 Hadoop-2.7.3\n\n#### 下载并解压 Hadoop\n\n此处使用官网发行的二进制运行包。在实际中的集群安装可以自己编译之后再安装。\n\n使用 hadoop 帐号登陆服务器 10.142.166.81，下载并解压 Hadoop：\n\n    cd /frin/hadoop\n    wget http://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz\n    tar xzvf hadoop-2.7.3.tar.gz\n\n#### 创建软链\n\n为了统一及方便管理，在 /usr/local 下创建 hadoop 软链，并修改软链的属主及组为 hadoop：\n\n    cd /usr/local\n    ln -s /letv/hadoop/hadoop-2.7.3 hadoop\n    chown -h hadoop hadoop\n    chgrp -h hadoop hadoop\n\n#### 设置HADOOP_PREFIX及HADOOP_HOME\n\n使用 root 帐号登陆服务器 10.142.166.81，编辑 /etc/profile，增加以下内容：\n\n    export HADOOP_PREFIX=/usr/local/hadoop\n    export HADOOP_HOME=/usr/local/hadoop\n\n编辑完成后，将 /etc/profile 分发到其他服务器。\n\n#### 配置 hosts\n\n使用 root 帐号登陆服务器 10.142.166.81，编辑 /etc/hosts，内容如下：\n\n    127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n    ::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n\n    10.142.166.81 frin-namenode1\n    10.142.166.119 frin-resourcemanager1\n    10.142.166.119 frin-jobhistoryserver\n\n    10.142.166.81 vm-10-142-166-81\n    10.142.166.119 vm-10-142-166-119\n    10.142.165.40 vm-10-142-165-40\n    10.142.165.41 vm-10-142-165-41\n    10.142.165.44 vm-10-142-165-44\n\n编辑完成后，将 /etc/profile 分发到其他服务器。\n\n#### etc/hadoop/core-site.xml 配置\n\n    <property>\n      <name>fs.defaultFS</name>\n      <value>hdfs://frin-namenode1:9000</value>\n    </property>\n    <property>\n      <name>io.file.buffer.size</name>\n      <value>131072</value>\n    </property>\n\n#### etc/hadoop/hdfs-site.xml 配置\n\n##### NameNode 配置\n\n    <!-- configurations for namenode -->\n    <property>\n      <name>dfs.namenode.name.dir</name>\n      <value>/data/dfs/name</value>\n    </property>\n    <property>\n      <name>dfs.hosts</name>\n      <value>/usr/local/hadoop/etc/hadoop/slaves</value>\n    </property>\n    <property>\n      <name>dfs.hosts.exclude</name>\n      <value>/usr/local/hadoop/etc/hadoop/exclude_hosts</value>\n    </property>\n    <property>\n      <name>dfs.blocksize</name>\n      <value>268435456</value>\n    </property>\n    <property>\n      <name>dfs.namenode.handler.count</name>\n      <value>30</value>\n    </property>\n\n配置 etc/hadoop/slaves 文件内容如下：\n\n    10.142.165.44\n    10.142.165.41\n    10.142.165.40\n\n创建空文件 etc/hadoop/exclude_hosts。\n\n##### DataNode 配置\n\n    <!-- configurations for datanode -->\n    <property>\n      <name>dfs.datanode.data.dir</name>\n      <value>/data/dfs/data</value>\n    </property>\n\n#### etc/hadoop/yarn-site.xml 配置\n\n##### ResourceManager 和  NodeManager 配置\n\n    <!-- configurations for ResourceManager and NodeManager -->\n    <property>\n      <name>yarn.log-aggregation-enable</name>\n      <value>true</value>\n    </property>\n    <property>\n      <name>yarn.nodemanager.remote-app-log-dir</name>\n      <value>/data/hadoop/yarn-logs</value>\n    </property>\n\n> 集群启动后，在 HDFS 上创建目录 /data/hadoop/yarn-logs，并修改目录权限为 777。\n\n##### ResourceManager 配置\n\n    <!-- configurations for ResourceManager -->\n    <property>\n      <name>yarn.resourcemanager.webapp.address</name>\n      <value>frin-resourcemanager1:8088</value>\n    </property>\n    <property>\n      <name>yarn.resourcemanager.hostname</name>\n      <value>frin-resourcemanager1</value>\n    </property>\n    <property>\n      <name>yarn.resourcemanager.scheduler.class</name>\n      <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>\n    </property>\n    <property>\n      <name>yarn.scheduler.minimum-allocation-mb</name>\n      <value>1024</value>\n    </property>\n    <property>\n      <name>yarn.scheduler.maximum-allocation-mb</name>\n      <value>8192</value>\n    </property>\n    <property>\n      <name>yarn.resourcemanager.nodes.include-path</name>\n      <value>/usr/local/hadoop/etc/hadoop/nodemanagers</value>\n    </property>\n\n##### NodeManager 配置\n\n    <!-- configurations for NodeManager -->\n    <property>\n      <name>yarn.nodemanager.resource.memory-mb</name>\n      <value>6144</value>\n    </property>\n    <property>\n      <name>yarn.nodemanager.vmem-pmem-ratio</name>\n      <value>1.8</value>\n    </property>\n    <property>\n      <name>yarn.nodemanager.local-dirs</name>\n      <value>/data/yarn/data</value>\n    </property>\n    <property>\n      <name>yarn.nodemanager.log-dirs</name>\n      <value>/data/yarn/log</value>\n    </property>\n    <property>\n      <name>yarn.nodemanager.aux-services</name>\n      <value>mapreduce_shuffle</value>\n    </property>\n\n##### History Server 配置\n\n    <!-- configurations for History Server -->\n    <property>\n      <name>yarn.log-aggregation.retain-seconds</name>\n      <value>86400</value>\n    </property>\n    <property>\n      <name>yarn.log-aggregation.retain-check-interval-seconds</name>\n      <value>1800</value>\n    </property>\n\n#### etc/hadoop/mapred-site.xml 配置\n\n##### MapReduce Applications 配置\n\n    <!-- configurations for MapReduce Applications -->\n    <property>\n      <name>mapreduce.framework.name</name>\n      <value>yarn</value>\n    </property>\n    <property>\n      <name>mapreduce.map.memory.mb</name>\n      <value>2048</value>\n    </property>\n    <property>\n      <name>mapreduce.map.java.opts</name>\n      <value>-Xmx1024M</value>\n    </property>\n    <property>\n      <name>mapreduce.reduce.memory.mb</name>\n      <value>4096</value>\n    </property>\n    <property>\n      <name>mapreduce.reduce.java.opts</name>\n      <value>-Xmx2048M</value>\n    </property>\n\n##### MapReduce JobHistory Server 配置\n\n    <!-- configurations for MapReduce JobHistory Server -->\n    <property>\n      <name>mapreduce.jobhistory.address</name>\n      <value>frin-jobhistoryserver:10020</value>\n    </property>\n    <property>\n      <name>mapreduce.jobhistory.webapp.address</name>\n      <value>frin-jobhistoryserver:19888</value>\n    </property>\n\n#### NodeManager 健康状态检测\n\n##### 检测脚本\n\n在 bin 目录下创建 nodemanager-health-checker.sh，在该脚本中可以编写 NodeManager 是否健康的判断逻辑：\n\n    #!/bin/bash\n    cd `dirname $0`\n\n##### 检测配置\n\n在 etc/hadoop/yarn-site.xml 中增加以下配置：\n\n    <!-- configurations for Monitoring Health of NodeManager -->\n    <property>\n      <name>yarn.nodemanager.health-checker.script.path</name>\n      <value>/usr/local/hadoop/bin/nodemanager-health-checker.sh</value>\n    </property>\n    <property>\n      <name>yarn.nodemanager.health-checker.interval-ms</name>\n      <value>300000</value>\n    </property>\n\n> 官网文档中的参数 yarn.nodemanager.health-checker.script.interval-ms 是错误的，正确的参数应该是 yarn.nodemanager.health-checker.interval-ms。我已经在 Jira 中提交了一个 Issue：<https://issues.apache.org/jira/browse/YARN-6274>。\n> 官网文档URL：<http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/ClusterSetup.html>\n\n##### 分发 Hadoop\n\n将已经配置好的 hadoop 整个目录分发到所有服务器。\n\n### Hadoop 集群启动\n\n#### 格式化 HDFS\n\n使用 hadoop 帐号登陆服务器 10.142.166.81，用以下命令格式化一个新的 HDFS 系统：\n\n     bin/hdfs namenode -format\n\n#### 启动 NameNode\n\n使用 hadoop 帐号登陆服务器 10.142.166.81，用以下命令启动 NameNode：\n\n    sbin/hadoop-daemon.sh start namenode\n\n#### 启动 DataNode\n\n在 slaves 中所有的服务器上用以下命令启动 DataNode：\n\n    sbin/hadoop-daemon.sh start datanode\n\n#### 启动 ResourceManager\n\n用 hadoop 帐号登陆服务器 10.142.166.119，用以下命令启动 Resourcemanager：\n\n    sbin/yarn-daemon.sh start resourcemanager\n\n#### 启动 NodeManager\n\n在 slaves 中所有的服务器上用以下命令启动 NodeManager\n\n    sbin/yarn-daemon.sh start nodemanager\n\n#### 启动 MapReduce JobHistory Server\n\n使用 hadoop 帐号登陆服务器 10.142.166.119，用以下命令启动 MapReduce JobHistory Server\n\n    sbin/mr-jobhistory-daemon.sh start historyserver\n\n### 集群状态检查\n\n将 hosts 配置添加到本地配置文件。\n\n#### NameNode\n\n访问网址：<http://frin-namenode1:50070/>。看到如下信息，说明服务正常：![NameNode](/uploads/20170303/namenode.png)\n\n#### ResourceManager\n\n访问网址：<http://frin-resourcemanager1:8088/>。看到如下信息，说明服务正常：![ResourceManager](/uploads/20170303/resourcemanager.png)\n\n#### MapReduce JobHistory Server\n\n访问网址：<http://frin-jobhistoryserver:19888/>。如果有 Job 运行，则界面如下：![MapReduce JobHistory Server](/uploads/20170303/mapreduce-jobhistory-server.png)","source":"_posts/hadoop-2-7-3-集群安装.md","raw":"title: hadoop 2.7.3 集群安装\ntags:\n  - Hadoop\ncategories:\n  - 大数据\n  - Hadoop\ndate: 2017-03-03 10:31:39\n---\n\n### 服务器信息\n\n#### 服务器列表\n\n- 10.142.166.81：NameNode\n- 10.142.166.119：ResourceManager / MapReduce JobHistory Server\n- 10.142.165.40 / 10.142.165.41 / 10.142.165.44：DataNode / NodeManager\n\n#### 服务器配置信息\n\n- OS：CentOS 6.6\n- 4 cpu\n- 8G Mem\n- 磁盘目录：\n  - /frin：80G\n  - /data：1T\n\n<!-- more -->\n\n### 准备工作\n\n#### 创建 hadoop 用户\n\n在所有服务器上创建 hadoop 用户，整个集群使用该用户进行管理。创建用户命令：\n\n    useradd hadoop\n\n#### SSH 免密钥\n\n配置服务器 10.142.166.81 的 hadoop 用户到其他服务器的 SSH 免密钥登陆。\n\n#### 目录权限\n\n修改所有服务器的 /data 目录的属主及组为 hadoop。\n\n在所有服务器上创建 /frin/hadoop 目录，并修改属主及组为 hadoop。命令如下：\n\n    cd /frin\n    mkdir hadoop\n    chown hadoop hadoop\n    chgrp hadoop hadoop\n\n### 安装 Hadoop-2.7.3\n\n#### 下载并解压 Hadoop\n\n此处使用官网发行的二进制运行包。在实际中的集群安装可以自己编译之后再安装。\n\n使用 hadoop 帐号登陆服务器 10.142.166.81，下载并解压 Hadoop：\n\n    cd /frin/hadoop\n    wget http://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz\n    tar xzvf hadoop-2.7.3.tar.gz\n\n#### 创建软链\n\n为了统一及方便管理，在 /usr/local 下创建 hadoop 软链，并修改软链的属主及组为 hadoop：\n\n    cd /usr/local\n    ln -s /letv/hadoop/hadoop-2.7.3 hadoop\n    chown -h hadoop hadoop\n    chgrp -h hadoop hadoop\n\n#### 设置HADOOP_PREFIX及HADOOP_HOME\n\n使用 root 帐号登陆服务器 10.142.166.81，编辑 /etc/profile，增加以下内容：\n\n    export HADOOP_PREFIX=/usr/local/hadoop\n    export HADOOP_HOME=/usr/local/hadoop\n\n编辑完成后，将 /etc/profile 分发到其他服务器。\n\n#### 配置 hosts\n\n使用 root 帐号登陆服务器 10.142.166.81，编辑 /etc/hosts，内容如下：\n\n    127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n    ::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n\n    10.142.166.81 frin-namenode1\n    10.142.166.119 frin-resourcemanager1\n    10.142.166.119 frin-jobhistoryserver\n\n    10.142.166.81 vm-10-142-166-81\n    10.142.166.119 vm-10-142-166-119\n    10.142.165.40 vm-10-142-165-40\n    10.142.165.41 vm-10-142-165-41\n    10.142.165.44 vm-10-142-165-44\n\n编辑完成后，将 /etc/profile 分发到其他服务器。\n\n#### etc/hadoop/core-site.xml 配置\n\n    <property>\n      <name>fs.defaultFS</name>\n      <value>hdfs://frin-namenode1:9000</value>\n    </property>\n    <property>\n      <name>io.file.buffer.size</name>\n      <value>131072</value>\n    </property>\n\n#### etc/hadoop/hdfs-site.xml 配置\n\n##### NameNode 配置\n\n    <!-- configurations for namenode -->\n    <property>\n      <name>dfs.namenode.name.dir</name>\n      <value>/data/dfs/name</value>\n    </property>\n    <property>\n      <name>dfs.hosts</name>\n      <value>/usr/local/hadoop/etc/hadoop/slaves</value>\n    </property>\n    <property>\n      <name>dfs.hosts.exclude</name>\n      <value>/usr/local/hadoop/etc/hadoop/exclude_hosts</value>\n    </property>\n    <property>\n      <name>dfs.blocksize</name>\n      <value>268435456</value>\n    </property>\n    <property>\n      <name>dfs.namenode.handler.count</name>\n      <value>30</value>\n    </property>\n\n配置 etc/hadoop/slaves 文件内容如下：\n\n    10.142.165.44\n    10.142.165.41\n    10.142.165.40\n\n创建空文件 etc/hadoop/exclude_hosts。\n\n##### DataNode 配置\n\n    <!-- configurations for datanode -->\n    <property>\n      <name>dfs.datanode.data.dir</name>\n      <value>/data/dfs/data</value>\n    </property>\n\n#### etc/hadoop/yarn-site.xml 配置\n\n##### ResourceManager 和  NodeManager 配置\n\n    <!-- configurations for ResourceManager and NodeManager -->\n    <property>\n      <name>yarn.log-aggregation-enable</name>\n      <value>true</value>\n    </property>\n    <property>\n      <name>yarn.nodemanager.remote-app-log-dir</name>\n      <value>/data/hadoop/yarn-logs</value>\n    </property>\n\n> 集群启动后，在 HDFS 上创建目录 /data/hadoop/yarn-logs，并修改目录权限为 777。\n\n##### ResourceManager 配置\n\n    <!-- configurations for ResourceManager -->\n    <property>\n      <name>yarn.resourcemanager.webapp.address</name>\n      <value>frin-resourcemanager1:8088</value>\n    </property>\n    <property>\n      <name>yarn.resourcemanager.hostname</name>\n      <value>frin-resourcemanager1</value>\n    </property>\n    <property>\n      <name>yarn.resourcemanager.scheduler.class</name>\n      <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>\n    </property>\n    <property>\n      <name>yarn.scheduler.minimum-allocation-mb</name>\n      <value>1024</value>\n    </property>\n    <property>\n      <name>yarn.scheduler.maximum-allocation-mb</name>\n      <value>8192</value>\n    </property>\n    <property>\n      <name>yarn.resourcemanager.nodes.include-path</name>\n      <value>/usr/local/hadoop/etc/hadoop/nodemanagers</value>\n    </property>\n\n##### NodeManager 配置\n\n    <!-- configurations for NodeManager -->\n    <property>\n      <name>yarn.nodemanager.resource.memory-mb</name>\n      <value>6144</value>\n    </property>\n    <property>\n      <name>yarn.nodemanager.vmem-pmem-ratio</name>\n      <value>1.8</value>\n    </property>\n    <property>\n      <name>yarn.nodemanager.local-dirs</name>\n      <value>/data/yarn/data</value>\n    </property>\n    <property>\n      <name>yarn.nodemanager.log-dirs</name>\n      <value>/data/yarn/log</value>\n    </property>\n    <property>\n      <name>yarn.nodemanager.aux-services</name>\n      <value>mapreduce_shuffle</value>\n    </property>\n\n##### History Server 配置\n\n    <!-- configurations for History Server -->\n    <property>\n      <name>yarn.log-aggregation.retain-seconds</name>\n      <value>86400</value>\n    </property>\n    <property>\n      <name>yarn.log-aggregation.retain-check-interval-seconds</name>\n      <value>1800</value>\n    </property>\n\n#### etc/hadoop/mapred-site.xml 配置\n\n##### MapReduce Applications 配置\n\n    <!-- configurations for MapReduce Applications -->\n    <property>\n      <name>mapreduce.framework.name</name>\n      <value>yarn</value>\n    </property>\n    <property>\n      <name>mapreduce.map.memory.mb</name>\n      <value>2048</value>\n    </property>\n    <property>\n      <name>mapreduce.map.java.opts</name>\n      <value>-Xmx1024M</value>\n    </property>\n    <property>\n      <name>mapreduce.reduce.memory.mb</name>\n      <value>4096</value>\n    </property>\n    <property>\n      <name>mapreduce.reduce.java.opts</name>\n      <value>-Xmx2048M</value>\n    </property>\n\n##### MapReduce JobHistory Server 配置\n\n    <!-- configurations for MapReduce JobHistory Server -->\n    <property>\n      <name>mapreduce.jobhistory.address</name>\n      <value>frin-jobhistoryserver:10020</value>\n    </property>\n    <property>\n      <name>mapreduce.jobhistory.webapp.address</name>\n      <value>frin-jobhistoryserver:19888</value>\n    </property>\n\n#### NodeManager 健康状态检测\n\n##### 检测脚本\n\n在 bin 目录下创建 nodemanager-health-checker.sh，在该脚本中可以编写 NodeManager 是否健康的判断逻辑：\n\n    #!/bin/bash\n    cd `dirname $0`\n\n##### 检测配置\n\n在 etc/hadoop/yarn-site.xml 中增加以下配置：\n\n    <!-- configurations for Monitoring Health of NodeManager -->\n    <property>\n      <name>yarn.nodemanager.health-checker.script.path</name>\n      <value>/usr/local/hadoop/bin/nodemanager-health-checker.sh</value>\n    </property>\n    <property>\n      <name>yarn.nodemanager.health-checker.interval-ms</name>\n      <value>300000</value>\n    </property>\n\n> 官网文档中的参数 yarn.nodemanager.health-checker.script.interval-ms 是错误的，正确的参数应该是 yarn.nodemanager.health-checker.interval-ms。我已经在 Jira 中提交了一个 Issue：<https://issues.apache.org/jira/browse/YARN-6274>。\n> 官网文档URL：<http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/ClusterSetup.html>\n\n##### 分发 Hadoop\n\n将已经配置好的 hadoop 整个目录分发到所有服务器。\n\n### Hadoop 集群启动\n\n#### 格式化 HDFS\n\n使用 hadoop 帐号登陆服务器 10.142.166.81，用以下命令格式化一个新的 HDFS 系统：\n\n     bin/hdfs namenode -format\n\n#### 启动 NameNode\n\n使用 hadoop 帐号登陆服务器 10.142.166.81，用以下命令启动 NameNode：\n\n    sbin/hadoop-daemon.sh start namenode\n\n#### 启动 DataNode\n\n在 slaves 中所有的服务器上用以下命令启动 DataNode：\n\n    sbin/hadoop-daemon.sh start datanode\n\n#### 启动 ResourceManager\n\n用 hadoop 帐号登陆服务器 10.142.166.119，用以下命令启动 Resourcemanager：\n\n    sbin/yarn-daemon.sh start resourcemanager\n\n#### 启动 NodeManager\n\n在 slaves 中所有的服务器上用以下命令启动 NodeManager\n\n    sbin/yarn-daemon.sh start nodemanager\n\n#### 启动 MapReduce JobHistory Server\n\n使用 hadoop 帐号登陆服务器 10.142.166.119，用以下命令启动 MapReduce JobHistory Server\n\n    sbin/mr-jobhistory-daemon.sh start historyserver\n\n### 集群状态检查\n\n将 hosts 配置添加到本地配置文件。\n\n#### NameNode\n\n访问网址：<http://frin-namenode1:50070/>。看到如下信息，说明服务正常：![NameNode](/uploads/20170303/namenode.png)\n\n#### ResourceManager\n\n访问网址：<http://frin-resourcemanager1:8088/>。看到如下信息，说明服务正常：![ResourceManager](/uploads/20170303/resourcemanager.png)\n\n#### MapReduce JobHistory Server\n\n访问网址：<http://frin-jobhistoryserver:19888/>。如果有 Job 运行，则界面如下：![MapReduce JobHistory Server](/uploads/20170303/mapreduce-jobhistory-server.png)","slug":"hadoop-2-7-3-集群安装","published":1,"updated":"2021-07-19T16:28:00.300Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphqx00dditd33kigfdo1","content":"<h3 id=\"服务器信息\"><a href=\"#服务器信息\" class=\"headerlink\" title=\"服务器信息\"></a>服务器信息</h3><h4 id=\"服务器列表\"><a href=\"#服务器列表\" class=\"headerlink\" title=\"服务器列表\"></a>服务器列表</h4><ul>\n<li>10.142.166.81：NameNode</li>\n<li>10.142.166.119：ResourceManager / MapReduce JobHistory Server</li>\n<li>10.142.165.40 / 10.142.165.41 / 10.142.165.44：DataNode / NodeManager</li>\n</ul>\n<h4 id=\"服务器配置信息\"><a href=\"#服务器配置信息\" class=\"headerlink\" title=\"服务器配置信息\"></a>服务器配置信息</h4><ul>\n<li>OS：CentOS 6.6</li>\n<li>4 cpu</li>\n<li>8G Mem</li>\n<li>磁盘目录：<ul>\n<li>/frin：80G</li>\n<li>/data：1T</li>\n</ul>\n</li>\n</ul>\n<span id=\"more\"></span>\n\n<h3 id=\"准备工作\"><a href=\"#准备工作\" class=\"headerlink\" title=\"准备工作\"></a>准备工作</h3><h4 id=\"创建-hadoop-用户\"><a href=\"#创建-hadoop-用户\" class=\"headerlink\" title=\"创建 hadoop 用户\"></a>创建 hadoop 用户</h4><p>在所有服务器上创建 hadoop 用户，整个集群使用该用户进行管理。创建用户命令：</p>\n<pre><code>useradd hadoop\n</code></pre>\n<h4 id=\"SSH-免密钥\"><a href=\"#SSH-免密钥\" class=\"headerlink\" title=\"SSH 免密钥\"></a>SSH 免密钥</h4><p>配置服务器 10.142.166.81 的 hadoop 用户到其他服务器的 SSH 免密钥登陆。</p>\n<h4 id=\"目录权限\"><a href=\"#目录权限\" class=\"headerlink\" title=\"目录权限\"></a>目录权限</h4><p>修改所有服务器的 /data 目录的属主及组为 hadoop。</p>\n<p>在所有服务器上创建 /frin/hadoop 目录，并修改属主及组为 hadoop。命令如下：</p>\n<pre><code>cd /frin\nmkdir hadoop\nchown hadoop hadoop\nchgrp hadoop hadoop\n</code></pre>\n<h3 id=\"安装-Hadoop-2-7-3\"><a href=\"#安装-Hadoop-2-7-3\" class=\"headerlink\" title=\"安装 Hadoop-2.7.3\"></a>安装 Hadoop-2.7.3</h3><h4 id=\"下载并解压-Hadoop\"><a href=\"#下载并解压-Hadoop\" class=\"headerlink\" title=\"下载并解压 Hadoop\"></a>下载并解压 Hadoop</h4><p>此处使用官网发行的二进制运行包。在实际中的集群安装可以自己编译之后再安装。</p>\n<p>使用 hadoop 帐号登陆服务器 10.142.166.81，下载并解压 Hadoop：</p>\n<pre><code>cd /frin/hadoop\nwget http://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz\ntar xzvf hadoop-2.7.3.tar.gz\n</code></pre>\n<h4 id=\"创建软链\"><a href=\"#创建软链\" class=\"headerlink\" title=\"创建软链\"></a>创建软链</h4><p>为了统一及方便管理，在 /usr/local 下创建 hadoop 软链，并修改软链的属主及组为 hadoop：</p>\n<pre><code>cd /usr/local\nln -s /letv/hadoop/hadoop-2.7.3 hadoop\nchown -h hadoop hadoop\nchgrp -h hadoop hadoop\n</code></pre>\n<h4 id=\"设置HADOOP-PREFIX及HADOOP-HOME\"><a href=\"#设置HADOOP-PREFIX及HADOOP-HOME\" class=\"headerlink\" title=\"设置HADOOP_PREFIX及HADOOP_HOME\"></a>设置HADOOP_PREFIX及HADOOP_HOME</h4><p>使用 root 帐号登陆服务器 10.142.166.81，编辑 /etc/profile，增加以下内容：</p>\n<pre><code>export HADOOP_PREFIX=/usr/local/hadoop\nexport HADOOP_HOME=/usr/local/hadoop\n</code></pre>\n<p>编辑完成后，将 /etc/profile 分发到其他服务器。</p>\n<h4 id=\"配置-hosts\"><a href=\"#配置-hosts\" class=\"headerlink\" title=\"配置 hosts\"></a>配置 hosts</h4><p>使用 root 帐号登陆服务器 10.142.166.81，编辑 /etc/hosts，内容如下：</p>\n<pre><code>127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n\n10.142.166.81 frin-namenode1\n10.142.166.119 frin-resourcemanager1\n10.142.166.119 frin-jobhistoryserver\n\n10.142.166.81 vm-10-142-166-81\n10.142.166.119 vm-10-142-166-119\n10.142.165.40 vm-10-142-165-40\n10.142.165.41 vm-10-142-165-41\n10.142.165.44 vm-10-142-165-44\n</code></pre>\n<p>编辑完成后，将 /etc/profile 分发到其他服务器。</p>\n<h4 id=\"etc-hadoop-core-site-xml-配置\"><a href=\"#etc-hadoop-core-site-xml-配置\" class=\"headerlink\" title=\"etc/hadoop/core-site.xml 配置\"></a>etc/hadoop/core-site.xml 配置</h4><pre><code>&lt;property&gt;\n  &lt;name&gt;fs.defaultFS&lt;/name&gt;\n  &lt;value&gt;hdfs://frin-namenode1:9000&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;io.file.buffer.size&lt;/name&gt;\n  &lt;value&gt;131072&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<h4 id=\"etc-hadoop-hdfs-site-xml-配置\"><a href=\"#etc-hadoop-hdfs-site-xml-配置\" class=\"headerlink\" title=\"etc/hadoop/hdfs-site.xml 配置\"></a>etc/hadoop/hdfs-site.xml 配置</h4><h5 id=\"NameNode-配置\"><a href=\"#NameNode-配置\" class=\"headerlink\" title=\"NameNode 配置\"></a>NameNode 配置</h5><pre><code>&lt;!-- configurations for namenode --&gt;\n&lt;property&gt;\n  &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;\n  &lt;value&gt;/data/dfs/name&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;dfs.hosts&lt;/name&gt;\n  &lt;value&gt;/usr/local/hadoop/etc/hadoop/slaves&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;dfs.hosts.exclude&lt;/name&gt;\n  &lt;value&gt;/usr/local/hadoop/etc/hadoop/exclude_hosts&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;dfs.blocksize&lt;/name&gt;\n  &lt;value&gt;268435456&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;dfs.namenode.handler.count&lt;/name&gt;\n  &lt;value&gt;30&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<p>配置 etc/hadoop/slaves 文件内容如下：</p>\n<pre><code>10.142.165.44\n10.142.165.41\n10.142.165.40\n</code></pre>\n<p>创建空文件 etc/hadoop/exclude_hosts。</p>\n<h5 id=\"DataNode-配置\"><a href=\"#DataNode-配置\" class=\"headerlink\" title=\"DataNode 配置\"></a>DataNode 配置</h5><pre><code>&lt;!-- configurations for datanode --&gt;\n&lt;property&gt;\n  &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;\n  &lt;value&gt;/data/dfs/data&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<h4 id=\"etc-hadoop-yarn-site-xml-配置\"><a href=\"#etc-hadoop-yarn-site-xml-配置\" class=\"headerlink\" title=\"etc/hadoop/yarn-site.xml 配置\"></a>etc/hadoop/yarn-site.xml 配置</h4><h5 id=\"ResourceManager-和-NodeManager-配置\"><a href=\"#ResourceManager-和-NodeManager-配置\" class=\"headerlink\" title=\"ResourceManager 和  NodeManager 配置\"></a>ResourceManager 和  NodeManager 配置</h5><pre><code>&lt;!-- configurations for ResourceManager and NodeManager --&gt;\n&lt;property&gt;\n  &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;\n  &lt;value&gt;true&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;yarn.nodemanager.remote-app-log-dir&lt;/name&gt;\n  &lt;value&gt;/data/hadoop/yarn-logs&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<blockquote>\n<p>集群启动后，在 HDFS 上创建目录 /data/hadoop/yarn-logs，并修改目录权限为 777。</p>\n</blockquote>\n<h5 id=\"ResourceManager-配置\"><a href=\"#ResourceManager-配置\" class=\"headerlink\" title=\"ResourceManager 配置\"></a>ResourceManager 配置</h5><pre><code>&lt;!-- configurations for ResourceManager --&gt;\n&lt;property&gt;\n  &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;\n  &lt;value&gt;frin-resourcemanager1:8088&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\n  &lt;value&gt;frin-resourcemanager1&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt;\n  &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;\n  &lt;value&gt;1024&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;\n  &lt;value&gt;8192&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;yarn.resourcemanager.nodes.include-path&lt;/name&gt;\n  &lt;value&gt;/usr/local/hadoop/etc/hadoop/nodemanagers&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<h5 id=\"NodeManager-配置\"><a href=\"#NodeManager-配置\" class=\"headerlink\" title=\"NodeManager 配置\"></a>NodeManager 配置</h5><pre><code>&lt;!-- configurations for NodeManager --&gt;\n&lt;property&gt;\n  &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;\n  &lt;value&gt;6144&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;\n  &lt;value&gt;1.8&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;yarn.nodemanager.local-dirs&lt;/name&gt;\n  &lt;value&gt;/data/yarn/data&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;yarn.nodemanager.log-dirs&lt;/name&gt;\n  &lt;value&gt;/data/yarn/log&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n  &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<h5 id=\"History-Server-配置\"><a href=\"#History-Server-配置\" class=\"headerlink\" title=\"History Server 配置\"></a>History Server 配置</h5><pre><code>&lt;!-- configurations for History Server --&gt;\n&lt;property&gt;\n  &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;\n  &lt;value&gt;86400&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;yarn.log-aggregation.retain-check-interval-seconds&lt;/name&gt;\n  &lt;value&gt;1800&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<h4 id=\"etc-hadoop-mapred-site-xml-配置\"><a href=\"#etc-hadoop-mapred-site-xml-配置\" class=\"headerlink\" title=\"etc/hadoop/mapred-site.xml 配置\"></a>etc/hadoop/mapred-site.xml 配置</h4><h5 id=\"MapReduce-Applications-配置\"><a href=\"#MapReduce-Applications-配置\" class=\"headerlink\" title=\"MapReduce Applications 配置\"></a>MapReduce Applications 配置</h5><pre><code>&lt;!-- configurations for MapReduce Applications --&gt;\n&lt;property&gt;\n  &lt;name&gt;mapreduce.framework.name&lt;/name&gt;\n  &lt;value&gt;yarn&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt;\n  &lt;value&gt;2048&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;mapreduce.map.java.opts&lt;/name&gt;\n  &lt;value&gt;-Xmx1024M&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt;\n  &lt;value&gt;4096&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;mapreduce.reduce.java.opts&lt;/name&gt;\n  &lt;value&gt;-Xmx2048M&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<h5 id=\"MapReduce-JobHistory-Server-配置\"><a href=\"#MapReduce-JobHistory-Server-配置\" class=\"headerlink\" title=\"MapReduce JobHistory Server 配置\"></a>MapReduce JobHistory Server 配置</h5><pre><code>&lt;!-- configurations for MapReduce JobHistory Server --&gt;\n&lt;property&gt;\n  &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;\n  &lt;value&gt;frin-jobhistoryserver:10020&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;\n  &lt;value&gt;frin-jobhistoryserver:19888&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<h4 id=\"NodeManager-健康状态检测\"><a href=\"#NodeManager-健康状态检测\" class=\"headerlink\" title=\"NodeManager 健康状态检测\"></a>NodeManager 健康状态检测</h4><h5 id=\"检测脚本\"><a href=\"#检测脚本\" class=\"headerlink\" title=\"检测脚本\"></a>检测脚本</h5><p>在 bin 目录下创建 nodemanager-health-checker.sh，在该脚本中可以编写 NodeManager 是否健康的判断逻辑：</p>\n<pre><code>#!/bin/bash\ncd `dirname $0`\n</code></pre>\n<h5 id=\"检测配置\"><a href=\"#检测配置\" class=\"headerlink\" title=\"检测配置\"></a>检测配置</h5><p>在 etc/hadoop/yarn-site.xml 中增加以下配置：</p>\n<pre><code>&lt;!-- configurations for Monitoring Health of NodeManager --&gt;\n&lt;property&gt;\n  &lt;name&gt;yarn.nodemanager.health-checker.script.path&lt;/name&gt;\n  &lt;value&gt;/usr/local/hadoop/bin/nodemanager-health-checker.sh&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;yarn.nodemanager.health-checker.interval-ms&lt;/name&gt;\n  &lt;value&gt;300000&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<blockquote>\n<p>官网文档中的参数 yarn.nodemanager.health-checker.script.interval-ms 是错误的，正确的参数应该是 yarn.nodemanager.health-checker.interval-ms。我已经在 Jira 中提交了一个 Issue：<a href=\"https://issues.apache.org/jira/browse/YARN-6274\">https://issues.apache.org/jira/browse/YARN-6274</a>。<br>官网文档URL：<a href=\"http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/ClusterSetup.html\">http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/ClusterSetup.html</a></p>\n</blockquote>\n<h5 id=\"分发-Hadoop\"><a href=\"#分发-Hadoop\" class=\"headerlink\" title=\"分发 Hadoop\"></a>分发 Hadoop</h5><p>将已经配置好的 hadoop 整个目录分发到所有服务器。</p>\n<h3 id=\"Hadoop-集群启动\"><a href=\"#Hadoop-集群启动\" class=\"headerlink\" title=\"Hadoop 集群启动\"></a>Hadoop 集群启动</h3><h4 id=\"格式化-HDFS\"><a href=\"#格式化-HDFS\" class=\"headerlink\" title=\"格式化 HDFS\"></a>格式化 HDFS</h4><p>使用 hadoop 帐号登陆服务器 10.142.166.81，用以下命令格式化一个新的 HDFS 系统：</p>\n<pre><code> bin/hdfs namenode -format\n</code></pre>\n<h4 id=\"启动-NameNode\"><a href=\"#启动-NameNode\" class=\"headerlink\" title=\"启动 NameNode\"></a>启动 NameNode</h4><p>使用 hadoop 帐号登陆服务器 10.142.166.81，用以下命令启动 NameNode：</p>\n<pre><code>sbin/hadoop-daemon.sh start namenode\n</code></pre>\n<h4 id=\"启动-DataNode\"><a href=\"#启动-DataNode\" class=\"headerlink\" title=\"启动 DataNode\"></a>启动 DataNode</h4><p>在 slaves 中所有的服务器上用以下命令启动 DataNode：</p>\n<pre><code>sbin/hadoop-daemon.sh start datanode\n</code></pre>\n<h4 id=\"启动-ResourceManager\"><a href=\"#启动-ResourceManager\" class=\"headerlink\" title=\"启动 ResourceManager\"></a>启动 ResourceManager</h4><p>用 hadoop 帐号登陆服务器 10.142.166.119，用以下命令启动 Resourcemanager：</p>\n<pre><code>sbin/yarn-daemon.sh start resourcemanager\n</code></pre>\n<h4 id=\"启动-NodeManager\"><a href=\"#启动-NodeManager\" class=\"headerlink\" title=\"启动 NodeManager\"></a>启动 NodeManager</h4><p>在 slaves 中所有的服务器上用以下命令启动 NodeManager</p>\n<pre><code>sbin/yarn-daemon.sh start nodemanager\n</code></pre>\n<h4 id=\"启动-MapReduce-JobHistory-Server\"><a href=\"#启动-MapReduce-JobHistory-Server\" class=\"headerlink\" title=\"启动 MapReduce JobHistory Server\"></a>启动 MapReduce JobHistory Server</h4><p>使用 hadoop 帐号登陆服务器 10.142.166.119，用以下命令启动 MapReduce JobHistory Server</p>\n<pre><code>sbin/mr-jobhistory-daemon.sh start historyserver\n</code></pre>\n<h3 id=\"集群状态检查\"><a href=\"#集群状态检查\" class=\"headerlink\" title=\"集群状态检查\"></a>集群状态检查</h3><p>将 hosts 配置添加到本地配置文件。</p>\n<h4 id=\"NameNode\"><a href=\"#NameNode\" class=\"headerlink\" title=\"NameNode\"></a>NameNode</h4><p>访问网址：<a href=\"http://frin-namenode1:50070/\">http://frin-namenode1:50070/</a>。看到如下信息，说明服务正常：<img src=\"/uploads/20170303/namenode.png\" alt=\"NameNode\"></p>\n<h4 id=\"ResourceManager\"><a href=\"#ResourceManager\" class=\"headerlink\" title=\"ResourceManager\"></a>ResourceManager</h4><p>访问网址：<a href=\"http://frin-resourcemanager1:8088/\">http://frin-resourcemanager1:8088/</a>。看到如下信息，说明服务正常：<img src=\"/uploads/20170303/resourcemanager.png\" alt=\"ResourceManager\"></p>\n<h4 id=\"MapReduce-JobHistory-Server\"><a href=\"#MapReduce-JobHistory-Server\" class=\"headerlink\" title=\"MapReduce JobHistory Server\"></a>MapReduce JobHistory Server</h4><p>访问网址：<a href=\"http://frin-jobhistoryserver:19888/\">http://frin-jobhistoryserver:19888/</a>。如果有 Job 运行，则界面如下：<img src=\"/uploads/20170303/mapreduce-jobhistory-server.png\" alt=\"MapReduce JobHistory Server\"></p>\n","site":{"data":{}},"excerpt":"<h3 id=\"服务器信息\"><a href=\"#服务器信息\" class=\"headerlink\" title=\"服务器信息\"></a>服务器信息</h3><h4 id=\"服务器列表\"><a href=\"#服务器列表\" class=\"headerlink\" title=\"服务器列表\"></a>服务器列表</h4><ul>\n<li>10.142.166.81：NameNode</li>\n<li>10.142.166.119：ResourceManager / MapReduce JobHistory Server</li>\n<li>10.142.165.40 / 10.142.165.41 / 10.142.165.44：DataNode / NodeManager</li>\n</ul>\n<h4 id=\"服务器配置信息\"><a href=\"#服务器配置信息\" class=\"headerlink\" title=\"服务器配置信息\"></a>服务器配置信息</h4><ul>\n<li>OS：CentOS 6.6</li>\n<li>4 cpu</li>\n<li>8G Mem</li>\n<li>磁盘目录：<ul>\n<li>/frin：80G</li>\n<li>/data：1T</li>\n</ul>\n</li>\n</ul>","more":"<h3 id=\"准备工作\"><a href=\"#准备工作\" class=\"headerlink\" title=\"准备工作\"></a>准备工作</h3><h4 id=\"创建-hadoop-用户\"><a href=\"#创建-hadoop-用户\" class=\"headerlink\" title=\"创建 hadoop 用户\"></a>创建 hadoop 用户</h4><p>在所有服务器上创建 hadoop 用户，整个集群使用该用户进行管理。创建用户命令：</p>\n<pre><code>useradd hadoop\n</code></pre>\n<h4 id=\"SSH-免密钥\"><a href=\"#SSH-免密钥\" class=\"headerlink\" title=\"SSH 免密钥\"></a>SSH 免密钥</h4><p>配置服务器 10.142.166.81 的 hadoop 用户到其他服务器的 SSH 免密钥登陆。</p>\n<h4 id=\"目录权限\"><a href=\"#目录权限\" class=\"headerlink\" title=\"目录权限\"></a>目录权限</h4><p>修改所有服务器的 /data 目录的属主及组为 hadoop。</p>\n<p>在所有服务器上创建 /frin/hadoop 目录，并修改属主及组为 hadoop。命令如下：</p>\n<pre><code>cd /frin\nmkdir hadoop\nchown hadoop hadoop\nchgrp hadoop hadoop\n</code></pre>\n<h3 id=\"安装-Hadoop-2-7-3\"><a href=\"#安装-Hadoop-2-7-3\" class=\"headerlink\" title=\"安装 Hadoop-2.7.3\"></a>安装 Hadoop-2.7.3</h3><h4 id=\"下载并解压-Hadoop\"><a href=\"#下载并解压-Hadoop\" class=\"headerlink\" title=\"下载并解压 Hadoop\"></a>下载并解压 Hadoop</h4><p>此处使用官网发行的二进制运行包。在实际中的集群安装可以自己编译之后再安装。</p>\n<p>使用 hadoop 帐号登陆服务器 10.142.166.81，下载并解压 Hadoop：</p>\n<pre><code>cd /frin/hadoop\nwget http://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz\ntar xzvf hadoop-2.7.3.tar.gz\n</code></pre>\n<h4 id=\"创建软链\"><a href=\"#创建软链\" class=\"headerlink\" title=\"创建软链\"></a>创建软链</h4><p>为了统一及方便管理，在 /usr/local 下创建 hadoop 软链，并修改软链的属主及组为 hadoop：</p>\n<pre><code>cd /usr/local\nln -s /letv/hadoop/hadoop-2.7.3 hadoop\nchown -h hadoop hadoop\nchgrp -h hadoop hadoop\n</code></pre>\n<h4 id=\"设置HADOOP-PREFIX及HADOOP-HOME\"><a href=\"#设置HADOOP-PREFIX及HADOOP-HOME\" class=\"headerlink\" title=\"设置HADOOP_PREFIX及HADOOP_HOME\"></a>设置HADOOP_PREFIX及HADOOP_HOME</h4><p>使用 root 帐号登陆服务器 10.142.166.81，编辑 /etc/profile，增加以下内容：</p>\n<pre><code>export HADOOP_PREFIX=/usr/local/hadoop\nexport HADOOP_HOME=/usr/local/hadoop\n</code></pre>\n<p>编辑完成后，将 /etc/profile 分发到其他服务器。</p>\n<h4 id=\"配置-hosts\"><a href=\"#配置-hosts\" class=\"headerlink\" title=\"配置 hosts\"></a>配置 hosts</h4><p>使用 root 帐号登陆服务器 10.142.166.81，编辑 /etc/hosts，内容如下：</p>\n<pre><code>127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n\n10.142.166.81 frin-namenode1\n10.142.166.119 frin-resourcemanager1\n10.142.166.119 frin-jobhistoryserver\n\n10.142.166.81 vm-10-142-166-81\n10.142.166.119 vm-10-142-166-119\n10.142.165.40 vm-10-142-165-40\n10.142.165.41 vm-10-142-165-41\n10.142.165.44 vm-10-142-165-44\n</code></pre>\n<p>编辑完成后，将 /etc/profile 分发到其他服务器。</p>\n<h4 id=\"etc-hadoop-core-site-xml-配置\"><a href=\"#etc-hadoop-core-site-xml-配置\" class=\"headerlink\" title=\"etc/hadoop/core-site.xml 配置\"></a>etc/hadoop/core-site.xml 配置</h4><pre><code>&lt;property&gt;\n  &lt;name&gt;fs.defaultFS&lt;/name&gt;\n  &lt;value&gt;hdfs://frin-namenode1:9000&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;io.file.buffer.size&lt;/name&gt;\n  &lt;value&gt;131072&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<h4 id=\"etc-hadoop-hdfs-site-xml-配置\"><a href=\"#etc-hadoop-hdfs-site-xml-配置\" class=\"headerlink\" title=\"etc/hadoop/hdfs-site.xml 配置\"></a>etc/hadoop/hdfs-site.xml 配置</h4><h5 id=\"NameNode-配置\"><a href=\"#NameNode-配置\" class=\"headerlink\" title=\"NameNode 配置\"></a>NameNode 配置</h5><pre><code>&lt;!-- configurations for namenode --&gt;\n&lt;property&gt;\n  &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;\n  &lt;value&gt;/data/dfs/name&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;dfs.hosts&lt;/name&gt;\n  &lt;value&gt;/usr/local/hadoop/etc/hadoop/slaves&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;dfs.hosts.exclude&lt;/name&gt;\n  &lt;value&gt;/usr/local/hadoop/etc/hadoop/exclude_hosts&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;dfs.blocksize&lt;/name&gt;\n  &lt;value&gt;268435456&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;dfs.namenode.handler.count&lt;/name&gt;\n  &lt;value&gt;30&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<p>配置 etc/hadoop/slaves 文件内容如下：</p>\n<pre><code>10.142.165.44\n10.142.165.41\n10.142.165.40\n</code></pre>\n<p>创建空文件 etc/hadoop/exclude_hosts。</p>\n<h5 id=\"DataNode-配置\"><a href=\"#DataNode-配置\" class=\"headerlink\" title=\"DataNode 配置\"></a>DataNode 配置</h5><pre><code>&lt;!-- configurations for datanode --&gt;\n&lt;property&gt;\n  &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;\n  &lt;value&gt;/data/dfs/data&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<h4 id=\"etc-hadoop-yarn-site-xml-配置\"><a href=\"#etc-hadoop-yarn-site-xml-配置\" class=\"headerlink\" title=\"etc/hadoop/yarn-site.xml 配置\"></a>etc/hadoop/yarn-site.xml 配置</h4><h5 id=\"ResourceManager-和-NodeManager-配置\"><a href=\"#ResourceManager-和-NodeManager-配置\" class=\"headerlink\" title=\"ResourceManager 和  NodeManager 配置\"></a>ResourceManager 和  NodeManager 配置</h5><pre><code>&lt;!-- configurations for ResourceManager and NodeManager --&gt;\n&lt;property&gt;\n  &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;\n  &lt;value&gt;true&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;yarn.nodemanager.remote-app-log-dir&lt;/name&gt;\n  &lt;value&gt;/data/hadoop/yarn-logs&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<blockquote>\n<p>集群启动后，在 HDFS 上创建目录 /data/hadoop/yarn-logs，并修改目录权限为 777。</p>\n</blockquote>\n<h5 id=\"ResourceManager-配置\"><a href=\"#ResourceManager-配置\" class=\"headerlink\" title=\"ResourceManager 配置\"></a>ResourceManager 配置</h5><pre><code>&lt;!-- configurations for ResourceManager --&gt;\n&lt;property&gt;\n  &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;\n  &lt;value&gt;frin-resourcemanager1:8088&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\n  &lt;value&gt;frin-resourcemanager1&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt;\n  &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;\n  &lt;value&gt;1024&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;\n  &lt;value&gt;8192&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;yarn.resourcemanager.nodes.include-path&lt;/name&gt;\n  &lt;value&gt;/usr/local/hadoop/etc/hadoop/nodemanagers&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<h5 id=\"NodeManager-配置\"><a href=\"#NodeManager-配置\" class=\"headerlink\" title=\"NodeManager 配置\"></a>NodeManager 配置</h5><pre><code>&lt;!-- configurations for NodeManager --&gt;\n&lt;property&gt;\n  &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;\n  &lt;value&gt;6144&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;\n  &lt;value&gt;1.8&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;yarn.nodemanager.local-dirs&lt;/name&gt;\n  &lt;value&gt;/data/yarn/data&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;yarn.nodemanager.log-dirs&lt;/name&gt;\n  &lt;value&gt;/data/yarn/log&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n  &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<h5 id=\"History-Server-配置\"><a href=\"#History-Server-配置\" class=\"headerlink\" title=\"History Server 配置\"></a>History Server 配置</h5><pre><code>&lt;!-- configurations for History Server --&gt;\n&lt;property&gt;\n  &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;\n  &lt;value&gt;86400&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;yarn.log-aggregation.retain-check-interval-seconds&lt;/name&gt;\n  &lt;value&gt;1800&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<h4 id=\"etc-hadoop-mapred-site-xml-配置\"><a href=\"#etc-hadoop-mapred-site-xml-配置\" class=\"headerlink\" title=\"etc/hadoop/mapred-site.xml 配置\"></a>etc/hadoop/mapred-site.xml 配置</h4><h5 id=\"MapReduce-Applications-配置\"><a href=\"#MapReduce-Applications-配置\" class=\"headerlink\" title=\"MapReduce Applications 配置\"></a>MapReduce Applications 配置</h5><pre><code>&lt;!-- configurations for MapReduce Applications --&gt;\n&lt;property&gt;\n  &lt;name&gt;mapreduce.framework.name&lt;/name&gt;\n  &lt;value&gt;yarn&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt;\n  &lt;value&gt;2048&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;mapreduce.map.java.opts&lt;/name&gt;\n  &lt;value&gt;-Xmx1024M&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt;\n  &lt;value&gt;4096&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;mapreduce.reduce.java.opts&lt;/name&gt;\n  &lt;value&gt;-Xmx2048M&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<h5 id=\"MapReduce-JobHistory-Server-配置\"><a href=\"#MapReduce-JobHistory-Server-配置\" class=\"headerlink\" title=\"MapReduce JobHistory Server 配置\"></a>MapReduce JobHistory Server 配置</h5><pre><code>&lt;!-- configurations for MapReduce JobHistory Server --&gt;\n&lt;property&gt;\n  &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;\n  &lt;value&gt;frin-jobhistoryserver:10020&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;\n  &lt;value&gt;frin-jobhistoryserver:19888&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<h4 id=\"NodeManager-健康状态检测\"><a href=\"#NodeManager-健康状态检测\" class=\"headerlink\" title=\"NodeManager 健康状态检测\"></a>NodeManager 健康状态检测</h4><h5 id=\"检测脚本\"><a href=\"#检测脚本\" class=\"headerlink\" title=\"检测脚本\"></a>检测脚本</h5><p>在 bin 目录下创建 nodemanager-health-checker.sh，在该脚本中可以编写 NodeManager 是否健康的判断逻辑：</p>\n<pre><code>#!/bin/bash\ncd `dirname $0`\n</code></pre>\n<h5 id=\"检测配置\"><a href=\"#检测配置\" class=\"headerlink\" title=\"检测配置\"></a>检测配置</h5><p>在 etc/hadoop/yarn-site.xml 中增加以下配置：</p>\n<pre><code>&lt;!-- configurations for Monitoring Health of NodeManager --&gt;\n&lt;property&gt;\n  &lt;name&gt;yarn.nodemanager.health-checker.script.path&lt;/name&gt;\n  &lt;value&gt;/usr/local/hadoop/bin/nodemanager-health-checker.sh&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;yarn.nodemanager.health-checker.interval-ms&lt;/name&gt;\n  &lt;value&gt;300000&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<blockquote>\n<p>官网文档中的参数 yarn.nodemanager.health-checker.script.interval-ms 是错误的，正确的参数应该是 yarn.nodemanager.health-checker.interval-ms。我已经在 Jira 中提交了一个 Issue：<a href=\"https://issues.apache.org/jira/browse/YARN-6274\">https://issues.apache.org/jira/browse/YARN-6274</a>。<br>官网文档URL：<a href=\"http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/ClusterSetup.html\">http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/ClusterSetup.html</a></p>\n</blockquote>\n<h5 id=\"分发-Hadoop\"><a href=\"#分发-Hadoop\" class=\"headerlink\" title=\"分发 Hadoop\"></a>分发 Hadoop</h5><p>将已经配置好的 hadoop 整个目录分发到所有服务器。</p>\n<h3 id=\"Hadoop-集群启动\"><a href=\"#Hadoop-集群启动\" class=\"headerlink\" title=\"Hadoop 集群启动\"></a>Hadoop 集群启动</h3><h4 id=\"格式化-HDFS\"><a href=\"#格式化-HDFS\" class=\"headerlink\" title=\"格式化 HDFS\"></a>格式化 HDFS</h4><p>使用 hadoop 帐号登陆服务器 10.142.166.81，用以下命令格式化一个新的 HDFS 系统：</p>\n<pre><code> bin/hdfs namenode -format\n</code></pre>\n<h4 id=\"启动-NameNode\"><a href=\"#启动-NameNode\" class=\"headerlink\" title=\"启动 NameNode\"></a>启动 NameNode</h4><p>使用 hadoop 帐号登陆服务器 10.142.166.81，用以下命令启动 NameNode：</p>\n<pre><code>sbin/hadoop-daemon.sh start namenode\n</code></pre>\n<h4 id=\"启动-DataNode\"><a href=\"#启动-DataNode\" class=\"headerlink\" title=\"启动 DataNode\"></a>启动 DataNode</h4><p>在 slaves 中所有的服务器上用以下命令启动 DataNode：</p>\n<pre><code>sbin/hadoop-daemon.sh start datanode\n</code></pre>\n<h4 id=\"启动-ResourceManager\"><a href=\"#启动-ResourceManager\" class=\"headerlink\" title=\"启动 ResourceManager\"></a>启动 ResourceManager</h4><p>用 hadoop 帐号登陆服务器 10.142.166.119，用以下命令启动 Resourcemanager：</p>\n<pre><code>sbin/yarn-daemon.sh start resourcemanager\n</code></pre>\n<h4 id=\"启动-NodeManager\"><a href=\"#启动-NodeManager\" class=\"headerlink\" title=\"启动 NodeManager\"></a>启动 NodeManager</h4><p>在 slaves 中所有的服务器上用以下命令启动 NodeManager</p>\n<pre><code>sbin/yarn-daemon.sh start nodemanager\n</code></pre>\n<h4 id=\"启动-MapReduce-JobHistory-Server\"><a href=\"#启动-MapReduce-JobHistory-Server\" class=\"headerlink\" title=\"启动 MapReduce JobHistory Server\"></a>启动 MapReduce JobHistory Server</h4><p>使用 hadoop 帐号登陆服务器 10.142.166.119，用以下命令启动 MapReduce JobHistory Server</p>\n<pre><code>sbin/mr-jobhistory-daemon.sh start historyserver\n</code></pre>\n<h3 id=\"集群状态检查\"><a href=\"#集群状态检查\" class=\"headerlink\" title=\"集群状态检查\"></a>集群状态检查</h3><p>将 hosts 配置添加到本地配置文件。</p>\n<h4 id=\"NameNode\"><a href=\"#NameNode\" class=\"headerlink\" title=\"NameNode\"></a>NameNode</h4><p>访问网址：<a href=\"http://frin-namenode1:50070/\">http://frin-namenode1:50070/</a>。看到如下信息，说明服务正常：<img src=\"/uploads/20170303/namenode.png\" alt=\"NameNode\"></p>\n<h4 id=\"ResourceManager\"><a href=\"#ResourceManager\" class=\"headerlink\" title=\"ResourceManager\"></a>ResourceManager</h4><p>访问网址：<a href=\"http://frin-resourcemanager1:8088/\">http://frin-resourcemanager1:8088/</a>。看到如下信息，说明服务正常：<img src=\"/uploads/20170303/resourcemanager.png\" alt=\"ResourceManager\"></p>\n<h4 id=\"MapReduce-JobHistory-Server\"><a href=\"#MapReduce-JobHistory-Server\" class=\"headerlink\" title=\"MapReduce JobHistory Server\"></a>MapReduce JobHistory Server</h4><p>访问网址：<a href=\"http://frin-jobhistoryserver:19888/\">http://frin-jobhistoryserver:19888/</a>。如果有 Job 运行，则界面如下：<img src=\"/uploads/20170303/mapreduce-jobhistory-server.png\" alt=\"MapReduce JobHistory Server\"></p>"},{"title":"hdfs-audit.log Permission denied","date":"2017-07-04T14:31:44.000Z","_content":"\n今天在列出 HDFS 文件时遇到异常 java.io.FileNotFoundException: /data/hadoop/logs/hdfs-audit.log (Permission denied)，详细异常信息如下：\n\n<!-- more -->\n\n    $ hadoop fs -ls /\n    log4j:ERROR setFile(null,true) call failed.\n    java.io.FileNotFoundException: /data/hadoop/logs/hdfs-audit.log (Permission denied)\n        at java.io.FileOutputStream.open0(Native Method)\n        at java.io.FileOutputStream.open(FileOutputStream.java:270)\n        at java.io.FileOutputStream.<init>(FileOutputStream.java:213)\n        at java.io.FileOutputStream.<init>(FileOutputStream.java:133)\n        at org.apache.log4j.FileAppender.setFile(FileAppender.java:294)\n        at org.apache.log4j.RollingFileAppender.setFile(RollingFileAppender.java:207)\n        at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165)\n        at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307)\n        at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172)\n        at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104)\n        at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:842)\n        at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)\n        at org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:672)\n        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:516)\n        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:580)\n        at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)\n        at org.apache.log4j.LogManager.<clinit>(LogManager.java:127)\n        at org.apache.log4j.Logger.getLogger(Logger.java:104)\n        at org.apache.commons.logging.impl.Log4JLogger.getLogger(Log4JLogger.java:262)\n        at org.apache.commons.logging.impl.Log4JLogger.<init>(Log4JLogger.java:108)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n        at org.apache.commons.logging.impl.LogFactoryImpl.createLogFromClass(LogFactoryImpl.java:1025)\n        at org.apache.commons.logging.impl.LogFactoryImpl.discoverLogImplementation(LogFactoryImpl.java:844)\n        at org.apache.commons.logging.impl.LogFactoryImpl.newInstance(LogFactoryImpl.java:541)\n        at org.apache.commons.logging.impl.LogFactoryImpl.getInstance(LogFactoryImpl.java:292)\n        at org.apache.commons.logging.impl.LogFactoryImpl.getInstance(LogFactoryImpl.java:269)\n        at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:657)\n        at org.apache.hadoop.fs.FsShell.<clinit>(FsShell.java:43)\n    Found 2 items\n    drwxr-xr-x   - hadoop   supergroup          0 2017-06-01 18:11 /data\n    drwxrwxrwx   - hadoop   supergroup          0 2017-05-11 20:29 /tmp\n\n根据错误信息很明显，是文件 /data/hadoop/logs/hdfs-audit.log 没有访问权限。因为 log4j.properties 中 hdfs audit 日志的配置是 hdfs.audit.logger=INFO,NullAppender，所以可以直接删除文件 /data/hadoop/logs/hdfs-audit.log。或者可以修改该文件的权限。\n","source":"_posts/hdfs-audit-log-Permission-denied.md","raw":"title: hdfs-audit.log Permission denied\ntags:\n  - Hadoop\n  - HDFS\ncategories:\n  - 大数据\n  - Hadoop\ndate: 2017-07-04 22:31:44\n---\n\n今天在列出 HDFS 文件时遇到异常 java.io.FileNotFoundException: /data/hadoop/logs/hdfs-audit.log (Permission denied)，详细异常信息如下：\n\n<!-- more -->\n\n    $ hadoop fs -ls /\n    log4j:ERROR setFile(null,true) call failed.\n    java.io.FileNotFoundException: /data/hadoop/logs/hdfs-audit.log (Permission denied)\n        at java.io.FileOutputStream.open0(Native Method)\n        at java.io.FileOutputStream.open(FileOutputStream.java:270)\n        at java.io.FileOutputStream.<init>(FileOutputStream.java:213)\n        at java.io.FileOutputStream.<init>(FileOutputStream.java:133)\n        at org.apache.log4j.FileAppender.setFile(FileAppender.java:294)\n        at org.apache.log4j.RollingFileAppender.setFile(RollingFileAppender.java:207)\n        at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165)\n        at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307)\n        at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172)\n        at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104)\n        at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:842)\n        at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)\n        at org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:672)\n        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:516)\n        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:580)\n        at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)\n        at org.apache.log4j.LogManager.<clinit>(LogManager.java:127)\n        at org.apache.log4j.Logger.getLogger(Logger.java:104)\n        at org.apache.commons.logging.impl.Log4JLogger.getLogger(Log4JLogger.java:262)\n        at org.apache.commons.logging.impl.Log4JLogger.<init>(Log4JLogger.java:108)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n        at org.apache.commons.logging.impl.LogFactoryImpl.createLogFromClass(LogFactoryImpl.java:1025)\n        at org.apache.commons.logging.impl.LogFactoryImpl.discoverLogImplementation(LogFactoryImpl.java:844)\n        at org.apache.commons.logging.impl.LogFactoryImpl.newInstance(LogFactoryImpl.java:541)\n        at org.apache.commons.logging.impl.LogFactoryImpl.getInstance(LogFactoryImpl.java:292)\n        at org.apache.commons.logging.impl.LogFactoryImpl.getInstance(LogFactoryImpl.java:269)\n        at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:657)\n        at org.apache.hadoop.fs.FsShell.<clinit>(FsShell.java:43)\n    Found 2 items\n    drwxr-xr-x   - hadoop   supergroup          0 2017-06-01 18:11 /data\n    drwxrwxrwx   - hadoop   supergroup          0 2017-05-11 20:29 /tmp\n\n根据错误信息很明显，是文件 /data/hadoop/logs/hdfs-audit.log 没有访问权限。因为 log4j.properties 中 hdfs audit 日志的配置是 hdfs.audit.logger=INFO,NullAppender，所以可以直接删除文件 /data/hadoop/logs/hdfs-audit.log。或者可以修改该文件的权限。\n","slug":"hdfs-audit-log-Permission-denied","published":1,"updated":"2021-07-19T16:28:00.300Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphqy00diitd3hbuy374w","content":"<p>今天在列出 HDFS 文件时遇到异常 java.io.FileNotFoundException: /data/hadoop/logs/hdfs-audit.log (Permission denied)，详细异常信息如下：</p>\n<span id=\"more\"></span>\n\n<pre><code>$ hadoop fs -ls /\nlog4j:ERROR setFile(null,true) call failed.\njava.io.FileNotFoundException: /data/hadoop/logs/hdfs-audit.log (Permission denied)\n    at java.io.FileOutputStream.open0(Native Method)\n    at java.io.FileOutputStream.open(FileOutputStream.java:270)\n    at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:213)\n    at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:133)\n    at org.apache.log4j.FileAppender.setFile(FileAppender.java:294)\n    at org.apache.log4j.RollingFileAppender.setFile(RollingFileAppender.java:207)\n    at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165)\n    at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307)\n    at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172)\n    at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104)\n    at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:842)\n    at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)\n    at org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:672)\n    at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:516)\n    at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:580)\n    at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)\n    at org.apache.log4j.LogManager.&lt;clinit&gt;(LogManager.java:127)\n    at org.apache.log4j.Logger.getLogger(Logger.java:104)\n    at org.apache.commons.logging.impl.Log4JLogger.getLogger(Log4JLogger.java:262)\n    at org.apache.commons.logging.impl.Log4JLogger.&lt;init&gt;(Log4JLogger.java:108)\n    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n    at org.apache.commons.logging.impl.LogFactoryImpl.createLogFromClass(LogFactoryImpl.java:1025)\n    at org.apache.commons.logging.impl.LogFactoryImpl.discoverLogImplementation(LogFactoryImpl.java:844)\n    at org.apache.commons.logging.impl.LogFactoryImpl.newInstance(LogFactoryImpl.java:541)\n    at org.apache.commons.logging.impl.LogFactoryImpl.getInstance(LogFactoryImpl.java:292)\n    at org.apache.commons.logging.impl.LogFactoryImpl.getInstance(LogFactoryImpl.java:269)\n    at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:657)\n    at org.apache.hadoop.fs.FsShell.&lt;clinit&gt;(FsShell.java:43)\nFound 2 items\ndrwxr-xr-x   - hadoop   supergroup          0 2017-06-01 18:11 /data\ndrwxrwxrwx   - hadoop   supergroup          0 2017-05-11 20:29 /tmp\n</code></pre>\n<p>根据错误信息很明显，是文件 /data/hadoop/logs/hdfs-audit.log 没有访问权限。因为 log4j.properties 中 hdfs audit 日志的配置是 hdfs.audit.logger=INFO,NullAppender，所以可以直接删除文件 /data/hadoop/logs/hdfs-audit.log。或者可以修改该文件的权限。</p>\n","site":{"data":{}},"excerpt":"<p>今天在列出 HDFS 文件时遇到异常 java.io.FileNotFoundException: /data/hadoop/logs/hdfs-audit.log (Permission denied)，详细异常信息如下：</p>","more":"<pre><code>$ hadoop fs -ls /\nlog4j:ERROR setFile(null,true) call failed.\njava.io.FileNotFoundException: /data/hadoop/logs/hdfs-audit.log (Permission denied)\n    at java.io.FileOutputStream.open0(Native Method)\n    at java.io.FileOutputStream.open(FileOutputStream.java:270)\n    at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:213)\n    at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:133)\n    at org.apache.log4j.FileAppender.setFile(FileAppender.java:294)\n    at org.apache.log4j.RollingFileAppender.setFile(RollingFileAppender.java:207)\n    at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165)\n    at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307)\n    at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172)\n    at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104)\n    at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:842)\n    at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)\n    at org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:672)\n    at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:516)\n    at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:580)\n    at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)\n    at org.apache.log4j.LogManager.&lt;clinit&gt;(LogManager.java:127)\n    at org.apache.log4j.Logger.getLogger(Logger.java:104)\n    at org.apache.commons.logging.impl.Log4JLogger.getLogger(Log4JLogger.java:262)\n    at org.apache.commons.logging.impl.Log4JLogger.&lt;init&gt;(Log4JLogger.java:108)\n    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n    at org.apache.commons.logging.impl.LogFactoryImpl.createLogFromClass(LogFactoryImpl.java:1025)\n    at org.apache.commons.logging.impl.LogFactoryImpl.discoverLogImplementation(LogFactoryImpl.java:844)\n    at org.apache.commons.logging.impl.LogFactoryImpl.newInstance(LogFactoryImpl.java:541)\n    at org.apache.commons.logging.impl.LogFactoryImpl.getInstance(LogFactoryImpl.java:292)\n    at org.apache.commons.logging.impl.LogFactoryImpl.getInstance(LogFactoryImpl.java:269)\n    at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:657)\n    at org.apache.hadoop.fs.FsShell.&lt;clinit&gt;(FsShell.java:43)\nFound 2 items\ndrwxr-xr-x   - hadoop   supergroup          0 2017-06-01 18:11 /data\ndrwxrwxrwx   - hadoop   supergroup          0 2017-05-11 20:29 /tmp\n</code></pre>\n<p>根据错误信息很明显，是文件 /data/hadoop/logs/hdfs-audit.log 没有访问权限。因为 log4j.properties 中 hdfs audit 日志的配置是 hdfs.audit.logger=INFO,NullAppender，所以可以直接删除文件 /data/hadoop/logs/hdfs-audit.log。或者可以修改该文件的权限。</p>"},{"title":"hexo generate 处理 #","date":"2016-09-11T13:28:31.000Z","_content":"\n今天在发布一篇博文的时候执行 hexo generate 发生错误，错误信息如下：\n\n    $ hexo generate\n    INFO  Start processing\n    FATAL Something's wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.html\n    Template render error: (unknown path)\n      Error: expected end of comment, got end of file\n\n<!-- more -->\n\n想到刚才文章内容中有 # 字符作为普通字符出现，但并没有做转义处理，包含 # 字符的内容如下：\n\n    ${#v1}\n\n经过实际测试确实是因为 # 导致的。\n\n首先想到用 Markdown 转义字符 \\ 进行转义，但问题依旧。\n\nhexo generate 目标是将 .md 文件内容转为 html 文件，html 可以对字符编码。从网上查了下 # 的 html 编码为 &\\#35; 替换后问题解决。\n    ","source":"_posts/hexo-generate-处理-hashes.md","raw":"title: 'hexo generate 处理 #'\ntags:\n  - Hexo\n  - Markdown\ncategories:\n  - 开发\n  - Node.js\ndate: 2016-09-11 21:28:31\n---\n\n今天在发布一篇博文的时候执行 hexo generate 发生错误，错误信息如下：\n\n    $ hexo generate\n    INFO  Start processing\n    FATAL Something's wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.html\n    Template render error: (unknown path)\n      Error: expected end of comment, got end of file\n\n<!-- more -->\n\n想到刚才文章内容中有 # 字符作为普通字符出现，但并没有做转义处理，包含 # 字符的内容如下：\n\n    ${#v1}\n\n经过实际测试确实是因为 # 导致的。\n\n首先想到用 Markdown 转义字符 \\ 进行转义，但问题依旧。\n\nhexo generate 目标是将 .md 文件内容转为 html 文件，html 可以对字符编码。从网上查了下 # 的 html 编码为 &\\#35; 替换后问题解决。\n    ","slug":"hexo-generate-处理-hashes","published":1,"updated":"2021-07-19T16:28:00.300Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphqz00dlitd34uhccx7u","content":"<p>今天在发布一篇博文的时候执行 hexo generate 发生错误，错误信息如下：</p>\n<pre><code>$ hexo generate\nINFO  Start processing\nFATAL Something&#39;s wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.html\nTemplate render error: (unknown path)\n  Error: expected end of comment, got end of file\n</code></pre>\n<span id=\"more\"></span>\n\n<p>想到刚才文章内容中有 # 字符作为普通字符出现，但并没有做转义处理，包含 # 字符的内容如下：</p>\n<pre><code>$&#123;#v1&#125;\n</code></pre>\n<p>经过实际测试确实是因为 # 导致的。</p>\n<p>首先想到用 Markdown 转义字符 \\ 进行转义，但问题依旧。</p>\n<p>hexo generate 目标是将 .md 文件内容转为 html 文件，html 可以对字符编码。从网上查了下 # 的 html 编码为 &amp;#35; 替换后问题解决。\n    </p>\n","site":{"data":{}},"excerpt":"<p>今天在发布一篇博文的时候执行 hexo generate 发生错误，错误信息如下：</p>\n<pre><code>$ hexo generate\nINFO  Start processing\nFATAL Something&#39;s wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.html\nTemplate render error: (unknown path)\n  Error: expected end of comment, got end of file\n</code></pre>","more":"<p>想到刚才文章内容中有 # 字符作为普通字符出现，但并没有做转义处理，包含 # 字符的内容如下：</p>\n<pre><code>$&#123;#v1&#125;\n</code></pre>\n<p>经过实际测试确实是因为 # 导致的。</p>\n<p>首先想到用 Markdown 转义字符 \\ 进行转义，但问题依旧。</p>\n<p>hexo generate 目标是将 .md 文件内容转为 html 文件，html 可以对字符编码。从网上查了下 # 的 html 编码为 &amp;#35; 替换后问题解决。\n    </p>"},{"title":"hive 内置字符串（String）函数","date":"2016-05-15T00:55:09.000Z","_content":"\nHive 支持以下内置字符串函数：\n\n1. int ascii(string str)：返回字符串第一个字符的 ASCII 码值\n2. string base64(binary bin)：将二进制参数转换为 base 64 编码的字符串\n\n<!-- more -->\n\n3. string concat(string|binary A, string|binary B...)：将作为参数的字符串或者二进制数据依次拼接，并返回拼接后的结果。例如：concat('foo', 'bar')，返回结果为‘foobar’。这个函数可以接受任意数量的字符串参数。\n4. array<struct<string,double&gt;&gt; context_ngrams(array<array<string&gt;&gt;, array<string&gt;, int K, int pf)：给定一个字符串作“语境”，返回一组标记句子的Top-k语境n-grams。更多信息参见 [StatisticsAndDataMining](https://cwiki.apache.org/confluence/display/Hive/StatisticsAndDataMining)\n5. string concat_ws(string SEP, string A, string B...)：像上面的 concat() 函数一样，不过可以自定义分隔符 SEP。\n6. string concat_ws(string SEP, array<string>)：像上面的 concat_ws() 函数一样，不过传入一个字符串数据作为参数。\n7. string decode(binary bin, string charset)：用提供的字符集（'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16' 中的一个）解码第一个参数为一个字符串。如果任一个参数为 null，则结果也为 null。\n8. binary encode(string src, string charset)：用提供的字符集（'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16' 中的一个）编码第一个参数为二进制。\n9. int find_in_set(string str, string strList)：返回 str 在 strList 中第一次出现的序号；strList 是一个逗号分隔的字符串。如果任何一个参数为 null，则返回 null。如果第一个参数包含逗号则返回 0。例如，find_in_set('ab', 'abc,b,ab,c,def') 返回 3。\n10. string format_number(number x, int d)：格式化数字 X 为像 '#,###,###.##' 的格式，四舍五入到 D 的小数点位置，并返回结果为一个字符串。如果 D 是 0，结果没有小数点或分数部分。\n11. string get_json_object(string json\\_string, string path)：基于给定的 json 路径从 json 字符串中提取 json 对象，返回提取 json 对象的 json 字符串。如果输入的 json 字符串非法则返回 null。备注：json 路径只能包含 [0-9a-z_] 字符，即，没有大写或特殊字符。并且键不能以数字开头。这是由于 Hive 列名称的限制。\n12. boolean n_file(string str, string filename)：如果字符串 str 在 filename 中整行出现则返回 true。\n13. int instr(string str, string substr)：返回 substr 在 str 中第一次出现的位置。如果任一个参数为 null，则返回 null；如果在 str 中找不到 substr，则返回 0。注意这个函数不是以 0 为基础的。str 中第一个字符的索引是 1。\n14. int length(string A)：返回字符串的长度。\n15. int locate(string substr, string str[, int pos])：返回 pos 位置之后 substr 在 str 中第一次出现的位置。\n16. string lower(string A) lcase(string A)： 返回全部字符转为小写之后的字符串。例如，lower('fOoBaR') 结果为 ‘foobar’。\n17. string lpad(string str, int len, string pad)：左边添加 pad 参数输入的字符使字符串长度为 len，然后返回该字符串。\n18. string ltrim(string A)：返回从 A 的开头（左手边）删除空白字符后的字符串。例如，ltrim(' foobar ') 结果为 ‘foobar’。\n19. array<struct<string,double&gt;&gt; ngrams(array<array<string&gt;&gt;, int N, int K, int pf)：返回一组标记句子的Top-k语境n-grams，就像 sentences() UDAF 返回的结果。参见 [StatisticsAndDataMining](https://cwiki.apache.org/confluence/display/Hive/StatisticsAndDataMining) 获取更多信息。\n20. string parse_url(string urlString, string partToExtract [, string keyToExtract])：返回指定的 URL 中的部分。partToExtract 合法的取值有 HOST、PATH、QUERY、REF、PROTOCOL、AUTHORITY、FILE 和 USERINFO。例如，parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'HOST') 返回 ‘facebook.com’。并且通过提供第三个参数，可以提取 QUERY 中普通的键值，例如，parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'QUERY', 'k1') 返回 ‘v1’。\n21. string printf(String format, Obj... args)：返回按照输入的格式输出格式化的字符串。\n22. string regexp_extract(string subject, string pattern, int index)：返回使用模式提取的字符串。例如，regexp_extract('foothebar', 'foo(.\\*?)(bar)', 2) 返回 ‘bar’。注意，使用预定义字符类型有一些必要的关注：使用 ‘\\s’ 作为第二个参数将匹配字母 s；匹配空白字符 ‘\\\\\\s’ 是必须的，等。‘index’ 参数是 Java 正则 Matcher group() 方法索引。查阅 docs/api/java/util/regex/Matcher.html 获取‘index’或 Java 正则 group() 方法的更多信息。\n23. string regexp_replace(string INITIAL_STRING, string PATTERN, string REPLACEMENT)：返回用 REPLACEMENT 的实例替换 INITIAL_STRING 中所有 PATTERN 中定义的匹配 java 正则表达式的子字符串的结果字符串。例如，regexp_replace(\"foobar\", \"oo|ar\", \"\") 返回‘fb’。注意，使用预定义字符类型有一些必要的关注：使用 ‘\\s’ 作为第二个参数将匹配字母 s；匹配空白字符 ‘\\\\\\s’ 是必须的，等。\n24. string repeat(string str, int n)：重复 str n 次。\n25. string reverse(string A)：返回反转后的字符串。\n26. string rpad(string str, int len, string pad)：右边添加 pad 参数输入的字符使字符串长度为 len，然后返回该字符串。\n27. string rtrim(string A)：返回从 A 的末尾（右手边）删除空白字符后的字符串。例如，rtrim(' foobar ') 结果为 ‘foobar’。\n28. array<array<string&gt;&gt; sentences(string str, string lang, string locale)：标记一个自然语言文本字符串为单词和句子，每个句子在适当的句子边界被拆分并且作为一个单词数组返回。‘lang’和‘locale’是可选参数。例如，sentences('Hello there! How are you?') 返回 ( (\"Hello\", \"there\"), (\"How\", \"are\", \"you\"))。\n29. string space(int n)：返回 n 空白的字符串。\n30. array split(string str, string pat)：从 pat 左右分解 str（ pat 是一个正则表达式）。\n31. map<string,string&gt; str_to_map(text[, delimiter1, delimiter2])：使用两个分隔符将文本分割为键值对。delimiter1 分割文本为键值对，delimiter2 拆分每个键值对。默认的第一分隔符是‘,’，第二个分隔符是‘＝’。\n32. string substr(string|binary A, int start) substring(string|binary A, int start)：返回从 start 位置到末尾的字符串 A 的子字符串或字节数组的部分。例如，substr('foobar', 4) 结果为 ‘bar’。\n33. string substr(string|binary A, int start, int len) substring(string|binary A, int start, int len)：返回从 start 位置开始长度为 len 的 A 的字符串或字节数组的部分。例如，substr('foobar', 4, 1) 返回‘b’。\n34. string substring_index(string A, string delim, int count)：返回字符串 A 中 delim 分隔符第 count 次匹配前的子字符串。如果 count 是正数，所有到左边最后分隔符（从左边计算）都会返回。如果 count 是负数，所有到右边最后分隔符（从右边计算）都会返回。当搜索 delim 时，Substring_index 是大小写敏感的。示例：substring_index('www.apache.org', '.', 2) = 'www.apache'。\n35. string translate(string|char|varchar input, string|char|varchar from, string|char|varchar to)：通过用 to 中对应的字符替换 from 中展示的字符转化 input 字符串。这跟 [PostgreSQL](http://www.postgresql.org/docs/9.1/interactive/functions-string.html) 中的 **translate** 函数相似。这个 UDF 的任一个参数为 NULL，结果也为 NULL。\n36. string trim(string A)：返回从 A 两端去除空白字符后的字符串。例如，trim(' foobar ') 结果为 ‘foobar’。\n37. binary unbase64(string str)：转换参数从一个 base 64 字符串为 BINARY。\n38. string upper(string A) ucase(string A)：返回转换字符串 A 中所有字符为大写后的字符串。例如，upper('fOoBaR') 结果为‘FOOBAR’。\n39. string initcap(string A)：返回字符串，每个单词的第一个字母为大写，所有其他字母为小写。单词以空白字符分割。\n40. int levenshtein(string A, string B)：返回两个字符串之间的 Levenshtein 距离。例如，levenshtein('kitten', 'sitting') 结果为 3。\n41. string soundex(string A)：返回字符串的 soundex 码。例如，soundex('Miller') 结果为 M460。\n","source":"_posts/hive-内置-String-函数.md","raw":"title: hive 内置字符串（String）函数\ntags:\n  - Hive\ncategories:\n  - 大数据\n  - Hive\ndate: 2016-05-15 08:55:09\n---\n\nHive 支持以下内置字符串函数：\n\n1. int ascii(string str)：返回字符串第一个字符的 ASCII 码值\n2. string base64(binary bin)：将二进制参数转换为 base 64 编码的字符串\n\n<!-- more -->\n\n3. string concat(string|binary A, string|binary B...)：将作为参数的字符串或者二进制数据依次拼接，并返回拼接后的结果。例如：concat('foo', 'bar')，返回结果为‘foobar’。这个函数可以接受任意数量的字符串参数。\n4. array<struct<string,double&gt;&gt; context_ngrams(array<array<string&gt;&gt;, array<string&gt;, int K, int pf)：给定一个字符串作“语境”，返回一组标记句子的Top-k语境n-grams。更多信息参见 [StatisticsAndDataMining](https://cwiki.apache.org/confluence/display/Hive/StatisticsAndDataMining)\n5. string concat_ws(string SEP, string A, string B...)：像上面的 concat() 函数一样，不过可以自定义分隔符 SEP。\n6. string concat_ws(string SEP, array<string>)：像上面的 concat_ws() 函数一样，不过传入一个字符串数据作为参数。\n7. string decode(binary bin, string charset)：用提供的字符集（'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16' 中的一个）解码第一个参数为一个字符串。如果任一个参数为 null，则结果也为 null。\n8. binary encode(string src, string charset)：用提供的字符集（'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16' 中的一个）编码第一个参数为二进制。\n9. int find_in_set(string str, string strList)：返回 str 在 strList 中第一次出现的序号；strList 是一个逗号分隔的字符串。如果任何一个参数为 null，则返回 null。如果第一个参数包含逗号则返回 0。例如，find_in_set('ab', 'abc,b,ab,c,def') 返回 3。\n10. string format_number(number x, int d)：格式化数字 X 为像 '#,###,###.##' 的格式，四舍五入到 D 的小数点位置，并返回结果为一个字符串。如果 D 是 0，结果没有小数点或分数部分。\n11. string get_json_object(string json\\_string, string path)：基于给定的 json 路径从 json 字符串中提取 json 对象，返回提取 json 对象的 json 字符串。如果输入的 json 字符串非法则返回 null。备注：json 路径只能包含 [0-9a-z_] 字符，即，没有大写或特殊字符。并且键不能以数字开头。这是由于 Hive 列名称的限制。\n12. boolean n_file(string str, string filename)：如果字符串 str 在 filename 中整行出现则返回 true。\n13. int instr(string str, string substr)：返回 substr 在 str 中第一次出现的位置。如果任一个参数为 null，则返回 null；如果在 str 中找不到 substr，则返回 0。注意这个函数不是以 0 为基础的。str 中第一个字符的索引是 1。\n14. int length(string A)：返回字符串的长度。\n15. int locate(string substr, string str[, int pos])：返回 pos 位置之后 substr 在 str 中第一次出现的位置。\n16. string lower(string A) lcase(string A)： 返回全部字符转为小写之后的字符串。例如，lower('fOoBaR') 结果为 ‘foobar’。\n17. string lpad(string str, int len, string pad)：左边添加 pad 参数输入的字符使字符串长度为 len，然后返回该字符串。\n18. string ltrim(string A)：返回从 A 的开头（左手边）删除空白字符后的字符串。例如，ltrim(' foobar ') 结果为 ‘foobar’。\n19. array<struct<string,double&gt;&gt; ngrams(array<array<string&gt;&gt;, int N, int K, int pf)：返回一组标记句子的Top-k语境n-grams，就像 sentences() UDAF 返回的结果。参见 [StatisticsAndDataMining](https://cwiki.apache.org/confluence/display/Hive/StatisticsAndDataMining) 获取更多信息。\n20. string parse_url(string urlString, string partToExtract [, string keyToExtract])：返回指定的 URL 中的部分。partToExtract 合法的取值有 HOST、PATH、QUERY、REF、PROTOCOL、AUTHORITY、FILE 和 USERINFO。例如，parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'HOST') 返回 ‘facebook.com’。并且通过提供第三个参数，可以提取 QUERY 中普通的键值，例如，parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'QUERY', 'k1') 返回 ‘v1’。\n21. string printf(String format, Obj... args)：返回按照输入的格式输出格式化的字符串。\n22. string regexp_extract(string subject, string pattern, int index)：返回使用模式提取的字符串。例如，regexp_extract('foothebar', 'foo(.\\*?)(bar)', 2) 返回 ‘bar’。注意，使用预定义字符类型有一些必要的关注：使用 ‘\\s’ 作为第二个参数将匹配字母 s；匹配空白字符 ‘\\\\\\s’ 是必须的，等。‘index’ 参数是 Java 正则 Matcher group() 方法索引。查阅 docs/api/java/util/regex/Matcher.html 获取‘index’或 Java 正则 group() 方法的更多信息。\n23. string regexp_replace(string INITIAL_STRING, string PATTERN, string REPLACEMENT)：返回用 REPLACEMENT 的实例替换 INITIAL_STRING 中所有 PATTERN 中定义的匹配 java 正则表达式的子字符串的结果字符串。例如，regexp_replace(\"foobar\", \"oo|ar\", \"\") 返回‘fb’。注意，使用预定义字符类型有一些必要的关注：使用 ‘\\s’ 作为第二个参数将匹配字母 s；匹配空白字符 ‘\\\\\\s’ 是必须的，等。\n24. string repeat(string str, int n)：重复 str n 次。\n25. string reverse(string A)：返回反转后的字符串。\n26. string rpad(string str, int len, string pad)：右边添加 pad 参数输入的字符使字符串长度为 len，然后返回该字符串。\n27. string rtrim(string A)：返回从 A 的末尾（右手边）删除空白字符后的字符串。例如，rtrim(' foobar ') 结果为 ‘foobar’。\n28. array<array<string&gt;&gt; sentences(string str, string lang, string locale)：标记一个自然语言文本字符串为单词和句子，每个句子在适当的句子边界被拆分并且作为一个单词数组返回。‘lang’和‘locale’是可选参数。例如，sentences('Hello there! How are you?') 返回 ( (\"Hello\", \"there\"), (\"How\", \"are\", \"you\"))。\n29. string space(int n)：返回 n 空白的字符串。\n30. array split(string str, string pat)：从 pat 左右分解 str（ pat 是一个正则表达式）。\n31. map<string,string&gt; str_to_map(text[, delimiter1, delimiter2])：使用两个分隔符将文本分割为键值对。delimiter1 分割文本为键值对，delimiter2 拆分每个键值对。默认的第一分隔符是‘,’，第二个分隔符是‘＝’。\n32. string substr(string|binary A, int start) substring(string|binary A, int start)：返回从 start 位置到末尾的字符串 A 的子字符串或字节数组的部分。例如，substr('foobar', 4) 结果为 ‘bar’。\n33. string substr(string|binary A, int start, int len) substring(string|binary A, int start, int len)：返回从 start 位置开始长度为 len 的 A 的字符串或字节数组的部分。例如，substr('foobar', 4, 1) 返回‘b’。\n34. string substring_index(string A, string delim, int count)：返回字符串 A 中 delim 分隔符第 count 次匹配前的子字符串。如果 count 是正数，所有到左边最后分隔符（从左边计算）都会返回。如果 count 是负数，所有到右边最后分隔符（从右边计算）都会返回。当搜索 delim 时，Substring_index 是大小写敏感的。示例：substring_index('www.apache.org', '.', 2) = 'www.apache'。\n35. string translate(string|char|varchar input, string|char|varchar from, string|char|varchar to)：通过用 to 中对应的字符替换 from 中展示的字符转化 input 字符串。这跟 [PostgreSQL](http://www.postgresql.org/docs/9.1/interactive/functions-string.html) 中的 **translate** 函数相似。这个 UDF 的任一个参数为 NULL，结果也为 NULL。\n36. string trim(string A)：返回从 A 两端去除空白字符后的字符串。例如，trim(' foobar ') 结果为 ‘foobar’。\n37. binary unbase64(string str)：转换参数从一个 base 64 字符串为 BINARY。\n38. string upper(string A) ucase(string A)：返回转换字符串 A 中所有字符为大写后的字符串。例如，upper('fOoBaR') 结果为‘FOOBAR’。\n39. string initcap(string A)：返回字符串，每个单词的第一个字母为大写，所有其他字母为小写。单词以空白字符分割。\n40. int levenshtein(string A, string B)：返回两个字符串之间的 Levenshtein 距离。例如，levenshtein('kitten', 'sitting') 结果为 3。\n41. string soundex(string A)：返回字符串的 soundex 码。例如，soundex('Miller') 结果为 M460。\n","slug":"hive-内置-String-函数","published":1,"updated":"2021-07-19T16:28:00.300Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphr100dqitd36jap55d6","content":"<p>Hive 支持以下内置字符串函数：</p>\n<ol>\n<li>int ascii(string str)：返回字符串第一个字符的 ASCII 码值</li>\n<li>string base64(binary bin)：将二进制参数转换为 base 64 编码的字符串</li>\n</ol>\n<span id=\"more\"></span>\n\n<ol start=\"3\">\n<li>string concat(string|binary A, string|binary B…)：将作为参数的字符串或者二进制数据依次拼接，并返回拼接后的结果。例如：concat(‘foo’, ‘bar’)，返回结果为‘foobar’。这个函数可以接受任意数量的字符串参数。</li>\n<li>array&lt;struct&lt;string,double&gt;&gt; context_ngrams(array&lt;array&lt;string&gt;&gt;, array&lt;string&gt;, int K, int pf)：给定一个字符串作“语境”，返回一组标记句子的Top-k语境n-grams。更多信息参见 <a href=\"https://cwiki.apache.org/confluence/display/Hive/StatisticsAndDataMining\">StatisticsAndDataMining</a></li>\n<li>string concat_ws(string SEP, string A, string B…)：像上面的 concat() 函数一样，不过可以自定义分隔符 SEP。</li>\n<li>string concat_ws(string SEP, array<string>)：像上面的 concat_ws() 函数一样，不过传入一个字符串数据作为参数。</li>\n<li>string decode(binary bin, string charset)：用提供的字符集（’US-ASCII’, ‘ISO-8859-1’, ‘UTF-8’, ‘UTF-16BE’, ‘UTF-16LE’, ‘UTF-16’ 中的一个）解码第一个参数为一个字符串。如果任一个参数为 null，则结果也为 null。</li>\n<li>binary encode(string src, string charset)：用提供的字符集（’US-ASCII’, ‘ISO-8859-1’, ‘UTF-8’, ‘UTF-16BE’, ‘UTF-16LE’, ‘UTF-16’ 中的一个）编码第一个参数为二进制。</li>\n<li>int find_in_set(string str, string strList)：返回 str 在 strList 中第一次出现的序号；strList 是一个逗号分隔的字符串。如果任何一个参数为 null，则返回 null。如果第一个参数包含逗号则返回 0。例如，find_in_set(‘ab’, ‘abc,b,ab,c,def’) 返回 3。</li>\n<li>string format_number(number x, int d)：格式化数字 X 为像 ‘#,###,###.##’ 的格式，四舍五入到 D 的小数点位置，并返回结果为一个字符串。如果 D 是 0，结果没有小数点或分数部分。</li>\n<li>string get_json_object(string json_string, string path)：基于给定的 json 路径从 json 字符串中提取 json 对象，返回提取 json 对象的 json 字符串。如果输入的 json 字符串非法则返回 null。备注：json 路径只能包含 [0-9a-z_] 字符，即，没有大写或特殊字符。并且键不能以数字开头。这是由于 Hive 列名称的限制。</li>\n<li>boolean n_file(string str, string filename)：如果字符串 str 在 filename 中整行出现则返回 true。</li>\n<li>int instr(string str, string substr)：返回 substr 在 str 中第一次出现的位置。如果任一个参数为 null，则返回 null；如果在 str 中找不到 substr，则返回 0。注意这个函数不是以 0 为基础的。str 中第一个字符的索引是 1。</li>\n<li>int length(string A)：返回字符串的长度。</li>\n<li>int locate(string substr, string str[, int pos])：返回 pos 位置之后 substr 在 str 中第一次出现的位置。</li>\n<li>string lower(string A) lcase(string A)： 返回全部字符转为小写之后的字符串。例如，lower(‘fOoBaR’) 结果为 ‘foobar’。</li>\n<li>string lpad(string str, int len, string pad)：左边添加 pad 参数输入的字符使字符串长度为 len，然后返回该字符串。</li>\n<li>string ltrim(string A)：返回从 A 的开头（左手边）删除空白字符后的字符串。例如，ltrim(‘ foobar ‘) 结果为 ‘foobar’。</li>\n<li>array&lt;struct&lt;string,double&gt;&gt; ngrams(array&lt;array&lt;string&gt;&gt;, int N, int K, int pf)：返回一组标记句子的Top-k语境n-grams，就像 sentences() UDAF 返回的结果。参见 <a href=\"https://cwiki.apache.org/confluence/display/Hive/StatisticsAndDataMining\">StatisticsAndDataMining</a> 获取更多信息。</li>\n<li>string parse_url(string urlString, string partToExtract [, string keyToExtract])：返回指定的 URL 中的部分。partToExtract 合法的取值有 HOST、PATH、QUERY、REF、PROTOCOL、AUTHORITY、FILE 和 USERINFO。例如，parse_url(‘<a href=\"http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1&#39;\">http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1&#39;</a>, ‘HOST’) 返回 ‘facebook.com’。并且通过提供第三个参数，可以提取 QUERY 中普通的键值，例如，parse_url(‘<a href=\"http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1&#39;\">http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1&#39;</a>, ‘QUERY’, ‘k1’) 返回 ‘v1’。</li>\n<li>string printf(String format, Obj… args)：返回按照输入的格式输出格式化的字符串。</li>\n<li>string regexp_extract(string subject, string pattern, int index)：返回使用模式提取的字符串。例如，regexp_extract(‘foothebar’, ‘foo(.*?)(bar)’, 2) 返回 ‘bar’。注意，使用预定义字符类型有一些必要的关注：使用 ‘\\s’ 作为第二个参数将匹配字母 s；匹配空白字符 ‘\\\\s’ 是必须的，等。‘index’ 参数是 Java 正则 Matcher group() 方法索引。查阅 docs/api/java/util/regex/Matcher.html 获取‘index’或 Java 正则 group() 方法的更多信息。</li>\n<li>string regexp_replace(string INITIAL_STRING, string PATTERN, string REPLACEMENT)：返回用 REPLACEMENT 的实例替换 INITIAL_STRING 中所有 PATTERN 中定义的匹配 java 正则表达式的子字符串的结果字符串。例如，regexp_replace(“foobar”, “oo|ar”, “”) 返回‘fb’。注意，使用预定义字符类型有一些必要的关注：使用 ‘\\s’ 作为第二个参数将匹配字母 s；匹配空白字符 ‘\\\\s’ 是必须的，等。</li>\n<li>string repeat(string str, int n)：重复 str n 次。</li>\n<li>string reverse(string A)：返回反转后的字符串。</li>\n<li>string rpad(string str, int len, string pad)：右边添加 pad 参数输入的字符使字符串长度为 len，然后返回该字符串。</li>\n<li>string rtrim(string A)：返回从 A 的末尾（右手边）删除空白字符后的字符串。例如，rtrim(‘ foobar ‘) 结果为 ‘foobar’。</li>\n<li>array&lt;array&lt;string&gt;&gt; sentences(string str, string lang, string locale)：标记一个自然语言文本字符串为单词和句子，每个句子在适当的句子边界被拆分并且作为一个单词数组返回。‘lang’和‘locale’是可选参数。例如，sentences(‘Hello there! How are you?’) 返回 ( (“Hello”, “there”), (“How”, “are”, “you”))。</li>\n<li>string space(int n)：返回 n 空白的字符串。</li>\n<li>array split(string str, string pat)：从 pat 左右分解 str（ pat 是一个正则表达式）。</li>\n<li>map&lt;string,string&gt; str_to_map(text[, delimiter1, delimiter2])：使用两个分隔符将文本分割为键值对。delimiter1 分割文本为键值对，delimiter2 拆分每个键值对。默认的第一分隔符是‘,’，第二个分隔符是‘＝’。</li>\n<li>string substr(string|binary A, int start) substring(string|binary A, int start)：返回从 start 位置到末尾的字符串 A 的子字符串或字节数组的部分。例如，substr(‘foobar’, 4) 结果为 ‘bar’。</li>\n<li>string substr(string|binary A, int start, int len) substring(string|binary A, int start, int len)：返回从 start 位置开始长度为 len 的 A 的字符串或字节数组的部分。例如，substr(‘foobar’, 4, 1) 返回‘b’。</li>\n<li>string substring_index(string A, string delim, int count)：返回字符串 A 中 delim 分隔符第 count 次匹配前的子字符串。如果 count 是正数，所有到左边最后分隔符（从左边计算）都会返回。如果 count 是负数，所有到右边最后分隔符（从右边计算）都会返回。当搜索 delim 时，Substring_index 是大小写敏感的。示例：substring_index(‘<a href=\"http://www.apache.org&/#39;\">www.apache.org&#39;</a>, ‘.’, 2) = ‘<a href=\"http://www.apache&/#39;%E3%80%82\">www.apache&#39;。</a></li>\n<li>string translate(string|char|varchar input, string|char|varchar from, string|char|varchar to)：通过用 to 中对应的字符替换 from 中展示的字符转化 input 字符串。这跟 <a href=\"http://www.postgresql.org/docs/9.1/interactive/functions-string.html\">PostgreSQL</a> 中的 <strong>translate</strong> 函数相似。这个 UDF 的任一个参数为 NULL，结果也为 NULL。</li>\n<li>string trim(string A)：返回从 A 两端去除空白字符后的字符串。例如，trim(‘ foobar ‘) 结果为 ‘foobar’。</li>\n<li>binary unbase64(string str)：转换参数从一个 base 64 字符串为 BINARY。</li>\n<li>string upper(string A) ucase(string A)：返回转换字符串 A 中所有字符为大写后的字符串。例如，upper(‘fOoBaR’) 结果为‘FOOBAR’。</li>\n<li>string initcap(string A)：返回字符串，每个单词的第一个字母为大写，所有其他字母为小写。单词以空白字符分割。</li>\n<li>int levenshtein(string A, string B)：返回两个字符串之间的 Levenshtein 距离。例如，levenshtein(‘kitten’, ‘sitting’) 结果为 3。</li>\n<li>string soundex(string A)：返回字符串的 soundex 码。例如，soundex(‘Miller’) 结果为 M460。</li>\n</ol>\n","site":{"data":{}},"excerpt":"<p>Hive 支持以下内置字符串函数：</p>\n<ol>\n<li>int ascii(string str)：返回字符串第一个字符的 ASCII 码值</li>\n<li>string base64(binary bin)：将二进制参数转换为 base 64 编码的字符串</li>\n</ol>","more":"<ol start=\"3\">\n<li>string concat(string|binary A, string|binary B…)：将作为参数的字符串或者二进制数据依次拼接，并返回拼接后的结果。例如：concat(‘foo’, ‘bar’)，返回结果为‘foobar’。这个函数可以接受任意数量的字符串参数。</li>\n<li>array&lt;struct&lt;string,double&gt;&gt; context_ngrams(array&lt;array&lt;string&gt;&gt;, array&lt;string&gt;, int K, int pf)：给定一个字符串作“语境”，返回一组标记句子的Top-k语境n-grams。更多信息参见 <a href=\"https://cwiki.apache.org/confluence/display/Hive/StatisticsAndDataMining\">StatisticsAndDataMining</a></li>\n<li>string concat_ws(string SEP, string A, string B…)：像上面的 concat() 函数一样，不过可以自定义分隔符 SEP。</li>\n<li>string concat_ws(string SEP, array<string>)：像上面的 concat_ws() 函数一样，不过传入一个字符串数据作为参数。</li>\n<li>string decode(binary bin, string charset)：用提供的字符集（’US-ASCII’, ‘ISO-8859-1’, ‘UTF-8’, ‘UTF-16BE’, ‘UTF-16LE’, ‘UTF-16’ 中的一个）解码第一个参数为一个字符串。如果任一个参数为 null，则结果也为 null。</li>\n<li>binary encode(string src, string charset)：用提供的字符集（’US-ASCII’, ‘ISO-8859-1’, ‘UTF-8’, ‘UTF-16BE’, ‘UTF-16LE’, ‘UTF-16’ 中的一个）编码第一个参数为二进制。</li>\n<li>int find_in_set(string str, string strList)：返回 str 在 strList 中第一次出现的序号；strList 是一个逗号分隔的字符串。如果任何一个参数为 null，则返回 null。如果第一个参数包含逗号则返回 0。例如，find_in_set(‘ab’, ‘abc,b,ab,c,def’) 返回 3。</li>\n<li>string format_number(number x, int d)：格式化数字 X 为像 ‘#,###,###.##’ 的格式，四舍五入到 D 的小数点位置，并返回结果为一个字符串。如果 D 是 0，结果没有小数点或分数部分。</li>\n<li>string get_json_object(string json_string, string path)：基于给定的 json 路径从 json 字符串中提取 json 对象，返回提取 json 对象的 json 字符串。如果输入的 json 字符串非法则返回 null。备注：json 路径只能包含 [0-9a-z_] 字符，即，没有大写或特殊字符。并且键不能以数字开头。这是由于 Hive 列名称的限制。</li>\n<li>boolean n_file(string str, string filename)：如果字符串 str 在 filename 中整行出现则返回 true。</li>\n<li>int instr(string str, string substr)：返回 substr 在 str 中第一次出现的位置。如果任一个参数为 null，则返回 null；如果在 str 中找不到 substr，则返回 0。注意这个函数不是以 0 为基础的。str 中第一个字符的索引是 1。</li>\n<li>int length(string A)：返回字符串的长度。</li>\n<li>int locate(string substr, string str[, int pos])：返回 pos 位置之后 substr 在 str 中第一次出现的位置。</li>\n<li>string lower(string A) lcase(string A)： 返回全部字符转为小写之后的字符串。例如，lower(‘fOoBaR’) 结果为 ‘foobar’。</li>\n<li>string lpad(string str, int len, string pad)：左边添加 pad 参数输入的字符使字符串长度为 len，然后返回该字符串。</li>\n<li>string ltrim(string A)：返回从 A 的开头（左手边）删除空白字符后的字符串。例如，ltrim(‘ foobar ‘) 结果为 ‘foobar’。</li>\n<li>array&lt;struct&lt;string,double&gt;&gt; ngrams(array&lt;array&lt;string&gt;&gt;, int N, int K, int pf)：返回一组标记句子的Top-k语境n-grams，就像 sentences() UDAF 返回的结果。参见 <a href=\"https://cwiki.apache.org/confluence/display/Hive/StatisticsAndDataMining\">StatisticsAndDataMining</a> 获取更多信息。</li>\n<li>string parse_url(string urlString, string partToExtract [, string keyToExtract])：返回指定的 URL 中的部分。partToExtract 合法的取值有 HOST、PATH、QUERY、REF、PROTOCOL、AUTHORITY、FILE 和 USERINFO。例如，parse_url(‘<a href=\"http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1&#39;\">http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1&#39;</a>, ‘HOST’) 返回 ‘facebook.com’。并且通过提供第三个参数，可以提取 QUERY 中普通的键值，例如，parse_url(‘<a href=\"http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1&#39;\">http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1&#39;</a>, ‘QUERY’, ‘k1’) 返回 ‘v1’。</li>\n<li>string printf(String format, Obj… args)：返回按照输入的格式输出格式化的字符串。</li>\n<li>string regexp_extract(string subject, string pattern, int index)：返回使用模式提取的字符串。例如，regexp_extract(‘foothebar’, ‘foo(.*?)(bar)’, 2) 返回 ‘bar’。注意，使用预定义字符类型有一些必要的关注：使用 ‘\\s’ 作为第二个参数将匹配字母 s；匹配空白字符 ‘\\\\s’ 是必须的，等。‘index’ 参数是 Java 正则 Matcher group() 方法索引。查阅 docs/api/java/util/regex/Matcher.html 获取‘index’或 Java 正则 group() 方法的更多信息。</li>\n<li>string regexp_replace(string INITIAL_STRING, string PATTERN, string REPLACEMENT)：返回用 REPLACEMENT 的实例替换 INITIAL_STRING 中所有 PATTERN 中定义的匹配 java 正则表达式的子字符串的结果字符串。例如，regexp_replace(“foobar”, “oo|ar”, “”) 返回‘fb’。注意，使用预定义字符类型有一些必要的关注：使用 ‘\\s’ 作为第二个参数将匹配字母 s；匹配空白字符 ‘\\\\s’ 是必须的，等。</li>\n<li>string repeat(string str, int n)：重复 str n 次。</li>\n<li>string reverse(string A)：返回反转后的字符串。</li>\n<li>string rpad(string str, int len, string pad)：右边添加 pad 参数输入的字符使字符串长度为 len，然后返回该字符串。</li>\n<li>string rtrim(string A)：返回从 A 的末尾（右手边）删除空白字符后的字符串。例如，rtrim(‘ foobar ‘) 结果为 ‘foobar’。</li>\n<li>array&lt;array&lt;string&gt;&gt; sentences(string str, string lang, string locale)：标记一个自然语言文本字符串为单词和句子，每个句子在适当的句子边界被拆分并且作为一个单词数组返回。‘lang’和‘locale’是可选参数。例如，sentences(‘Hello there! How are you?’) 返回 ( (“Hello”, “there”), (“How”, “are”, “you”))。</li>\n<li>string space(int n)：返回 n 空白的字符串。</li>\n<li>array split(string str, string pat)：从 pat 左右分解 str（ pat 是一个正则表达式）。</li>\n<li>map&lt;string,string&gt; str_to_map(text[, delimiter1, delimiter2])：使用两个分隔符将文本分割为键值对。delimiter1 分割文本为键值对，delimiter2 拆分每个键值对。默认的第一分隔符是‘,’，第二个分隔符是‘＝’。</li>\n<li>string substr(string|binary A, int start) substring(string|binary A, int start)：返回从 start 位置到末尾的字符串 A 的子字符串或字节数组的部分。例如，substr(‘foobar’, 4) 结果为 ‘bar’。</li>\n<li>string substr(string|binary A, int start, int len) substring(string|binary A, int start, int len)：返回从 start 位置开始长度为 len 的 A 的字符串或字节数组的部分。例如，substr(‘foobar’, 4, 1) 返回‘b’。</li>\n<li>string substring_index(string A, string delim, int count)：返回字符串 A 中 delim 分隔符第 count 次匹配前的子字符串。如果 count 是正数，所有到左边最后分隔符（从左边计算）都会返回。如果 count 是负数，所有到右边最后分隔符（从右边计算）都会返回。当搜索 delim 时，Substring_index 是大小写敏感的。示例：substring_index(‘<a href=\"http://www.apache.org&/#39;\">www.apache.org&#39;</a>, ‘.’, 2) = ‘<a href=\"http://www.apache&/#39;%E3%80%82\">www.apache&#39;。</a></li>\n<li>string translate(string|char|varchar input, string|char|varchar from, string|char|varchar to)：通过用 to 中对应的字符替换 from 中展示的字符转化 input 字符串。这跟 <a href=\"http://www.postgresql.org/docs/9.1/interactive/functions-string.html\">PostgreSQL</a> 中的 <strong>translate</strong> 函数相似。这个 UDF 的任一个参数为 NULL，结果也为 NULL。</li>\n<li>string trim(string A)：返回从 A 两端去除空白字符后的字符串。例如，trim(‘ foobar ‘) 结果为 ‘foobar’。</li>\n<li>binary unbase64(string str)：转换参数从一个 base 64 字符串为 BINARY。</li>\n<li>string upper(string A) ucase(string A)：返回转换字符串 A 中所有字符为大写后的字符串。例如，upper(‘fOoBaR’) 结果为‘FOOBAR’。</li>\n<li>string initcap(string A)：返回字符串，每个单词的第一个字母为大写，所有其他字母为小写。单词以空白字符分割。</li>\n<li>int levenshtein(string A, string B)：返回两个字符串之间的 Levenshtein 距离。例如，levenshtein(‘kitten’, ‘sitting’) 结果为 3。</li>\n<li>string soundex(string A)：返回字符串的 soundex 码。例如，soundex(‘Miller’) 结果为 M460。</li>\n</ol>"},{"title":"lftp 的 sftp 使用时遇到的坑","date":"2019-05-07T13:37:40.000Z","_content":"\n今天在使用 lftp 的 sftp 时候一直处于 connecting 状态，并且没有其他提示信息。状态如下图所示：\n![lftp sftp](/uploads/20190507/lftp-sftp.png)\n\n<!-- more -->\n\n导致这个问题的原因是，在 sftp 第一次连接服务器的时候需要接收服务端服务器的 fingerprint，这个过程需要用户通过输入进行确认，如下图所示。但是这些提示信息被 lftp 掩盖了，所以 connecting 状态其实是 sftp 一直在等待用户的输入。解决方法是先在命令行使用 sftp 连接一次服务器，然后在使用 lftp。\n！[Are you sure you want to continue connecting](/uploads/20190507/continue-connecting.png)","source":"_posts/lftp-的-sftp-使用时遇到的坑.md","raw":"title: lftp 的 sftp 使用时遇到的坑\ndate: 2019-05-07 21:37:40\ntags:\n- Linux\ncategories:\n- 操作系统\n- Linux\n---\n\n今天在使用 lftp 的 sftp 时候一直处于 connecting 状态，并且没有其他提示信息。状态如下图所示：\n![lftp sftp](/uploads/20190507/lftp-sftp.png)\n\n<!-- more -->\n\n导致这个问题的原因是，在 sftp 第一次连接服务器的时候需要接收服务端服务器的 fingerprint，这个过程需要用户通过输入进行确认，如下图所示。但是这些提示信息被 lftp 掩盖了，所以 connecting 状态其实是 sftp 一直在等待用户的输入。解决方法是先在命令行使用 sftp 连接一次服务器，然后在使用 lftp。\n！[Are you sure you want to continue connecting](/uploads/20190507/continue-connecting.png)","slug":"lftp-的-sftp-使用时遇到的坑","published":1,"updated":"2021-07-19T16:28:00.300Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphr200dtitd3g4nw5enl","content":"<p>今天在使用 lftp 的 sftp 时候一直处于 connecting 状态，并且没有其他提示信息。状态如下图所示：<br><img src=\"/uploads/20190507/lftp-sftp.png\" alt=\"lftp sftp\"></p>\n<span id=\"more\"></span>\n\n<p>导致这个问题的原因是，在 sftp 第一次连接服务器的时候需要接收服务端服务器的 fingerprint，这个过程需要用户通过输入进行确认，如下图所示。但是这些提示信息被 lftp 掩盖了，所以 connecting 状态其实是 sftp 一直在等待用户的输入。解决方法是先在命令行使用 sftp 连接一次服务器，然后在使用 lftp。<br>！<a href=\"/uploads/20190507/continue-connecting.png\">Are you sure you want to continue connecting</a></p>\n","site":{"data":{}},"excerpt":"<p>今天在使用 lftp 的 sftp 时候一直处于 connecting 状态，并且没有其他提示信息。状态如下图所示：<br><img src=\"/uploads/20190507/lftp-sftp.png\" alt=\"lftp sftp\"></p>","more":"<p>导致这个问题的原因是，在 sftp 第一次连接服务器的时候需要接收服务端服务器的 fingerprint，这个过程需要用户通过输入进行确认，如下图所示。但是这些提示信息被 lftp 掩盖了，所以 connecting 状态其实是 sftp 一直在等待用户的输入。解决方法是先在命令行使用 sftp 连接一次服务器，然后在使用 lftp。<br>！<a href=\"/uploads/20190507/continue-connecting.png\">Are you sure you want to continue connecting</a></p>"},{"title":"javap","date":"2016-10-15T16:28:10.000Z","_content":"\n反汇编一个或多个 class 文件。\n\n<!-- more -->\n\n### 摘要\n\n**javap** [*options*] *classfile...*\n\n#### *options*\n\n命令行选项。具体选项参见“选项”一节。\n\n#### *classfile*\n\n一个或多个用空格分隔的被注释的类，例如：DocFooter.class。可以通过文件名或者 URL（如：file:///home/user/myproject/src/DocFooter.class）指定一个在类路径中的类。\n\n### 描述\n\n*javap* 命令反汇编一个或多个类文件。输出依赖于使用的选项。当不使用选项时，*javap* 命令打印传递给它的类的包、protected 和 public 属性、方法。*javap* 命令打印输出到标准输入。\n\n### 选项\n\n#### -help --help -?\n\n打印 *javap* 命令的帮助信息。\n\n#### -version\n\n打印发布信息。\n\n#### -l\n\n输出行号和本地变量表。\n\n#### -public\n\n仅显示公共类和成员。\n\n#### -protected\n\n显示受保护的/公共类和成员。\n\n#### -private -p\n\n显示所有类和成员。\n\n#### -Joption\n\n传递指定的选项给 JVM。例如：\n\n    javap -J-version\n    javap -J-Djava.security.manager -J-Djava.security.policy=MyPolicy MyClassName\n\n关于 JVM 选项的更多信息，参见 [java](http://docs.oracle.com/javase/8/docs/technotes/tools/unix/java.html#CBBFHAJA) 命令文档。\n\n#### -s\n\n输出内部类型签名。\n\n#### -sysinfo\n\n显示正在处理的类的系统信息 (路径、大小、日期、MD5 散列)。\n\n#### -constants\n\n显示静态最终常量。\n\n#### -c\n\n对代码进行反汇编。\n\n#### -verbose\n\n打印方法参数和本地变量的数量以及栈区大小。\n\n#### -classpath *path*\n\n指定查找用户类文件的位。当设置了这个变量将覆盖默认的或 CLASSPATH 环境变量。\n\n#### -bootclasspath *path*\n\n覆盖引导类文件的位置。默认的，引导类是那些实现了位于 jre/lib/rt.jar 和其他几个 JAR 文件的核心 Java 平台的类。\n\n#### -extdir *dirs*\n\n指定javap搜索已安装的java扩展的位置，默认的java扩展的位置为 java.ext.dirs 的值。\n\n### 实例\n\n编译下面的 HelloWorld.java 类：\n\n    public class HelloWorld {\n\n      public static void main(String[] args) {\n        System.out.println(\"Hello World!\");\n      }\n    }\n\njavap HelloWorld.class 命令的输出如下：\n\n    Compiled from \"HelloWorld.java\"\n    public class HelloWorld {\n      public HelloWorld();\n      public static void main(java.lang.String[]);\n    }\n\njavap -c HelloWorld.class 命令的输出如下：\n\n    Compiled from \"HelloWorld.java\"\n    public class HelloWorld {\n      public HelloWorld();\n        Code:\n           0: aload_0       \n           1: invokespecial #1                  // Method java/lang/Object.\"<init>\":()V\n           4: return        \n\n      public static void main(java.lang.String[]);\n        Code:\n           0: getstatic     #2                  // Field java/lang/System.out:Ljava/io/PrintStream;\n           3: ldc           #3                  // String Hello World!\n           5: invokevirtual #4                  // Method java/io/PrintStream.println:(Ljava/lang/String;)V\n           8: return        \n    }\n","source":"_posts/javap.md","raw":"title: javap\ntags:\n  - Java\ncategories:\n  - 语言\n  - Java\ndate: 2016-10-16 00:28:10\n---\n\n反汇编一个或多个 class 文件。\n\n<!-- more -->\n\n### 摘要\n\n**javap** [*options*] *classfile...*\n\n#### *options*\n\n命令行选项。具体选项参见“选项”一节。\n\n#### *classfile*\n\n一个或多个用空格分隔的被注释的类，例如：DocFooter.class。可以通过文件名或者 URL（如：file:///home/user/myproject/src/DocFooter.class）指定一个在类路径中的类。\n\n### 描述\n\n*javap* 命令反汇编一个或多个类文件。输出依赖于使用的选项。当不使用选项时，*javap* 命令打印传递给它的类的包、protected 和 public 属性、方法。*javap* 命令打印输出到标准输入。\n\n### 选项\n\n#### -help --help -?\n\n打印 *javap* 命令的帮助信息。\n\n#### -version\n\n打印发布信息。\n\n#### -l\n\n输出行号和本地变量表。\n\n#### -public\n\n仅显示公共类和成员。\n\n#### -protected\n\n显示受保护的/公共类和成员。\n\n#### -private -p\n\n显示所有类和成员。\n\n#### -Joption\n\n传递指定的选项给 JVM。例如：\n\n    javap -J-version\n    javap -J-Djava.security.manager -J-Djava.security.policy=MyPolicy MyClassName\n\n关于 JVM 选项的更多信息，参见 [java](http://docs.oracle.com/javase/8/docs/technotes/tools/unix/java.html#CBBFHAJA) 命令文档。\n\n#### -s\n\n输出内部类型签名。\n\n#### -sysinfo\n\n显示正在处理的类的系统信息 (路径、大小、日期、MD5 散列)。\n\n#### -constants\n\n显示静态最终常量。\n\n#### -c\n\n对代码进行反汇编。\n\n#### -verbose\n\n打印方法参数和本地变量的数量以及栈区大小。\n\n#### -classpath *path*\n\n指定查找用户类文件的位。当设置了这个变量将覆盖默认的或 CLASSPATH 环境变量。\n\n#### -bootclasspath *path*\n\n覆盖引导类文件的位置。默认的，引导类是那些实现了位于 jre/lib/rt.jar 和其他几个 JAR 文件的核心 Java 平台的类。\n\n#### -extdir *dirs*\n\n指定javap搜索已安装的java扩展的位置，默认的java扩展的位置为 java.ext.dirs 的值。\n\n### 实例\n\n编译下面的 HelloWorld.java 类：\n\n    public class HelloWorld {\n\n      public static void main(String[] args) {\n        System.out.println(\"Hello World!\");\n      }\n    }\n\njavap HelloWorld.class 命令的输出如下：\n\n    Compiled from \"HelloWorld.java\"\n    public class HelloWorld {\n      public HelloWorld();\n      public static void main(java.lang.String[]);\n    }\n\njavap -c HelloWorld.class 命令的输出如下：\n\n    Compiled from \"HelloWorld.java\"\n    public class HelloWorld {\n      public HelloWorld();\n        Code:\n           0: aload_0       \n           1: invokespecial #1                  // Method java/lang/Object.\"<init>\":()V\n           4: return        \n\n      public static void main(java.lang.String[]);\n        Code:\n           0: getstatic     #2                  // Field java/lang/System.out:Ljava/io/PrintStream;\n           3: ldc           #3                  // String Hello World!\n           5: invokevirtual #4                  // Method java/io/PrintStream.println:(Ljava/lang/String;)V\n           8: return        \n    }\n","slug":"javap","published":1,"updated":"2021-07-19T16:28:00.300Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphr300dyitd31ovlfg8d","content":"<p>反汇编一个或多个 class 文件。</p>\n<span id=\"more\"></span>\n\n<h3 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h3><p><strong>javap</strong> [<em>options</em>] <em>classfile…</em></p>\n<h4 id=\"options\"><a href=\"#options\" class=\"headerlink\" title=\"options\"></a><em>options</em></h4><p>命令行选项。具体选项参见“选项”一节。</p>\n<h4 id=\"classfile\"><a href=\"#classfile\" class=\"headerlink\" title=\"classfile\"></a><em>classfile</em></h4><p>一个或多个用空格分隔的被注释的类，例如：DocFooter.class。可以通过文件名或者 URL（如：file:///home/user/myproject/src/DocFooter.class）指定一个在类路径中的类。</p>\n<h3 id=\"描述\"><a href=\"#描述\" class=\"headerlink\" title=\"描述\"></a>描述</h3><p><em>javap</em> 命令反汇编一个或多个类文件。输出依赖于使用的选项。当不使用选项时，<em>javap</em> 命令打印传递给它的类的包、protected 和 public 属性、方法。<em>javap</em> 命令打印输出到标准输入。</p>\n<h3 id=\"选项\"><a href=\"#选项\" class=\"headerlink\" title=\"选项\"></a>选项</h3><h4 id=\"help-–help\"><a href=\"#help-–help\" class=\"headerlink\" title=\"-help –help -?\"></a>-help –help -?</h4><p>打印 <em>javap</em> 命令的帮助信息。</p>\n<h4 id=\"version\"><a href=\"#version\" class=\"headerlink\" title=\"-version\"></a>-version</h4><p>打印发布信息。</p>\n<h4 id=\"l\"><a href=\"#l\" class=\"headerlink\" title=\"-l\"></a>-l</h4><p>输出行号和本地变量表。</p>\n<h4 id=\"public\"><a href=\"#public\" class=\"headerlink\" title=\"-public\"></a>-public</h4><p>仅显示公共类和成员。</p>\n<h4 id=\"protected\"><a href=\"#protected\" class=\"headerlink\" title=\"-protected\"></a>-protected</h4><p>显示受保护的/公共类和成员。</p>\n<h4 id=\"private-p\"><a href=\"#private-p\" class=\"headerlink\" title=\"-private -p\"></a>-private -p</h4><p>显示所有类和成员。</p>\n<h4 id=\"Joption\"><a href=\"#Joption\" class=\"headerlink\" title=\"-Joption\"></a>-Joption</h4><p>传递指定的选项给 JVM。例如：</p>\n<pre><code>javap -J-version\njavap -J-Djava.security.manager -J-Djava.security.policy=MyPolicy MyClassName\n</code></pre>\n<p>关于 JVM 选项的更多信息，参见 <a href=\"http://docs.oracle.com/javase/8/docs/technotes/tools/unix/java.html#CBBFHAJA\">java</a> 命令文档。</p>\n<h4 id=\"s\"><a href=\"#s\" class=\"headerlink\" title=\"-s\"></a>-s</h4><p>输出内部类型签名。</p>\n<h4 id=\"sysinfo\"><a href=\"#sysinfo\" class=\"headerlink\" title=\"-sysinfo\"></a>-sysinfo</h4><p>显示正在处理的类的系统信息 (路径、大小、日期、MD5 散列)。</p>\n<h4 id=\"constants\"><a href=\"#constants\" class=\"headerlink\" title=\"-constants\"></a>-constants</h4><p>显示静态最终常量。</p>\n<h4 id=\"c\"><a href=\"#c\" class=\"headerlink\" title=\"-c\"></a>-c</h4><p>对代码进行反汇编。</p>\n<h4 id=\"verbose\"><a href=\"#verbose\" class=\"headerlink\" title=\"-verbose\"></a>-verbose</h4><p>打印方法参数和本地变量的数量以及栈区大小。</p>\n<h4 id=\"classpath-path\"><a href=\"#classpath-path\" class=\"headerlink\" title=\"-classpath path\"></a>-classpath <em>path</em></h4><p>指定查找用户类文件的位。当设置了这个变量将覆盖默认的或 CLASSPATH 环境变量。</p>\n<h4 id=\"bootclasspath-path\"><a href=\"#bootclasspath-path\" class=\"headerlink\" title=\"-bootclasspath path\"></a>-bootclasspath <em>path</em></h4><p>覆盖引导类文件的位置。默认的，引导类是那些实现了位于 jre/lib/rt.jar 和其他几个 JAR 文件的核心 Java 平台的类。</p>\n<h4 id=\"extdir-dirs\"><a href=\"#extdir-dirs\" class=\"headerlink\" title=\"-extdir dirs\"></a>-extdir <em>dirs</em></h4><p>指定javap搜索已安装的java扩展的位置，默认的java扩展的位置为 java.ext.dirs 的值。</p>\n<h3 id=\"实例\"><a href=\"#实例\" class=\"headerlink\" title=\"实例\"></a>实例</h3><p>编译下面的 HelloWorld.java 类：</p>\n<pre><code>public class HelloWorld &#123;\n\n  public static void main(String[] args) &#123;\n    System.out.println(&quot;Hello World!&quot;);\n  &#125;\n&#125;\n</code></pre>\n<p>javap HelloWorld.class 命令的输出如下：</p>\n<pre><code>Compiled from &quot;HelloWorld.java&quot;\npublic class HelloWorld &#123;\n  public HelloWorld();\n  public static void main(java.lang.String[]);\n&#125;\n</code></pre>\n<p>javap -c HelloWorld.class 命令的输出如下：</p>\n<pre><code>Compiled from &quot;HelloWorld.java&quot;\npublic class HelloWorld &#123;\n  public HelloWorld();\n    Code:\n       0: aload_0       \n       1: invokespecial #1                  // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V\n       4: return        \n\n  public static void main(java.lang.String[]);\n    Code:\n       0: getstatic     #2                  // Field java/lang/System.out:Ljava/io/PrintStream;\n       3: ldc           #3                  // String Hello World!\n       5: invokevirtual #4                  // Method java/io/PrintStream.println:(Ljava/lang/String;)V\n       8: return        \n&#125;\n</code></pre>\n","site":{"data":{}},"excerpt":"<p>反汇编一个或多个 class 文件。</p>","more":"<h3 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h3><p><strong>javap</strong> [<em>options</em>] <em>classfile…</em></p>\n<h4 id=\"options\"><a href=\"#options\" class=\"headerlink\" title=\"options\"></a><em>options</em></h4><p>命令行选项。具体选项参见“选项”一节。</p>\n<h4 id=\"classfile\"><a href=\"#classfile\" class=\"headerlink\" title=\"classfile\"></a><em>classfile</em></h4><p>一个或多个用空格分隔的被注释的类，例如：DocFooter.class。可以通过文件名或者 URL（如：file:///home/user/myproject/src/DocFooter.class）指定一个在类路径中的类。</p>\n<h3 id=\"描述\"><a href=\"#描述\" class=\"headerlink\" title=\"描述\"></a>描述</h3><p><em>javap</em> 命令反汇编一个或多个类文件。输出依赖于使用的选项。当不使用选项时，<em>javap</em> 命令打印传递给它的类的包、protected 和 public 属性、方法。<em>javap</em> 命令打印输出到标准输入。</p>\n<h3 id=\"选项\"><a href=\"#选项\" class=\"headerlink\" title=\"选项\"></a>选项</h3><h4 id=\"help-–help\"><a href=\"#help-–help\" class=\"headerlink\" title=\"-help –help -?\"></a>-help –help -?</h4><p>打印 <em>javap</em> 命令的帮助信息。</p>\n<h4 id=\"version\"><a href=\"#version\" class=\"headerlink\" title=\"-version\"></a>-version</h4><p>打印发布信息。</p>\n<h4 id=\"l\"><a href=\"#l\" class=\"headerlink\" title=\"-l\"></a>-l</h4><p>输出行号和本地变量表。</p>\n<h4 id=\"public\"><a href=\"#public\" class=\"headerlink\" title=\"-public\"></a>-public</h4><p>仅显示公共类和成员。</p>\n<h4 id=\"protected\"><a href=\"#protected\" class=\"headerlink\" title=\"-protected\"></a>-protected</h4><p>显示受保护的/公共类和成员。</p>\n<h4 id=\"private-p\"><a href=\"#private-p\" class=\"headerlink\" title=\"-private -p\"></a>-private -p</h4><p>显示所有类和成员。</p>\n<h4 id=\"Joption\"><a href=\"#Joption\" class=\"headerlink\" title=\"-Joption\"></a>-Joption</h4><p>传递指定的选项给 JVM。例如：</p>\n<pre><code>javap -J-version\njavap -J-Djava.security.manager -J-Djava.security.policy=MyPolicy MyClassName\n</code></pre>\n<p>关于 JVM 选项的更多信息，参见 <a href=\"http://docs.oracle.com/javase/8/docs/technotes/tools/unix/java.html#CBBFHAJA\">java</a> 命令文档。</p>\n<h4 id=\"s\"><a href=\"#s\" class=\"headerlink\" title=\"-s\"></a>-s</h4><p>输出内部类型签名。</p>\n<h4 id=\"sysinfo\"><a href=\"#sysinfo\" class=\"headerlink\" title=\"-sysinfo\"></a>-sysinfo</h4><p>显示正在处理的类的系统信息 (路径、大小、日期、MD5 散列)。</p>\n<h4 id=\"constants\"><a href=\"#constants\" class=\"headerlink\" title=\"-constants\"></a>-constants</h4><p>显示静态最终常量。</p>\n<h4 id=\"c\"><a href=\"#c\" class=\"headerlink\" title=\"-c\"></a>-c</h4><p>对代码进行反汇编。</p>\n<h4 id=\"verbose\"><a href=\"#verbose\" class=\"headerlink\" title=\"-verbose\"></a>-verbose</h4><p>打印方法参数和本地变量的数量以及栈区大小。</p>\n<h4 id=\"classpath-path\"><a href=\"#classpath-path\" class=\"headerlink\" title=\"-classpath path\"></a>-classpath <em>path</em></h4><p>指定查找用户类文件的位。当设置了这个变量将覆盖默认的或 CLASSPATH 环境变量。</p>\n<h4 id=\"bootclasspath-path\"><a href=\"#bootclasspath-path\" class=\"headerlink\" title=\"-bootclasspath path\"></a>-bootclasspath <em>path</em></h4><p>覆盖引导类文件的位置。默认的，引导类是那些实现了位于 jre/lib/rt.jar 和其他几个 JAR 文件的核心 Java 平台的类。</p>\n<h4 id=\"extdir-dirs\"><a href=\"#extdir-dirs\" class=\"headerlink\" title=\"-extdir dirs\"></a>-extdir <em>dirs</em></h4><p>指定javap搜索已安装的java扩展的位置，默认的java扩展的位置为 java.ext.dirs 的值。</p>\n<h3 id=\"实例\"><a href=\"#实例\" class=\"headerlink\" title=\"实例\"></a>实例</h3><p>编译下面的 HelloWorld.java 类：</p>\n<pre><code>public class HelloWorld &#123;\n\n  public static void main(String[] args) &#123;\n    System.out.println(&quot;Hello World!&quot;);\n  &#125;\n&#125;\n</code></pre>\n<p>javap HelloWorld.class 命令的输出如下：</p>\n<pre><code>Compiled from &quot;HelloWorld.java&quot;\npublic class HelloWorld &#123;\n  public HelloWorld();\n  public static void main(java.lang.String[]);\n&#125;\n</code></pre>\n<p>javap -c HelloWorld.class 命令的输出如下：</p>\n<pre><code>Compiled from &quot;HelloWorld.java&quot;\npublic class HelloWorld &#123;\n  public HelloWorld();\n    Code:\n       0: aload_0       \n       1: invokespecial #1                  // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V\n       4: return        \n\n  public static void main(java.lang.String[]);\n    Code:\n       0: getstatic     #2                  // Field java/lang/System.out:Ljava/io/PrintStream;\n       3: ldc           #3                  // String Hello World!\n       5: invokevirtual #4                  // Method java/io/PrintStream.println:(Ljava/lang/String;)V\n       8: return        \n&#125;\n</code></pre>"},{"title":"mongoose安装","date":"2015-12-07T01:51:59.000Z","_content":"\n首先，已经安装了 MongoDB 和 Node.js 。\n然后，用 npm 命令安装 Mongoose 。\n\n\t$ npm install mongoose\n\n<!-- more -->\n\n### gyp ERR! build error\n如果安装过程中遇到以下错误，请尝试下面的解决方法：\n\n\tmake: *** [Release/obj.target/kerberos/lib/kerberos.o] Error 1\n\tmake: Leaving directory `/letv/DataInspector/node_modules/mongoose/node_modules/mongodb/node_modules/kerberos/build'\n\tgyp ERR! build error\n\tgyp ERR! stack Error: `make` failed with exit code: 2\n\tgyp ERR! stack     at ChildProcess.onExit (/letv/nodejs/lib/node_modules/npm/node_modules/node-gyp/lib/build.js:270:23)\n\tgyp ERR! stack     at emitTwo (events.js:87:13)\n\tgyp ERR! stack     at ChildProcess.emit (events.js:172:7)\n\tgyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:200:12)\n\tgyp ERR! System Linux 2.6.32-926.504.30.3.letv.el6.x86_64\n\tgyp ERR! command \"/letv/nodejs/bin/node\" \"/letv/nodejs/lib/node_modules/npm/node_modules/node-gyp/bin/node-gyp.js\" \"rebuild\"\n\tgyp ERR! cwd /letv/DataInspector/node_modules/mongoose/node_modules/mongodb/node_modules/kerberos\n\tgyp ERR! node -v v4.2.2\n\tgyp ERR! node-gyp -v v3.0.3\n\tgyp ERR! not ok\n\n（1）检查 python 是否安装以及版本号\n\n\t$ python -V\n\tPython 2.6.6\n\n（2）检查 g++ 版本\n\n这个检查是借鉴网友的做法。（这块儿真心不懂，感谢度娘和goole，还有热血网友^_^）\n如果g++低于 4.8 需要升级g++版本。\n\n\t$ g++ -v\n\tUsing built-in specs.\n\tTarget: x86_64-redhat-linux\n\tConfigured with: ../configure --prefix=/usr --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-bootstrap --enable-shared --enable-threads=posix --enable-checking=release --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-languages=c,c++,objc,obj-c++,java,fortran,ada --enable-java-awt=gtk --disable-dssi --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-1.5.0.0/jre --enable-libgcj-multifile --enable-java-maintainer-mode --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --disable-libjava-multilib --with-ppl --with-cloog --with-tune=generic --with-arch_32=i686 --build=x86_64-redhat-linux\n\tThread model: posix\n\tgcc version 4.4.7 20120313 (Red Hat 4.4.7-11) (GCC)\n\n升级方法参考：<http://xg2007524.blog.51cto.com/869106/1337465>\n如果参考博文中的第二步“下载编译所需依赖库”不能自动完成的话，可以根据 contrib/download_prerequisites 中的内容手动下载，并运行相应的命令完成操作。\n\n完成 gcc 升级后，需要替换系统默认的g++\n\n\t$ cd /usr/bin\n\t$ rm -f g++\n\t$ ln -s /usr/local/bin/g++ g++\n\t$ g++ --version\n\tg++ (GCC) 4.8.2\n\tCopyright (C) 2013 Free Software Foundation, Inc.\n\tThis is free software; see the source for copying conditions.  There is NO\n\twarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\ng++ 升级完成，重新安装 mongoose 成功。\n","source":"_posts/mongoose安装.md","raw":"title: mongoose安装\ntags:\n  - Node.js\n  - mongoose\n  - MongoDB\ncategories:\n  - 开发\n  - 环境搭建\ndate: 2015-12-07 09:51:59\n---\n\n首先，已经安装了 MongoDB 和 Node.js 。\n然后，用 npm 命令安装 Mongoose 。\n\n\t$ npm install mongoose\n\n<!-- more -->\n\n### gyp ERR! build error\n如果安装过程中遇到以下错误，请尝试下面的解决方法：\n\n\tmake: *** [Release/obj.target/kerberos/lib/kerberos.o] Error 1\n\tmake: Leaving directory `/letv/DataInspector/node_modules/mongoose/node_modules/mongodb/node_modules/kerberos/build'\n\tgyp ERR! build error\n\tgyp ERR! stack Error: `make` failed with exit code: 2\n\tgyp ERR! stack     at ChildProcess.onExit (/letv/nodejs/lib/node_modules/npm/node_modules/node-gyp/lib/build.js:270:23)\n\tgyp ERR! stack     at emitTwo (events.js:87:13)\n\tgyp ERR! stack     at ChildProcess.emit (events.js:172:7)\n\tgyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:200:12)\n\tgyp ERR! System Linux 2.6.32-926.504.30.3.letv.el6.x86_64\n\tgyp ERR! command \"/letv/nodejs/bin/node\" \"/letv/nodejs/lib/node_modules/npm/node_modules/node-gyp/bin/node-gyp.js\" \"rebuild\"\n\tgyp ERR! cwd /letv/DataInspector/node_modules/mongoose/node_modules/mongodb/node_modules/kerberos\n\tgyp ERR! node -v v4.2.2\n\tgyp ERR! node-gyp -v v3.0.3\n\tgyp ERR! not ok\n\n（1）检查 python 是否安装以及版本号\n\n\t$ python -V\n\tPython 2.6.6\n\n（2）检查 g++ 版本\n\n这个检查是借鉴网友的做法。（这块儿真心不懂，感谢度娘和goole，还有热血网友^_^）\n如果g++低于 4.8 需要升级g++版本。\n\n\t$ g++ -v\n\tUsing built-in specs.\n\tTarget: x86_64-redhat-linux\n\tConfigured with: ../configure --prefix=/usr --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-bootstrap --enable-shared --enable-threads=posix --enable-checking=release --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-languages=c,c++,objc,obj-c++,java,fortran,ada --enable-java-awt=gtk --disable-dssi --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-1.5.0.0/jre --enable-libgcj-multifile --enable-java-maintainer-mode --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --disable-libjava-multilib --with-ppl --with-cloog --with-tune=generic --with-arch_32=i686 --build=x86_64-redhat-linux\n\tThread model: posix\n\tgcc version 4.4.7 20120313 (Red Hat 4.4.7-11) (GCC)\n\n升级方法参考：<http://xg2007524.blog.51cto.com/869106/1337465>\n如果参考博文中的第二步“下载编译所需依赖库”不能自动完成的话，可以根据 contrib/download_prerequisites 中的内容手动下载，并运行相应的命令完成操作。\n\n完成 gcc 升级后，需要替换系统默认的g++\n\n\t$ cd /usr/bin\n\t$ rm -f g++\n\t$ ln -s /usr/local/bin/g++ g++\n\t$ g++ --version\n\tg++ (GCC) 4.8.2\n\tCopyright (C) 2013 Free Software Foundation, Inc.\n\tThis is free software; see the source for copying conditions.  There is NO\n\twarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\ng++ 升级完成，重新安装 mongoose 成功。\n","slug":"mongoose安装","published":1,"updated":"2021-07-19T16:28:00.300Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphr400e1itd39rik1ijg","content":"<p>首先，已经安装了 MongoDB 和 Node.js 。<br>然后，用 npm 命令安装 Mongoose 。</p>\n<pre><code>$ npm install mongoose\n</code></pre>\n<span id=\"more\"></span>\n\n<h3 id=\"gyp-ERR-build-error\"><a href=\"#gyp-ERR-build-error\" class=\"headerlink\" title=\"gyp ERR! build error\"></a>gyp ERR! build error</h3><p>如果安装过程中遇到以下错误，请尝试下面的解决方法：</p>\n<pre><code>make: *** [Release/obj.target/kerberos/lib/kerberos.o] Error 1\nmake: Leaving directory `/letv/DataInspector/node_modules/mongoose/node_modules/mongodb/node_modules/kerberos/build&#39;\ngyp ERR! build error\ngyp ERR! stack Error: `make` failed with exit code: 2\ngyp ERR! stack     at ChildProcess.onExit (/letv/nodejs/lib/node_modules/npm/node_modules/node-gyp/lib/build.js:270:23)\ngyp ERR! stack     at emitTwo (events.js:87:13)\ngyp ERR! stack     at ChildProcess.emit (events.js:172:7)\ngyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:200:12)\ngyp ERR! System Linux 2.6.32-926.504.30.3.letv.el6.x86_64\ngyp ERR! command &quot;/letv/nodejs/bin/node&quot; &quot;/letv/nodejs/lib/node_modules/npm/node_modules/node-gyp/bin/node-gyp.js&quot; &quot;rebuild&quot;\ngyp ERR! cwd /letv/DataInspector/node_modules/mongoose/node_modules/mongodb/node_modules/kerberos\ngyp ERR! node -v v4.2.2\ngyp ERR! node-gyp -v v3.0.3\ngyp ERR! not ok\n</code></pre>\n<p>（1）检查 python 是否安装以及版本号</p>\n<pre><code>$ python -V\nPython 2.6.6\n</code></pre>\n<p>（2）检查 g++ 版本</p>\n<p>这个检查是借鉴网友的做法。（这块儿真心不懂，感谢度娘和goole，还有热血网友^_^）<br>如果g++低于 4.8 需要升级g++版本。</p>\n<pre><code>$ g++ -v\nUsing built-in specs.\nTarget: x86_64-redhat-linux\nConfigured with: ../configure --prefix=/usr --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-bootstrap --enable-shared --enable-threads=posix --enable-checking=release --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-languages=c,c++,objc,obj-c++,java,fortran,ada --enable-java-awt=gtk --disable-dssi --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-1.5.0.0/jre --enable-libgcj-multifile --enable-java-maintainer-mode --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --disable-libjava-multilib --with-ppl --with-cloog --with-tune=generic --with-arch_32=i686 --build=x86_64-redhat-linux\nThread model: posix\ngcc version 4.4.7 20120313 (Red Hat 4.4.7-11) (GCC)\n</code></pre>\n<p>升级方法参考：<a href=\"http://xg2007524.blog.51cto.com/869106/1337465\">http://xg2007524.blog.51cto.com/869106/1337465</a><br>如果参考博文中的第二步“下载编译所需依赖库”不能自动完成的话，可以根据 contrib/download_prerequisites 中的内容手动下载，并运行相应的命令完成操作。</p>\n<p>完成 gcc 升级后，需要替换系统默认的g++</p>\n<pre><code>$ cd /usr/bin\n$ rm -f g++\n$ ln -s /usr/local/bin/g++ g++\n$ g++ --version\ng++ (GCC) 4.8.2\nCopyright (C) 2013 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n</code></pre>\n<p>g++ 升级完成，重新安装 mongoose 成功。</p>\n","site":{"data":{}},"excerpt":"<p>首先，已经安装了 MongoDB 和 Node.js 。<br>然后，用 npm 命令安装 Mongoose 。</p>\n<pre><code>$ npm install mongoose\n</code></pre>","more":"<h3 id=\"gyp-ERR-build-error\"><a href=\"#gyp-ERR-build-error\" class=\"headerlink\" title=\"gyp ERR! build error\"></a>gyp ERR! build error</h3><p>如果安装过程中遇到以下错误，请尝试下面的解决方法：</p>\n<pre><code>make: *** [Release/obj.target/kerberos/lib/kerberos.o] Error 1\nmake: Leaving directory `/letv/DataInspector/node_modules/mongoose/node_modules/mongodb/node_modules/kerberos/build&#39;\ngyp ERR! build error\ngyp ERR! stack Error: `make` failed with exit code: 2\ngyp ERR! stack     at ChildProcess.onExit (/letv/nodejs/lib/node_modules/npm/node_modules/node-gyp/lib/build.js:270:23)\ngyp ERR! stack     at emitTwo (events.js:87:13)\ngyp ERR! stack     at ChildProcess.emit (events.js:172:7)\ngyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:200:12)\ngyp ERR! System Linux 2.6.32-926.504.30.3.letv.el6.x86_64\ngyp ERR! command &quot;/letv/nodejs/bin/node&quot; &quot;/letv/nodejs/lib/node_modules/npm/node_modules/node-gyp/bin/node-gyp.js&quot; &quot;rebuild&quot;\ngyp ERR! cwd /letv/DataInspector/node_modules/mongoose/node_modules/mongodb/node_modules/kerberos\ngyp ERR! node -v v4.2.2\ngyp ERR! node-gyp -v v3.0.3\ngyp ERR! not ok\n</code></pre>\n<p>（1）检查 python 是否安装以及版本号</p>\n<pre><code>$ python -V\nPython 2.6.6\n</code></pre>\n<p>（2）检查 g++ 版本</p>\n<p>这个检查是借鉴网友的做法。（这块儿真心不懂，感谢度娘和goole，还有热血网友^_^）<br>如果g++低于 4.8 需要升级g++版本。</p>\n<pre><code>$ g++ -v\nUsing built-in specs.\nTarget: x86_64-redhat-linux\nConfigured with: ../configure --prefix=/usr --mandir=/usr/share/man --infodir=/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-bootstrap --enable-shared --enable-threads=posix --enable-checking=release --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-languages=c,c++,objc,obj-c++,java,fortran,ada --enable-java-awt=gtk --disable-dssi --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-1.5.0.0/jre --enable-libgcj-multifile --enable-java-maintainer-mode --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --disable-libjava-multilib --with-ppl --with-cloog --with-tune=generic --with-arch_32=i686 --build=x86_64-redhat-linux\nThread model: posix\ngcc version 4.4.7 20120313 (Red Hat 4.4.7-11) (GCC)\n</code></pre>\n<p>升级方法参考：<a href=\"http://xg2007524.blog.51cto.com/869106/1337465\">http://xg2007524.blog.51cto.com/869106/1337465</a><br>如果参考博文中的第二步“下载编译所需依赖库”不能自动完成的话，可以根据 contrib/download_prerequisites 中的内容手动下载，并运行相应的命令完成操作。</p>\n<p>完成 gcc 升级后，需要替换系统默认的g++</p>\n<pre><code>$ cd /usr/bin\n$ rm -f g++\n$ ln -s /usr/local/bin/g++ g++\n$ g++ --version\ng++ (GCC) 4.8.2\nCopyright (C) 2013 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n</code></pre>\n<p>g++ 升级完成，重新安装 mongoose 成功。</p>"},{"title":"nginx ngx_http_core_module location指令","date":"2016-01-15T13:33:10.000Z","_content":"\n\n\t语法：location [ = | ~ | ~* | ^~ ] uri { ... }\n\t     location @name { ... }\n\t默认：—\n\t上下文：server, location\n\n为请求 URI（路径）设置配置。\n\n<!-- more -->\n\n路径匹配在 URI 规范化以后进行。规范化，就是先将 URI 中形如 “%XX” 的编码字符进行解码，再解析 URI 中的相对路径 “.” 和 “..” 部分，另外还可能会[压缩](http://nginx.org/en/docs/http/ngx_http_core_module.html#merge_slashes)相邻的两个或多个斜线成为一个斜线。\n\n可以使用前缀字符串或者正则表达式定义路径。使用正则表达式需要在路径开始添加 “~*” 前缀修饰语 （不区分大小写），或者 “~” 修饰语（区分大小写）。为了匹配请求的 URI 路径，nginx 先检查前缀字符串定义的路径 (前缀路径)。在这些路径中，最长匹配前缀的路径会被选中并记住。然后 nginx 按在配置文件中的出现顺序检查正则表达式路径。第一个匹配的路径找到后检查会停止，并使用相应的配置。如果找不到匹配的正则表达式路径，那么就使用前面被记住的前缀路径的配置。\n\n路径配置块可以嵌套，下面会提到一些例外。\n\n像 Mac OS X 和 Cygwin 这种不区分大小写的操作系统，匹配前缀字符串忽略大小写(0.7.7)。但是，比较仅限于单字节的编码区域(one-byte locale)。\n\n正则表达式中可以包含匹配组(0.7.40)，结果可以被后面的其他指令使用。\n\n如果最大前缀匹配的路径以“^~”开始，那么nginx不再检查正则表达式。\n\n使用“=”前缀可以定义URI和路径的精确匹配。如果找到精确匹配，则终止查找。例如，如果 “/” 请求频繁，定义 “location = /” 将提高这些请求的处理速度，因为查找过程在第一次比较以后即结束。这样的路径明显不可能包含嵌套路径。\n\n>在 0.7.1 到 0.8.41 的版本中，如果一个请求匹配没有“=” 和“^~”修饰符前缀的路径，查找也会终止，且正则表达式也不会检查。\n\n让我们以实力说明上面的情况：\n\n\tlocation = / {\n      [ configuration A ]\n\t}\n\n\tlocation / {\n      [ configuration B ]\n\t}\n\n\tlocation /documents/ {\n      [ configuration C ]\n\t}\n\n\tlocation ^~ /images/ {\n      [ configuration D ]\n\t}\n\n\tlocation ~* \\.(gif|jpg|jpeg)$ {\n      [ configuration E ]\n\t}\n\n“/” 请求将匹配配置 A，“/index.html” 请求将匹配配置B，“/documents/document.html” 请求将匹配配置C，“/images/1.gif” 请求将匹配配置D，“/documents/1.jpg” 请求将匹配配置E。\n\n“@” 前缀定义一个命名的路径。这种路径不会用作常规请求的处理，仅用作请求转发。它们不能嵌套，并且不能包含嵌套路径。\n\n如果一个路径用带有斜杠结尾的前缀字符串定义的，并且请求被 [proxy_pass](http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_pass)、[fastcgi_pass](http://nginx.org/en/docs/http/ngx_http_fastcgi_module.html#fastcgi_pass)、[uwsgi_pass](http://nginx.org/en/docs/http/ngx_http_uwsgi_module.html#uwsgi_pass)、[scgi_pass](http://nginx.org/en/docs/http/ngx_http_scgi_module.html#scgi_pass) 或者 [memcached_pass](http://nginx.org/en/docs/http/ngx_http_memcached_module.html#memcached_pass)中的一个处理，那么特定的处理会执行。在响应 URI 与该字符串相等的请求，但没有结尾的斜杠，一个带有 301 代码的固定转发将返回给追加斜杠的被请求 URI。如果不喜欢这样，可以像下面这样定义精确匹配 URI 的路径：\n\n\tlocation /user/ {\n      proxy_pass http://user.example.com;\n\t}\n\n\tlocation = /user {\n      proxy_pass http://login.example.com;\n\t}\n","source":"_posts/nginx-ngx-http-core-module-location指令.md","raw":"title: nginx ngx_http_core_module location指令\ntags:\n  - Nginx\ncategories:\n  - 开发工具\n  - Nginx\ndate: 2016-01-15 21:33:10\n---\n\n\n\t语法：location [ = | ~ | ~* | ^~ ] uri { ... }\n\t     location @name { ... }\n\t默认：—\n\t上下文：server, location\n\n为请求 URI（路径）设置配置。\n\n<!-- more -->\n\n路径匹配在 URI 规范化以后进行。规范化，就是先将 URI 中形如 “%XX” 的编码字符进行解码，再解析 URI 中的相对路径 “.” 和 “..” 部分，另外还可能会[压缩](http://nginx.org/en/docs/http/ngx_http_core_module.html#merge_slashes)相邻的两个或多个斜线成为一个斜线。\n\n可以使用前缀字符串或者正则表达式定义路径。使用正则表达式需要在路径开始添加 “~*” 前缀修饰语 （不区分大小写），或者 “~” 修饰语（区分大小写）。为了匹配请求的 URI 路径，nginx 先检查前缀字符串定义的路径 (前缀路径)。在这些路径中，最长匹配前缀的路径会被选中并记住。然后 nginx 按在配置文件中的出现顺序检查正则表达式路径。第一个匹配的路径找到后检查会停止，并使用相应的配置。如果找不到匹配的正则表达式路径，那么就使用前面被记住的前缀路径的配置。\n\n路径配置块可以嵌套，下面会提到一些例外。\n\n像 Mac OS X 和 Cygwin 这种不区分大小写的操作系统，匹配前缀字符串忽略大小写(0.7.7)。但是，比较仅限于单字节的编码区域(one-byte locale)。\n\n正则表达式中可以包含匹配组(0.7.40)，结果可以被后面的其他指令使用。\n\n如果最大前缀匹配的路径以“^~”开始，那么nginx不再检查正则表达式。\n\n使用“=”前缀可以定义URI和路径的精确匹配。如果找到精确匹配，则终止查找。例如，如果 “/” 请求频繁，定义 “location = /” 将提高这些请求的处理速度，因为查找过程在第一次比较以后即结束。这样的路径明显不可能包含嵌套路径。\n\n>在 0.7.1 到 0.8.41 的版本中，如果一个请求匹配没有“=” 和“^~”修饰符前缀的路径，查找也会终止，且正则表达式也不会检查。\n\n让我们以实力说明上面的情况：\n\n\tlocation = / {\n      [ configuration A ]\n\t}\n\n\tlocation / {\n      [ configuration B ]\n\t}\n\n\tlocation /documents/ {\n      [ configuration C ]\n\t}\n\n\tlocation ^~ /images/ {\n      [ configuration D ]\n\t}\n\n\tlocation ~* \\.(gif|jpg|jpeg)$ {\n      [ configuration E ]\n\t}\n\n“/” 请求将匹配配置 A，“/index.html” 请求将匹配配置B，“/documents/document.html” 请求将匹配配置C，“/images/1.gif” 请求将匹配配置D，“/documents/1.jpg” 请求将匹配配置E。\n\n“@” 前缀定义一个命名的路径。这种路径不会用作常规请求的处理，仅用作请求转发。它们不能嵌套，并且不能包含嵌套路径。\n\n如果一个路径用带有斜杠结尾的前缀字符串定义的，并且请求被 [proxy_pass](http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_pass)、[fastcgi_pass](http://nginx.org/en/docs/http/ngx_http_fastcgi_module.html#fastcgi_pass)、[uwsgi_pass](http://nginx.org/en/docs/http/ngx_http_uwsgi_module.html#uwsgi_pass)、[scgi_pass](http://nginx.org/en/docs/http/ngx_http_scgi_module.html#scgi_pass) 或者 [memcached_pass](http://nginx.org/en/docs/http/ngx_http_memcached_module.html#memcached_pass)中的一个处理，那么特定的处理会执行。在响应 URI 与该字符串相等的请求，但没有结尾的斜杠，一个带有 301 代码的固定转发将返回给追加斜杠的被请求 URI。如果不喜欢这样，可以像下面这样定义精确匹配 URI 的路径：\n\n\tlocation /user/ {\n      proxy_pass http://user.example.com;\n\t}\n\n\tlocation = /user {\n      proxy_pass http://login.example.com;\n\t}\n","slug":"nginx-ngx-http-core-module-location指令","published":1,"updated":"2021-07-19T16:28:00.300Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphr600e6itd3bxdv3zf2","content":"<pre><code>语法：location [ = | ~ | ~* | ^~ ] uri &#123; ... &#125;\n     location @name &#123; ... &#125;\n默认：—\n上下文：server, location\n</code></pre>\n<p>为请求 URI（路径）设置配置。</p>\n<span id=\"more\"></span>\n\n<p>路径匹配在 URI 规范化以后进行。规范化，就是先将 URI 中形如 “%XX” 的编码字符进行解码，再解析 URI 中的相对路径 “.” 和 “..” 部分，另外还可能会<a href=\"http://nginx.org/en/docs/http/ngx_http_core_module.html#merge_slashes\">压缩</a>相邻的两个或多个斜线成为一个斜线。</p>\n<p>可以使用前缀字符串或者正则表达式定义路径。使用正则表达式需要在路径开始添加 “<del>*” 前缀修饰语 （不区分大小写），或者 “</del>” 修饰语（区分大小写）。为了匹配请求的 URI 路径，nginx 先检查前缀字符串定义的路径 (前缀路径)。在这些路径中，最长匹配前缀的路径会被选中并记住。然后 nginx 按在配置文件中的出现顺序检查正则表达式路径。第一个匹配的路径找到后检查会停止，并使用相应的配置。如果找不到匹配的正则表达式路径，那么就使用前面被记住的前缀路径的配置。</p>\n<p>路径配置块可以嵌套，下面会提到一些例外。</p>\n<p>像 Mac OS X 和 Cygwin 这种不区分大小写的操作系统，匹配前缀字符串忽略大小写(0.7.7)。但是，比较仅限于单字节的编码区域(one-byte locale)。</p>\n<p>正则表达式中可以包含匹配组(0.7.40)，结果可以被后面的其他指令使用。</p>\n<p>如果最大前缀匹配的路径以“^~”开始，那么nginx不再检查正则表达式。</p>\n<p>使用“=”前缀可以定义URI和路径的精确匹配。如果找到精确匹配，则终止查找。例如，如果 “/” 请求频繁，定义 “location = /” 将提高这些请求的处理速度，因为查找过程在第一次比较以后即结束。这样的路径明显不可能包含嵌套路径。</p>\n<blockquote>\n<p>在 0.7.1 到 0.8.41 的版本中，如果一个请求匹配没有“=” 和“^~”修饰符前缀的路径，查找也会终止，且正则表达式也不会检查。</p>\n</blockquote>\n<p>让我们以实力说明上面的情况：</p>\n<pre><code>location = / &#123;\n  [ configuration A ]\n&#125;\n\nlocation / &#123;\n  [ configuration B ]\n&#125;\n\nlocation /documents/ &#123;\n  [ configuration C ]\n&#125;\n\nlocation ^~ /images/ &#123;\n  [ configuration D ]\n&#125;\n\nlocation ~* \\.(gif|jpg|jpeg)$ &#123;\n  [ configuration E ]\n&#125;\n</code></pre>\n<p>“/” 请求将匹配配置 A，“/index.html” 请求将匹配配置B，“/documents/document.html” 请求将匹配配置C，“/images/1.gif” 请求将匹配配置D，“/documents/1.jpg” 请求将匹配配置E。</p>\n<p>“@” 前缀定义一个命名的路径。这种路径不会用作常规请求的处理，仅用作请求转发。它们不能嵌套，并且不能包含嵌套路径。</p>\n<p>如果一个路径用带有斜杠结尾的前缀字符串定义的，并且请求被 <a href=\"http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_pass\">proxy_pass</a>、<a href=\"http://nginx.org/en/docs/http/ngx_http_fastcgi_module.html#fastcgi_pass\">fastcgi_pass</a>、<a href=\"http://nginx.org/en/docs/http/ngx_http_uwsgi_module.html#uwsgi_pass\">uwsgi_pass</a>、<a href=\"http://nginx.org/en/docs/http/ngx_http_scgi_module.html#scgi_pass\">scgi_pass</a> 或者 <a href=\"http://nginx.org/en/docs/http/ngx_http_memcached_module.html#memcached_pass\">memcached_pass</a>中的一个处理，那么特定的处理会执行。在响应 URI 与该字符串相等的请求，但没有结尾的斜杠，一个带有 301 代码的固定转发将返回给追加斜杠的被请求 URI。如果不喜欢这样，可以像下面这样定义精确匹配 URI 的路径：</p>\n<pre><code>location /user/ &#123;\n  proxy_pass http://user.example.com;\n&#125;\n\nlocation = /user &#123;\n  proxy_pass http://login.example.com;\n&#125;\n</code></pre>\n","site":{"data":{}},"excerpt":"<pre><code>语法：location [ = | ~ | ~* | ^~ ] uri &#123; ... &#125;\n     location @name &#123; ... &#125;\n默认：—\n上下文：server, location\n</code></pre>\n<p>为请求 URI（路径）设置配置。</p>","more":"<p>路径匹配在 URI 规范化以后进行。规范化，就是先将 URI 中形如 “%XX” 的编码字符进行解码，再解析 URI 中的相对路径 “.” 和 “..” 部分，另外还可能会<a href=\"http://nginx.org/en/docs/http/ngx_http_core_module.html#merge_slashes\">压缩</a>相邻的两个或多个斜线成为一个斜线。</p>\n<p>可以使用前缀字符串或者正则表达式定义路径。使用正则表达式需要在路径开始添加 “<del>*” 前缀修饰语 （不区分大小写），或者 “</del>” 修饰语（区分大小写）。为了匹配请求的 URI 路径，nginx 先检查前缀字符串定义的路径 (前缀路径)。在这些路径中，最长匹配前缀的路径会被选中并记住。然后 nginx 按在配置文件中的出现顺序检查正则表达式路径。第一个匹配的路径找到后检查会停止，并使用相应的配置。如果找不到匹配的正则表达式路径，那么就使用前面被记住的前缀路径的配置。</p>\n<p>路径配置块可以嵌套，下面会提到一些例外。</p>\n<p>像 Mac OS X 和 Cygwin 这种不区分大小写的操作系统，匹配前缀字符串忽略大小写(0.7.7)。但是，比较仅限于单字节的编码区域(one-byte locale)。</p>\n<p>正则表达式中可以包含匹配组(0.7.40)，结果可以被后面的其他指令使用。</p>\n<p>如果最大前缀匹配的路径以“^~”开始，那么nginx不再检查正则表达式。</p>\n<p>使用“=”前缀可以定义URI和路径的精确匹配。如果找到精确匹配，则终止查找。例如，如果 “/” 请求频繁，定义 “location = /” 将提高这些请求的处理速度，因为查找过程在第一次比较以后即结束。这样的路径明显不可能包含嵌套路径。</p>\n<blockquote>\n<p>在 0.7.1 到 0.8.41 的版本中，如果一个请求匹配没有“=” 和“^~”修饰符前缀的路径，查找也会终止，且正则表达式也不会检查。</p>\n</blockquote>\n<p>让我们以实力说明上面的情况：</p>\n<pre><code>location = / &#123;\n  [ configuration A ]\n&#125;\n\nlocation / &#123;\n  [ configuration B ]\n&#125;\n\nlocation /documents/ &#123;\n  [ configuration C ]\n&#125;\n\nlocation ^~ /images/ &#123;\n  [ configuration D ]\n&#125;\n\nlocation ~* \\.(gif|jpg|jpeg)$ &#123;\n  [ configuration E ]\n&#125;\n</code></pre>\n<p>“/” 请求将匹配配置 A，“/index.html” 请求将匹配配置B，“/documents/document.html” 请求将匹配配置C，“/images/1.gif” 请求将匹配配置D，“/documents/1.jpg” 请求将匹配配置E。</p>\n<p>“@” 前缀定义一个命名的路径。这种路径不会用作常规请求的处理，仅用作请求转发。它们不能嵌套，并且不能包含嵌套路径。</p>\n<p>如果一个路径用带有斜杠结尾的前缀字符串定义的，并且请求被 <a href=\"http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_pass\">proxy_pass</a>、<a href=\"http://nginx.org/en/docs/http/ngx_http_fastcgi_module.html#fastcgi_pass\">fastcgi_pass</a>、<a href=\"http://nginx.org/en/docs/http/ngx_http_uwsgi_module.html#uwsgi_pass\">uwsgi_pass</a>、<a href=\"http://nginx.org/en/docs/http/ngx_http_scgi_module.html#scgi_pass\">scgi_pass</a> 或者 <a href=\"http://nginx.org/en/docs/http/ngx_http_memcached_module.html#memcached_pass\">memcached_pass</a>中的一个处理，那么特定的处理会执行。在响应 URI 与该字符串相等的请求，但没有结尾的斜杠，一个带有 301 代码的固定转发将返回给追加斜杠的被请求 URI。如果不喜欢这样，可以像下面这样定义精确匹配 URI 的路径：</p>\n<pre><code>location /user/ &#123;\n  proxy_pass http://user.example.com;\n&#125;\n\nlocation = /user &#123;\n  proxy_pass http://login.example.com;\n&#125;\n</code></pre>"},{"title":"nginx ngx_http_core_module 内置变量","date":"2016-01-01T08:14:31.000Z","_content":"\nngx_http_core_module 模块支持与 Apache 服务器相同名称的内置变量。首先，这些都是表示客户端请求头字段的变量，例如 $http_user_agent 、$http_cookie 等等。还有其他变量：\n\n<!-- more -->\n\n$arg_name\n\n请求行中的参数名称\n\n$args\n\n请求行中的参数\n\n$binary_remote_addr\n\n二进制形式的客户端地址，值的长度总是 4 个字节\n\n$body_bytes_sent\n\n发送给客户端的字节数，不算响应头；这个变量兼容 mod_log_config Apache 模块的“%B”参数。\n\n$bytes_sent\n\n发送给客户端的字节数(1.3.8, 1.2.5)\n\n$connection\n\n连接序列数(1.3.8, 1.2.5)\n\n$connection_requests\n\n一个连接产生的当前请求数 (1.3.8, 1.2.5)\n\n$content_length\n\n请求头变量“Content-Length”\n\n$content_type\n\n请求头变量“Content-Type”\n\n$cookie_name\n\ncookie 名称\n\n$document_root\n\n当前请求的 root 或 alias 指令的值\n\n$document_uri\n\n同 $uri\n\n$host\n\n按照这个优先级顺序：请求行的主机名，请求头参数“Host” 中的主机名，匹配请求的服务器名称\n\n$hostname\n\n主机名称\n\n$http_*name*\n\n任意请求头参数；变量名的最后部分是转成小写的参数名，并且用下划线替换破折号。\n\n$https\n\n“on”如果连接运行在 SSL 模式，否则是一个空字符串。\n\n$is_args\n\n“?”如果请求行有参数，否则是一个空字符串。\n\n$limit_rate\n\n设置这个变量启用响应频率限制；参见 [limit_rate](http://nginx.org/en/docs/http/ngx_http_core_module.html#limit_rate)\n\n$msec\n\n以毫秒为单位的当前时间的秒数 (1.3.9, 1.2.6)\n\n$nginx_version\n\nnginx 版本\n\n$pid\n\n工作进程的 PID\n\n$pipe\n\n如果请求来自管道通信，值为“p”，否则为“.” (1.3.12, 1.2.7)\n\n$proxy_protocol_addr\n\n获取代理代理协议头的客户端地址，如果是直接访问，该值为空字符串。(1.5.12)  \n代理协议必须预先在 [listen](http://nginx.org/en/docs/http/ngx_http_core_module.html#listen) 指令中通过设置 proxy_protocol 参数启用。\n\n$query_string\n\n同 $args\n\n$realpath_root\n\n当前请求的文档根目录或别名的真实路径，会将所有符号连接转换为真实路径。\n\n$remote_addr\n\n客户端地址\n\n$remote_port\n\n客户端端口\n\n$remote_user\n\n用于HTTP基础认证服务的用户名\n\n$request\n\n完整的原始请求行\n\n$request_body\n\n请求体  \n此变量可在 location 中使用，将请求主体通过 proxy_pass, fastcgi_pass, uwsgi_pass, 和 scgi_pass 传递给下一级的代理服务器。\n\n$request_body_file\n\n将客户端请求主体保存在临时文件中。处理过程接收后，需要删除这个文件。如果一直将请求体写入一个文件， [client_body_in_file_only](http://nginx.org/en/docs/http/ngx_http_core_module.html#client_body_in_file_only) 需要被启用。如果一个临时文件的名称是在代理请求中传递的，或者在请求中传递给一个 FastCGI/uwsgi/SCGI 服务器， 传递请求体需要分别禁用 [proxy_pass_request_body off](http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_pass_request_body) 、 [fastcgi_pass_request_body off](http://nginx.org/en/docs/http/ngx_http_fastcgi_module.html#fastcgi_pass_request_body) 、 [uwsgi_pass_request_body off](http://nginx.org/en/docs/http/ngx_http_uwsgi_module.html#uwsgi_pass_request_body) 或者 [scgi_pass_request_body off](http://nginx.org/en/docs/http/ngx_http_scgi_module.html#scgi_pass_request_body) 指令。\n\n$request_completion\n\n如果请求完成，则值为“OK”，否则为空字符串。\n\n$request_filename\n\n当前请求的文件路径，基于 [root](http://nginx.org/en/docs/http/ngx_http_core_module.html#root) 或者 [alias](http://nginx.org/en/docs/http/ngx_http_core_module.html#alias) 指令，以及 URI。\n\n$request_length\n\n请求长度（包含请求行、头和请求体）(1.3.12, 1.2.7)\n\n$request_method\n\n请求方法，通常是 “GET” 或者 “POST”\n\n$request_time\n\n请求处理时间秒数，精度到毫秒(1.3.9, 1.2.6)；从客户端第一个字节被读取开始。\n\n$request_uri\n\n完整的请求 URI （带参数）\n\n$scheme\n\n请求方案，“http” 或 “https”\n\n$sent_http\\_name\n\n自定义影响头字段；变量名的最后部分是破折号替换为下划线并且转换为小写字符的字段名字。\n\n$server_addr\n\n接受请求的服务器地址  \n计算这个变量的值通常需要一个系统调用。为了避免系统调用，[listen](http://nginx.org/en/docs/http/ngx_http_core_module.html#listen)指令必须指定地址，并且使用绑定参数。\n\n$server_name\n\n接受请求的服务器名称\n\n$server_port\n\n接受请求的服务器端口\n\n$server_protocol\n\n请求协议，通常是 “HTTP/1.0” 、 “HTTP/1.1” 或者 “[HTTP/2.0](http://nginx.org/en/docs/http/ngx_http_v2_module.html)”\n\n$status\n\n响应状态(1.3.2, 1.2.2)\n\n$tcpinfo_rtt, $tcpinfo_rttvar, $tcpinfo_snd_cwnd, $tcpinfo_rcv_space\n\n关于客户端 TCP 连接的信息；在支持 TCP_INFO 套接字的系统上可用。\n\n$time_iso8601\n\nISO 8601 标准格式的本地时间(1.3.12, 1.2.7)\n\n$time_local\n\nCommon Log 格式的本地时间(1.3.12, 1.2.7)\n\n$uri\n\n当前请求[标准化](http://nginx.org/en/docs/http/ngx_http_core_module.html#location)的 URI  \n$uri 的值在请求处理过程中可能改变，例如，当内部转发，或者使用索引文件时。\n","source":"_posts/nginx-ngx-http-core-module内置变量.md","raw":"title: nginx ngx_http_core_module 内置变量\ntags:\n  - Nginx\ncategories:\n  - 开发工具\n  - Nginx\ndate: 2016-01-01 16:14:31\n---\n\nngx_http_core_module 模块支持与 Apache 服务器相同名称的内置变量。首先，这些都是表示客户端请求头字段的变量，例如 $http_user_agent 、$http_cookie 等等。还有其他变量：\n\n<!-- more -->\n\n$arg_name\n\n请求行中的参数名称\n\n$args\n\n请求行中的参数\n\n$binary_remote_addr\n\n二进制形式的客户端地址，值的长度总是 4 个字节\n\n$body_bytes_sent\n\n发送给客户端的字节数，不算响应头；这个变量兼容 mod_log_config Apache 模块的“%B”参数。\n\n$bytes_sent\n\n发送给客户端的字节数(1.3.8, 1.2.5)\n\n$connection\n\n连接序列数(1.3.8, 1.2.5)\n\n$connection_requests\n\n一个连接产生的当前请求数 (1.3.8, 1.2.5)\n\n$content_length\n\n请求头变量“Content-Length”\n\n$content_type\n\n请求头变量“Content-Type”\n\n$cookie_name\n\ncookie 名称\n\n$document_root\n\n当前请求的 root 或 alias 指令的值\n\n$document_uri\n\n同 $uri\n\n$host\n\n按照这个优先级顺序：请求行的主机名，请求头参数“Host” 中的主机名，匹配请求的服务器名称\n\n$hostname\n\n主机名称\n\n$http_*name*\n\n任意请求头参数；变量名的最后部分是转成小写的参数名，并且用下划线替换破折号。\n\n$https\n\n“on”如果连接运行在 SSL 模式，否则是一个空字符串。\n\n$is_args\n\n“?”如果请求行有参数，否则是一个空字符串。\n\n$limit_rate\n\n设置这个变量启用响应频率限制；参见 [limit_rate](http://nginx.org/en/docs/http/ngx_http_core_module.html#limit_rate)\n\n$msec\n\n以毫秒为单位的当前时间的秒数 (1.3.9, 1.2.6)\n\n$nginx_version\n\nnginx 版本\n\n$pid\n\n工作进程的 PID\n\n$pipe\n\n如果请求来自管道通信，值为“p”，否则为“.” (1.3.12, 1.2.7)\n\n$proxy_protocol_addr\n\n获取代理代理协议头的客户端地址，如果是直接访问，该值为空字符串。(1.5.12)  \n代理协议必须预先在 [listen](http://nginx.org/en/docs/http/ngx_http_core_module.html#listen) 指令中通过设置 proxy_protocol 参数启用。\n\n$query_string\n\n同 $args\n\n$realpath_root\n\n当前请求的文档根目录或别名的真实路径，会将所有符号连接转换为真实路径。\n\n$remote_addr\n\n客户端地址\n\n$remote_port\n\n客户端端口\n\n$remote_user\n\n用于HTTP基础认证服务的用户名\n\n$request\n\n完整的原始请求行\n\n$request_body\n\n请求体  \n此变量可在 location 中使用，将请求主体通过 proxy_pass, fastcgi_pass, uwsgi_pass, 和 scgi_pass 传递给下一级的代理服务器。\n\n$request_body_file\n\n将客户端请求主体保存在临时文件中。处理过程接收后，需要删除这个文件。如果一直将请求体写入一个文件， [client_body_in_file_only](http://nginx.org/en/docs/http/ngx_http_core_module.html#client_body_in_file_only) 需要被启用。如果一个临时文件的名称是在代理请求中传递的，或者在请求中传递给一个 FastCGI/uwsgi/SCGI 服务器， 传递请求体需要分别禁用 [proxy_pass_request_body off](http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_pass_request_body) 、 [fastcgi_pass_request_body off](http://nginx.org/en/docs/http/ngx_http_fastcgi_module.html#fastcgi_pass_request_body) 、 [uwsgi_pass_request_body off](http://nginx.org/en/docs/http/ngx_http_uwsgi_module.html#uwsgi_pass_request_body) 或者 [scgi_pass_request_body off](http://nginx.org/en/docs/http/ngx_http_scgi_module.html#scgi_pass_request_body) 指令。\n\n$request_completion\n\n如果请求完成，则值为“OK”，否则为空字符串。\n\n$request_filename\n\n当前请求的文件路径，基于 [root](http://nginx.org/en/docs/http/ngx_http_core_module.html#root) 或者 [alias](http://nginx.org/en/docs/http/ngx_http_core_module.html#alias) 指令，以及 URI。\n\n$request_length\n\n请求长度（包含请求行、头和请求体）(1.3.12, 1.2.7)\n\n$request_method\n\n请求方法，通常是 “GET” 或者 “POST”\n\n$request_time\n\n请求处理时间秒数，精度到毫秒(1.3.9, 1.2.6)；从客户端第一个字节被读取开始。\n\n$request_uri\n\n完整的请求 URI （带参数）\n\n$scheme\n\n请求方案，“http” 或 “https”\n\n$sent_http\\_name\n\n自定义影响头字段；变量名的最后部分是破折号替换为下划线并且转换为小写字符的字段名字。\n\n$server_addr\n\n接受请求的服务器地址  \n计算这个变量的值通常需要一个系统调用。为了避免系统调用，[listen](http://nginx.org/en/docs/http/ngx_http_core_module.html#listen)指令必须指定地址，并且使用绑定参数。\n\n$server_name\n\n接受请求的服务器名称\n\n$server_port\n\n接受请求的服务器端口\n\n$server_protocol\n\n请求协议，通常是 “HTTP/1.0” 、 “HTTP/1.1” 或者 “[HTTP/2.0](http://nginx.org/en/docs/http/ngx_http_v2_module.html)”\n\n$status\n\n响应状态(1.3.2, 1.2.2)\n\n$tcpinfo_rtt, $tcpinfo_rttvar, $tcpinfo_snd_cwnd, $tcpinfo_rcv_space\n\n关于客户端 TCP 连接的信息；在支持 TCP_INFO 套接字的系统上可用。\n\n$time_iso8601\n\nISO 8601 标准格式的本地时间(1.3.12, 1.2.7)\n\n$time_local\n\nCommon Log 格式的本地时间(1.3.12, 1.2.7)\n\n$uri\n\n当前请求[标准化](http://nginx.org/en/docs/http/ngx_http_core_module.html#location)的 URI  \n$uri 的值在请求处理过程中可能改变，例如，当内部转发，或者使用索引文件时。\n","slug":"nginx-ngx-http-core-module内置变量","published":1,"updated":"2021-07-19T16:28:00.304Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphr800e9itd3exev62k6","content":"<p>ngx_http_core_module 模块支持与 Apache 服务器相同名称的内置变量。首先，这些都是表示客户端请求头字段的变量，例如 $http_user_agent 、$http_cookie 等等。还有其他变量：</p>\n<span id=\"more\"></span>\n\n<p>$arg_name</p>\n<p>请求行中的参数名称</p>\n<p>$args</p>\n<p>请求行中的参数</p>\n<p>$binary_remote_addr</p>\n<p>二进制形式的客户端地址，值的长度总是 4 个字节</p>\n<p>$body_bytes_sent</p>\n<p>发送给客户端的字节数，不算响应头；这个变量兼容 mod_log_config Apache 模块的“%B”参数。</p>\n<p>$bytes_sent</p>\n<p>发送给客户端的字节数(1.3.8, 1.2.5)</p>\n<p>$connection</p>\n<p>连接序列数(1.3.8, 1.2.5)</p>\n<p>$connection_requests</p>\n<p>一个连接产生的当前请求数 (1.3.8, 1.2.5)</p>\n<p>$content_length</p>\n<p>请求头变量“Content-Length”</p>\n<p>$content_type</p>\n<p>请求头变量“Content-Type”</p>\n<p>$cookie_name</p>\n<p>cookie 名称</p>\n<p>$document_root</p>\n<p>当前请求的 root 或 alias 指令的值</p>\n<p>$document_uri</p>\n<p>同 $uri</p>\n<p>$host</p>\n<p>按照这个优先级顺序：请求行的主机名，请求头参数“Host” 中的主机名，匹配请求的服务器名称</p>\n<p>$hostname</p>\n<p>主机名称</p>\n<p>$http_<em>name</em></p>\n<p>任意请求头参数；变量名的最后部分是转成小写的参数名，并且用下划线替换破折号。</p>\n<p>$https</p>\n<p>“on”如果连接运行在 SSL 模式，否则是一个空字符串。</p>\n<p>$is_args</p>\n<p>“?”如果请求行有参数，否则是一个空字符串。</p>\n<p>$limit_rate</p>\n<p>设置这个变量启用响应频率限制；参见 <a href=\"http://nginx.org/en/docs/http/ngx_http_core_module.html#limit_rate\">limit_rate</a></p>\n<p>$msec</p>\n<p>以毫秒为单位的当前时间的秒数 (1.3.9, 1.2.6)</p>\n<p>$nginx_version</p>\n<p>nginx 版本</p>\n<p>$pid</p>\n<p>工作进程的 PID</p>\n<p>$pipe</p>\n<p>如果请求来自管道通信，值为“p”，否则为“.” (1.3.12, 1.2.7)</p>\n<p>$proxy_protocol_addr</p>\n<p>获取代理代理协议头的客户端地址，如果是直接访问，该值为空字符串。(1.5.12)<br>代理协议必须预先在 <a href=\"http://nginx.org/en/docs/http/ngx_http_core_module.html#listen\">listen</a> 指令中通过设置 proxy_protocol 参数启用。</p>\n<p>$query_string</p>\n<p>同 $args</p>\n<p>$realpath_root</p>\n<p>当前请求的文档根目录或别名的真实路径，会将所有符号连接转换为真实路径。</p>\n<p>$remote_addr</p>\n<p>客户端地址</p>\n<p>$remote_port</p>\n<p>客户端端口</p>\n<p>$remote_user</p>\n<p>用于HTTP基础认证服务的用户名</p>\n<p>$request</p>\n<p>完整的原始请求行</p>\n<p>$request_body</p>\n<p>请求体<br>此变量可在 location 中使用，将请求主体通过 proxy_pass, fastcgi_pass, uwsgi_pass, 和 scgi_pass 传递给下一级的代理服务器。</p>\n<p>$request_body_file</p>\n<p>将客户端请求主体保存在临时文件中。处理过程接收后，需要删除这个文件。如果一直将请求体写入一个文件， <a href=\"http://nginx.org/en/docs/http/ngx_http_core_module.html#client_body_in_file_only\">client_body_in_file_only</a> 需要被启用。如果一个临时文件的名称是在代理请求中传递的，或者在请求中传递给一个 FastCGI/uwsgi/SCGI 服务器， 传递请求体需要分别禁用 <a href=\"http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_pass_request_body\">proxy_pass_request_body off</a> 、 <a href=\"http://nginx.org/en/docs/http/ngx_http_fastcgi_module.html#fastcgi_pass_request_body\">fastcgi_pass_request_body off</a> 、 <a href=\"http://nginx.org/en/docs/http/ngx_http_uwsgi_module.html#uwsgi_pass_request_body\">uwsgi_pass_request_body off</a> 或者 <a href=\"http://nginx.org/en/docs/http/ngx_http_scgi_module.html#scgi_pass_request_body\">scgi_pass_request_body off</a> 指令。</p>\n<p>$request_completion</p>\n<p>如果请求完成，则值为“OK”，否则为空字符串。</p>\n<p>$request_filename</p>\n<p>当前请求的文件路径，基于 <a href=\"http://nginx.org/en/docs/http/ngx_http_core_module.html#root\">root</a> 或者 <a href=\"http://nginx.org/en/docs/http/ngx_http_core_module.html#alias\">alias</a> 指令，以及 URI。</p>\n<p>$request_length</p>\n<p>请求长度（包含请求行、头和请求体）(1.3.12, 1.2.7)</p>\n<p>$request_method</p>\n<p>请求方法，通常是 “GET” 或者 “POST”</p>\n<p>$request_time</p>\n<p>请求处理时间秒数，精度到毫秒(1.3.9, 1.2.6)；从客户端第一个字节被读取开始。</p>\n<p>$request_uri</p>\n<p>完整的请求 URI （带参数）</p>\n<p>$scheme</p>\n<p>请求方案，“http” 或 “https”</p>\n<p>$sent_http_name</p>\n<p>自定义影响头字段；变量名的最后部分是破折号替换为下划线并且转换为小写字符的字段名字。</p>\n<p>$server_addr</p>\n<p>接受请求的服务器地址<br>计算这个变量的值通常需要一个系统调用。为了避免系统调用，<a href=\"http://nginx.org/en/docs/http/ngx_http_core_module.html#listen\">listen</a>指令必须指定地址，并且使用绑定参数。</p>\n<p>$server_name</p>\n<p>接受请求的服务器名称</p>\n<p>$server_port</p>\n<p>接受请求的服务器端口</p>\n<p>$server_protocol</p>\n<p>请求协议，通常是 “HTTP/1.0” 、 “HTTP/1.1” 或者 “<a href=\"http://nginx.org/en/docs/http/ngx_http_v2_module.html\">HTTP/2.0</a>”</p>\n<p>$status</p>\n<p>响应状态(1.3.2, 1.2.2)</p>\n<p>$tcpinfo_rtt, $tcpinfo_rttvar, $tcpinfo_snd_cwnd, $tcpinfo_rcv_space</p>\n<p>关于客户端 TCP 连接的信息；在支持 TCP_INFO 套接字的系统上可用。</p>\n<p>$time_iso8601</p>\n<p>ISO 8601 标准格式的本地时间(1.3.12, 1.2.7)</p>\n<p>$time_local</p>\n<p>Common Log 格式的本地时间(1.3.12, 1.2.7)</p>\n<p>$uri</p>\n<p>当前请求<a href=\"http://nginx.org/en/docs/http/ngx_http_core_module.html#location\">标准化</a>的 URI<br>$uri 的值在请求处理过程中可能改变，例如，当内部转发，或者使用索引文件时。</p>\n","site":{"data":{}},"excerpt":"<p>ngx_http_core_module 模块支持与 Apache 服务器相同名称的内置变量。首先，这些都是表示客户端请求头字段的变量，例如 $http_user_agent 、$http_cookie 等等。还有其他变量：</p>","more":"<p>$arg_name</p>\n<p>请求行中的参数名称</p>\n<p>$args</p>\n<p>请求行中的参数</p>\n<p>$binary_remote_addr</p>\n<p>二进制形式的客户端地址，值的长度总是 4 个字节</p>\n<p>$body_bytes_sent</p>\n<p>发送给客户端的字节数，不算响应头；这个变量兼容 mod_log_config Apache 模块的“%B”参数。</p>\n<p>$bytes_sent</p>\n<p>发送给客户端的字节数(1.3.8, 1.2.5)</p>\n<p>$connection</p>\n<p>连接序列数(1.3.8, 1.2.5)</p>\n<p>$connection_requests</p>\n<p>一个连接产生的当前请求数 (1.3.8, 1.2.5)</p>\n<p>$content_length</p>\n<p>请求头变量“Content-Length”</p>\n<p>$content_type</p>\n<p>请求头变量“Content-Type”</p>\n<p>$cookie_name</p>\n<p>cookie 名称</p>\n<p>$document_root</p>\n<p>当前请求的 root 或 alias 指令的值</p>\n<p>$document_uri</p>\n<p>同 $uri</p>\n<p>$host</p>\n<p>按照这个优先级顺序：请求行的主机名，请求头参数“Host” 中的主机名，匹配请求的服务器名称</p>\n<p>$hostname</p>\n<p>主机名称</p>\n<p>$http_<em>name</em></p>\n<p>任意请求头参数；变量名的最后部分是转成小写的参数名，并且用下划线替换破折号。</p>\n<p>$https</p>\n<p>“on”如果连接运行在 SSL 模式，否则是一个空字符串。</p>\n<p>$is_args</p>\n<p>“?”如果请求行有参数，否则是一个空字符串。</p>\n<p>$limit_rate</p>\n<p>设置这个变量启用响应频率限制；参见 <a href=\"http://nginx.org/en/docs/http/ngx_http_core_module.html#limit_rate\">limit_rate</a></p>\n<p>$msec</p>\n<p>以毫秒为单位的当前时间的秒数 (1.3.9, 1.2.6)</p>\n<p>$nginx_version</p>\n<p>nginx 版本</p>\n<p>$pid</p>\n<p>工作进程的 PID</p>\n<p>$pipe</p>\n<p>如果请求来自管道通信，值为“p”，否则为“.” (1.3.12, 1.2.7)</p>\n<p>$proxy_protocol_addr</p>\n<p>获取代理代理协议头的客户端地址，如果是直接访问，该值为空字符串。(1.5.12)<br>代理协议必须预先在 <a href=\"http://nginx.org/en/docs/http/ngx_http_core_module.html#listen\">listen</a> 指令中通过设置 proxy_protocol 参数启用。</p>\n<p>$query_string</p>\n<p>同 $args</p>\n<p>$realpath_root</p>\n<p>当前请求的文档根目录或别名的真实路径，会将所有符号连接转换为真实路径。</p>\n<p>$remote_addr</p>\n<p>客户端地址</p>\n<p>$remote_port</p>\n<p>客户端端口</p>\n<p>$remote_user</p>\n<p>用于HTTP基础认证服务的用户名</p>\n<p>$request</p>\n<p>完整的原始请求行</p>\n<p>$request_body</p>\n<p>请求体<br>此变量可在 location 中使用，将请求主体通过 proxy_pass, fastcgi_pass, uwsgi_pass, 和 scgi_pass 传递给下一级的代理服务器。</p>\n<p>$request_body_file</p>\n<p>将客户端请求主体保存在临时文件中。处理过程接收后，需要删除这个文件。如果一直将请求体写入一个文件， <a href=\"http://nginx.org/en/docs/http/ngx_http_core_module.html#client_body_in_file_only\">client_body_in_file_only</a> 需要被启用。如果一个临时文件的名称是在代理请求中传递的，或者在请求中传递给一个 FastCGI/uwsgi/SCGI 服务器， 传递请求体需要分别禁用 <a href=\"http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_pass_request_body\">proxy_pass_request_body off</a> 、 <a href=\"http://nginx.org/en/docs/http/ngx_http_fastcgi_module.html#fastcgi_pass_request_body\">fastcgi_pass_request_body off</a> 、 <a href=\"http://nginx.org/en/docs/http/ngx_http_uwsgi_module.html#uwsgi_pass_request_body\">uwsgi_pass_request_body off</a> 或者 <a href=\"http://nginx.org/en/docs/http/ngx_http_scgi_module.html#scgi_pass_request_body\">scgi_pass_request_body off</a> 指令。</p>\n<p>$request_completion</p>\n<p>如果请求完成，则值为“OK”，否则为空字符串。</p>\n<p>$request_filename</p>\n<p>当前请求的文件路径，基于 <a href=\"http://nginx.org/en/docs/http/ngx_http_core_module.html#root\">root</a> 或者 <a href=\"http://nginx.org/en/docs/http/ngx_http_core_module.html#alias\">alias</a> 指令，以及 URI。</p>\n<p>$request_length</p>\n<p>请求长度（包含请求行、头和请求体）(1.3.12, 1.2.7)</p>\n<p>$request_method</p>\n<p>请求方法，通常是 “GET” 或者 “POST”</p>\n<p>$request_time</p>\n<p>请求处理时间秒数，精度到毫秒(1.3.9, 1.2.6)；从客户端第一个字节被读取开始。</p>\n<p>$request_uri</p>\n<p>完整的请求 URI （带参数）</p>\n<p>$scheme</p>\n<p>请求方案，“http” 或 “https”</p>\n<p>$sent_http_name</p>\n<p>自定义影响头字段；变量名的最后部分是破折号替换为下划线并且转换为小写字符的字段名字。</p>\n<p>$server_addr</p>\n<p>接受请求的服务器地址<br>计算这个变量的值通常需要一个系统调用。为了避免系统调用，<a href=\"http://nginx.org/en/docs/http/ngx_http_core_module.html#listen\">listen</a>指令必须指定地址，并且使用绑定参数。</p>\n<p>$server_name</p>\n<p>接受请求的服务器名称</p>\n<p>$server_port</p>\n<p>接受请求的服务器端口</p>\n<p>$server_protocol</p>\n<p>请求协议，通常是 “HTTP/1.0” 、 “HTTP/1.1” 或者 “<a href=\"http://nginx.org/en/docs/http/ngx_http_v2_module.html\">HTTP/2.0</a>”</p>\n<p>$status</p>\n<p>响应状态(1.3.2, 1.2.2)</p>\n<p>$tcpinfo_rtt, $tcpinfo_rttvar, $tcpinfo_snd_cwnd, $tcpinfo_rcv_space</p>\n<p>关于客户端 TCP 连接的信息；在支持 TCP_INFO 套接字的系统上可用。</p>\n<p>$time_iso8601</p>\n<p>ISO 8601 标准格式的本地时间(1.3.12, 1.2.7)</p>\n<p>$time_local</p>\n<p>Common Log 格式的本地时间(1.3.12, 1.2.7)</p>\n<p>$uri</p>\n<p>当前请求<a href=\"http://nginx.org/en/docs/http/ngx_http_core_module.html#location\">标准化</a>的 URI<br>$uri 的值在请求处理过程中可能改变，例如，当内部转发，或者使用索引文件时。</p>"},{"title":"no matching cipher found 问题一次解决经历","date":"2019-12-10T07:27:10.000Z","_content":"\n本次问题解决纯属蒙对了，原理不清楚。\n\n当我从一台 CentOS 7.3 的服务器通过 ssh 登录另外一台 CentOS 6.8 的服务器时出现以下错误信息：\n\n    [root@192-168-72-75 .ssh]# ssh -p65522 bddev@192.168.72.208\n    no matching cipher found: client arcfour server chacha20-poly1305@openssh.com,aes128-ctr,aes192-ctr,aes256-ctr,aes128-gcm@openssh.com,aes256-gcm@openssh.com\n\n解决方法是在“192-168-72-75”服务器上修改 /etc/ssh/ssh_config 配置文件，添加以下内容：\n\n    Host *\n    \t GSSAPIAuthentication yes\n         Ciphers aes128-ctr,aes192-ctr,aes256-ctr,arcfour256,arcfour128,aes128-cbc,3des-cbc\n         MACs hmac-md5,hmac-sha1,umac-64@openssh.com,hmac-ripemd160\n\n以上内容在原配置文件中本来就有，可能只是注释掉了，修改的时候仔细核对下。","source":"_posts/no-matching-cipher-found-问题一次解决经历.md","raw":"title: no matching cipher found 问题一次解决经历\ndate: 2019-12-10 15:27:10\ntags:\n- CentOS\n- Linux\ncategories:\n- 操作系统\n- Linux\n---\n\n本次问题解决纯属蒙对了，原理不清楚。\n\n当我从一台 CentOS 7.3 的服务器通过 ssh 登录另外一台 CentOS 6.8 的服务器时出现以下错误信息：\n\n    [root@192-168-72-75 .ssh]# ssh -p65522 bddev@192.168.72.208\n    no matching cipher found: client arcfour server chacha20-poly1305@openssh.com,aes128-ctr,aes192-ctr,aes256-ctr,aes128-gcm@openssh.com,aes256-gcm@openssh.com\n\n解决方法是在“192-168-72-75”服务器上修改 /etc/ssh/ssh_config 配置文件，添加以下内容：\n\n    Host *\n    \t GSSAPIAuthentication yes\n         Ciphers aes128-ctr,aes192-ctr,aes256-ctr,arcfour256,arcfour128,aes128-cbc,3des-cbc\n         MACs hmac-md5,hmac-sha1,umac-64@openssh.com,hmac-ripemd160\n\n以上内容在原配置文件中本来就有，可能只是注释掉了，修改的时候仔细核对下。","slug":"no-matching-cipher-found-问题一次解决经历","published":1,"updated":"2021-07-19T16:28:00.332Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphr900eeitd33cx3ev68","content":"<p>本次问题解决纯属蒙对了，原理不清楚。</p>\n<p>当我从一台 CentOS 7.3 的服务器通过 ssh 登录另外一台 CentOS 6.8 的服务器时出现以下错误信息：</p>\n<pre><code>[root@192-168-72-75 .ssh]# ssh -p65522 bddev@192.168.72.208\nno matching cipher found: client arcfour server chacha20-poly1305@openssh.com,aes128-ctr,aes192-ctr,aes256-ctr,aes128-gcm@openssh.com,aes256-gcm@openssh.com\n</code></pre>\n<p>解决方法是在“192-168-72-75”服务器上修改 /etc/ssh/ssh_config 配置文件，添加以下内容：</p>\n<pre><code>Host *\n     GSSAPIAuthentication yes\n     Ciphers aes128-ctr,aes192-ctr,aes256-ctr,arcfour256,arcfour128,aes128-cbc,3des-cbc\n     MACs hmac-md5,hmac-sha1,umac-64@openssh.com,hmac-ripemd160\n</code></pre>\n<p>以上内容在原配置文件中本来就有，可能只是注释掉了，修改的时候仔细核对下。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>本次问题解决纯属蒙对了，原理不清楚。</p>\n<p>当我从一台 CentOS 7.3 的服务器通过 ssh 登录另外一台 CentOS 6.8 的服务器时出现以下错误信息：</p>\n<pre><code>[root@192-168-72-75 .ssh]# ssh -p65522 bddev@192.168.72.208\nno matching cipher found: client arcfour server chacha20-poly1305@openssh.com,aes128-ctr,aes192-ctr,aes256-ctr,aes128-gcm@openssh.com,aes256-gcm@openssh.com\n</code></pre>\n<p>解决方法是在“192-168-72-75”服务器上修改 /etc/ssh/ssh_config 配置文件，添加以下内容：</p>\n<pre><code>Host *\n     GSSAPIAuthentication yes\n     Ciphers aes128-ctr,aes192-ctr,aes256-ctr,arcfour256,arcfour128,aes128-cbc,3des-cbc\n     MACs hmac-md5,hmac-sha1,umac-64@openssh.com,hmac-ripemd160\n</code></pre>\n<p>以上内容在原配置文件中本来就有，可能只是注释掉了，修改的时候仔细核对下。</p>\n"},{"title":"npm 代理","date":"2020-09-17T03:25:50.000Z","_content":"\n加快 npm 安装的代理设置：\n\n使用以下命令查看当前的库：\n\n    npm config get registry\n    \n默认为：https://registry.npmjs.org/。因为是国外的站点，所以会很慢。\n\n用set命令换成阿里的镜像：\n\n    npm config set registry https://registry.npm.taobao.org\n","source":"_posts/npm-代理.md","raw":"title: npm 代理\ndate: 2020-09-17 11:25:50\ntags:\n- Node.js\ncategories:\n- 开发\n- Node.js\n---\n\n加快 npm 安装的代理设置：\n\n使用以下命令查看当前的库：\n\n    npm config get registry\n    \n默认为：https://registry.npmjs.org/。因为是国外的站点，所以会很慢。\n\n用set命令换成阿里的镜像：\n\n    npm config set registry https://registry.npm.taobao.org\n","slug":"npm-代理","published":1,"updated":"2021-07-19T16:27:59.900Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphrb00ehitd34lco69wu","content":"<p>加快 npm 安装的代理设置：</p>\n<p>使用以下命令查看当前的库：</p>\n<pre><code>npm config get registry\n</code></pre>\n<p>默认为：<a href=\"https://registry.npmjs.org/%E3%80%82%E5%9B%A0%E4%B8%BA%E6%98%AF%E5%9B%BD%E5%A4%96%E7%9A%84%E7%AB%99%E7%82%B9%EF%BC%8C%E6%89%80%E4%BB%A5%E4%BC%9A%E5%BE%88%E6%85%A2%E3%80%82\">https://registry.npmjs.org/。因为是国外的站点，所以会很慢。</a></p>\n<p>用set命令换成阿里的镜像：</p>\n<pre><code>npm config set registry https://registry.npm.taobao.org\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>加快 npm 安装的代理设置：</p>\n<p>使用以下命令查看当前的库：</p>\n<pre><code>npm config get registry\n</code></pre>\n<p>默认为：<a href=\"https://registry.npmjs.org/%E3%80%82%E5%9B%A0%E4%B8%BA%E6%98%AF%E5%9B%BD%E5%A4%96%E7%9A%84%E7%AB%99%E7%82%B9%EF%BC%8C%E6%89%80%E4%BB%A5%E4%BC%9A%E5%BE%88%E6%85%A2%E3%80%82\">https://registry.npmjs.org/。因为是国外的站点，所以会很慢。</a></p>\n<p>用set命令换成阿里的镜像：</p>\n<pre><code>npm config set registry https://registry.npm.taobao.org\n</code></pre>\n"},{"title":"org.apache.kafka.common.errors.TimeoutException","date":"2017-08-24T04:24:19.000Z","_content":"\n\n使用 kafka-console-producer.sh 向远端 Kafka 写入数据时遇到以下错误：\n\n    $ bin/kafka-console-producer.sh --broker-list 172.16.72.202:9092 --topic test\n    This is a message\n    [2017-08-24 11:47:48,286] ERROR Error when sending message to topic test with key: null, value: 17 bytes with error: (org.apache.kafka.clients.producer.internals.ErrorLoggingCallback)\n    org.apache.kafka.common.errors.TimeoutException: Expiring 1 record(s) for test-0: 1523 ms has passed since batch creation plus linger time\n    八月 24, 2017 11:50:55 上午 sun.rmi.transport.tcp.TCPTransport$AcceptLoop executeAcceptLoop\n    警告: RMI TCP Accept-0: accept loop for ServerSocket[addr=0.0.0.0/0.0.0.0,localport=38175] throws\n    java.io.IOException: The server sockets created using the LocalRMIServerSocketFactory only accept connections from clients running on the host where the RMI remote objects have been exported.\n    \tat sun.management.jmxremote.LocalRMIServerSocketFactory$1.accept(LocalRMIServerSocketFactory.java:114)\n    \tat sun.rmi.transport.tcp.TCPTransport$AcceptLoop.executeAcceptLoop(TCPTransport.java:400)\n    \tat sun.rmi.transport.tcp.TCPTransport$AcceptLoop.run(TCPTransport.java:372)\n     \tat java.lang.Thread.run(Thread.java:748)\n        \n在没有配置 advertised.host.name 的情况下，Kafka 并没有广播我们配置的 host.name，而是广播了主机配置的 hostname。远端的客户端并没有配置 hosts，所以自然是连接不上这个 hostname 的，所以在远端客户端配置 hosts。在客户端 /etc/hosts 中添加以下内容后问题解决：\n\n    172.16.72.202 172-16-72-202\n","source":"_posts/org-apache-kafka-common-errors-TimeoutException.md","raw":"title: org.apache.kafka.common.errors.TimeoutException\ntags:\n  - 大数据\n  - Kafka\ncategories:\n  - 大数据\n  - Kafka\ndate: 2017-08-24 12:24:19\n---\n\n\n使用 kafka-console-producer.sh 向远端 Kafka 写入数据时遇到以下错误：\n\n    $ bin/kafka-console-producer.sh --broker-list 172.16.72.202:9092 --topic test\n    This is a message\n    [2017-08-24 11:47:48,286] ERROR Error when sending message to topic test with key: null, value: 17 bytes with error: (org.apache.kafka.clients.producer.internals.ErrorLoggingCallback)\n    org.apache.kafka.common.errors.TimeoutException: Expiring 1 record(s) for test-0: 1523 ms has passed since batch creation plus linger time\n    八月 24, 2017 11:50:55 上午 sun.rmi.transport.tcp.TCPTransport$AcceptLoop executeAcceptLoop\n    警告: RMI TCP Accept-0: accept loop for ServerSocket[addr=0.0.0.0/0.0.0.0,localport=38175] throws\n    java.io.IOException: The server sockets created using the LocalRMIServerSocketFactory only accept connections from clients running on the host where the RMI remote objects have been exported.\n    \tat sun.management.jmxremote.LocalRMIServerSocketFactory$1.accept(LocalRMIServerSocketFactory.java:114)\n    \tat sun.rmi.transport.tcp.TCPTransport$AcceptLoop.executeAcceptLoop(TCPTransport.java:400)\n    \tat sun.rmi.transport.tcp.TCPTransport$AcceptLoop.run(TCPTransport.java:372)\n     \tat java.lang.Thread.run(Thread.java:748)\n        \n在没有配置 advertised.host.name 的情况下，Kafka 并没有广播我们配置的 host.name，而是广播了主机配置的 hostname。远端的客户端并没有配置 hosts，所以自然是连接不上这个 hostname 的，所以在远端客户端配置 hosts。在客户端 /etc/hosts 中添加以下内容后问题解决：\n\n    172.16.72.202 172-16-72-202\n","slug":"org-apache-kafka-common-errors-TimeoutException","published":1,"updated":"2021-07-19T16:28:00.304Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphrd00emitd3dh89h6a1","content":"<p>使用 kafka-console-producer.sh 向远端 Kafka 写入数据时遇到以下错误：</p>\n<pre><code>$ bin/kafka-console-producer.sh --broker-list 172.16.72.202:9092 --topic test\nThis is a message\n[2017-08-24 11:47:48,286] ERROR Error when sending message to topic test with key: null, value: 17 bytes with error: (org.apache.kafka.clients.producer.internals.ErrorLoggingCallback)\norg.apache.kafka.common.errors.TimeoutException: Expiring 1 record(s) for test-0: 1523 ms has passed since batch creation plus linger time\n八月 24, 2017 11:50:55 上午 sun.rmi.transport.tcp.TCPTransport$AcceptLoop executeAcceptLoop\n警告: RMI TCP Accept-0: accept loop for ServerSocket[addr=0.0.0.0/0.0.0.0,localport=38175] throws\njava.io.IOException: The server sockets created using the LocalRMIServerSocketFactory only accept connections from clients running on the host where the RMI remote objects have been exported.\n    at sun.management.jmxremote.LocalRMIServerSocketFactory$1.accept(LocalRMIServerSocketFactory.java:114)\n    at sun.rmi.transport.tcp.TCPTransport$AcceptLoop.executeAcceptLoop(TCPTransport.java:400)\n    at sun.rmi.transport.tcp.TCPTransport$AcceptLoop.run(TCPTransport.java:372)\n     at java.lang.Thread.run(Thread.java:748)\n    \n</code></pre>\n<p>在没有配置 advertised.host.name 的情况下，Kafka 并没有广播我们配置的 host.name，而是广播了主机配置的 hostname。远端的客户端并没有配置 hosts，所以自然是连接不上这个 hostname 的，所以在远端客户端配置 hosts。在客户端 /etc/hosts 中添加以下内容后问题解决：</p>\n<pre><code>172.16.72.202 172-16-72-202\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>使用 kafka-console-producer.sh 向远端 Kafka 写入数据时遇到以下错误：</p>\n<pre><code>$ bin/kafka-console-producer.sh --broker-list 172.16.72.202:9092 --topic test\nThis is a message\n[2017-08-24 11:47:48,286] ERROR Error when sending message to topic test with key: null, value: 17 bytes with error: (org.apache.kafka.clients.producer.internals.ErrorLoggingCallback)\norg.apache.kafka.common.errors.TimeoutException: Expiring 1 record(s) for test-0: 1523 ms has passed since batch creation plus linger time\n八月 24, 2017 11:50:55 上午 sun.rmi.transport.tcp.TCPTransport$AcceptLoop executeAcceptLoop\n警告: RMI TCP Accept-0: accept loop for ServerSocket[addr=0.0.0.0/0.0.0.0,localport=38175] throws\njava.io.IOException: The server sockets created using the LocalRMIServerSocketFactory only accept connections from clients running on the host where the RMI remote objects have been exported.\n    at sun.management.jmxremote.LocalRMIServerSocketFactory$1.accept(LocalRMIServerSocketFactory.java:114)\n    at sun.rmi.transport.tcp.TCPTransport$AcceptLoop.executeAcceptLoop(TCPTransport.java:400)\n    at sun.rmi.transport.tcp.TCPTransport$AcceptLoop.run(TCPTransport.java:372)\n     at java.lang.Thread.run(Thread.java:748)\n    \n</code></pre>\n<p>在没有配置 advertised.host.name 的情况下，Kafka 并没有广播我们配置的 host.name，而是广播了主机配置的 hostname。远端的客户端并没有配置 hosts，所以自然是连接不上这个 hostname 的，所以在远端客户端配置 hosts。在客户端 /etc/hosts 中添加以下内容后问题解决：</p>\n<pre><code>172.16.72.202 172-16-72-202\n</code></pre>\n"},{"title":"pip 安装指定版本的包","date":"2019-03-19T09:33:40.000Z","_content":"\n使用以下命令安装指定版本的包：\n\n    # pip install pyspark==2.3.3\n","source":"_posts/pip-安装指定版本的包.md","raw":"title: pip 安装指定版本的包\ndate: 2019-03-19 17:33:40\ntags:\n- Python3\ncategories:\n- 语言\n- Python3\n---\n\n使用以下命令安装指定版本的包：\n\n    # pip install pyspark==2.3.3\n","slug":"pip-安装指定版本的包","published":1,"updated":"2021-07-19T16:28:00.304Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphre00epitd3hm155ijx","content":"<p>使用以下命令安装指定版本的包：</p>\n<pre><code># pip install pyspark==2.3.3\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>使用以下命令安装指定版本的包：</p>\n<pre><code># pip install pyspark==2.3.3\n</code></pre>\n"},{"title":"react native windows 系统本地图片不能显示问题","date":"2015-12-20T03:53:57.000Z","_content":"\n\n在 windows 开发环境下，使用 react native 的 Image UI 组件通过相对路径加载本地图片显示不出来，可以通过修改 react native 源代码解决。但是，通过这种方式不确定是否会引起其他平台下新的问题。\n\n将文件 packager/react-packager/src/Bundler/index.js 中的下面一行：\n\n\thttpServerLocation: path.join('/assets', path.dirname(relPath)),\n\n修改为：\n\n\thttpServerLocation: path.join('assets', path.dirname(relPath)),\n","source":"_posts/react-native-windows-系统本地图片不能显示问题.md","raw":"title: react native windows 系统本地图片不能显示问题\ntags:\n  - react native\ncategories:\n  - 语言\n  - react native\ndate: 2015-12-20 11:53:57\n---\n\n\n在 windows 开发环境下，使用 react native 的 Image UI 组件通过相对路径加载本地图片显示不出来，可以通过修改 react native 源代码解决。但是，通过这种方式不确定是否会引起其他平台下新的问题。\n\n将文件 packager/react-packager/src/Bundler/index.js 中的下面一行：\n\n\thttpServerLocation: path.join('/assets', path.dirname(relPath)),\n\n修改为：\n\n\thttpServerLocation: path.join('assets', path.dirname(relPath)),\n","slug":"react-native-windows-系统本地图片不能显示问题","published":1,"updated":"2021-07-19T16:28:00.304Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphrg00euitd3hpnd06p0","content":"<p>在 windows 开发环境下，使用 react native 的 Image UI 组件通过相对路径加载本地图片显示不出来，可以通过修改 react native 源代码解决。但是，通过这种方式不确定是否会引起其他平台下新的问题。</p>\n<p>将文件 packager/react-packager/src/Bundler/index.js 中的下面一行：</p>\n<pre><code>httpServerLocation: path.join(&#39;/assets&#39;, path.dirname(relPath)),\n</code></pre>\n<p>修改为：</p>\n<pre><code>httpServerLocation: path.join(&#39;assets&#39;, path.dirname(relPath)),\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>在 windows 开发环境下，使用 react native 的 Image UI 组件通过相对路径加载本地图片显示不出来，可以通过修改 react native 源代码解决。但是，通过这种方式不确定是否会引起其他平台下新的问题。</p>\n<p>将文件 packager/react-packager/src/Bundler/index.js 中的下面一行：</p>\n<pre><code>httpServerLocation: path.join(&#39;/assets&#39;, path.dirname(relPath)),\n</code></pre>\n<p>修改为：</p>\n<pre><code>httpServerLocation: path.join(&#39;assets&#39;, path.dirname(relPath)),\n</code></pre>\n"},{"title":"sed变量中特殊字符/处理方式","date":"2021-04-26T06:07:31.000Z","_content":"\n如果变量值中包含斜杠（/）特殊字符，在使用sed命令的做行内字符串替换时可以使用井号（#）做为sed语法分隔符，如下：\n\n    ```Shell\n    GITLAB_PROJECT_CLONE_URL=ssh://git@192.168.1.10:50022/test/Documentation.git\n    sed -i \"11s#GITLAB_PROJECT_CLONE_URL#$GITLAB_PROJECT_CLONE_URL#\" config.xml\n    ```","source":"_posts/sed变量中特殊字符-处理方式.md","raw":"title: sed变量中特殊字符/处理方式\ndate: 2021-04-26 14:07:31\ntags:\n- Shell\n- Linux\ncategories:\n- 语言\n- Shell\n---\n\n如果变量值中包含斜杠（/）特殊字符，在使用sed命令的做行内字符串替换时可以使用井号（#）做为sed语法分隔符，如下：\n\n    ```Shell\n    GITLAB_PROJECT_CLONE_URL=ssh://git@192.168.1.10:50022/test/Documentation.git\n    sed -i \"11s#GITLAB_PROJECT_CLONE_URL#$GITLAB_PROJECT_CLONE_URL#\" config.xml\n    ```","slug":"sed变量中特殊字符-处理方式","published":1,"updated":"2021-07-19T16:28:00.156Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphrh00exitd3grfmd0y8","content":"<p>如果变量值中包含斜杠（/）特殊字符，在使用sed命令的做行内字符串替换时可以使用井号（#）做为sed语法分隔符，如下：</p>\n<pre><code><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GITLAB_PROJECT_CLONE_URL=ssh://git@192.168.1.10:50022/test/Documentation.git</span><br><span class=\"line\">sed -i &quot;11s#GITLAB_PROJECT_CLONE_URL#$GITLAB_PROJECT_CLONE_URL#&quot; config.xml</span><br></pre></td></tr></table></figure>\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>如果变量值中包含斜杠（/）特殊字符，在使用sed命令的做行内字符串替换时可以使用井号（#）做为sed语法分隔符，如下：</p>\n<pre><code><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GITLAB_PROJECT_CLONE_URL=ssh://git@192.168.1.10:50022/test/Documentation.git</span><br><span class=\"line\">sed -i &quot;11s#GITLAB_PROJECT_CLONE_URL#$GITLAB_PROJECT_CLONE_URL#&quot; config.xml</span><br></pre></td></tr></table></figure>\n</code></pre>\n"},{"title":"shell -c","date":"2020-12-03T02:57:48.000Z","_content":"\nshell -c {string}：表示命令从-c后的字符串读取。在需要使用管道或者重定向需要sudo时很有用，如下：\n\n\t$ sudo find ../*/exportFiles -mtime +15 -name \"*\" | xargs -I {} rm -rf {}\n\trm: cannot remove `i_20201108_CONTENTINFO_xy_001.dat': Permission denied\n\t\n按照以下方式处理即可：\n\n\t$ sudo sh -c 'find ../*/exportFiles -mtime +15 -name \"*\" | xargs -I {} rm -rf {}'","source":"_posts/shell-c.md","raw":"title: shell -c\ndate: 2020-12-03 10:57:48\ntags:\n- Linux\n- Shell\ncategories:\n- 操作系统\n- Linux\n---\n\nshell -c {string}：表示命令从-c后的字符串读取。在需要使用管道或者重定向需要sudo时很有用，如下：\n\n\t$ sudo find ../*/exportFiles -mtime +15 -name \"*\" | xargs -I {} rm -rf {}\n\trm: cannot remove `i_20201108_CONTENTINFO_xy_001.dat': Permission denied\n\t\n按照以下方式处理即可：\n\n\t$ sudo sh -c 'find ../*/exportFiles -mtime +15 -name \"*\" | xargs -I {} rm -rf {}'","slug":"shell-c","published":1,"updated":"2021-07-19T16:28:00.084Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphri00f1itd37eg4bpds","content":"<p>shell -c {string}：表示命令从-c后的字符串读取。在需要使用管道或者重定向需要sudo时很有用，如下：</p>\n<pre><code>$ sudo find ../*/exportFiles -mtime +15 -name &quot;*&quot; | xargs -I &#123;&#125; rm -rf &#123;&#125;\nrm: cannot remove `i_20201108_CONTENTINFO_xy_001.dat&#39;: Permission denied\n</code></pre>\n<p>按照以下方式处理即可：</p>\n<pre><code>$ sudo sh -c &#39;find ../*/exportFiles -mtime +15 -name &quot;*&quot; | xargs -I &#123;&#125; rm -rf &#123;&#125;&#39;\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>shell -c {string}：表示命令从-c后的字符串读取。在需要使用管道或者重定向需要sudo时很有用，如下：</p>\n<pre><code>$ sudo find ../*/exportFiles -mtime +15 -name &quot;*&quot; | xargs -I &#123;&#125; rm -rf &#123;&#125;\nrm: cannot remove `i_20201108_CONTENTINFO_xy_001.dat&#39;: Permission denied\n</code></pre>\n<p>按照以下方式处理即可：</p>\n<pre><code>$ sudo sh -c &#39;find ../*/exportFiles -mtime +15 -name &quot;*&quot; | xargs -I &#123;&#125; rm -rf &#123;&#125;&#39;\n</code></pre>\n"},{"title":"spacemacs gnuplot","date":"2017-02-27T10:14:46.000Z","_content":"\n### 环境\n\n- Ubuntu 16.10\n- Emacs 24\n\n### 安装过程\n\n#### spacemacs 安装\n\n1. 安装 Emacs\n\n    sudo apt-get install emacs\n\n<!-- more -->\n\n2. 安装 spacemacs\n\n（1）如果已经存在 Emacs 配置文件，首先备份：\n\n    cd ~\n    mv .emacs.d .emacs.d.bak\n    mv .emacs .emacs.bak\n\n不要忘记备份并移动 ~/.emacs 文件，否则 Spacemacs 将不能加载，因为这个文件阻止从适当的初始化文件加载。\n\n（2）clone 配置仓库：\n\n    git clone https://github.com/syl20bnr/spacemacs ~/.emacs.d\n\nmaster 分支是稳定不能修改的分支，不要做任何修改，否则会破坏更新机制。使用 develop 分支可以安全的手动处理更新。\n\n（3）【可选】安装 [Source Code Pro](https://github.com/adobe-fonts/source-code-pro) 字体：\n\n即使在终端运行也需要改变终端字体设置。\n\n（4）启动 Emacs。Spacemacs 会自动安装它需要的包。如果看到跟包下载相关的错误，那么可以尝试通过以下命令启动 emacs 禁用 HTTPS 协议：\n\n    emacs --insecure\n\n或者可以设置隐藏文件中的 dotspacemacs-elpa-https 为 nil，这样启动 emacs 的时候就不需要 --insecure 参数了。可以清理 .emacs.d/elpa 目录，这样任何已经下载的可能引起错误的包会重新安装。\n\n（5）重新启动 emacs 完成安装。\n\n#### spacemacs 安装镜像\n\n如果不使用镜像 spacemacs 安装会很慢，甚至卡死。我使用的是 [ELPA 镜像](http://www.4gamers.cn/)。\n\n#### 安装 gnuplot\n\n    apt-get install gnuplot\n\n### gnuplot 测试\n\n编辑 org 文件：\n\n    #+PLOT: title:\"Citas\" ind:1 deps:(3) type:2d with:histograms set:\"yrange [0:]\"\n         | Sede      | Max cites | H-index |\n         |-----------+-----------+---------|\n         | Chile     |    257.72 |   21.39 |\n         | Leeds     |    165.77 |   19.68 |\n         | Sao Paolo |     71.00 |   11.50 |\n         | Stockholm |    134.19 |   14.33 |\n         | Morelia   |    257.56 |   17.67 |\n\n运行命令 M-x org-plot/gnuplot\n\n> 如果命令运行报错：cannot open load file no such file or directory, gnuplot。原因是没有安装 gnuplot.el。运行命令 M-x package-install RET gnuplot RET\n> 参考网址：<https://github.com/bruceravel/gnuplot-mode>","source":"_posts/spacemacs-gnuplot.md","raw":"title: spacemacs gnuplot\ntags:\n  - Emacs\ncategories:\n  - 开发工具\n  - Emacs\ndate: 2017-02-27 18:14:46\n---\n\n### 环境\n\n- Ubuntu 16.10\n- Emacs 24\n\n### 安装过程\n\n#### spacemacs 安装\n\n1. 安装 Emacs\n\n    sudo apt-get install emacs\n\n<!-- more -->\n\n2. 安装 spacemacs\n\n（1）如果已经存在 Emacs 配置文件，首先备份：\n\n    cd ~\n    mv .emacs.d .emacs.d.bak\n    mv .emacs .emacs.bak\n\n不要忘记备份并移动 ~/.emacs 文件，否则 Spacemacs 将不能加载，因为这个文件阻止从适当的初始化文件加载。\n\n（2）clone 配置仓库：\n\n    git clone https://github.com/syl20bnr/spacemacs ~/.emacs.d\n\nmaster 分支是稳定不能修改的分支，不要做任何修改，否则会破坏更新机制。使用 develop 分支可以安全的手动处理更新。\n\n（3）【可选】安装 [Source Code Pro](https://github.com/adobe-fonts/source-code-pro) 字体：\n\n即使在终端运行也需要改变终端字体设置。\n\n（4）启动 Emacs。Spacemacs 会自动安装它需要的包。如果看到跟包下载相关的错误，那么可以尝试通过以下命令启动 emacs 禁用 HTTPS 协议：\n\n    emacs --insecure\n\n或者可以设置隐藏文件中的 dotspacemacs-elpa-https 为 nil，这样启动 emacs 的时候就不需要 --insecure 参数了。可以清理 .emacs.d/elpa 目录，这样任何已经下载的可能引起错误的包会重新安装。\n\n（5）重新启动 emacs 完成安装。\n\n#### spacemacs 安装镜像\n\n如果不使用镜像 spacemacs 安装会很慢，甚至卡死。我使用的是 [ELPA 镜像](http://www.4gamers.cn/)。\n\n#### 安装 gnuplot\n\n    apt-get install gnuplot\n\n### gnuplot 测试\n\n编辑 org 文件：\n\n    #+PLOT: title:\"Citas\" ind:1 deps:(3) type:2d with:histograms set:\"yrange [0:]\"\n         | Sede      | Max cites | H-index |\n         |-----------+-----------+---------|\n         | Chile     |    257.72 |   21.39 |\n         | Leeds     |    165.77 |   19.68 |\n         | Sao Paolo |     71.00 |   11.50 |\n         | Stockholm |    134.19 |   14.33 |\n         | Morelia   |    257.56 |   17.67 |\n\n运行命令 M-x org-plot/gnuplot\n\n> 如果命令运行报错：cannot open load file no such file or directory, gnuplot。原因是没有安装 gnuplot.el。运行命令 M-x package-install RET gnuplot RET\n> 参考网址：<https://github.com/bruceravel/gnuplot-mode>","slug":"spacemacs-gnuplot","published":1,"updated":"2021-07-19T16:28:00.304Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphrk00f5itd37ydw2smx","content":"<h3 id=\"环境\"><a href=\"#环境\" class=\"headerlink\" title=\"环境\"></a>环境</h3><ul>\n<li>Ubuntu 16.10</li>\n<li>Emacs 24</li>\n</ul>\n<h3 id=\"安装过程\"><a href=\"#安装过程\" class=\"headerlink\" title=\"安装过程\"></a>安装过程</h3><h4 id=\"spacemacs-安装\"><a href=\"#spacemacs-安装\" class=\"headerlink\" title=\"spacemacs 安装\"></a>spacemacs 安装</h4><ol>\n<li><p>安装 Emacs</p>\n<p> sudo apt-get install emacs</p>\n</li>\n</ol>\n<span id=\"more\"></span>\n\n<ol start=\"2\">\n<li>安装 spacemacs</li>\n</ol>\n<p>（1）如果已经存在 Emacs 配置文件，首先备份：</p>\n<pre><code>cd ~\nmv .emacs.d .emacs.d.bak\nmv .emacs .emacs.bak\n</code></pre>\n<p>不要忘记备份并移动 ~/.emacs 文件，否则 Spacemacs 将不能加载，因为这个文件阻止从适当的初始化文件加载。</p>\n<p>（2）clone 配置仓库：</p>\n<pre><code>git clone https://github.com/syl20bnr/spacemacs ~/.emacs.d\n</code></pre>\n<p>master 分支是稳定不能修改的分支，不要做任何修改，否则会破坏更新机制。使用 develop 分支可以安全的手动处理更新。</p>\n<p>（3）【可选】安装 <a href=\"https://github.com/adobe-fonts/source-code-pro\">Source Code Pro</a> 字体：</p>\n<p>即使在终端运行也需要改变终端字体设置。</p>\n<p>（4）启动 Emacs。Spacemacs 会自动安装它需要的包。如果看到跟包下载相关的错误，那么可以尝试通过以下命令启动 emacs 禁用 HTTPS 协议：</p>\n<pre><code>emacs --insecure\n</code></pre>\n<p>或者可以设置隐藏文件中的 dotspacemacs-elpa-https 为 nil，这样启动 emacs 的时候就不需要 –insecure 参数了。可以清理 .emacs.d/elpa 目录，这样任何已经下载的可能引起错误的包会重新安装。</p>\n<p>（5）重新启动 emacs 完成安装。</p>\n<h4 id=\"spacemacs-安装镜像\"><a href=\"#spacemacs-安装镜像\" class=\"headerlink\" title=\"spacemacs 安装镜像\"></a>spacemacs 安装镜像</h4><p>如果不使用镜像 spacemacs 安装会很慢，甚至卡死。我使用的是 <a href=\"http://www.4gamers.cn/\">ELPA 镜像</a>。</p>\n<h4 id=\"安装-gnuplot\"><a href=\"#安装-gnuplot\" class=\"headerlink\" title=\"安装 gnuplot\"></a>安装 gnuplot</h4><pre><code>apt-get install gnuplot\n</code></pre>\n<h3 id=\"gnuplot-测试\"><a href=\"#gnuplot-测试\" class=\"headerlink\" title=\"gnuplot 测试\"></a>gnuplot 测试</h3><p>编辑 org 文件：</p>\n<pre><code>#+PLOT: title:&quot;Citas&quot; ind:1 deps:(3) type:2d with:histograms set:&quot;yrange [0:]&quot;\n     | Sede      | Max cites | H-index |\n     |-----------+-----------+---------|\n     | Chile     |    257.72 |   21.39 |\n     | Leeds     |    165.77 |   19.68 |\n     | Sao Paolo |     71.00 |   11.50 |\n     | Stockholm |    134.19 |   14.33 |\n     | Morelia   |    257.56 |   17.67 |\n</code></pre>\n<p>运行命令 M-x org-plot/gnuplot</p>\n<blockquote>\n<p>如果命令运行报错：cannot open load file no such file or directory, gnuplot。原因是没有安装 gnuplot.el。运行命令 M-x package-install RET gnuplot RET<br>参考网址：<a href=\"https://github.com/bruceravel/gnuplot-mode\">https://github.com/bruceravel/gnuplot-mode</a></p>\n</blockquote>\n","site":{"data":{}},"excerpt":"<h3 id=\"环境\"><a href=\"#环境\" class=\"headerlink\" title=\"环境\"></a>环境</h3><ul>\n<li>Ubuntu 16.10</li>\n<li>Emacs 24</li>\n</ul>\n<h3 id=\"安装过程\"><a href=\"#安装过程\" class=\"headerlink\" title=\"安装过程\"></a>安装过程</h3><h4 id=\"spacemacs-安装\"><a href=\"#spacemacs-安装\" class=\"headerlink\" title=\"spacemacs 安装\"></a>spacemacs 安装</h4><ol>\n<li><p>安装 Emacs</p>\n<p> sudo apt-get install emacs</p>\n</li>\n</ol>","more":"<ol start=\"2\">\n<li>安装 spacemacs</li>\n</ol>\n<p>（1）如果已经存在 Emacs 配置文件，首先备份：</p>\n<pre><code>cd ~\nmv .emacs.d .emacs.d.bak\nmv .emacs .emacs.bak\n</code></pre>\n<p>不要忘记备份并移动 ~/.emacs 文件，否则 Spacemacs 将不能加载，因为这个文件阻止从适当的初始化文件加载。</p>\n<p>（2）clone 配置仓库：</p>\n<pre><code>git clone https://github.com/syl20bnr/spacemacs ~/.emacs.d\n</code></pre>\n<p>master 分支是稳定不能修改的分支，不要做任何修改，否则会破坏更新机制。使用 develop 分支可以安全的手动处理更新。</p>\n<p>（3）【可选】安装 <a href=\"https://github.com/adobe-fonts/source-code-pro\">Source Code Pro</a> 字体：</p>\n<p>即使在终端运行也需要改变终端字体设置。</p>\n<p>（4）启动 Emacs。Spacemacs 会自动安装它需要的包。如果看到跟包下载相关的错误，那么可以尝试通过以下命令启动 emacs 禁用 HTTPS 协议：</p>\n<pre><code>emacs --insecure\n</code></pre>\n<p>或者可以设置隐藏文件中的 dotspacemacs-elpa-https 为 nil，这样启动 emacs 的时候就不需要 –insecure 参数了。可以清理 .emacs.d/elpa 目录，这样任何已经下载的可能引起错误的包会重新安装。</p>\n<p>（5）重新启动 emacs 完成安装。</p>\n<h4 id=\"spacemacs-安装镜像\"><a href=\"#spacemacs-安装镜像\" class=\"headerlink\" title=\"spacemacs 安装镜像\"></a>spacemacs 安装镜像</h4><p>如果不使用镜像 spacemacs 安装会很慢，甚至卡死。我使用的是 <a href=\"http://www.4gamers.cn/\">ELPA 镜像</a>。</p>\n<h4 id=\"安装-gnuplot\"><a href=\"#安装-gnuplot\" class=\"headerlink\" title=\"安装 gnuplot\"></a>安装 gnuplot</h4><pre><code>apt-get install gnuplot\n</code></pre>\n<h3 id=\"gnuplot-测试\"><a href=\"#gnuplot-测试\" class=\"headerlink\" title=\"gnuplot 测试\"></a>gnuplot 测试</h3><p>编辑 org 文件：</p>\n<pre><code>#+PLOT: title:&quot;Citas&quot; ind:1 deps:(3) type:2d with:histograms set:&quot;yrange [0:]&quot;\n     | Sede      | Max cites | H-index |\n     |-----------+-----------+---------|\n     | Chile     |    257.72 |   21.39 |\n     | Leeds     |    165.77 |   19.68 |\n     | Sao Paolo |     71.00 |   11.50 |\n     | Stockholm |    134.19 |   14.33 |\n     | Morelia   |    257.56 |   17.67 |\n</code></pre>\n<p>运行命令 M-x org-plot/gnuplot</p>\n<blockquote>\n<p>如果命令运行报错：cannot open load file no such file or directory, gnuplot。原因是没有安装 gnuplot.el。运行命令 M-x package-install RET gnuplot RET<br>参考网址：<a href=\"https://github.com/bruceravel/gnuplot-mode\">https://github.com/bruceravel/gnuplot-mode</a></p>\n</blockquote>"},{"title":"timed out waiting for input: auto-logout","date":"2015-11-26T01:39:12.000Z","_content":"\n发生这个的原因是系统设置了等待用户输入 time out （超时）参数。登陆系统，执行以下命令可以查看参数值：\n\n\t[root@vm-10-176-30-167 ~]# echo $TMOUT\n\t120\n\n<!-- more -->\n\n当前参数是 120 秒，所以很快就超时自动退出。解决方法是在 /etc/profile 中修改该参数的值：\n\n\t[root@vm-10-176-30-167 ~]# vi /etc/profile\n\n在 /etc/profile 末尾增加一下配置，将超时时间改为 10 小时。\n\t\n\tTMOUT=36000\n\texport TMOUT\n\n保存，退出系统并重新登录，或者执行以下命令生效新配置参数：\n\n\t[root@vm-10-176-30-167 ~]# source /etc/profile\n\t[root@vm-10-176-30-167 ~]# echo $TMOUT\n\t36000\n","source":"_posts/timed-out-waiting-for-input-auto-logout.md","raw":"title: 'timed out waiting for input: auto-logout'\ntags:\n  - Linux\ncategories:\n  - 操作系统\n  - Linux\ndate: 2015-11-26 09:39:12\n---\n\n发生这个的原因是系统设置了等待用户输入 time out （超时）参数。登陆系统，执行以下命令可以查看参数值：\n\n\t[root@vm-10-176-30-167 ~]# echo $TMOUT\n\t120\n\n<!-- more -->\n\n当前参数是 120 秒，所以很快就超时自动退出。解决方法是在 /etc/profile 中修改该参数的值：\n\n\t[root@vm-10-176-30-167 ~]# vi /etc/profile\n\n在 /etc/profile 末尾增加一下配置，将超时时间改为 10 小时。\n\t\n\tTMOUT=36000\n\texport TMOUT\n\n保存，退出系统并重新登录，或者执行以下命令生效新配置参数：\n\n\t[root@vm-10-176-30-167 ~]# source /etc/profile\n\t[root@vm-10-176-30-167 ~]# echo $TMOUT\n\t36000\n","slug":"timed-out-waiting-for-input-auto-logout","published":1,"updated":"2021-07-19T16:28:00.304Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphrl00f9itd3a0myhbf8","content":"<p>发生这个的原因是系统设置了等待用户输入 time out （超时）参数。登陆系统，执行以下命令可以查看参数值：</p>\n<pre><code>[root@vm-10-176-30-167 ~]# echo $TMOUT\n120\n</code></pre>\n<span id=\"more\"></span>\n\n<p>当前参数是 120 秒，所以很快就超时自动退出。解决方法是在 /etc/profile 中修改该参数的值：</p>\n<pre><code>[root@vm-10-176-30-167 ~]# vi /etc/profile\n</code></pre>\n<p>在 /etc/profile 末尾增加一下配置，将超时时间改为 10 小时。</p>\n<pre><code>TMOUT=36000\nexport TMOUT\n</code></pre>\n<p>保存，退出系统并重新登录，或者执行以下命令生效新配置参数：</p>\n<pre><code>[root@vm-10-176-30-167 ~]# source /etc/profile\n[root@vm-10-176-30-167 ~]# echo $TMOUT\n36000\n</code></pre>\n","site":{"data":{}},"excerpt":"<p>发生这个的原因是系统设置了等待用户输入 time out （超时）参数。登陆系统，执行以下命令可以查看参数值：</p>\n<pre><code>[root@vm-10-176-30-167 ~]# echo $TMOUT\n120\n</code></pre>","more":"<p>当前参数是 120 秒，所以很快就超时自动退出。解决方法是在 /etc/profile 中修改该参数的值：</p>\n<pre><code>[root@vm-10-176-30-167 ~]# vi /etc/profile\n</code></pre>\n<p>在 /etc/profile 末尾增加一下配置，将超时时间改为 10 小时。</p>\n<pre><code>TMOUT=36000\nexport TMOUT\n</code></pre>\n<p>保存，退出系统并重新登录，或者执行以下命令生效新配置参数：</p>\n<pre><code>[root@vm-10-176-30-167 ~]# source /etc/profile\n[root@vm-10-176-30-167 ~]# echo $TMOUT\n36000\n</code></pre>"},{"title":"[: ==: unary operator expected","date":"2016-06-26T06:04:01.000Z","_content":"\nShell 脚本报错：“line 10: [: =: unary operator expected”。根据提示信息找到报错的程序是：\n\n    if [ $OPERATION == \"scp\" ]; then\n\n报错原因是变量 OPERATION 的值为空，那么程序就变成了：\n\n    if [ == \"scp\" ]; then\n\n显然 [ 和 \"scp\" 不相等并且缺少了 [ 符号，所以报了这样的错误。检查程序发现是 OPERATION 在声明的时候写错导致的。这样的错误还是很隐蔽的，所以为了增强程序的健壮性可以用下面的写法：\n\n    if [ \"${OPERATION}x\" == \"scpx\" ]; then\n","source":"_posts/unary-operator-expected.md","raw":"title: '[: ==: unary operator expected'\ntags:\n  - Linux\n  - Shell\ncategories:\n  - 语言\n  - Shell\ndate: 2016-06-26 14:04:01\n---\n\nShell 脚本报错：“line 10: [: =: unary operator expected”。根据提示信息找到报错的程序是：\n\n    if [ $OPERATION == \"scp\" ]; then\n\n报错原因是变量 OPERATION 的值为空，那么程序就变成了：\n\n    if [ == \"scp\" ]; then\n\n显然 [ 和 \"scp\" 不相等并且缺少了 [ 符号，所以报了这样的错误。检查程序发现是 OPERATION 在声明的时候写错导致的。这样的错误还是很隐蔽的，所以为了增强程序的健壮性可以用下面的写法：\n\n    if [ \"${OPERATION}x\" == \"scpx\" ]; then\n","slug":"unary-operator-expected","published":1,"updated":"2021-07-19T16:28:00.304Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphrm00fditd33umk7vzj","content":"<p>Shell 脚本报错：“line 10: [: =: unary operator expected”。根据提示信息找到报错的程序是：</p>\n<pre><code>if [ $OPERATION == &quot;scp&quot; ]; then\n</code></pre>\n<p>报错原因是变量 OPERATION 的值为空，那么程序就变成了：</p>\n<pre><code>if [ == &quot;scp&quot; ]; then\n</code></pre>\n<p>显然 [ 和 “scp” 不相等并且缺少了 [ 符号，所以报了这样的错误。检查程序发现是 OPERATION 在声明的时候写错导致的。这样的错误还是很隐蔽的，所以为了增强程序的健壮性可以用下面的写法：</p>\n<pre><code>if [ &quot;$&#123;OPERATION&#125;x&quot; == &quot;scpx&quot; ]; then\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>Shell 脚本报错：“line 10: [: =: unary operator expected”。根据提示信息找到报错的程序是：</p>\n<pre><code>if [ $OPERATION == &quot;scp&quot; ]; then\n</code></pre>\n<p>报错原因是变量 OPERATION 的值为空，那么程序就变成了：</p>\n<pre><code>if [ == &quot;scp&quot; ]; then\n</code></pre>\n<p>显然 [ 和 “scp” 不相等并且缺少了 [ 符号，所以报了这样的错误。检查程序发现是 OPERATION 在声明的时候写错导致的。这样的错误还是很隐蔽的，所以为了增强程序的健壮性可以用下面的写法：</p>\n<pre><code>if [ &quot;$&#123;OPERATION&#125;x&quot; == &quot;scpx&quot; ]; then\n</code></pre>\n"},{"title":"使用 Rsyslog 将 Nginx Access Log 写入 Kafka","date":"2019-03-15T01:41:45.000Z","_content":"### 环境说明\n\n- CentOS Linux release 7.3.1611\n- kafka_2.12-0.10.2.2\n- nginx/1.12.2\n- rsyslog-8.24.0-34.el7.x86_64.rpm\n\n<!-- more -->\n\n### 创建测试 Topic\n\n    $ ./kafka-topics.sh --zookeeper 192.168.72.25:2181/kafka --create --topic develop-test-topic --partitions 10 --replication-factor 3\n\n### Rsyslog 安装\n\n一般系统自带 Rsyslog 服务无需另外安装。但是因为数据需要通过 Rsyslog 的 omkafka 模块写入到 Kafka，而 omkafka 在 Rsyslog 的 v8.7.0+ 版本才支持，所以要看当前系统中 Rsyslog 的版本是否需要升级。使用以下命令查看：\n\n    # rsyslogd -v\n    rsyslogd 7.4.7, compiled with:\n    \tFEATURE_REGEXP:\t\t\t\tYes\n    \tFEATURE_LARGEFILE:\t\t\tNo\n    \tGSSAPI Kerberos 5 support:\t\tYes\n    \tFEATURE_DEBUG (debug build, slow code):\tNo\n    \t32bit Atomic operations supported:\tYes\n    \t64bit Atomic operations supported:\tYes\n    \tRuntime Instrumentation (slow code):\tNo\n    \tuuid support:\t\t\t\tYes\n\n    See http://www.rsyslog.com for more information.\n\n执行以下命令安装：\n\n    # sudo yum install rsyslog\n\n安装依赖关系如下：![](/uploads/20190315/Rsyslog依赖关系.png)\n\n### 添加 omkafka 模块\n\n    # yum install rsyslog-kafka\n\n### Rsyslog Client Nginx 配置\n\n注意，Nginx 1.7.1 之后才支持 syslog 的方式处理日志。具体配置项参见官网文档[Logging to syslog](https://nginx.org/en/docs/syslog.html)。\n\nNginx 配置主要是日志格式和 Access Log 配置项：\n\n    log_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" '\n                      '$status $body_bytes_sent \"$http_referer\" '\n                      '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n\n    server {\n        listen 11000;\n\n        location / {\n            proxy_pass http://10.16.0.144:11000;\n            access_log syslog:server=localhost,facility=local7,tag=nginx11000Root,severity=info main;\n        }\n    }\n\n### Rsyslog Server 端配置\n\nRsyslog 的主配置文件 /etc/rsyslog.conf，其中会包含引入 /etc/rsyslog.d 下扩展名为 conf 的配置文件。\n\n修改配置文件 /etc/rsyslog.conf 将下面两行前面的注释去掉：\n\n    $ModLoad imudp\n    $UDPServerRun 514\n\n在 /etc/rsyslog.d 目录下创建 rsyslog_nginx_kafka_cluster.conf，配置内容如下：\n\n    module(load=\"imudp\")\n    input(type=\"imudp\" port=\"514\")\n\n    # nginx access log ==> rsyslog server(local) ==> kafka\n    module(load=\"omkafka\")\n\n    template(name=\"nginx-11000-root\" type=\"string\" string=\"%msg%\")\n\n    if $inputname == \"imudp\" then {\n        if ($programname == \"nginx11000Root\") then\n            action(type=\"omkafka\"\n                template=\"nginx-11000-root\"\n                broker=[\"192.168.72.10:9092\",\"192.168.72.20:9092\",\"192.168.72.25:9092\",\"192.168.72.26:9092\",\"192.168.72.27:9092\",\"192.168.72.48:9092\",\"192.168.72.55:9092\",\"192.168.72.80:9092\",\"192.168.72.81:9092\",\"192.168.72.97:9092\"]\n                topic=\"develop-test-topic\"\n                partitions.auto=\"on\"\n                confParam=[\n                    \"socket.keepalive.enable=true\"\n                ]\n            )\n    }\n\n    :rawmsg, contains, \"nginx11000Root\" ~\n\n### 联调测试\n\n启动 Rsyslog 服务：\n\n    # service rsyslog start\n    Redirecting to /bin/systemctl start  rsyslog.service\n\n### 遇到的问题\n#### syslog tag 只能包含字母和数字\n\n    # nginx -t\n    nginx: [emerg] syslog \"tag\" only allows alphanumeric characters and underscore in     /etc/nginx/conf.d/jx-11000-jenkins149-36-144.conf:7\n    nginx: configuration file /etc/nginx/nginx.conf test failed\n\n#### 'omkafka' is unknown\n\nRsyslog 中没有包含 omkafka 模块，需要另外安装。查看 /var/log/messages 日志信息会有以下提示：\n\n    # tail -f messages\n    Mar 15 15:13:40 192-168-72-29 systemd: Started System Logging Service.\n    Mar 15 15:13:40 192-168-72-29 rsyslogd: could not load module '/usr/lib64/rsyslog/omkafka.so', dlopen:     /usr/lib64/rsyslog/omkafka.so: cannot open shared object file: No such file or directory  [v8.24.0-34.el7 try     http://www.rsyslog.com/e/2066 ]\n    Mar 15 15:13:40 192-168-72-29 rsyslogd: could not load module '/usr/lib64/rsyslog/omkafka.so', dlopen:     /usr/lib64/rsyslog/omkafka.so: cannot open shared object file: No such file or directory  [v8.24.0-34.el7 try     http://www.rsyslog.com/e/2066 ]\n    Mar 15 15:13:40 192-168-72-29 rsyslogd: module name 'omkafka' is unknown [v8.24.0-34.el7 try     http://www.rsyslog.com/e/2209 ]\n    Mar 15 15:13:40 192-168-72-29 rsyslogd: module name 'omkafka' is unknown [v8.24.0-34.el7 try     http://www.rsyslog.com/e/2209 ]\n\n#### CentOS 6.5 升级 Rsyslog\n\nCentOS 6.5 自带的 Rsyslog 版本是 rsyslogd 5.8.10。按照以下方式安装新版本：\n\n    # cd /etc/yum.repos.d/\n    # wget http://rpms.adiscon.com/v8-stable/rsyslog.repo\n    # yum install rsyslog\n","source":"_posts/使用-Rsyslog-将-Nginx-Access-Log-写入-Kafka.md","raw":"title: 使用 Rsyslog 将 Nginx Access Log 写入 Kafka\ndate: 2019-03-15 09:41:45\ntags:\n- Rsyslog\n- Nginx\n- Kafka\ncategories:\n- 大数据\n- Rsyslog\n---\n### 环境说明\n\n- CentOS Linux release 7.3.1611\n- kafka_2.12-0.10.2.2\n- nginx/1.12.2\n- rsyslog-8.24.0-34.el7.x86_64.rpm\n\n<!-- more -->\n\n### 创建测试 Topic\n\n    $ ./kafka-topics.sh --zookeeper 192.168.72.25:2181/kafka --create --topic develop-test-topic --partitions 10 --replication-factor 3\n\n### Rsyslog 安装\n\n一般系统自带 Rsyslog 服务无需另外安装。但是因为数据需要通过 Rsyslog 的 omkafka 模块写入到 Kafka，而 omkafka 在 Rsyslog 的 v8.7.0+ 版本才支持，所以要看当前系统中 Rsyslog 的版本是否需要升级。使用以下命令查看：\n\n    # rsyslogd -v\n    rsyslogd 7.4.7, compiled with:\n    \tFEATURE_REGEXP:\t\t\t\tYes\n    \tFEATURE_LARGEFILE:\t\t\tNo\n    \tGSSAPI Kerberos 5 support:\t\tYes\n    \tFEATURE_DEBUG (debug build, slow code):\tNo\n    \t32bit Atomic operations supported:\tYes\n    \t64bit Atomic operations supported:\tYes\n    \tRuntime Instrumentation (slow code):\tNo\n    \tuuid support:\t\t\t\tYes\n\n    See http://www.rsyslog.com for more information.\n\n执行以下命令安装：\n\n    # sudo yum install rsyslog\n\n安装依赖关系如下：![](/uploads/20190315/Rsyslog依赖关系.png)\n\n### 添加 omkafka 模块\n\n    # yum install rsyslog-kafka\n\n### Rsyslog Client Nginx 配置\n\n注意，Nginx 1.7.1 之后才支持 syslog 的方式处理日志。具体配置项参见官网文档[Logging to syslog](https://nginx.org/en/docs/syslog.html)。\n\nNginx 配置主要是日志格式和 Access Log 配置项：\n\n    log_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" '\n                      '$status $body_bytes_sent \"$http_referer\" '\n                      '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n\n    server {\n        listen 11000;\n\n        location / {\n            proxy_pass http://10.16.0.144:11000;\n            access_log syslog:server=localhost,facility=local7,tag=nginx11000Root,severity=info main;\n        }\n    }\n\n### Rsyslog Server 端配置\n\nRsyslog 的主配置文件 /etc/rsyslog.conf，其中会包含引入 /etc/rsyslog.d 下扩展名为 conf 的配置文件。\n\n修改配置文件 /etc/rsyslog.conf 将下面两行前面的注释去掉：\n\n    $ModLoad imudp\n    $UDPServerRun 514\n\n在 /etc/rsyslog.d 目录下创建 rsyslog_nginx_kafka_cluster.conf，配置内容如下：\n\n    module(load=\"imudp\")\n    input(type=\"imudp\" port=\"514\")\n\n    # nginx access log ==> rsyslog server(local) ==> kafka\n    module(load=\"omkafka\")\n\n    template(name=\"nginx-11000-root\" type=\"string\" string=\"%msg%\")\n\n    if $inputname == \"imudp\" then {\n        if ($programname == \"nginx11000Root\") then\n            action(type=\"omkafka\"\n                template=\"nginx-11000-root\"\n                broker=[\"192.168.72.10:9092\",\"192.168.72.20:9092\",\"192.168.72.25:9092\",\"192.168.72.26:9092\",\"192.168.72.27:9092\",\"192.168.72.48:9092\",\"192.168.72.55:9092\",\"192.168.72.80:9092\",\"192.168.72.81:9092\",\"192.168.72.97:9092\"]\n                topic=\"develop-test-topic\"\n                partitions.auto=\"on\"\n                confParam=[\n                    \"socket.keepalive.enable=true\"\n                ]\n            )\n    }\n\n    :rawmsg, contains, \"nginx11000Root\" ~\n\n### 联调测试\n\n启动 Rsyslog 服务：\n\n    # service rsyslog start\n    Redirecting to /bin/systemctl start  rsyslog.service\n\n### 遇到的问题\n#### syslog tag 只能包含字母和数字\n\n    # nginx -t\n    nginx: [emerg] syslog \"tag\" only allows alphanumeric characters and underscore in     /etc/nginx/conf.d/jx-11000-jenkins149-36-144.conf:7\n    nginx: configuration file /etc/nginx/nginx.conf test failed\n\n#### 'omkafka' is unknown\n\nRsyslog 中没有包含 omkafka 模块，需要另外安装。查看 /var/log/messages 日志信息会有以下提示：\n\n    # tail -f messages\n    Mar 15 15:13:40 192-168-72-29 systemd: Started System Logging Service.\n    Mar 15 15:13:40 192-168-72-29 rsyslogd: could not load module '/usr/lib64/rsyslog/omkafka.so', dlopen:     /usr/lib64/rsyslog/omkafka.so: cannot open shared object file: No such file or directory  [v8.24.0-34.el7 try     http://www.rsyslog.com/e/2066 ]\n    Mar 15 15:13:40 192-168-72-29 rsyslogd: could not load module '/usr/lib64/rsyslog/omkafka.so', dlopen:     /usr/lib64/rsyslog/omkafka.so: cannot open shared object file: No such file or directory  [v8.24.0-34.el7 try     http://www.rsyslog.com/e/2066 ]\n    Mar 15 15:13:40 192-168-72-29 rsyslogd: module name 'omkafka' is unknown [v8.24.0-34.el7 try     http://www.rsyslog.com/e/2209 ]\n    Mar 15 15:13:40 192-168-72-29 rsyslogd: module name 'omkafka' is unknown [v8.24.0-34.el7 try     http://www.rsyslog.com/e/2209 ]\n\n#### CentOS 6.5 升级 Rsyslog\n\nCentOS 6.5 自带的 Rsyslog 版本是 rsyslogd 5.8.10。按照以下方式安装新版本：\n\n    # cd /etc/yum.repos.d/\n    # wget http://rpms.adiscon.com/v8-stable/rsyslog.repo\n    # yum install rsyslog\n","slug":"使用-Rsyslog-将-Nginx-Access-Log-写入-Kafka","published":1,"updated":"2021-07-19T16:28:00.304Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphro00fhitd3bzz6hnjp","content":"<h3 id=\"环境说明\"><a href=\"#环境说明\" class=\"headerlink\" title=\"环境说明\"></a>环境说明</h3><ul>\n<li>CentOS Linux release 7.3.1611</li>\n<li>kafka_2.12-0.10.2.2</li>\n<li>nginx/1.12.2</li>\n<li>rsyslog-8.24.0-34.el7.x86_64.rpm</li>\n</ul>\n<span id=\"more\"></span>\n\n<h3 id=\"创建测试-Topic\"><a href=\"#创建测试-Topic\" class=\"headerlink\" title=\"创建测试 Topic\"></a>创建测试 Topic</h3><pre><code>$ ./kafka-topics.sh --zookeeper 192.168.72.25:2181/kafka --create --topic develop-test-topic --partitions 10 --replication-factor 3\n</code></pre>\n<h3 id=\"Rsyslog-安装\"><a href=\"#Rsyslog-安装\" class=\"headerlink\" title=\"Rsyslog 安装\"></a>Rsyslog 安装</h3><p>一般系统自带 Rsyslog 服务无需另外安装。但是因为数据需要通过 Rsyslog 的 omkafka 模块写入到 Kafka，而 omkafka 在 Rsyslog 的 v8.7.0+ 版本才支持，所以要看当前系统中 Rsyslog 的版本是否需要升级。使用以下命令查看：</p>\n<pre><code># rsyslogd -v\nrsyslogd 7.4.7, compiled with:\n    FEATURE_REGEXP:                Yes\n    FEATURE_LARGEFILE:            No\n    GSSAPI Kerberos 5 support:        Yes\n    FEATURE_DEBUG (debug build, slow code):    No\n    32bit Atomic operations supported:    Yes\n    64bit Atomic operations supported:    Yes\n    Runtime Instrumentation (slow code):    No\n    uuid support:                Yes\n\nSee http://www.rsyslog.com for more information.\n</code></pre>\n<p>执行以下命令安装：</p>\n<pre><code># sudo yum install rsyslog\n</code></pre>\n<p>安装依赖关系如下：<img src=\"/uploads/20190315/Rsyslog%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB.png\"></p>\n<h3 id=\"添加-omkafka-模块\"><a href=\"#添加-omkafka-模块\" class=\"headerlink\" title=\"添加 omkafka 模块\"></a>添加 omkafka 模块</h3><pre><code># yum install rsyslog-kafka\n</code></pre>\n<h3 id=\"Rsyslog-Client-Nginx-配置\"><a href=\"#Rsyslog-Client-Nginx-配置\" class=\"headerlink\" title=\"Rsyslog Client Nginx 配置\"></a>Rsyslog Client Nginx 配置</h3><p>注意，Nginx 1.7.1 之后才支持 syslog 的方式处理日志。具体配置项参见官网文档<a href=\"https://nginx.org/en/docs/syslog.html\">Logging to syslog</a>。</p>\n<p>Nginx 配置主要是日志格式和 Access Log 配置项：</p>\n<pre><code>log_format  main  &#39;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#39;\n                  &#39;$status $body_bytes_sent &quot;$http_referer&quot; &#39;\n                  &#39;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#39;;\n\nserver &#123;\n    listen 11000;\n\n    location / &#123;\n        proxy_pass http://10.16.0.144:11000;\n        access_log syslog:server=localhost,facility=local7,tag=nginx11000Root,severity=info main;\n    &#125;\n&#125;\n</code></pre>\n<h3 id=\"Rsyslog-Server-端配置\"><a href=\"#Rsyslog-Server-端配置\" class=\"headerlink\" title=\"Rsyslog Server 端配置\"></a>Rsyslog Server 端配置</h3><p>Rsyslog 的主配置文件 /etc/rsyslog.conf，其中会包含引入 /etc/rsyslog.d 下扩展名为 conf 的配置文件。</p>\n<p>修改配置文件 /etc/rsyslog.conf 将下面两行前面的注释去掉：</p>\n<pre><code>$ModLoad imudp\n$UDPServerRun 514\n</code></pre>\n<p>在 /etc/rsyslog.d 目录下创建 rsyslog_nginx_kafka_cluster.conf，配置内容如下：</p>\n<pre><code>module(load=&quot;imudp&quot;)\ninput(type=&quot;imudp&quot; port=&quot;514&quot;)\n\n# nginx access log ==&gt; rsyslog server(local) ==&gt; kafka\nmodule(load=&quot;omkafka&quot;)\n\ntemplate(name=&quot;nginx-11000-root&quot; type=&quot;string&quot; string=&quot;%msg%&quot;)\n\nif $inputname == &quot;imudp&quot; then &#123;\n    if ($programname == &quot;nginx11000Root&quot;) then\n        action(type=&quot;omkafka&quot;\n            template=&quot;nginx-11000-root&quot;\n            broker=[&quot;192.168.72.10:9092&quot;,&quot;192.168.72.20:9092&quot;,&quot;192.168.72.25:9092&quot;,&quot;192.168.72.26:9092&quot;,&quot;192.168.72.27:9092&quot;,&quot;192.168.72.48:9092&quot;,&quot;192.168.72.55:9092&quot;,&quot;192.168.72.80:9092&quot;,&quot;192.168.72.81:9092&quot;,&quot;192.168.72.97:9092&quot;]\n            topic=&quot;develop-test-topic&quot;\n            partitions.auto=&quot;on&quot;\n            confParam=[\n                &quot;socket.keepalive.enable=true&quot;\n            ]\n        )\n&#125;\n\n:rawmsg, contains, &quot;nginx11000Root&quot; ~\n</code></pre>\n<h3 id=\"联调测试\"><a href=\"#联调测试\" class=\"headerlink\" title=\"联调测试\"></a>联调测试</h3><p>启动 Rsyslog 服务：</p>\n<pre><code># service rsyslog start\nRedirecting to /bin/systemctl start  rsyslog.service\n</code></pre>\n<h3 id=\"遇到的问题\"><a href=\"#遇到的问题\" class=\"headerlink\" title=\"遇到的问题\"></a>遇到的问题</h3><h4 id=\"syslog-tag-只能包含字母和数字\"><a href=\"#syslog-tag-只能包含字母和数字\" class=\"headerlink\" title=\"syslog tag 只能包含字母和数字\"></a>syslog tag 只能包含字母和数字</h4><pre><code># nginx -t\nnginx: [emerg] syslog &quot;tag&quot; only allows alphanumeric characters and underscore in     /etc/nginx/conf.d/jx-11000-jenkins149-36-144.conf:7\nnginx: configuration file /etc/nginx/nginx.conf test failed\n</code></pre>\n<h4 id=\"‘omkafka’-is-unknown\"><a href=\"#‘omkafka’-is-unknown\" class=\"headerlink\" title=\"‘omkafka’ is unknown\"></a>‘omkafka’ is unknown</h4><p>Rsyslog 中没有包含 omkafka 模块，需要另外安装。查看 /var/log/messages 日志信息会有以下提示：</p>\n<pre><code># tail -f messages\nMar 15 15:13:40 192-168-72-29 systemd: Started System Logging Service.\nMar 15 15:13:40 192-168-72-29 rsyslogd: could not load module &#39;/usr/lib64/rsyslog/omkafka.so&#39;, dlopen:     /usr/lib64/rsyslog/omkafka.so: cannot open shared object file: No such file or directory  [v8.24.0-34.el7 try     http://www.rsyslog.com/e/2066 ]\nMar 15 15:13:40 192-168-72-29 rsyslogd: could not load module &#39;/usr/lib64/rsyslog/omkafka.so&#39;, dlopen:     /usr/lib64/rsyslog/omkafka.so: cannot open shared object file: No such file or directory  [v8.24.0-34.el7 try     http://www.rsyslog.com/e/2066 ]\nMar 15 15:13:40 192-168-72-29 rsyslogd: module name &#39;omkafka&#39; is unknown [v8.24.0-34.el7 try     http://www.rsyslog.com/e/2209 ]\nMar 15 15:13:40 192-168-72-29 rsyslogd: module name &#39;omkafka&#39; is unknown [v8.24.0-34.el7 try     http://www.rsyslog.com/e/2209 ]\n</code></pre>\n<h4 id=\"CentOS-6-5-升级-Rsyslog\"><a href=\"#CentOS-6-5-升级-Rsyslog\" class=\"headerlink\" title=\"CentOS 6.5 升级 Rsyslog\"></a>CentOS 6.5 升级 Rsyslog</h4><p>CentOS 6.5 自带的 Rsyslog 版本是 rsyslogd 5.8.10。按照以下方式安装新版本：</p>\n<pre><code># cd /etc/yum.repos.d/\n# wget http://rpms.adiscon.com/v8-stable/rsyslog.repo\n# yum install rsyslog\n</code></pre>\n","site":{"data":{}},"excerpt":"<h3 id=\"环境说明\"><a href=\"#环境说明\" class=\"headerlink\" title=\"环境说明\"></a>环境说明</h3><ul>\n<li>CentOS Linux release 7.3.1611</li>\n<li>kafka_2.12-0.10.2.2</li>\n<li>nginx/1.12.2</li>\n<li>rsyslog-8.24.0-34.el7.x86_64.rpm</li>\n</ul>","more":"<h3 id=\"创建测试-Topic\"><a href=\"#创建测试-Topic\" class=\"headerlink\" title=\"创建测试 Topic\"></a>创建测试 Topic</h3><pre><code>$ ./kafka-topics.sh --zookeeper 192.168.72.25:2181/kafka --create --topic develop-test-topic --partitions 10 --replication-factor 3\n</code></pre>\n<h3 id=\"Rsyslog-安装\"><a href=\"#Rsyslog-安装\" class=\"headerlink\" title=\"Rsyslog 安装\"></a>Rsyslog 安装</h3><p>一般系统自带 Rsyslog 服务无需另外安装。但是因为数据需要通过 Rsyslog 的 omkafka 模块写入到 Kafka，而 omkafka 在 Rsyslog 的 v8.7.0+ 版本才支持，所以要看当前系统中 Rsyslog 的版本是否需要升级。使用以下命令查看：</p>\n<pre><code># rsyslogd -v\nrsyslogd 7.4.7, compiled with:\n    FEATURE_REGEXP:                Yes\n    FEATURE_LARGEFILE:            No\n    GSSAPI Kerberos 5 support:        Yes\n    FEATURE_DEBUG (debug build, slow code):    No\n    32bit Atomic operations supported:    Yes\n    64bit Atomic operations supported:    Yes\n    Runtime Instrumentation (slow code):    No\n    uuid support:                Yes\n\nSee http://www.rsyslog.com for more information.\n</code></pre>\n<p>执行以下命令安装：</p>\n<pre><code># sudo yum install rsyslog\n</code></pre>\n<p>安装依赖关系如下：<img src=\"/uploads/20190315/Rsyslog%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB.png\"></p>\n<h3 id=\"添加-omkafka-模块\"><a href=\"#添加-omkafka-模块\" class=\"headerlink\" title=\"添加 omkafka 模块\"></a>添加 omkafka 模块</h3><pre><code># yum install rsyslog-kafka\n</code></pre>\n<h3 id=\"Rsyslog-Client-Nginx-配置\"><a href=\"#Rsyslog-Client-Nginx-配置\" class=\"headerlink\" title=\"Rsyslog Client Nginx 配置\"></a>Rsyslog Client Nginx 配置</h3><p>注意，Nginx 1.7.1 之后才支持 syslog 的方式处理日志。具体配置项参见官网文档<a href=\"https://nginx.org/en/docs/syslog.html\">Logging to syslog</a>。</p>\n<p>Nginx 配置主要是日志格式和 Access Log 配置项：</p>\n<pre><code>log_format  main  &#39;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#39;\n                  &#39;$status $body_bytes_sent &quot;$http_referer&quot; &#39;\n                  &#39;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#39;;\n\nserver &#123;\n    listen 11000;\n\n    location / &#123;\n        proxy_pass http://10.16.0.144:11000;\n        access_log syslog:server=localhost,facility=local7,tag=nginx11000Root,severity=info main;\n    &#125;\n&#125;\n</code></pre>\n<h3 id=\"Rsyslog-Server-端配置\"><a href=\"#Rsyslog-Server-端配置\" class=\"headerlink\" title=\"Rsyslog Server 端配置\"></a>Rsyslog Server 端配置</h3><p>Rsyslog 的主配置文件 /etc/rsyslog.conf，其中会包含引入 /etc/rsyslog.d 下扩展名为 conf 的配置文件。</p>\n<p>修改配置文件 /etc/rsyslog.conf 将下面两行前面的注释去掉：</p>\n<pre><code>$ModLoad imudp\n$UDPServerRun 514\n</code></pre>\n<p>在 /etc/rsyslog.d 目录下创建 rsyslog_nginx_kafka_cluster.conf，配置内容如下：</p>\n<pre><code>module(load=&quot;imudp&quot;)\ninput(type=&quot;imudp&quot; port=&quot;514&quot;)\n\n# nginx access log ==&gt; rsyslog server(local) ==&gt; kafka\nmodule(load=&quot;omkafka&quot;)\n\ntemplate(name=&quot;nginx-11000-root&quot; type=&quot;string&quot; string=&quot;%msg%&quot;)\n\nif $inputname == &quot;imudp&quot; then &#123;\n    if ($programname == &quot;nginx11000Root&quot;) then\n        action(type=&quot;omkafka&quot;\n            template=&quot;nginx-11000-root&quot;\n            broker=[&quot;192.168.72.10:9092&quot;,&quot;192.168.72.20:9092&quot;,&quot;192.168.72.25:9092&quot;,&quot;192.168.72.26:9092&quot;,&quot;192.168.72.27:9092&quot;,&quot;192.168.72.48:9092&quot;,&quot;192.168.72.55:9092&quot;,&quot;192.168.72.80:9092&quot;,&quot;192.168.72.81:9092&quot;,&quot;192.168.72.97:9092&quot;]\n            topic=&quot;develop-test-topic&quot;\n            partitions.auto=&quot;on&quot;\n            confParam=[\n                &quot;socket.keepalive.enable=true&quot;\n            ]\n        )\n&#125;\n\n:rawmsg, contains, &quot;nginx11000Root&quot; ~\n</code></pre>\n<h3 id=\"联调测试\"><a href=\"#联调测试\" class=\"headerlink\" title=\"联调测试\"></a>联调测试</h3><p>启动 Rsyslog 服务：</p>\n<pre><code># service rsyslog start\nRedirecting to /bin/systemctl start  rsyslog.service\n</code></pre>\n<h3 id=\"遇到的问题\"><a href=\"#遇到的问题\" class=\"headerlink\" title=\"遇到的问题\"></a>遇到的问题</h3><h4 id=\"syslog-tag-只能包含字母和数字\"><a href=\"#syslog-tag-只能包含字母和数字\" class=\"headerlink\" title=\"syslog tag 只能包含字母和数字\"></a>syslog tag 只能包含字母和数字</h4><pre><code># nginx -t\nnginx: [emerg] syslog &quot;tag&quot; only allows alphanumeric characters and underscore in     /etc/nginx/conf.d/jx-11000-jenkins149-36-144.conf:7\nnginx: configuration file /etc/nginx/nginx.conf test failed\n</code></pre>\n<h4 id=\"‘omkafka’-is-unknown\"><a href=\"#‘omkafka’-is-unknown\" class=\"headerlink\" title=\"‘omkafka’ is unknown\"></a>‘omkafka’ is unknown</h4><p>Rsyslog 中没有包含 omkafka 模块，需要另外安装。查看 /var/log/messages 日志信息会有以下提示：</p>\n<pre><code># tail -f messages\nMar 15 15:13:40 192-168-72-29 systemd: Started System Logging Service.\nMar 15 15:13:40 192-168-72-29 rsyslogd: could not load module &#39;/usr/lib64/rsyslog/omkafka.so&#39;, dlopen:     /usr/lib64/rsyslog/omkafka.so: cannot open shared object file: No such file or directory  [v8.24.0-34.el7 try     http://www.rsyslog.com/e/2066 ]\nMar 15 15:13:40 192-168-72-29 rsyslogd: could not load module &#39;/usr/lib64/rsyslog/omkafka.so&#39;, dlopen:     /usr/lib64/rsyslog/omkafka.so: cannot open shared object file: No such file or directory  [v8.24.0-34.el7 try     http://www.rsyslog.com/e/2066 ]\nMar 15 15:13:40 192-168-72-29 rsyslogd: module name &#39;omkafka&#39; is unknown [v8.24.0-34.el7 try     http://www.rsyslog.com/e/2209 ]\nMar 15 15:13:40 192-168-72-29 rsyslogd: module name &#39;omkafka&#39; is unknown [v8.24.0-34.el7 try     http://www.rsyslog.com/e/2209 ]\n</code></pre>\n<h4 id=\"CentOS-6-5-升级-Rsyslog\"><a href=\"#CentOS-6-5-升级-Rsyslog\" class=\"headerlink\" title=\"CentOS 6.5 升级 Rsyslog\"></a>CentOS 6.5 升级 Rsyslog</h4><p>CentOS 6.5 自带的 Rsyslog 版本是 rsyslogd 5.8.10。按照以下方式安装新版本：</p>\n<pre><code># cd /etc/yum.repos.d/\n# wget http://rpms.adiscon.com/v8-stable/rsyslog.repo\n# yum install rsyslog\n</code></pre>"},{"title":"修改 Linux 符号链接的属主和组","date":"2017-03-03T03:11:48.000Z","_content":"\n\n使用以下命令修改 Linux 系统符号链接的数据和组：\n\n    chown -h user /path\n    chgrp -h user /path\n\n<!-- more -->\n\n当遇到不熟悉的命令，man 是一个好习惯：\n\n    man chown\n    -h, --no-dereference\n              affect  each  symbolic  link  instead  of any referenced file (useful only on systems that can\n              change the ownership of a symlink)","source":"_posts/修改-Linux-符号链接的属主和组.md","raw":"title: 修改 Linux 符号链接的属主和组\ntags:\n  - Linux\ncategories:\n  - 操作系统\n  - Linux\ndate: 2017-03-03 11:11:48\n---\n\n\n使用以下命令修改 Linux 系统符号链接的数据和组：\n\n    chown -h user /path\n    chgrp -h user /path\n\n<!-- more -->\n\n当遇到不熟悉的命令，man 是一个好习惯：\n\n    man chown\n    -h, --no-dereference\n              affect  each  symbolic  link  instead  of any referenced file (useful only on systems that can\n              change the ownership of a symlink)","slug":"修改-Linux-符号链接的属主和组","published":1,"updated":"2021-07-19T16:28:00.304Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphrp00flitd39m96g0ef","content":"<p>使用以下命令修改 Linux 系统符号链接的数据和组：</p>\n<pre><code>chown -h user /path\nchgrp -h user /path\n</code></pre>\n<span id=\"more\"></span>\n\n<p>当遇到不熟悉的命令，man 是一个好习惯：</p>\n<pre><code>man chown\n-h, --no-dereference\n          affect  each  symbolic  link  instead  of any referenced file (useful only on systems that can\n          change the ownership of a symlink)\n</code></pre>\n","site":{"data":{}},"excerpt":"<p>使用以下命令修改 Linux 系统符号链接的数据和组：</p>\n<pre><code>chown -h user /path\nchgrp -h user /path\n</code></pre>","more":"<p>当遇到不熟悉的命令，man 是一个好习惯：</p>\n<pre><code>man chown\n-h, --no-dereference\n          affect  each  symbolic  link  instead  of any referenced file (useful only on systems that can\n          change the ownership of a symlink)\n</code></pre>"},{"title":"修改MySQL参数max_connections","date":"2019-10-17T03:38:51.000Z","_content":"编辑配置文件 /etc/my.cnf，修改以下参数：\n\n    max_connections=2000\n\nMySQL 管理员登陆 MySQL 数据库，在命令行执行：\n\n    mysql> set GLOBAL max_connections=3000;\n    Query OK, 0 rows affected (0.01 sec)\n    mysql> show variables like '%connections%';\n    +----------------------+-------+\n    | Variable_name        | Value |\n    +----------------------+-------+\n    | max_connections      | 3000  |\n    | max_user_connections | 0     |\n    +----------------------+-------+\n    2 rows in set (0.00 sec)","source":"_posts/修改MySQL参数max-connections.md","raw":"title: 修改MySQL参数max_connections\ndate: 2019-10-17 11:38:51\ntags:\n- MySQL\ncategories:\n- 数据库\n- MySQL\n---\n编辑配置文件 /etc/my.cnf，修改以下参数：\n\n    max_connections=2000\n\nMySQL 管理员登陆 MySQL 数据库，在命令行执行：\n\n    mysql> set GLOBAL max_connections=3000;\n    Query OK, 0 rows affected (0.01 sec)\n    mysql> show variables like '%connections%';\n    +----------------------+-------+\n    | Variable_name        | Value |\n    +----------------------+-------+\n    | max_connections      | 3000  |\n    | max_user_connections | 0     |\n    +----------------------+-------+\n    2 rows in set (0.00 sec)","slug":"修改MySQL参数max-connections","published":1,"updated":"2021-07-19T16:28:00.304Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphrr00fpitd31n4j1ily","content":"<p>编辑配置文件 /etc/my.cnf，修改以下参数：</p>\n<pre><code>max_connections=2000\n</code></pre>\n<p>MySQL 管理员登陆 MySQL 数据库，在命令行执行：</p>\n<pre><code>mysql&gt; set GLOBAL max_connections=3000;\nQuery OK, 0 rows affected (0.01 sec)\nmysql&gt; show variables like &#39;%connections%&#39;;\n+----------------------+-------+\n| Variable_name        | Value |\n+----------------------+-------+\n| max_connections      | 3000  |\n| max_user_connections | 0     |\n+----------------------+-------+\n2 rows in set (0.00 sec)\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>编辑配置文件 /etc/my.cnf，修改以下参数：</p>\n<pre><code>max_connections=2000\n</code></pre>\n<p>MySQL 管理员登陆 MySQL 数据库，在命令行执行：</p>\n<pre><code>mysql&gt; set GLOBAL max_connections=3000;\nQuery OK, 0 rows affected (0.01 sec)\nmysql&gt; show variables like &#39;%connections%&#39;;\n+----------------------+-------+\n| Variable_name        | Value |\n+----------------------+-------+\n| max_connections      | 3000  |\n| max_user_connections | 0     |\n+----------------------+-------+\n2 rows in set (0.00 sec)\n</code></pre>\n"},{"title":"博傻理论","date":"2020-05-19T14:54:34.000Z","_content":"\n博傻理论（greater fool theory)，是指在资本市场中（如股票、期货市场）：人们之所以完全不管某个东西的真实价值而愿意花高价购买，是因为他们预期会有一个更大的笨蛋会花更高的价格从他们那儿把它买走。博傻理论告诉人们的最重要的一个道理是：在这个世界上，傻不可怕，可怕的是做最后一个傻子。","source":"_posts/博傻理论.md","raw":"title: 博傻理论\ndate: 2020-05-19 22:54:34\ntags:\n- 经济学\ncategories:\n- 经济学\n---\n\n博傻理论（greater fool theory)，是指在资本市场中（如股票、期货市场）：人们之所以完全不管某个东西的真实价值而愿意花高价购买，是因为他们预期会有一个更大的笨蛋会花更高的价格从他们那儿把它买走。博傻理论告诉人们的最重要的一个道理是：在这个世界上，傻不可怕，可怕的是做最后一个傻子。","slug":"博傻理论","published":1,"updated":"2021-07-19T16:28:00.348Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphrs00ftitd3bmqcd5ml","content":"<p>博傻理论（greater fool theory)，是指在资本市场中（如股票、期货市场）：人们之所以完全不管某个东西的真实价值而愿意花高价购买，是因为他们预期会有一个更大的笨蛋会花更高的价格从他们那儿把它买走。博傻理论告诉人们的最重要的一个道理是：在这个世界上，傻不可怕，可怕的是做最后一个傻子。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>博傻理论（greater fool theory)，是指在资本市场中（如股票、期货市场）：人们之所以完全不管某个东西的真实价值而愿意花高价购买，是因为他们预期会有一个更大的笨蛋会花更高的价格从他们那儿把它买走。博傻理论告诉人们的最重要的一个道理是：在这个世界上，傻不可怕，可怕的是做最后一个傻子。</p>\n"},{"title":"在 Linux 平台上安装 64 位 JDK","date":"2016-06-20T15:38:53.000Z","_content":"\n这个过程使用二进制打包文件（.tar.gz）为 64 位 Linux 安装 Java 开发包（JDK）。\n\n<!-- more -->\n\n本教程使用文件：jdk-8uversion-linux-x64.tar.gz\n\n1. 下载文件\n\n下载地址：<http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html>\n\n下载前必须先同意许可协议。这个二进制文件可以被所有用户安装（不只是 root 用户）在任何有写入权限的位置。但是，只有 root 用户可以安装 JDK 到系统位置。\n\n2. 切换目录到要安装 JDK 的位置，然后移动二进制 .tar.gz 文件到当前目录。\n\n3. 打开 tar 包并安装 JDK\n\n    # tar zxvf jdk-8uversion-linux-x64.tar.gz\n\nJava 开发包文件被安装在当前目录下叫 jdk1.8.0_version 的目录中。\n\n4. 如果想节省磁盘空间则删除 .tar.gz 文件。\n\n5. 配置 Path\n\n使用 vi 编辑 profile 文件：\n\n    # vi /etc/profile\n\n添加以下内容：\n\n    JAVA_HOME=/usr/local/lib/jdk1.8.0_91\n    export JAVA_HOME\n\n    PATH=$PATH:$JAVA_HOME/bin\n    export PATH\n\n重新登陆或者使用 source 命令生效最新配置：\n\n    source /etc/profile\n\n6. 检查安装结果\n\n    # java -version\n    ava version \"1.8.0_91\"\n    Java(TM) SE Runtime Environment (build 1.8.0_91-b14)\n    Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode)\n\n如果看到类似上面的信息则说明安装成功。\n","source":"_posts/在-Linux-平台上安装-64-位-JDK.md","raw":"title: 在 Linux 平台上安装 64 位 JDK\ntags:\n  - Java\ncategories:\n  - 语言\n  - Java\ndate: 2016-06-20 23:38:53\n---\n\n这个过程使用二进制打包文件（.tar.gz）为 64 位 Linux 安装 Java 开发包（JDK）。\n\n<!-- more -->\n\n本教程使用文件：jdk-8uversion-linux-x64.tar.gz\n\n1. 下载文件\n\n下载地址：<http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html>\n\n下载前必须先同意许可协议。这个二进制文件可以被所有用户安装（不只是 root 用户）在任何有写入权限的位置。但是，只有 root 用户可以安装 JDK 到系统位置。\n\n2. 切换目录到要安装 JDK 的位置，然后移动二进制 .tar.gz 文件到当前目录。\n\n3. 打开 tar 包并安装 JDK\n\n    # tar zxvf jdk-8uversion-linux-x64.tar.gz\n\nJava 开发包文件被安装在当前目录下叫 jdk1.8.0_version 的目录中。\n\n4. 如果想节省磁盘空间则删除 .tar.gz 文件。\n\n5. 配置 Path\n\n使用 vi 编辑 profile 文件：\n\n    # vi /etc/profile\n\n添加以下内容：\n\n    JAVA_HOME=/usr/local/lib/jdk1.8.0_91\n    export JAVA_HOME\n\n    PATH=$PATH:$JAVA_HOME/bin\n    export PATH\n\n重新登陆或者使用 source 命令生效最新配置：\n\n    source /etc/profile\n\n6. 检查安装结果\n\n    # java -version\n    ava version \"1.8.0_91\"\n    Java(TM) SE Runtime Environment (build 1.8.0_91-b14)\n    Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode)\n\n如果看到类似上面的信息则说明安装成功。\n","slug":"在-Linux-平台上安装-64-位-JDK","published":1,"updated":"2021-07-19T16:28:00.304Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphrt00fxitd3fjrte5n6","content":"<p>这个过程使用二进制打包文件（.tar.gz）为 64 位 Linux 安装 Java 开发包（JDK）。</p>\n<span id=\"more\"></span>\n\n<p>本教程使用文件：jdk-8uversion-linux-x64.tar.gz</p>\n<ol>\n<li>下载文件</li>\n</ol>\n<p>下载地址：<a href=\"http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html\">http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html</a></p>\n<p>下载前必须先同意许可协议。这个二进制文件可以被所有用户安装（不只是 root 用户）在任何有写入权限的位置。但是，只有 root 用户可以安装 JDK 到系统位置。</p>\n<ol start=\"2\">\n<li><p>切换目录到要安装 JDK 的位置，然后移动二进制 .tar.gz 文件到当前目录。</p>\n</li>\n<li><p>打开 tar 包并安装 JDK</p>\n<h1 id=\"tar-zxvf-jdk-8uversion-linux-x64-tar-gz\"><a href=\"#tar-zxvf-jdk-8uversion-linux-x64-tar-gz\" class=\"headerlink\" title=\"tar zxvf jdk-8uversion-linux-x64.tar.gz\"></a>tar zxvf jdk-8uversion-linux-x64.tar.gz</h1></li>\n</ol>\n<p>Java 开发包文件被安装在当前目录下叫 jdk1.8.0_version 的目录中。</p>\n<ol start=\"4\">\n<li><p>如果想节省磁盘空间则删除 .tar.gz 文件。</p>\n</li>\n<li><p>配置 Path</p>\n</li>\n</ol>\n<p>使用 vi 编辑 profile 文件：</p>\n<pre><code># vi /etc/profile\n</code></pre>\n<p>添加以下内容：</p>\n<pre><code>JAVA_HOME=/usr/local/lib/jdk1.8.0_91\nexport JAVA_HOME\n\nPATH=$PATH:$JAVA_HOME/bin\nexport PATH\n</code></pre>\n<p>重新登陆或者使用 source 命令生效最新配置：</p>\n<pre><code>source /etc/profile\n</code></pre>\n<ol start=\"6\">\n<li><p>检查安装结果</p>\n<h1 id=\"java-version\"><a href=\"#java-version\" class=\"headerlink\" title=\"java -version\"></a>java -version</h1><p> ava version “1.8.0_91”<br> Java(TM) SE Runtime Environment (build 1.8.0_91-b14)<br> Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode)</p>\n</li>\n</ol>\n<p>如果看到类似上面的信息则说明安装成功。</p>\n","site":{"data":{}},"excerpt":"<p>这个过程使用二进制打包文件（.tar.gz）为 64 位 Linux 安装 Java 开发包（JDK）。</p>","more":"<p>本教程使用文件：jdk-8uversion-linux-x64.tar.gz</p>\n<ol>\n<li>下载文件</li>\n</ol>\n<p>下载地址：<a href=\"http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html\">http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html</a></p>\n<p>下载前必须先同意许可协议。这个二进制文件可以被所有用户安装（不只是 root 用户）在任何有写入权限的位置。但是，只有 root 用户可以安装 JDK 到系统位置。</p>\n<ol start=\"2\">\n<li><p>切换目录到要安装 JDK 的位置，然后移动二进制 .tar.gz 文件到当前目录。</p>\n</li>\n<li><p>打开 tar 包并安装 JDK</p>\n<h1 id=\"tar-zxvf-jdk-8uversion-linux-x64-tar-gz\"><a href=\"#tar-zxvf-jdk-8uversion-linux-x64-tar-gz\" class=\"headerlink\" title=\"tar zxvf jdk-8uversion-linux-x64.tar.gz\"></a>tar zxvf jdk-8uversion-linux-x64.tar.gz</h1></li>\n</ol>\n<p>Java 开发包文件被安装在当前目录下叫 jdk1.8.0_version 的目录中。</p>\n<ol start=\"4\">\n<li><p>如果想节省磁盘空间则删除 .tar.gz 文件。</p>\n</li>\n<li><p>配置 Path</p>\n</li>\n</ol>\n<p>使用 vi 编辑 profile 文件：</p>\n<pre><code># vi /etc/profile\n</code></pre>\n<p>添加以下内容：</p>\n<pre><code>JAVA_HOME=/usr/local/lib/jdk1.8.0_91\nexport JAVA_HOME\n\nPATH=$PATH:$JAVA_HOME/bin\nexport PATH\n</code></pre>\n<p>重新登陆或者使用 source 命令生效最新配置：</p>\n<pre><code>source /etc/profile\n</code></pre>\n<ol start=\"6\">\n<li><p>检查安装结果</p>\n<h1 id=\"java-version\"><a href=\"#java-version\" class=\"headerlink\" title=\"java -version\"></a>java -version</h1><p> ava version “1.8.0_91”<br> Java(TM) SE Runtime Environment (build 1.8.0_91-b14)<br> Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode)</p>\n</li>\n</ol>\n<p>如果看到类似上面的信息则说明安装成功。</p>"},{"title":"基于 Zookeeper 的 HDFS QJM（Quorum Journal Manager） HA 自动 Failover","date":"2017-04-05T09:04:26.000Z","_content":"\n### 组件\n\n#### Zookeeper\n\nHDFS 自动 failover 的实现基于 Zookeeper 的以下能力：\n\n- **失败检查**：每台 NameNode 服务器在 Zookeeper 中维持一个持久会话。如果服务器崩溃，Zookeeper 会话就会超时，并通知另外一台 NameNode 触发 failover。\n- **活跃 NameNode 选举**：Zookeeper 提供了一种简单机制唯一选举一台活跃节点。如果当前活跃的 NameNode 崩溃，另外一个台节点可以在 Zookeeper 中获取一个特定且唯一的锁，表明它应该变为下一个状态。\n\n<!-- more -->\n\n#### ZKFailoverController (ZKFC)\n\nZKFailoverController (ZKFC) 是一个 Zookeeper 的客户端，它负责监测和管理 NameNode 的状态。运行 NameNode 的机器也会运行一个 ZKFC。ZKFC 负责：\n\n- **健康监测**：ZKFC 使用一个健康检查命令周期的 ping 它本地的 NameNode。只要节点响应及时，ZKFC 认为 NameNode 是健康的。如果节点崩溃、卡住或进入不健康状态，健康监测会标识它是不健康的。\n\n- **ZooKeeper 会话管理**：本地 NameNode 健康时，ZKFC 在 Zookeeper 中保持一个打开的会话。如果本地的 NameNode 是活跃的，它也会保持一个特定的“lock”znode。这个锁使用了 Zookeeper 支持的“ephemeral”节点；如果会话过期，这个锁节点被自动删除。\n\n- **基于 Zookeeper 的选举**：如果本地 NameNode 是健康的，并且 ZKFC 发现当前没有其他节点保持锁 znode，它自己将尝试请求这个锁。如果成功，那么它“赢得选举”，并且负责运行 failover 来使它本地的 NameNode 活跃。failover 进程与之前描述的手动 failover 一样：首先，如果需要的话之前活跃的节点会被杀死；然后，本地 NameNode 变为活跃状态。\n\n### 部署 Zookeeper\n\n见我的另外一篇博客[Zookeeper 集群搭建](http://zhang-jc.github.io/2017/04/01/Zookeeper-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/)。\n\n### 停止集群\n\n在开始配置之前先停止集群服务。\n\n### 配置自动 failover\n\n#### hdfs-site.xml\n\n添加如下配置：\n\n    <property>\n      <name>dfs.ha.automatic-failover.enabled</name>\n      <value>true</value>\n    </property>\n\n#### core-site.xml\n\n添加如下配置：\n\n    <property>\n      <name>ha.zookeeper.quorum</name>\n      <value>frin-zookeeper1:2181,frin-zookeeper2:2181,frin-zookeeper3:2181</value>\n    </property>\n\n### 初始化 ZooKeeper 中的 HA 状态\n\n从一台 NameNode 主机上用以下命令初始化：\n\n    bin/hdfs zkfc -formatZK\n\n### 启动集群\n\n启动 NameNode 和 DataNode。\n\n在两台 NameNode 主机上使用下面的命令启动 zkfc 守护进程：\n\n    sbin/hadoop-daemon.sh start zkfc\n\n### 安全访问 ZooKeeper\n\n如果我们运行的是一个安全集群，那我们也希望确保存在 ZooKeeper 中的信息也是安全的。这可以防止恶意的客户端修改 ZooKeeper 中的元数据或触发一个潜在错误的 failover。\n\n#### core-site.xml\n\n为了保护 Zookeeper 中的信息，首先在 core-site.xml 中添加如下配置：\n\n    <property>\n      <name>ha.zookeeper.auth</name>\n      <value>@/usr/local/hadoop/etc/hadoop/zk-auth.txt</value>\n    </property>\n    <property>\n      <name>ha.zookeeper.acl</name>\n      <value>@/usr/local/hadoop/etc/hadoop/zk-acl.txt</value>\n    </property>\n\n注意，配置中的“@”字符表示配置不是内部的值，而是指向磁盘上的文件。\n\n第一个文件指定了 Zookeeper 授权列表，与 ZK CLI 中使用的格式一样。例如：\n\n    digest:hdfs-zkfcs:mypassword\n\nhdfs-zkfcs 是 Zookeeper 中一个唯一的用户名，mypassword 是密码。\n\n在 NameNode 节点 $HADOOP_HOME/etc/hadoop/ 目录下创建文件 zk-auth.txt 和 zk-acl.txt。zk-auth.txt 配置文件中的内容为：\n\n    digest:hdfs-zkfcs:frin-cluster-hdfs-zkfcs\n\n#### ZooKeeper ACL\n\n生成一个跟上面授权对应的 ZooKeeper ACL，使用下面的命令：\n\n    java -cp /usr/local/zookeeper/lib/*:/usr/local/zookeeper/zookeeper-3.4.10.jar org.apache.zookeeper.server.auth.DigestAuthenticationProvider hdfs-zkfcs:frin-cluster-hdfs-zkfcs\n\n拷贝输出内容中“->”之后的内容，粘贴到 zk-acls.txt 文件中，并且要以“digest:”作为前缀。例如：\n\n    digest:hdfs-zkfcs:+ub6J925c+Pbmbd74jrgDRKLe5o=:rwcda\n\n#### 初始化 Zookeeper 中的状态\n\n为了这些 ACL 生效，像上面描述的重新执行 zkfc -formatZK 命令。\n\n做完这些后，从 ZK CLI 确认 ACL，如下面：\n\n    [zk: frin-zookeeper1:2181(CONNECTED) 3] getAcl /hadoop-ha\n    'digest,'hdfs-zkfcs:+ub6J925c+Pbmbd74jrgDRKLe5o=\n    : cdrwa\n\n### 验证自动 failover\n\n从 WebUI 找出活跃的 NameNode，使用 kill -9 &lt;NameNode 进程的 PID&gt; 模拟 JVM 崩溃，也可以重启服务器或者使服务器断网。这些情况发生后，我们希望另外一台 NameNode 能在几秒钟内自动变为活跃状态。监测失败及触发 failover 需要的时间依赖 ha.zookeeper.session-timeout.ms 配置项，默认为 5 秒。\n\n### 自动 Failover FAQ\n\n#### 是否需要按照特定的顺序启动 ZKFC 和 NameNode？\n\n不。在任何一个给定的节点上，可以在相关的 NameNode 进程启动之前或者之后启动 ZKFC。\n\n#### 还应该添加什么样的监测？\n\n应该在运行 NameNode 的主机上添加监测确保 ZKFC 保持运行。例如，在 Zookeeper 失败的一些场景，ZKFC 可能异常退出，应该重启 ZKFC 确保系统准备好 failover。\n\n另外，应该监测 Zookeeper 集群的每台服务器。如果 Zookeeper 崩溃，那么自动 failover 将不起作用。\n\n#### 如果 Zookeeper 停止会发生什么？\n\n如果 ZooKeeper 集群崩溃，自动 failover 将不会被触发。然而，HDFS 会继续运行不受影响。当 Zookeeper 重新启动，HDFS 会重新连接。\n\n#### 是否可以指定主/优先的 NameNode？\n\n不。当前，这是不支持的。哪个 NameNode 先启动哪个节点将变为活跃的。可以选择以特定的顺序启动集群，先启动作为优先节点的 NameNode 节点。\n\n#### 当配置了自动 failover 后，如何发起手动 failover？\n\n即使配置了自动 failover，可以使用同样的 hdfs haadmin 命令发起手动 failover。它将执行相应的 failover。\n\n### 启用 HA 的 HDFS 升级/定型/回退\n\n当切换 HDFS 的版本时，有时新的软件可以简单的安装并重启集群。然而，有时升级正在运行的 HDFS 版本时需要修改磁盘上的数据。这种情况下，必须在安装完新软件后使用 HDFS 的升级/定型/回退工具。在  HA 的情况下这个过程会变得复杂，因为  NN 依赖的磁盘上的元数据是分布定义的，在一对 HA NN 上，在使用 QJM 共享 edits 的情况下也存储在 JournalNode 上。这一节描述在 HA 的情况下使用 HDFS 升级/定型/回退工具的过程。\n\n#### 执行 HA 升级\n\n必须做下面的操作：\n\n1. 正常关闭所有 NN，并安装新软件。\n2. 启动所有的 JN。注意，当执行升级、回退或者定型操作时所有的 JN 都是运行的，这是极为重要的。如果在运行这些操作时任何 JN 停止了，那么操作都会失败。\n3. 使用 -upgrade 标志启动一台 NN。\n4. 开始，在 HA 情况下这台 NN 通常不会进入 standby 状态。而是，这台 NN 会立即进入 active 状态，执行它本地存储目录的更新，并执行共享 edit 日志的更新。\n5. 此时，另外一台 NN 与已经升级的 NN 是不同步的。为了同步及恢复 HA 安装，需要使用 -bootstrapStandby 标志重新引导这台 NN 运行。使用 -upgrade 标志启动这台 NN 是错误的。\n\n注意，在定型或者回退升级之前的任何时候想重启 NameNode，应该正常启动 NN，不使用任何特殊的启动标志。\n\n#### 定型 HA 升级\n\n当 NN 都正在运行且一台为 active，操作者应该使用 hdfs dfsadmin -finalizeUpgrade 命令定型 HA 升级。此时，活跃的 NN 将执行共享 edit 日志的定型，并且包含之前 FS 状态的存储目录的这台 NN 会删除它本地的状态。\n\n#### 执行升级回退\n\n停止两台 NN。在发起升级过程的那台 NN 上运行回退命令，它将在本地目录、共享日志、NFS 或 JN 上执行回退。之后，启动这台 NN，并在另外一个 NN 上运行 -bootstrapStandby，以使两台 NN 同步到回退后的文件系统状态。\n\n### 问题排查\n\n#### code:NOAUTH\n\n在测试自动切换时未能成功，检查 zkfc 的 log 发现以下错误信息：\n\n    java.lang.RuntimeException: ZK Failover Controller failed: Received create error from Zookeeper. code:NOAUTH for path /hadoop-ha/frin-cluster/ActiveStandbyElectorLock\n\n可以发现是因为没有授权所以访问 Zookeeper 中的锁信息失败。经排查是配置安全访问 Zookeeper 时修改了 core-site.xml 中的配置，但配置没有分发到其他服务器。\n\n#### .ssh/id_rsa\n\n在测试自动切换时未能成功，检查 zkfc 的 log 发现以下错误信息：\n\n    2017-04-05 15:01:06,635 WARN org.apache.hadoop.ha.SshFenceByTcpPort: Unable to create SSH session\n    com.jcraft.jsch.JSchException: java.io.FileNotFoundException: /home/hadoop/.ssh/id_rsa (No such file or directory)\n            at com.jcraft.jsch.IdentityFile.newInstance(IdentityFile.java:98)\n            at com.jcraft.jsch.JSch.addIdentity(JSch.java:206)\n            at com.jcraft.jsch.JSch.addIdentity(JSch.java:192)\n            at org.apache.hadoop.ha.SshFenceByTcpPort.createSession(SshFenceByTcpPort.java:122)\n            at org.apache.hadoop.ha.SshFenceByTcpPort.tryFence(SshFenceByTcpPort.java:91)\n            at org.apache.hadoop.ha.NodeFencer.fence(NodeFencer.java:97)\n            at org.apache.hadoop.ha.ZKFailoverController.doFence(ZKFailoverController.java:532)\n            at org.apache.hadoop.ha.ZKFailoverController.fenceOldActive(ZKFailoverController.java:505)\n            at org.apache.hadoop.ha.ZKFailoverController.access$1100(ZKFailoverController.java:61)\n            at org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks.fenceOldActive(ZKFailoverController.java:892)\n            at org.apache.hadoop.ha.ActiveStandbyElector.fenceOldActive(ActiveStandbyElector.java:910)\n            at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:809)\n            at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:418)\n            at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)\n            at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)\n    Caused by: java.io.FileNotFoundException: /home/hadoop/.ssh/id_rsa (No such file or directory)\n            at java.io.FileInputStream.open(Native Method)\n            at java.io.FileInputStream.<init>(FileInputStream.java:146)\n            at java.io.FileInputStream.<init>(FileInputStream.java:101)\n            at com.jcraft.jsch.IdentityFile.newInstance(IdentityFile.java:83)\n            ... 14 more\n\n检查发现 NameNode2 服务器未创建 ssh key，使用命令 ssh-keygen -t rsa 命令创建，再次测试，出现如下报错信息：\n\n    2017-04-05 15:12:23,351 WARN org.apache.hadoop.ha.SshFenceByTcpPort: Unable to connect to frin-namenode1 as user hadoop\n    com.jcraft.jsch.JSchException: Auth fail\n            at com.jcraft.jsch.Session.connect(Session.java:452)\n            at org.apache.hadoop.ha.SshFenceByTcpPort.tryFence(SshFenceByTcpPort.java:100)\n            at org.apache.hadoop.ha.NodeFencer.fence(NodeFencer.java:97)\n            at org.apache.hadoop.ha.ZKFailoverController.doFence(ZKFailoverController.java:532)\n            at org.apache.hadoop.ha.ZKFailoverController.fenceOldActive(ZKFailoverController.java:505)\n            at org.apache.hadoop.ha.ZKFailoverController.access$1100(ZKFailoverController.java:61)\n            at org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks.fenceOldActive(ZKFailoverController.java:892)\n            at org.apache.hadoop.ha.ActiveStandbyElector.fenceOldActive(ActiveStandbyElector.java:910)\n            at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:809)\n            at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:418)\n            at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)\n            at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)\n\n配置 NameNode2 到 NameNode1 SSH 免密钥登陆后，自动切换成功。","source":"_posts/基于-Zookeeper-的-HDFS-QJM（Quorum-Journal-Manager）-HA-自动-Failover.md","raw":"title: 基于 Zookeeper 的 HDFS QJM（Quorum Journal Manager） HA 自动 Failover\ntags:\n  - HDFS\n  - Zookeeper\ncategories:\n  - 大数据\n  - Hadoop\ndate: 2017-04-05 17:04:26\n---\n\n### 组件\n\n#### Zookeeper\n\nHDFS 自动 failover 的实现基于 Zookeeper 的以下能力：\n\n- **失败检查**：每台 NameNode 服务器在 Zookeeper 中维持一个持久会话。如果服务器崩溃，Zookeeper 会话就会超时，并通知另外一台 NameNode 触发 failover。\n- **活跃 NameNode 选举**：Zookeeper 提供了一种简单机制唯一选举一台活跃节点。如果当前活跃的 NameNode 崩溃，另外一个台节点可以在 Zookeeper 中获取一个特定且唯一的锁，表明它应该变为下一个状态。\n\n<!-- more -->\n\n#### ZKFailoverController (ZKFC)\n\nZKFailoverController (ZKFC) 是一个 Zookeeper 的客户端，它负责监测和管理 NameNode 的状态。运行 NameNode 的机器也会运行一个 ZKFC。ZKFC 负责：\n\n- **健康监测**：ZKFC 使用一个健康检查命令周期的 ping 它本地的 NameNode。只要节点响应及时，ZKFC 认为 NameNode 是健康的。如果节点崩溃、卡住或进入不健康状态，健康监测会标识它是不健康的。\n\n- **ZooKeeper 会话管理**：本地 NameNode 健康时，ZKFC 在 Zookeeper 中保持一个打开的会话。如果本地的 NameNode 是活跃的，它也会保持一个特定的“lock”znode。这个锁使用了 Zookeeper 支持的“ephemeral”节点；如果会话过期，这个锁节点被自动删除。\n\n- **基于 Zookeeper 的选举**：如果本地 NameNode 是健康的，并且 ZKFC 发现当前没有其他节点保持锁 znode，它自己将尝试请求这个锁。如果成功，那么它“赢得选举”，并且负责运行 failover 来使它本地的 NameNode 活跃。failover 进程与之前描述的手动 failover 一样：首先，如果需要的话之前活跃的节点会被杀死；然后，本地 NameNode 变为活跃状态。\n\n### 部署 Zookeeper\n\n见我的另外一篇博客[Zookeeper 集群搭建](http://zhang-jc.github.io/2017/04/01/Zookeeper-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/)。\n\n### 停止集群\n\n在开始配置之前先停止集群服务。\n\n### 配置自动 failover\n\n#### hdfs-site.xml\n\n添加如下配置：\n\n    <property>\n      <name>dfs.ha.automatic-failover.enabled</name>\n      <value>true</value>\n    </property>\n\n#### core-site.xml\n\n添加如下配置：\n\n    <property>\n      <name>ha.zookeeper.quorum</name>\n      <value>frin-zookeeper1:2181,frin-zookeeper2:2181,frin-zookeeper3:2181</value>\n    </property>\n\n### 初始化 ZooKeeper 中的 HA 状态\n\n从一台 NameNode 主机上用以下命令初始化：\n\n    bin/hdfs zkfc -formatZK\n\n### 启动集群\n\n启动 NameNode 和 DataNode。\n\n在两台 NameNode 主机上使用下面的命令启动 zkfc 守护进程：\n\n    sbin/hadoop-daemon.sh start zkfc\n\n### 安全访问 ZooKeeper\n\n如果我们运行的是一个安全集群，那我们也希望确保存在 ZooKeeper 中的信息也是安全的。这可以防止恶意的客户端修改 ZooKeeper 中的元数据或触发一个潜在错误的 failover。\n\n#### core-site.xml\n\n为了保护 Zookeeper 中的信息，首先在 core-site.xml 中添加如下配置：\n\n    <property>\n      <name>ha.zookeeper.auth</name>\n      <value>@/usr/local/hadoop/etc/hadoop/zk-auth.txt</value>\n    </property>\n    <property>\n      <name>ha.zookeeper.acl</name>\n      <value>@/usr/local/hadoop/etc/hadoop/zk-acl.txt</value>\n    </property>\n\n注意，配置中的“@”字符表示配置不是内部的值，而是指向磁盘上的文件。\n\n第一个文件指定了 Zookeeper 授权列表，与 ZK CLI 中使用的格式一样。例如：\n\n    digest:hdfs-zkfcs:mypassword\n\nhdfs-zkfcs 是 Zookeeper 中一个唯一的用户名，mypassword 是密码。\n\n在 NameNode 节点 $HADOOP_HOME/etc/hadoop/ 目录下创建文件 zk-auth.txt 和 zk-acl.txt。zk-auth.txt 配置文件中的内容为：\n\n    digest:hdfs-zkfcs:frin-cluster-hdfs-zkfcs\n\n#### ZooKeeper ACL\n\n生成一个跟上面授权对应的 ZooKeeper ACL，使用下面的命令：\n\n    java -cp /usr/local/zookeeper/lib/*:/usr/local/zookeeper/zookeeper-3.4.10.jar org.apache.zookeeper.server.auth.DigestAuthenticationProvider hdfs-zkfcs:frin-cluster-hdfs-zkfcs\n\n拷贝输出内容中“->”之后的内容，粘贴到 zk-acls.txt 文件中，并且要以“digest:”作为前缀。例如：\n\n    digest:hdfs-zkfcs:+ub6J925c+Pbmbd74jrgDRKLe5o=:rwcda\n\n#### 初始化 Zookeeper 中的状态\n\n为了这些 ACL 生效，像上面描述的重新执行 zkfc -formatZK 命令。\n\n做完这些后，从 ZK CLI 确认 ACL，如下面：\n\n    [zk: frin-zookeeper1:2181(CONNECTED) 3] getAcl /hadoop-ha\n    'digest,'hdfs-zkfcs:+ub6J925c+Pbmbd74jrgDRKLe5o=\n    : cdrwa\n\n### 验证自动 failover\n\n从 WebUI 找出活跃的 NameNode，使用 kill -9 &lt;NameNode 进程的 PID&gt; 模拟 JVM 崩溃，也可以重启服务器或者使服务器断网。这些情况发生后，我们希望另外一台 NameNode 能在几秒钟内自动变为活跃状态。监测失败及触发 failover 需要的时间依赖 ha.zookeeper.session-timeout.ms 配置项，默认为 5 秒。\n\n### 自动 Failover FAQ\n\n#### 是否需要按照特定的顺序启动 ZKFC 和 NameNode？\n\n不。在任何一个给定的节点上，可以在相关的 NameNode 进程启动之前或者之后启动 ZKFC。\n\n#### 还应该添加什么样的监测？\n\n应该在运行 NameNode 的主机上添加监测确保 ZKFC 保持运行。例如，在 Zookeeper 失败的一些场景，ZKFC 可能异常退出，应该重启 ZKFC 确保系统准备好 failover。\n\n另外，应该监测 Zookeeper 集群的每台服务器。如果 Zookeeper 崩溃，那么自动 failover 将不起作用。\n\n#### 如果 Zookeeper 停止会发生什么？\n\n如果 ZooKeeper 集群崩溃，自动 failover 将不会被触发。然而，HDFS 会继续运行不受影响。当 Zookeeper 重新启动，HDFS 会重新连接。\n\n#### 是否可以指定主/优先的 NameNode？\n\n不。当前，这是不支持的。哪个 NameNode 先启动哪个节点将变为活跃的。可以选择以特定的顺序启动集群，先启动作为优先节点的 NameNode 节点。\n\n#### 当配置了自动 failover 后，如何发起手动 failover？\n\n即使配置了自动 failover，可以使用同样的 hdfs haadmin 命令发起手动 failover。它将执行相应的 failover。\n\n### 启用 HA 的 HDFS 升级/定型/回退\n\n当切换 HDFS 的版本时，有时新的软件可以简单的安装并重启集群。然而，有时升级正在运行的 HDFS 版本时需要修改磁盘上的数据。这种情况下，必须在安装完新软件后使用 HDFS 的升级/定型/回退工具。在  HA 的情况下这个过程会变得复杂，因为  NN 依赖的磁盘上的元数据是分布定义的，在一对 HA NN 上，在使用 QJM 共享 edits 的情况下也存储在 JournalNode 上。这一节描述在 HA 的情况下使用 HDFS 升级/定型/回退工具的过程。\n\n#### 执行 HA 升级\n\n必须做下面的操作：\n\n1. 正常关闭所有 NN，并安装新软件。\n2. 启动所有的 JN。注意，当执行升级、回退或者定型操作时所有的 JN 都是运行的，这是极为重要的。如果在运行这些操作时任何 JN 停止了，那么操作都会失败。\n3. 使用 -upgrade 标志启动一台 NN。\n4. 开始，在 HA 情况下这台 NN 通常不会进入 standby 状态。而是，这台 NN 会立即进入 active 状态，执行它本地存储目录的更新，并执行共享 edit 日志的更新。\n5. 此时，另外一台 NN 与已经升级的 NN 是不同步的。为了同步及恢复 HA 安装，需要使用 -bootstrapStandby 标志重新引导这台 NN 运行。使用 -upgrade 标志启动这台 NN 是错误的。\n\n注意，在定型或者回退升级之前的任何时候想重启 NameNode，应该正常启动 NN，不使用任何特殊的启动标志。\n\n#### 定型 HA 升级\n\n当 NN 都正在运行且一台为 active，操作者应该使用 hdfs dfsadmin -finalizeUpgrade 命令定型 HA 升级。此时，活跃的 NN 将执行共享 edit 日志的定型，并且包含之前 FS 状态的存储目录的这台 NN 会删除它本地的状态。\n\n#### 执行升级回退\n\n停止两台 NN。在发起升级过程的那台 NN 上运行回退命令，它将在本地目录、共享日志、NFS 或 JN 上执行回退。之后，启动这台 NN，并在另外一个 NN 上运行 -bootstrapStandby，以使两台 NN 同步到回退后的文件系统状态。\n\n### 问题排查\n\n#### code:NOAUTH\n\n在测试自动切换时未能成功，检查 zkfc 的 log 发现以下错误信息：\n\n    java.lang.RuntimeException: ZK Failover Controller failed: Received create error from Zookeeper. code:NOAUTH for path /hadoop-ha/frin-cluster/ActiveStandbyElectorLock\n\n可以发现是因为没有授权所以访问 Zookeeper 中的锁信息失败。经排查是配置安全访问 Zookeeper 时修改了 core-site.xml 中的配置，但配置没有分发到其他服务器。\n\n#### .ssh/id_rsa\n\n在测试自动切换时未能成功，检查 zkfc 的 log 发现以下错误信息：\n\n    2017-04-05 15:01:06,635 WARN org.apache.hadoop.ha.SshFenceByTcpPort: Unable to create SSH session\n    com.jcraft.jsch.JSchException: java.io.FileNotFoundException: /home/hadoop/.ssh/id_rsa (No such file or directory)\n            at com.jcraft.jsch.IdentityFile.newInstance(IdentityFile.java:98)\n            at com.jcraft.jsch.JSch.addIdentity(JSch.java:206)\n            at com.jcraft.jsch.JSch.addIdentity(JSch.java:192)\n            at org.apache.hadoop.ha.SshFenceByTcpPort.createSession(SshFenceByTcpPort.java:122)\n            at org.apache.hadoop.ha.SshFenceByTcpPort.tryFence(SshFenceByTcpPort.java:91)\n            at org.apache.hadoop.ha.NodeFencer.fence(NodeFencer.java:97)\n            at org.apache.hadoop.ha.ZKFailoverController.doFence(ZKFailoverController.java:532)\n            at org.apache.hadoop.ha.ZKFailoverController.fenceOldActive(ZKFailoverController.java:505)\n            at org.apache.hadoop.ha.ZKFailoverController.access$1100(ZKFailoverController.java:61)\n            at org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks.fenceOldActive(ZKFailoverController.java:892)\n            at org.apache.hadoop.ha.ActiveStandbyElector.fenceOldActive(ActiveStandbyElector.java:910)\n            at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:809)\n            at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:418)\n            at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)\n            at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)\n    Caused by: java.io.FileNotFoundException: /home/hadoop/.ssh/id_rsa (No such file or directory)\n            at java.io.FileInputStream.open(Native Method)\n            at java.io.FileInputStream.<init>(FileInputStream.java:146)\n            at java.io.FileInputStream.<init>(FileInputStream.java:101)\n            at com.jcraft.jsch.IdentityFile.newInstance(IdentityFile.java:83)\n            ... 14 more\n\n检查发现 NameNode2 服务器未创建 ssh key，使用命令 ssh-keygen -t rsa 命令创建，再次测试，出现如下报错信息：\n\n    2017-04-05 15:12:23,351 WARN org.apache.hadoop.ha.SshFenceByTcpPort: Unable to connect to frin-namenode1 as user hadoop\n    com.jcraft.jsch.JSchException: Auth fail\n            at com.jcraft.jsch.Session.connect(Session.java:452)\n            at org.apache.hadoop.ha.SshFenceByTcpPort.tryFence(SshFenceByTcpPort.java:100)\n            at org.apache.hadoop.ha.NodeFencer.fence(NodeFencer.java:97)\n            at org.apache.hadoop.ha.ZKFailoverController.doFence(ZKFailoverController.java:532)\n            at org.apache.hadoop.ha.ZKFailoverController.fenceOldActive(ZKFailoverController.java:505)\n            at org.apache.hadoop.ha.ZKFailoverController.access$1100(ZKFailoverController.java:61)\n            at org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks.fenceOldActive(ZKFailoverController.java:892)\n            at org.apache.hadoop.ha.ActiveStandbyElector.fenceOldActive(ActiveStandbyElector.java:910)\n            at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:809)\n            at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:418)\n            at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)\n            at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)\n\n配置 NameNode2 到 NameNode1 SSH 免密钥登陆后，自动切换成功。","slug":"基于-Zookeeper-的-HDFS-QJM（Quorum-Journal-Manager）-HA-自动-Failover","published":1,"updated":"2021-07-19T16:28:00.308Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphrv00g1itd3hemehpk9","content":"<h3 id=\"组件\"><a href=\"#组件\" class=\"headerlink\" title=\"组件\"></a>组件</h3><h4 id=\"Zookeeper\"><a href=\"#Zookeeper\" class=\"headerlink\" title=\"Zookeeper\"></a>Zookeeper</h4><p>HDFS 自动 failover 的实现基于 Zookeeper 的以下能力：</p>\n<ul>\n<li><strong>失败检查</strong>：每台 NameNode 服务器在 Zookeeper 中维持一个持久会话。如果服务器崩溃，Zookeeper 会话就会超时，并通知另外一台 NameNode 触发 failover。</li>\n<li><strong>活跃 NameNode 选举</strong>：Zookeeper 提供了一种简单机制唯一选举一台活跃节点。如果当前活跃的 NameNode 崩溃，另外一个台节点可以在 Zookeeper 中获取一个特定且唯一的锁，表明它应该变为下一个状态。</li>\n</ul>\n<span id=\"more\"></span>\n\n<h4 id=\"ZKFailoverController-ZKFC\"><a href=\"#ZKFailoverController-ZKFC\" class=\"headerlink\" title=\"ZKFailoverController (ZKFC)\"></a>ZKFailoverController (ZKFC)</h4><p>ZKFailoverController (ZKFC) 是一个 Zookeeper 的客户端，它负责监测和管理 NameNode 的状态。运行 NameNode 的机器也会运行一个 ZKFC。ZKFC 负责：</p>\n<ul>\n<li><p><strong>健康监测</strong>：ZKFC 使用一个健康检查命令周期的 ping 它本地的 NameNode。只要节点响应及时，ZKFC 认为 NameNode 是健康的。如果节点崩溃、卡住或进入不健康状态，健康监测会标识它是不健康的。</p>\n</li>\n<li><p><strong>ZooKeeper 会话管理</strong>：本地 NameNode 健康时，ZKFC 在 Zookeeper 中保持一个打开的会话。如果本地的 NameNode 是活跃的，它也会保持一个特定的“lock”znode。这个锁使用了 Zookeeper 支持的“ephemeral”节点；如果会话过期，这个锁节点被自动删除。</p>\n</li>\n<li><p><strong>基于 Zookeeper 的选举</strong>：如果本地 NameNode 是健康的，并且 ZKFC 发现当前没有其他节点保持锁 znode，它自己将尝试请求这个锁。如果成功，那么它“赢得选举”，并且负责运行 failover 来使它本地的 NameNode 活跃。failover 进程与之前描述的手动 failover 一样：首先，如果需要的话之前活跃的节点会被杀死；然后，本地 NameNode 变为活跃状态。</p>\n</li>\n</ul>\n<h3 id=\"部署-Zookeeper\"><a href=\"#部署-Zookeeper\" class=\"headerlink\" title=\"部署 Zookeeper\"></a>部署 Zookeeper</h3><p>见我的另外一篇博客<a href=\"http://zhang-jc.github.io/2017/04/01/Zookeeper-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/\">Zookeeper 集群搭建</a>。</p>\n<h3 id=\"停止集群\"><a href=\"#停止集群\" class=\"headerlink\" title=\"停止集群\"></a>停止集群</h3><p>在开始配置之前先停止集群服务。</p>\n<h3 id=\"配置自动-failover\"><a href=\"#配置自动-failover\" class=\"headerlink\" title=\"配置自动 failover\"></a>配置自动 failover</h3><h4 id=\"hdfs-site-xml\"><a href=\"#hdfs-site-xml\" class=\"headerlink\" title=\"hdfs-site.xml\"></a>hdfs-site.xml</h4><p>添加如下配置：</p>\n<pre><code>&lt;property&gt;\n  &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;\n  &lt;value&gt;true&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<h4 id=\"core-site-xml\"><a href=\"#core-site-xml\" class=\"headerlink\" title=\"core-site.xml\"></a>core-site.xml</h4><p>添加如下配置：</p>\n<pre><code>&lt;property&gt;\n  &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;\n  &lt;value&gt;frin-zookeeper1:2181,frin-zookeeper2:2181,frin-zookeeper3:2181&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<h3 id=\"初始化-ZooKeeper-中的-HA-状态\"><a href=\"#初始化-ZooKeeper-中的-HA-状态\" class=\"headerlink\" title=\"初始化 ZooKeeper 中的 HA 状态\"></a>初始化 ZooKeeper 中的 HA 状态</h3><p>从一台 NameNode 主机上用以下命令初始化：</p>\n<pre><code>bin/hdfs zkfc -formatZK\n</code></pre>\n<h3 id=\"启动集群\"><a href=\"#启动集群\" class=\"headerlink\" title=\"启动集群\"></a>启动集群</h3><p>启动 NameNode 和 DataNode。</p>\n<p>在两台 NameNode 主机上使用下面的命令启动 zkfc 守护进程：</p>\n<pre><code>sbin/hadoop-daemon.sh start zkfc\n</code></pre>\n<h3 id=\"安全访问-ZooKeeper\"><a href=\"#安全访问-ZooKeeper\" class=\"headerlink\" title=\"安全访问 ZooKeeper\"></a>安全访问 ZooKeeper</h3><p>如果我们运行的是一个安全集群，那我们也希望确保存在 ZooKeeper 中的信息也是安全的。这可以防止恶意的客户端修改 ZooKeeper 中的元数据或触发一个潜在错误的 failover。</p>\n<h4 id=\"core-site-xml-1\"><a href=\"#core-site-xml-1\" class=\"headerlink\" title=\"core-site.xml\"></a>core-site.xml</h4><p>为了保护 Zookeeper 中的信息，首先在 core-site.xml 中添加如下配置：</p>\n<pre><code>&lt;property&gt;\n  &lt;name&gt;ha.zookeeper.auth&lt;/name&gt;\n  &lt;value&gt;@/usr/local/hadoop/etc/hadoop/zk-auth.txt&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;ha.zookeeper.acl&lt;/name&gt;\n  &lt;value&gt;@/usr/local/hadoop/etc/hadoop/zk-acl.txt&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<p>注意，配置中的“@”字符表示配置不是内部的值，而是指向磁盘上的文件。</p>\n<p>第一个文件指定了 Zookeeper 授权列表，与 ZK CLI 中使用的格式一样。例如：</p>\n<pre><code>digest:hdfs-zkfcs:mypassword\n</code></pre>\n<p>hdfs-zkfcs 是 Zookeeper 中一个唯一的用户名，mypassword 是密码。</p>\n<p>在 NameNode 节点 $HADOOP_HOME/etc/hadoop/ 目录下创建文件 zk-auth.txt 和 zk-acl.txt。zk-auth.txt 配置文件中的内容为：</p>\n<pre><code>digest:hdfs-zkfcs:frin-cluster-hdfs-zkfcs\n</code></pre>\n<h4 id=\"ZooKeeper-ACL\"><a href=\"#ZooKeeper-ACL\" class=\"headerlink\" title=\"ZooKeeper ACL\"></a>ZooKeeper ACL</h4><p>生成一个跟上面授权对应的 ZooKeeper ACL，使用下面的命令：</p>\n<pre><code>java -cp /usr/local/zookeeper/lib/*:/usr/local/zookeeper/zookeeper-3.4.10.jar org.apache.zookeeper.server.auth.DigestAuthenticationProvider hdfs-zkfcs:frin-cluster-hdfs-zkfcs\n</code></pre>\n<p>拷贝输出内容中“-&gt;”之后的内容，粘贴到 zk-acls.txt 文件中，并且要以“digest:”作为前缀。例如：</p>\n<pre><code>digest:hdfs-zkfcs:+ub6J925c+Pbmbd74jrgDRKLe5o=:rwcda\n</code></pre>\n<h4 id=\"初始化-Zookeeper-中的状态\"><a href=\"#初始化-Zookeeper-中的状态\" class=\"headerlink\" title=\"初始化 Zookeeper 中的状态\"></a>初始化 Zookeeper 中的状态</h4><p>为了这些 ACL 生效，像上面描述的重新执行 zkfc -formatZK 命令。</p>\n<p>做完这些后，从 ZK CLI 确认 ACL，如下面：</p>\n<pre><code>[zk: frin-zookeeper1:2181(CONNECTED) 3] getAcl /hadoop-ha\n&#39;digest,&#39;hdfs-zkfcs:+ub6J925c+Pbmbd74jrgDRKLe5o=\n: cdrwa\n</code></pre>\n<h3 id=\"验证自动-failover\"><a href=\"#验证自动-failover\" class=\"headerlink\" title=\"验证自动 failover\"></a>验证自动 failover</h3><p>从 WebUI 找出活跃的 NameNode，使用 kill -9 &lt;NameNode 进程的 PID&gt; 模拟 JVM 崩溃，也可以重启服务器或者使服务器断网。这些情况发生后，我们希望另外一台 NameNode 能在几秒钟内自动变为活跃状态。监测失败及触发 failover 需要的时间依赖 ha.zookeeper.session-timeout.ms 配置项，默认为 5 秒。</p>\n<h3 id=\"自动-Failover-FAQ\"><a href=\"#自动-Failover-FAQ\" class=\"headerlink\" title=\"自动 Failover FAQ\"></a>自动 Failover FAQ</h3><h4 id=\"是否需要按照特定的顺序启动-ZKFC-和-NameNode？\"><a href=\"#是否需要按照特定的顺序启动-ZKFC-和-NameNode？\" class=\"headerlink\" title=\"是否需要按照特定的顺序启动 ZKFC 和 NameNode？\"></a>是否需要按照特定的顺序启动 ZKFC 和 NameNode？</h4><p>不。在任何一个给定的节点上，可以在相关的 NameNode 进程启动之前或者之后启动 ZKFC。</p>\n<h4 id=\"还应该添加什么样的监测？\"><a href=\"#还应该添加什么样的监测？\" class=\"headerlink\" title=\"还应该添加什么样的监测？\"></a>还应该添加什么样的监测？</h4><p>应该在运行 NameNode 的主机上添加监测确保 ZKFC 保持运行。例如，在 Zookeeper 失败的一些场景，ZKFC 可能异常退出，应该重启 ZKFC 确保系统准备好 failover。</p>\n<p>另外，应该监测 Zookeeper 集群的每台服务器。如果 Zookeeper 崩溃，那么自动 failover 将不起作用。</p>\n<h4 id=\"如果-Zookeeper-停止会发生什么？\"><a href=\"#如果-Zookeeper-停止会发生什么？\" class=\"headerlink\" title=\"如果 Zookeeper 停止会发生什么？\"></a>如果 Zookeeper 停止会发生什么？</h4><p>如果 ZooKeeper 集群崩溃，自动 failover 将不会被触发。然而，HDFS 会继续运行不受影响。当 Zookeeper 重新启动，HDFS 会重新连接。</p>\n<h4 id=\"是否可以指定主-优先的-NameNode？\"><a href=\"#是否可以指定主-优先的-NameNode？\" class=\"headerlink\" title=\"是否可以指定主/优先的 NameNode？\"></a>是否可以指定主/优先的 NameNode？</h4><p>不。当前，这是不支持的。哪个 NameNode 先启动哪个节点将变为活跃的。可以选择以特定的顺序启动集群，先启动作为优先节点的 NameNode 节点。</p>\n<h4 id=\"当配置了自动-failover-后，如何发起手动-failover？\"><a href=\"#当配置了自动-failover-后，如何发起手动-failover？\" class=\"headerlink\" title=\"当配置了自动 failover 后，如何发起手动 failover？\"></a>当配置了自动 failover 后，如何发起手动 failover？</h4><p>即使配置了自动 failover，可以使用同样的 hdfs haadmin 命令发起手动 failover。它将执行相应的 failover。</p>\n<h3 id=\"启用-HA-的-HDFS-升级-定型-回退\"><a href=\"#启用-HA-的-HDFS-升级-定型-回退\" class=\"headerlink\" title=\"启用 HA 的 HDFS 升级/定型/回退\"></a>启用 HA 的 HDFS 升级/定型/回退</h3><p>当切换 HDFS 的版本时，有时新的软件可以简单的安装并重启集群。然而，有时升级正在运行的 HDFS 版本时需要修改磁盘上的数据。这种情况下，必须在安装完新软件后使用 HDFS 的升级/定型/回退工具。在  HA 的情况下这个过程会变得复杂，因为  NN 依赖的磁盘上的元数据是分布定义的，在一对 HA NN 上，在使用 QJM 共享 edits 的情况下也存储在 JournalNode 上。这一节描述在 HA 的情况下使用 HDFS 升级/定型/回退工具的过程。</p>\n<h4 id=\"执行-HA-升级\"><a href=\"#执行-HA-升级\" class=\"headerlink\" title=\"执行 HA 升级\"></a>执行 HA 升级</h4><p>必须做下面的操作：</p>\n<ol>\n<li>正常关闭所有 NN，并安装新软件。</li>\n<li>启动所有的 JN。注意，当执行升级、回退或者定型操作时所有的 JN 都是运行的，这是极为重要的。如果在运行这些操作时任何 JN 停止了，那么操作都会失败。</li>\n<li>使用 -upgrade 标志启动一台 NN。</li>\n<li>开始，在 HA 情况下这台 NN 通常不会进入 standby 状态。而是，这台 NN 会立即进入 active 状态，执行它本地存储目录的更新，并执行共享 edit 日志的更新。</li>\n<li>此时，另外一台 NN 与已经升级的 NN 是不同步的。为了同步及恢复 HA 安装，需要使用 -bootstrapStandby 标志重新引导这台 NN 运行。使用 -upgrade 标志启动这台 NN 是错误的。</li>\n</ol>\n<p>注意，在定型或者回退升级之前的任何时候想重启 NameNode，应该正常启动 NN，不使用任何特殊的启动标志。</p>\n<h4 id=\"定型-HA-升级\"><a href=\"#定型-HA-升级\" class=\"headerlink\" title=\"定型 HA 升级\"></a>定型 HA 升级</h4><p>当 NN 都正在运行且一台为 active，操作者应该使用 hdfs dfsadmin -finalizeUpgrade 命令定型 HA 升级。此时，活跃的 NN 将执行共享 edit 日志的定型，并且包含之前 FS 状态的存储目录的这台 NN 会删除它本地的状态。</p>\n<h4 id=\"执行升级回退\"><a href=\"#执行升级回退\" class=\"headerlink\" title=\"执行升级回退\"></a>执行升级回退</h4><p>停止两台 NN。在发起升级过程的那台 NN 上运行回退命令，它将在本地目录、共享日志、NFS 或 JN 上执行回退。之后，启动这台 NN，并在另外一个 NN 上运行 -bootstrapStandby，以使两台 NN 同步到回退后的文件系统状态。</p>\n<h3 id=\"问题排查\"><a href=\"#问题排查\" class=\"headerlink\" title=\"问题排查\"></a>问题排查</h3><h4 id=\"code-NOAUTH\"><a href=\"#code-NOAUTH\" class=\"headerlink\" title=\"code:NOAUTH\"></a>code:NOAUTH</h4><p>在测试自动切换时未能成功，检查 zkfc 的 log 发现以下错误信息：</p>\n<pre><code>java.lang.RuntimeException: ZK Failover Controller failed: Received create error from Zookeeper. code:NOAUTH for path /hadoop-ha/frin-cluster/ActiveStandbyElectorLock\n</code></pre>\n<p>可以发现是因为没有授权所以访问 Zookeeper 中的锁信息失败。经排查是配置安全访问 Zookeeper 时修改了 core-site.xml 中的配置，但配置没有分发到其他服务器。</p>\n<h4 id=\"ssh-id-rsa\"><a href=\"#ssh-id-rsa\" class=\"headerlink\" title=\".ssh/id_rsa\"></a>.ssh/id_rsa</h4><p>在测试自动切换时未能成功，检查 zkfc 的 log 发现以下错误信息：</p>\n<pre><code>2017-04-05 15:01:06,635 WARN org.apache.hadoop.ha.SshFenceByTcpPort: Unable to create SSH session\ncom.jcraft.jsch.JSchException: java.io.FileNotFoundException: /home/hadoop/.ssh/id_rsa (No such file or directory)\n        at com.jcraft.jsch.IdentityFile.newInstance(IdentityFile.java:98)\n        at com.jcraft.jsch.JSch.addIdentity(JSch.java:206)\n        at com.jcraft.jsch.JSch.addIdentity(JSch.java:192)\n        at org.apache.hadoop.ha.SshFenceByTcpPort.createSession(SshFenceByTcpPort.java:122)\n        at org.apache.hadoop.ha.SshFenceByTcpPort.tryFence(SshFenceByTcpPort.java:91)\n        at org.apache.hadoop.ha.NodeFencer.fence(NodeFencer.java:97)\n        at org.apache.hadoop.ha.ZKFailoverController.doFence(ZKFailoverController.java:532)\n        at org.apache.hadoop.ha.ZKFailoverController.fenceOldActive(ZKFailoverController.java:505)\n        at org.apache.hadoop.ha.ZKFailoverController.access$1100(ZKFailoverController.java:61)\n        at org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks.fenceOldActive(ZKFailoverController.java:892)\n        at org.apache.hadoop.ha.ActiveStandbyElector.fenceOldActive(ActiveStandbyElector.java:910)\n        at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:809)\n        at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:418)\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)\nCaused by: java.io.FileNotFoundException: /home/hadoop/.ssh/id_rsa (No such file or directory)\n        at java.io.FileInputStream.open(Native Method)\n        at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:146)\n        at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:101)\n        at com.jcraft.jsch.IdentityFile.newInstance(IdentityFile.java:83)\n        ... 14 more\n</code></pre>\n<p>检查发现 NameNode2 服务器未创建 ssh key，使用命令 ssh-keygen -t rsa 命令创建，再次测试，出现如下报错信息：</p>\n<pre><code>2017-04-05 15:12:23,351 WARN org.apache.hadoop.ha.SshFenceByTcpPort: Unable to connect to frin-namenode1 as user hadoop\ncom.jcraft.jsch.JSchException: Auth fail\n        at com.jcraft.jsch.Session.connect(Session.java:452)\n        at org.apache.hadoop.ha.SshFenceByTcpPort.tryFence(SshFenceByTcpPort.java:100)\n        at org.apache.hadoop.ha.NodeFencer.fence(NodeFencer.java:97)\n        at org.apache.hadoop.ha.ZKFailoverController.doFence(ZKFailoverController.java:532)\n        at org.apache.hadoop.ha.ZKFailoverController.fenceOldActive(ZKFailoverController.java:505)\n        at org.apache.hadoop.ha.ZKFailoverController.access$1100(ZKFailoverController.java:61)\n        at org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks.fenceOldActive(ZKFailoverController.java:892)\n        at org.apache.hadoop.ha.ActiveStandbyElector.fenceOldActive(ActiveStandbyElector.java:910)\n        at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:809)\n        at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:418)\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)\n</code></pre>\n<p>配置 NameNode2 到 NameNode1 SSH 免密钥登陆后，自动切换成功。</p>\n","site":{"data":{}},"excerpt":"<h3 id=\"组件\"><a href=\"#组件\" class=\"headerlink\" title=\"组件\"></a>组件</h3><h4 id=\"Zookeeper\"><a href=\"#Zookeeper\" class=\"headerlink\" title=\"Zookeeper\"></a>Zookeeper</h4><p>HDFS 自动 failover 的实现基于 Zookeeper 的以下能力：</p>\n<ul>\n<li><strong>失败检查</strong>：每台 NameNode 服务器在 Zookeeper 中维持一个持久会话。如果服务器崩溃，Zookeeper 会话就会超时，并通知另外一台 NameNode 触发 failover。</li>\n<li><strong>活跃 NameNode 选举</strong>：Zookeeper 提供了一种简单机制唯一选举一台活跃节点。如果当前活跃的 NameNode 崩溃，另外一个台节点可以在 Zookeeper 中获取一个特定且唯一的锁，表明它应该变为下一个状态。</li>\n</ul>","more":"<h4 id=\"ZKFailoverController-ZKFC\"><a href=\"#ZKFailoverController-ZKFC\" class=\"headerlink\" title=\"ZKFailoverController (ZKFC)\"></a>ZKFailoverController (ZKFC)</h4><p>ZKFailoverController (ZKFC) 是一个 Zookeeper 的客户端，它负责监测和管理 NameNode 的状态。运行 NameNode 的机器也会运行一个 ZKFC。ZKFC 负责：</p>\n<ul>\n<li><p><strong>健康监测</strong>：ZKFC 使用一个健康检查命令周期的 ping 它本地的 NameNode。只要节点响应及时，ZKFC 认为 NameNode 是健康的。如果节点崩溃、卡住或进入不健康状态，健康监测会标识它是不健康的。</p>\n</li>\n<li><p><strong>ZooKeeper 会话管理</strong>：本地 NameNode 健康时，ZKFC 在 Zookeeper 中保持一个打开的会话。如果本地的 NameNode 是活跃的，它也会保持一个特定的“lock”znode。这个锁使用了 Zookeeper 支持的“ephemeral”节点；如果会话过期，这个锁节点被自动删除。</p>\n</li>\n<li><p><strong>基于 Zookeeper 的选举</strong>：如果本地 NameNode 是健康的，并且 ZKFC 发现当前没有其他节点保持锁 znode，它自己将尝试请求这个锁。如果成功，那么它“赢得选举”，并且负责运行 failover 来使它本地的 NameNode 活跃。failover 进程与之前描述的手动 failover 一样：首先，如果需要的话之前活跃的节点会被杀死；然后，本地 NameNode 变为活跃状态。</p>\n</li>\n</ul>\n<h3 id=\"部署-Zookeeper\"><a href=\"#部署-Zookeeper\" class=\"headerlink\" title=\"部署 Zookeeper\"></a>部署 Zookeeper</h3><p>见我的另外一篇博客<a href=\"http://zhang-jc.github.io/2017/04/01/Zookeeper-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/\">Zookeeper 集群搭建</a>。</p>\n<h3 id=\"停止集群\"><a href=\"#停止集群\" class=\"headerlink\" title=\"停止集群\"></a>停止集群</h3><p>在开始配置之前先停止集群服务。</p>\n<h3 id=\"配置自动-failover\"><a href=\"#配置自动-failover\" class=\"headerlink\" title=\"配置自动 failover\"></a>配置自动 failover</h3><h4 id=\"hdfs-site-xml\"><a href=\"#hdfs-site-xml\" class=\"headerlink\" title=\"hdfs-site.xml\"></a>hdfs-site.xml</h4><p>添加如下配置：</p>\n<pre><code>&lt;property&gt;\n  &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;\n  &lt;value&gt;true&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<h4 id=\"core-site-xml\"><a href=\"#core-site-xml\" class=\"headerlink\" title=\"core-site.xml\"></a>core-site.xml</h4><p>添加如下配置：</p>\n<pre><code>&lt;property&gt;\n  &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;\n  &lt;value&gt;frin-zookeeper1:2181,frin-zookeeper2:2181,frin-zookeeper3:2181&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<h3 id=\"初始化-ZooKeeper-中的-HA-状态\"><a href=\"#初始化-ZooKeeper-中的-HA-状态\" class=\"headerlink\" title=\"初始化 ZooKeeper 中的 HA 状态\"></a>初始化 ZooKeeper 中的 HA 状态</h3><p>从一台 NameNode 主机上用以下命令初始化：</p>\n<pre><code>bin/hdfs zkfc -formatZK\n</code></pre>\n<h3 id=\"启动集群\"><a href=\"#启动集群\" class=\"headerlink\" title=\"启动集群\"></a>启动集群</h3><p>启动 NameNode 和 DataNode。</p>\n<p>在两台 NameNode 主机上使用下面的命令启动 zkfc 守护进程：</p>\n<pre><code>sbin/hadoop-daemon.sh start zkfc\n</code></pre>\n<h3 id=\"安全访问-ZooKeeper\"><a href=\"#安全访问-ZooKeeper\" class=\"headerlink\" title=\"安全访问 ZooKeeper\"></a>安全访问 ZooKeeper</h3><p>如果我们运行的是一个安全集群，那我们也希望确保存在 ZooKeeper 中的信息也是安全的。这可以防止恶意的客户端修改 ZooKeeper 中的元数据或触发一个潜在错误的 failover。</p>\n<h4 id=\"core-site-xml-1\"><a href=\"#core-site-xml-1\" class=\"headerlink\" title=\"core-site.xml\"></a>core-site.xml</h4><p>为了保护 Zookeeper 中的信息，首先在 core-site.xml 中添加如下配置：</p>\n<pre><code>&lt;property&gt;\n  &lt;name&gt;ha.zookeeper.auth&lt;/name&gt;\n  &lt;value&gt;@/usr/local/hadoop/etc/hadoop/zk-auth.txt&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;ha.zookeeper.acl&lt;/name&gt;\n  &lt;value&gt;@/usr/local/hadoop/etc/hadoop/zk-acl.txt&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<p>注意，配置中的“@”字符表示配置不是内部的值，而是指向磁盘上的文件。</p>\n<p>第一个文件指定了 Zookeeper 授权列表，与 ZK CLI 中使用的格式一样。例如：</p>\n<pre><code>digest:hdfs-zkfcs:mypassword\n</code></pre>\n<p>hdfs-zkfcs 是 Zookeeper 中一个唯一的用户名，mypassword 是密码。</p>\n<p>在 NameNode 节点 $HADOOP_HOME/etc/hadoop/ 目录下创建文件 zk-auth.txt 和 zk-acl.txt。zk-auth.txt 配置文件中的内容为：</p>\n<pre><code>digest:hdfs-zkfcs:frin-cluster-hdfs-zkfcs\n</code></pre>\n<h4 id=\"ZooKeeper-ACL\"><a href=\"#ZooKeeper-ACL\" class=\"headerlink\" title=\"ZooKeeper ACL\"></a>ZooKeeper ACL</h4><p>生成一个跟上面授权对应的 ZooKeeper ACL，使用下面的命令：</p>\n<pre><code>java -cp /usr/local/zookeeper/lib/*:/usr/local/zookeeper/zookeeper-3.4.10.jar org.apache.zookeeper.server.auth.DigestAuthenticationProvider hdfs-zkfcs:frin-cluster-hdfs-zkfcs\n</code></pre>\n<p>拷贝输出内容中“-&gt;”之后的内容，粘贴到 zk-acls.txt 文件中，并且要以“digest:”作为前缀。例如：</p>\n<pre><code>digest:hdfs-zkfcs:+ub6J925c+Pbmbd74jrgDRKLe5o=:rwcda\n</code></pre>\n<h4 id=\"初始化-Zookeeper-中的状态\"><a href=\"#初始化-Zookeeper-中的状态\" class=\"headerlink\" title=\"初始化 Zookeeper 中的状态\"></a>初始化 Zookeeper 中的状态</h4><p>为了这些 ACL 生效，像上面描述的重新执行 zkfc -formatZK 命令。</p>\n<p>做完这些后，从 ZK CLI 确认 ACL，如下面：</p>\n<pre><code>[zk: frin-zookeeper1:2181(CONNECTED) 3] getAcl /hadoop-ha\n&#39;digest,&#39;hdfs-zkfcs:+ub6J925c+Pbmbd74jrgDRKLe5o=\n: cdrwa\n</code></pre>\n<h3 id=\"验证自动-failover\"><a href=\"#验证自动-failover\" class=\"headerlink\" title=\"验证自动 failover\"></a>验证自动 failover</h3><p>从 WebUI 找出活跃的 NameNode，使用 kill -9 &lt;NameNode 进程的 PID&gt; 模拟 JVM 崩溃，也可以重启服务器或者使服务器断网。这些情况发生后，我们希望另外一台 NameNode 能在几秒钟内自动变为活跃状态。监测失败及触发 failover 需要的时间依赖 ha.zookeeper.session-timeout.ms 配置项，默认为 5 秒。</p>\n<h3 id=\"自动-Failover-FAQ\"><a href=\"#自动-Failover-FAQ\" class=\"headerlink\" title=\"自动 Failover FAQ\"></a>自动 Failover FAQ</h3><h4 id=\"是否需要按照特定的顺序启动-ZKFC-和-NameNode？\"><a href=\"#是否需要按照特定的顺序启动-ZKFC-和-NameNode？\" class=\"headerlink\" title=\"是否需要按照特定的顺序启动 ZKFC 和 NameNode？\"></a>是否需要按照特定的顺序启动 ZKFC 和 NameNode？</h4><p>不。在任何一个给定的节点上，可以在相关的 NameNode 进程启动之前或者之后启动 ZKFC。</p>\n<h4 id=\"还应该添加什么样的监测？\"><a href=\"#还应该添加什么样的监测？\" class=\"headerlink\" title=\"还应该添加什么样的监测？\"></a>还应该添加什么样的监测？</h4><p>应该在运行 NameNode 的主机上添加监测确保 ZKFC 保持运行。例如，在 Zookeeper 失败的一些场景，ZKFC 可能异常退出，应该重启 ZKFC 确保系统准备好 failover。</p>\n<p>另外，应该监测 Zookeeper 集群的每台服务器。如果 Zookeeper 崩溃，那么自动 failover 将不起作用。</p>\n<h4 id=\"如果-Zookeeper-停止会发生什么？\"><a href=\"#如果-Zookeeper-停止会发生什么？\" class=\"headerlink\" title=\"如果 Zookeeper 停止会发生什么？\"></a>如果 Zookeeper 停止会发生什么？</h4><p>如果 ZooKeeper 集群崩溃，自动 failover 将不会被触发。然而，HDFS 会继续运行不受影响。当 Zookeeper 重新启动，HDFS 会重新连接。</p>\n<h4 id=\"是否可以指定主-优先的-NameNode？\"><a href=\"#是否可以指定主-优先的-NameNode？\" class=\"headerlink\" title=\"是否可以指定主/优先的 NameNode？\"></a>是否可以指定主/优先的 NameNode？</h4><p>不。当前，这是不支持的。哪个 NameNode 先启动哪个节点将变为活跃的。可以选择以特定的顺序启动集群，先启动作为优先节点的 NameNode 节点。</p>\n<h4 id=\"当配置了自动-failover-后，如何发起手动-failover？\"><a href=\"#当配置了自动-failover-后，如何发起手动-failover？\" class=\"headerlink\" title=\"当配置了自动 failover 后，如何发起手动 failover？\"></a>当配置了自动 failover 后，如何发起手动 failover？</h4><p>即使配置了自动 failover，可以使用同样的 hdfs haadmin 命令发起手动 failover。它将执行相应的 failover。</p>\n<h3 id=\"启用-HA-的-HDFS-升级-定型-回退\"><a href=\"#启用-HA-的-HDFS-升级-定型-回退\" class=\"headerlink\" title=\"启用 HA 的 HDFS 升级/定型/回退\"></a>启用 HA 的 HDFS 升级/定型/回退</h3><p>当切换 HDFS 的版本时，有时新的软件可以简单的安装并重启集群。然而，有时升级正在运行的 HDFS 版本时需要修改磁盘上的数据。这种情况下，必须在安装完新软件后使用 HDFS 的升级/定型/回退工具。在  HA 的情况下这个过程会变得复杂，因为  NN 依赖的磁盘上的元数据是分布定义的，在一对 HA NN 上，在使用 QJM 共享 edits 的情况下也存储在 JournalNode 上。这一节描述在 HA 的情况下使用 HDFS 升级/定型/回退工具的过程。</p>\n<h4 id=\"执行-HA-升级\"><a href=\"#执行-HA-升级\" class=\"headerlink\" title=\"执行 HA 升级\"></a>执行 HA 升级</h4><p>必须做下面的操作：</p>\n<ol>\n<li>正常关闭所有 NN，并安装新软件。</li>\n<li>启动所有的 JN。注意，当执行升级、回退或者定型操作时所有的 JN 都是运行的，这是极为重要的。如果在运行这些操作时任何 JN 停止了，那么操作都会失败。</li>\n<li>使用 -upgrade 标志启动一台 NN。</li>\n<li>开始，在 HA 情况下这台 NN 通常不会进入 standby 状态。而是，这台 NN 会立即进入 active 状态，执行它本地存储目录的更新，并执行共享 edit 日志的更新。</li>\n<li>此时，另外一台 NN 与已经升级的 NN 是不同步的。为了同步及恢复 HA 安装，需要使用 -bootstrapStandby 标志重新引导这台 NN 运行。使用 -upgrade 标志启动这台 NN 是错误的。</li>\n</ol>\n<p>注意，在定型或者回退升级之前的任何时候想重启 NameNode，应该正常启动 NN，不使用任何特殊的启动标志。</p>\n<h4 id=\"定型-HA-升级\"><a href=\"#定型-HA-升级\" class=\"headerlink\" title=\"定型 HA 升级\"></a>定型 HA 升级</h4><p>当 NN 都正在运行且一台为 active，操作者应该使用 hdfs dfsadmin -finalizeUpgrade 命令定型 HA 升级。此时，活跃的 NN 将执行共享 edit 日志的定型，并且包含之前 FS 状态的存储目录的这台 NN 会删除它本地的状态。</p>\n<h4 id=\"执行升级回退\"><a href=\"#执行升级回退\" class=\"headerlink\" title=\"执行升级回退\"></a>执行升级回退</h4><p>停止两台 NN。在发起升级过程的那台 NN 上运行回退命令，它将在本地目录、共享日志、NFS 或 JN 上执行回退。之后，启动这台 NN，并在另外一个 NN 上运行 -bootstrapStandby，以使两台 NN 同步到回退后的文件系统状态。</p>\n<h3 id=\"问题排查\"><a href=\"#问题排查\" class=\"headerlink\" title=\"问题排查\"></a>问题排查</h3><h4 id=\"code-NOAUTH\"><a href=\"#code-NOAUTH\" class=\"headerlink\" title=\"code:NOAUTH\"></a>code:NOAUTH</h4><p>在测试自动切换时未能成功，检查 zkfc 的 log 发现以下错误信息：</p>\n<pre><code>java.lang.RuntimeException: ZK Failover Controller failed: Received create error from Zookeeper. code:NOAUTH for path /hadoop-ha/frin-cluster/ActiveStandbyElectorLock\n</code></pre>\n<p>可以发现是因为没有授权所以访问 Zookeeper 中的锁信息失败。经排查是配置安全访问 Zookeeper 时修改了 core-site.xml 中的配置，但配置没有分发到其他服务器。</p>\n<h4 id=\"ssh-id-rsa\"><a href=\"#ssh-id-rsa\" class=\"headerlink\" title=\".ssh/id_rsa\"></a>.ssh/id_rsa</h4><p>在测试自动切换时未能成功，检查 zkfc 的 log 发现以下错误信息：</p>\n<pre><code>2017-04-05 15:01:06,635 WARN org.apache.hadoop.ha.SshFenceByTcpPort: Unable to create SSH session\ncom.jcraft.jsch.JSchException: java.io.FileNotFoundException: /home/hadoop/.ssh/id_rsa (No such file or directory)\n        at com.jcraft.jsch.IdentityFile.newInstance(IdentityFile.java:98)\n        at com.jcraft.jsch.JSch.addIdentity(JSch.java:206)\n        at com.jcraft.jsch.JSch.addIdentity(JSch.java:192)\n        at org.apache.hadoop.ha.SshFenceByTcpPort.createSession(SshFenceByTcpPort.java:122)\n        at org.apache.hadoop.ha.SshFenceByTcpPort.tryFence(SshFenceByTcpPort.java:91)\n        at org.apache.hadoop.ha.NodeFencer.fence(NodeFencer.java:97)\n        at org.apache.hadoop.ha.ZKFailoverController.doFence(ZKFailoverController.java:532)\n        at org.apache.hadoop.ha.ZKFailoverController.fenceOldActive(ZKFailoverController.java:505)\n        at org.apache.hadoop.ha.ZKFailoverController.access$1100(ZKFailoverController.java:61)\n        at org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks.fenceOldActive(ZKFailoverController.java:892)\n        at org.apache.hadoop.ha.ActiveStandbyElector.fenceOldActive(ActiveStandbyElector.java:910)\n        at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:809)\n        at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:418)\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)\nCaused by: java.io.FileNotFoundException: /home/hadoop/.ssh/id_rsa (No such file or directory)\n        at java.io.FileInputStream.open(Native Method)\n        at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:146)\n        at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:101)\n        at com.jcraft.jsch.IdentityFile.newInstance(IdentityFile.java:83)\n        ... 14 more\n</code></pre>\n<p>检查发现 NameNode2 服务器未创建 ssh key，使用命令 ssh-keygen -t rsa 命令创建，再次测试，出现如下报错信息：</p>\n<pre><code>2017-04-05 15:12:23,351 WARN org.apache.hadoop.ha.SshFenceByTcpPort: Unable to connect to frin-namenode1 as user hadoop\ncom.jcraft.jsch.JSchException: Auth fail\n        at com.jcraft.jsch.Session.connect(Session.java:452)\n        at org.apache.hadoop.ha.SshFenceByTcpPort.tryFence(SshFenceByTcpPort.java:100)\n        at org.apache.hadoop.ha.NodeFencer.fence(NodeFencer.java:97)\n        at org.apache.hadoop.ha.ZKFailoverController.doFence(ZKFailoverController.java:532)\n        at org.apache.hadoop.ha.ZKFailoverController.fenceOldActive(ZKFailoverController.java:505)\n        at org.apache.hadoop.ha.ZKFailoverController.access$1100(ZKFailoverController.java:61)\n        at org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks.fenceOldActive(ZKFailoverController.java:892)\n        at org.apache.hadoop.ha.ActiveStandbyElector.fenceOldActive(ActiveStandbyElector.java:910)\n        at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:809)\n        at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:418)\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)\n</code></pre>\n<p>配置 NameNode2 到 NameNode1 SSH 免密钥登陆后，自动切换成功。</p>"},{"title":"基于 hexo + github 的个人博客系统搭建","date":"2016-05-21T14:58:56.000Z","_content":"\n### 工具介绍\n#### Hexo\n\nHexo 是一个快速、简单，并且强大的博客框架。主要有以下特性：\n\n1. Node.js 带来不可思议的生成速度。建立数百个文件只需要几秒钟。\n2. 支持 GitHub 所有推荐的 Markdown 特性。你甚至可以在 Hexo 中使用大部分 Octopress 的插件。\n3. 只需要一个命令就可以部署个人博客到 GitHub、Heroku 或者其他站点。\n4. Hexo 有一个强大的插件系统。可以安装很多 Jade 插件和 CoffeeScript 插件。\n\n<!-- more -->\n\n#### github\n\nGitHub 提供一个快速的、灵活的、协作的开发过程，让你可以自己工作或者跟其他人一起工作。\n\n此处主要利用 GitHub 提供的 GitHub pages 特性。详细说明请查看[官方文档](https://pages.github.com/)。\n\n#### Oray\n\nOray 是一家域名服务提供商。可以根据自己的喜好选择其他域名服务商，如万网。域名服务一般都会提供 URL 跳转的功能：访问域名可跳转到一个网址上，实现不加端口号即可访问服务器（显性URL，会改变地址栏中的网址，隐性URL则反之）。此处正式利用这种功能。\n\n### 整体架构\n\n![个人博客架构](/uploads/20160521/personal-blog-site.png)\n\n1. source repository（local）：本地代码仓库。在本地搭建 node.js 环境，并运行个人博客系统作为开发调试环境。\n2. source repository（github）：github 上创建的代码仓库。通过 git push 将个人代码提交到 github 代码仓库。\n3. static repository（local）：个人博客在本地生成的静态内容仓库。使用 hexo generate 命令完成。\n4. 使用 Oray 域名服务的原因是可以使用自己喜欢的个性化的域名。\n\n### 总结\n\n采用这种方式作为个人博客的好处是：\n1. 过程简单，只要有基础英文文档阅读能力。按照官网介绍操作就好。不用开发也可以完成。\n2. 基于 GitHub 服务稳定；\n3. Hexo 功能强大，插件丰富，而且有丰富的主题可以选择。\n4. 文章编写使用 Markdown，能让你更专注于文章内容，不用关心展示。\n5. 成本极低。只需要域名服务付费，每年不到一百元。\n","source":"_posts/基于-hexo-github-的个人博客系统搭建.md","raw":"title: 基于 hexo + github 的个人博客系统搭建\ntags:\n  - Hexo\n  - GitHub\n  - Node.js\ncategories:\n  - 开发\n  - Node.js\ndate: 2016-05-21 22:58:56\n---\n\n### 工具介绍\n#### Hexo\n\nHexo 是一个快速、简单，并且强大的博客框架。主要有以下特性：\n\n1. Node.js 带来不可思议的生成速度。建立数百个文件只需要几秒钟。\n2. 支持 GitHub 所有推荐的 Markdown 特性。你甚至可以在 Hexo 中使用大部分 Octopress 的插件。\n3. 只需要一个命令就可以部署个人博客到 GitHub、Heroku 或者其他站点。\n4. Hexo 有一个强大的插件系统。可以安装很多 Jade 插件和 CoffeeScript 插件。\n\n<!-- more -->\n\n#### github\n\nGitHub 提供一个快速的、灵活的、协作的开发过程，让你可以自己工作或者跟其他人一起工作。\n\n此处主要利用 GitHub 提供的 GitHub pages 特性。详细说明请查看[官方文档](https://pages.github.com/)。\n\n#### Oray\n\nOray 是一家域名服务提供商。可以根据自己的喜好选择其他域名服务商，如万网。域名服务一般都会提供 URL 跳转的功能：访问域名可跳转到一个网址上，实现不加端口号即可访问服务器（显性URL，会改变地址栏中的网址，隐性URL则反之）。此处正式利用这种功能。\n\n### 整体架构\n\n![个人博客架构](/uploads/20160521/personal-blog-site.png)\n\n1. source repository（local）：本地代码仓库。在本地搭建 node.js 环境，并运行个人博客系统作为开发调试环境。\n2. source repository（github）：github 上创建的代码仓库。通过 git push 将个人代码提交到 github 代码仓库。\n3. static repository（local）：个人博客在本地生成的静态内容仓库。使用 hexo generate 命令完成。\n4. 使用 Oray 域名服务的原因是可以使用自己喜欢的个性化的域名。\n\n### 总结\n\n采用这种方式作为个人博客的好处是：\n1. 过程简单，只要有基础英文文档阅读能力。按照官网介绍操作就好。不用开发也可以完成。\n2. 基于 GitHub 服务稳定；\n3. Hexo 功能强大，插件丰富，而且有丰富的主题可以选择。\n4. 文章编写使用 Markdown，能让你更专注于文章内容，不用关心展示。\n5. 成本极低。只需要域名服务付费，每年不到一百元。\n","slug":"基于-hexo-github-的个人博客系统搭建","published":1,"updated":"2021-07-19T16:28:00.308Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphrw00g5itd34j67e5ie","content":"<h3 id=\"工具介绍\"><a href=\"#工具介绍\" class=\"headerlink\" title=\"工具介绍\"></a>工具介绍</h3><h4 id=\"Hexo\"><a href=\"#Hexo\" class=\"headerlink\" title=\"Hexo\"></a>Hexo</h4><p>Hexo 是一个快速、简单，并且强大的博客框架。主要有以下特性：</p>\n<ol>\n<li>Node.js 带来不可思议的生成速度。建立数百个文件只需要几秒钟。</li>\n<li>支持 GitHub 所有推荐的 Markdown 特性。你甚至可以在 Hexo 中使用大部分 Octopress 的插件。</li>\n<li>只需要一个命令就可以部署个人博客到 GitHub、Heroku 或者其他站点。</li>\n<li>Hexo 有一个强大的插件系统。可以安装很多 Jade 插件和 CoffeeScript 插件。</li>\n</ol>\n<span id=\"more\"></span>\n\n<h4 id=\"github\"><a href=\"#github\" class=\"headerlink\" title=\"github\"></a>github</h4><p>GitHub 提供一个快速的、灵活的、协作的开发过程，让你可以自己工作或者跟其他人一起工作。</p>\n<p>此处主要利用 GitHub 提供的 GitHub pages 特性。详细说明请查看<a href=\"https://pages.github.com/\">官方文档</a>。</p>\n<h4 id=\"Oray\"><a href=\"#Oray\" class=\"headerlink\" title=\"Oray\"></a>Oray</h4><p>Oray 是一家域名服务提供商。可以根据自己的喜好选择其他域名服务商，如万网。域名服务一般都会提供 URL 跳转的功能：访问域名可跳转到一个网址上，实现不加端口号即可访问服务器（显性URL，会改变地址栏中的网址，隐性URL则反之）。此处正式利用这种功能。</p>\n<h3 id=\"整体架构\"><a href=\"#整体架构\" class=\"headerlink\" title=\"整体架构\"></a>整体架构</h3><p><img src=\"/uploads/20160521/personal-blog-site.png\" alt=\"个人博客架构\"></p>\n<ol>\n<li>source repository（local）：本地代码仓库。在本地搭建 node.js 环境，并运行个人博客系统作为开发调试环境。</li>\n<li>source repository（github）：github 上创建的代码仓库。通过 git push 将个人代码提交到 github 代码仓库。</li>\n<li>static repository（local）：个人博客在本地生成的静态内容仓库。使用 hexo generate 命令完成。</li>\n<li>使用 Oray 域名服务的原因是可以使用自己喜欢的个性化的域名。</li>\n</ol>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>采用这种方式作为个人博客的好处是：</p>\n<ol>\n<li>过程简单，只要有基础英文文档阅读能力。按照官网介绍操作就好。不用开发也可以完成。</li>\n<li>基于 GitHub 服务稳定；</li>\n<li>Hexo 功能强大，插件丰富，而且有丰富的主题可以选择。</li>\n<li>文章编写使用 Markdown，能让你更专注于文章内容，不用关心展示。</li>\n<li>成本极低。只需要域名服务付费，每年不到一百元。</li>\n</ol>\n","site":{"data":{}},"excerpt":"<h3 id=\"工具介绍\"><a href=\"#工具介绍\" class=\"headerlink\" title=\"工具介绍\"></a>工具介绍</h3><h4 id=\"Hexo\"><a href=\"#Hexo\" class=\"headerlink\" title=\"Hexo\"></a>Hexo</h4><p>Hexo 是一个快速、简单，并且强大的博客框架。主要有以下特性：</p>\n<ol>\n<li>Node.js 带来不可思议的生成速度。建立数百个文件只需要几秒钟。</li>\n<li>支持 GitHub 所有推荐的 Markdown 特性。你甚至可以在 Hexo 中使用大部分 Octopress 的插件。</li>\n<li>只需要一个命令就可以部署个人博客到 GitHub、Heroku 或者其他站点。</li>\n<li>Hexo 有一个强大的插件系统。可以安装很多 Jade 插件和 CoffeeScript 插件。</li>\n</ol>","more":"<h4 id=\"github\"><a href=\"#github\" class=\"headerlink\" title=\"github\"></a>github</h4><p>GitHub 提供一个快速的、灵活的、协作的开发过程，让你可以自己工作或者跟其他人一起工作。</p>\n<p>此处主要利用 GitHub 提供的 GitHub pages 特性。详细说明请查看<a href=\"https://pages.github.com/\">官方文档</a>。</p>\n<h4 id=\"Oray\"><a href=\"#Oray\" class=\"headerlink\" title=\"Oray\"></a>Oray</h4><p>Oray 是一家域名服务提供商。可以根据自己的喜好选择其他域名服务商，如万网。域名服务一般都会提供 URL 跳转的功能：访问域名可跳转到一个网址上，实现不加端口号即可访问服务器（显性URL，会改变地址栏中的网址，隐性URL则反之）。此处正式利用这种功能。</p>\n<h3 id=\"整体架构\"><a href=\"#整体架构\" class=\"headerlink\" title=\"整体架构\"></a>整体架构</h3><p><img src=\"/uploads/20160521/personal-blog-site.png\" alt=\"个人博客架构\"></p>\n<ol>\n<li>source repository（local）：本地代码仓库。在本地搭建 node.js 环境，并运行个人博客系统作为开发调试环境。</li>\n<li>source repository（github）：github 上创建的代码仓库。通过 git push 将个人代码提交到 github 代码仓库。</li>\n<li>static repository（local）：个人博客在本地生成的静态内容仓库。使用 hexo generate 命令完成。</li>\n<li>使用 Oray 域名服务的原因是可以使用自己喜欢的个性化的域名。</li>\n</ol>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>采用这种方式作为个人博客的好处是：</p>\n<ol>\n<li>过程简单，只要有基础英文文档阅读能力。按照官网介绍操作就好。不用开发也可以完成。</li>\n<li>基于 GitHub 服务稳定；</li>\n<li>Hexo 功能强大，插件丰富，而且有丰富的主题可以选择。</li>\n<li>文章编写使用 Markdown，能让你更专注于文章内容，不用关心展示。</li>\n<li>成本极低。只需要域名服务付费，每年不到一百元。</li>\n</ol>"},{"title":"基于QJM（Quorum Journal Manager）实现 HDFS HA","date":"2017-03-27T07:42:38.000Z","_content":"\n### 综述\n\n#### 集群搭建\n\nHadoop 集群搭建及服务器信息见我的另外一篇博客[hadoop-2-7-3-集群安装](http://zhang-jc.github.io/2017/03/03/hadoop-2-7-3-%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/)\n\n#### 服务器列表\n\n- 10.142.166.81：NameNode1/ JournalNode1\n- 10.142.166.119：NameNode2 / ResourceManager / JournalNode2 / MapReduce JobHistory Server\n- 10.142.165.40：DataNode / NodeManager / JournalNode3\n- 10.142.165.41 / 10.142.165.44：DataNode / NodeManager\n\n<!-- more -->\n\n### 架构\n\n![Quorum Journal Manager](/uploads/20170327/hdfs-ha-qjm.png)\n\n为了防止出现“脑裂”情况， JournalNodes 同时只允许一个 NameNode 写入数据。实现 HA 后， Secondary NameNode、CheckpointNode、BackupNode 就都不需要了。\n\n### 配置过程\n\n#### hdfs-site.xml 配置\n\n##### dfs.nameservices\n\n    <property>\n      <name>dfs.nameservices</name>\n      <value>frin-cluster</value>\n    <property>\n\n##### dfs.ha.namenodes.[nameservice id]\n\n    <property>\n      <name>dfs.ha.namenodes.frin-cluster</name>\n      <value>frin-namenode1,frin-namenode2</value>\n    </property>\n\n目前，一个 nameservice 最多只能配置两台 NameNode。\n\n##### dfs.namenode.rpc-address.[nameservice id].[namenode id]\n\n    <property>\n      <name>dfs.namenode.rpc-address.frin-cluster.frin-namenode1</name>\n      <value>frin-namenode1:8020</value>\n    </property>\n    <property>\n      <name>dfs.namenode.rpc-address.frin-cluster.frin-namenode2</name>\n      <value>frin-namenode2:8020</value>\n    </property>\n\n##### dfs.namenode.http-address.[nameservice id].[namenode id]\n\n    <property>\n      <name>dfs.namenode.http-address.frin-cluster.frin-namenode1</name>\n      <value>frin-namenode1:50070</value>\n    </property>\n    <property>\n      <name>dfs.namenode.http-address.frin-cluster.frin-namenode2</name>\n      <value>frin-namenode2:50070</value>\n    </property>\n\n##### dfs.namenode.shared.edits.dir\n\n    <property>\n      <name>dfs.namenode.shared.edits.dir</name>\n      <value>qjournal://frin-journalnode1:8485;frin-journalnode2:8485;frin-journalnode3:8485/frin-cluster</value>\n    </property>\n\n##### dfs.client.failover.proxy.provider.[nameservice id]\n\n    <property>\n      <name>dfs.client.failover.proxy.provider.frin-cluster</name>\n      <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>\n    </property>\n\n##### dfs.ha.fencing.methods\n\n    <property>\n      <name>dfs.ha.fencing.methods</name>\n      <value>sshfence</value>\n    </property>\n    <property>\n      <name>dfs.ha.fencing.ssh.private-key-files</name>\n      <value>/home/hadoop/.ssh/id_rsa</value>\n    <property>\n    <property>\n      <name>dfs.ha.fencing.ssh.connect-timeout</name>\n      <value>30000</value>\n    </property>\n\n##### dfs.journalnode.edits.dir\n\n    <property>\n      <name>dfs.journalnode.edits.dir</name>\n      <value>/data/dfs/journal/edits</value>\n    </property>\n\n#### core-site.xml fs.defaultFS 配置\n\n    <property>\n      <name>fs.defaultFS</name>\n      <value>hdfs://frin-namenode1:9000</value>\n    </property>\n\n改为\n\n    <property>\n      <name>fs.defaultFS</name>\n      <value>hdfs://frin-cluster</value>\n   </property>\n\n#### 创建目录：\n\n在journalnode节点上建立目录：/data/dfs/journal/edits\n\n#### hosts 配置\n\n/etc/hosts 中添加如下内容：\n\n    10.142.166.81 frin-namenode1\n    10.142.166.119 frin-namenode2\n\n    10.142.166.81 frin-journalnode1\n    10.142.166.119 frin-journalnode2\n    10.142.165.40 frin-journalnode3\n\n    10.142.166.119 frin-resourcemanager1\n    10.142.166.119 frin-jobhistoryserver\n\n    10.142.166.81 vm-10-142-166-81\n    10.142.166.119 vm-10-142-166-119\n    10.142.165.40 vm-10-142-165-40\n    10.142.165.41 vm-10-142-165-41\n    10.142.165.44 vm-10-142-165-44\n\n### 启动 HDFS HA\n\n#### 停止 NameNode\n\n    hadoop-daemon.sh stop namenode\n\n#### 分发配置\n\n将以下修改后的配置文件分发到集群所有节点：\n\n    core-site.xml\n    hdfs-site.xml\n\n#### 重启DataNodes\n\n    hadoop-daemon.sh stop datanode\n    hadoop-daemon.sh start datanode\n\n#### 启动 JournalNode\n\n    hadoop-daemon.sh start journalnode\n\n#### 启动 NameNode1\n\n##### 初始化 JournalNode\n\n从非 HA 改为 HA 的 NameNode 需要用 NameNode 本地 edits 目录中的数据初始化 JornalNode：\n\n    hdfs namenode -initializeSharedEdits\n\n##### 启动 NameNode1\n\n    hadoop-daemon.sh start namenode\n\n##### 是 NameNode1 变为 Active\n\n如果 NameNode1 启动后状态为 standby，则用以下命令将 NameNode1 状态变为 active：\n\n    hdfs haadmin -transitionToActive frin-namenode1\n\n#### 启动 NameNode2\n\n##### 初始化 NameNode2\n\n从非 HA 改为 HA 的 NameNode，使用如下命令将 NameNode1 上的元数据信息拷贝到 NameNode2：\n\n    hdfs namenode -bootstrapStandby\n\n##### 启动 NameNode2\n\n    hadoop-daemon.sh start namenode\n\n### 验证\n\n访问 NameNode1 的 Web 站点，可以看到 NameNode1 的状态为 Active：![namenode1.png](/uploads/20170327/namenode1.png)\n\n访问 NameNode2 的 Web 站点，可以看到 NameNode2 的状态为 standby：![namenode2.png](/uploads/20170327/namenode2.png)","source":"_posts/基于QJM（Quorum-Journal-Manager）实现-HDFS-HA.md","raw":"title: 基于QJM（Quorum Journal Manager）实现 HDFS HA\ntags:\n  - Hadoop\n  - HDFS\ncategories:\n  - 大数据\n  - Hadoop\ndate: 2017-03-27 15:42:38\n---\n\n### 综述\n\n#### 集群搭建\n\nHadoop 集群搭建及服务器信息见我的另外一篇博客[hadoop-2-7-3-集群安装](http://zhang-jc.github.io/2017/03/03/hadoop-2-7-3-%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/)\n\n#### 服务器列表\n\n- 10.142.166.81：NameNode1/ JournalNode1\n- 10.142.166.119：NameNode2 / ResourceManager / JournalNode2 / MapReduce JobHistory Server\n- 10.142.165.40：DataNode / NodeManager / JournalNode3\n- 10.142.165.41 / 10.142.165.44：DataNode / NodeManager\n\n<!-- more -->\n\n### 架构\n\n![Quorum Journal Manager](/uploads/20170327/hdfs-ha-qjm.png)\n\n为了防止出现“脑裂”情况， JournalNodes 同时只允许一个 NameNode 写入数据。实现 HA 后， Secondary NameNode、CheckpointNode、BackupNode 就都不需要了。\n\n### 配置过程\n\n#### hdfs-site.xml 配置\n\n##### dfs.nameservices\n\n    <property>\n      <name>dfs.nameservices</name>\n      <value>frin-cluster</value>\n    <property>\n\n##### dfs.ha.namenodes.[nameservice id]\n\n    <property>\n      <name>dfs.ha.namenodes.frin-cluster</name>\n      <value>frin-namenode1,frin-namenode2</value>\n    </property>\n\n目前，一个 nameservice 最多只能配置两台 NameNode。\n\n##### dfs.namenode.rpc-address.[nameservice id].[namenode id]\n\n    <property>\n      <name>dfs.namenode.rpc-address.frin-cluster.frin-namenode1</name>\n      <value>frin-namenode1:8020</value>\n    </property>\n    <property>\n      <name>dfs.namenode.rpc-address.frin-cluster.frin-namenode2</name>\n      <value>frin-namenode2:8020</value>\n    </property>\n\n##### dfs.namenode.http-address.[nameservice id].[namenode id]\n\n    <property>\n      <name>dfs.namenode.http-address.frin-cluster.frin-namenode1</name>\n      <value>frin-namenode1:50070</value>\n    </property>\n    <property>\n      <name>dfs.namenode.http-address.frin-cluster.frin-namenode2</name>\n      <value>frin-namenode2:50070</value>\n    </property>\n\n##### dfs.namenode.shared.edits.dir\n\n    <property>\n      <name>dfs.namenode.shared.edits.dir</name>\n      <value>qjournal://frin-journalnode1:8485;frin-journalnode2:8485;frin-journalnode3:8485/frin-cluster</value>\n    </property>\n\n##### dfs.client.failover.proxy.provider.[nameservice id]\n\n    <property>\n      <name>dfs.client.failover.proxy.provider.frin-cluster</name>\n      <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>\n    </property>\n\n##### dfs.ha.fencing.methods\n\n    <property>\n      <name>dfs.ha.fencing.methods</name>\n      <value>sshfence</value>\n    </property>\n    <property>\n      <name>dfs.ha.fencing.ssh.private-key-files</name>\n      <value>/home/hadoop/.ssh/id_rsa</value>\n    <property>\n    <property>\n      <name>dfs.ha.fencing.ssh.connect-timeout</name>\n      <value>30000</value>\n    </property>\n\n##### dfs.journalnode.edits.dir\n\n    <property>\n      <name>dfs.journalnode.edits.dir</name>\n      <value>/data/dfs/journal/edits</value>\n    </property>\n\n#### core-site.xml fs.defaultFS 配置\n\n    <property>\n      <name>fs.defaultFS</name>\n      <value>hdfs://frin-namenode1:9000</value>\n    </property>\n\n改为\n\n    <property>\n      <name>fs.defaultFS</name>\n      <value>hdfs://frin-cluster</value>\n   </property>\n\n#### 创建目录：\n\n在journalnode节点上建立目录：/data/dfs/journal/edits\n\n#### hosts 配置\n\n/etc/hosts 中添加如下内容：\n\n    10.142.166.81 frin-namenode1\n    10.142.166.119 frin-namenode2\n\n    10.142.166.81 frin-journalnode1\n    10.142.166.119 frin-journalnode2\n    10.142.165.40 frin-journalnode3\n\n    10.142.166.119 frin-resourcemanager1\n    10.142.166.119 frin-jobhistoryserver\n\n    10.142.166.81 vm-10-142-166-81\n    10.142.166.119 vm-10-142-166-119\n    10.142.165.40 vm-10-142-165-40\n    10.142.165.41 vm-10-142-165-41\n    10.142.165.44 vm-10-142-165-44\n\n### 启动 HDFS HA\n\n#### 停止 NameNode\n\n    hadoop-daemon.sh stop namenode\n\n#### 分发配置\n\n将以下修改后的配置文件分发到集群所有节点：\n\n    core-site.xml\n    hdfs-site.xml\n\n#### 重启DataNodes\n\n    hadoop-daemon.sh stop datanode\n    hadoop-daemon.sh start datanode\n\n#### 启动 JournalNode\n\n    hadoop-daemon.sh start journalnode\n\n#### 启动 NameNode1\n\n##### 初始化 JournalNode\n\n从非 HA 改为 HA 的 NameNode 需要用 NameNode 本地 edits 目录中的数据初始化 JornalNode：\n\n    hdfs namenode -initializeSharedEdits\n\n##### 启动 NameNode1\n\n    hadoop-daemon.sh start namenode\n\n##### 是 NameNode1 变为 Active\n\n如果 NameNode1 启动后状态为 standby，则用以下命令将 NameNode1 状态变为 active：\n\n    hdfs haadmin -transitionToActive frin-namenode1\n\n#### 启动 NameNode2\n\n##### 初始化 NameNode2\n\n从非 HA 改为 HA 的 NameNode，使用如下命令将 NameNode1 上的元数据信息拷贝到 NameNode2：\n\n    hdfs namenode -bootstrapStandby\n\n##### 启动 NameNode2\n\n    hadoop-daemon.sh start namenode\n\n### 验证\n\n访问 NameNode1 的 Web 站点，可以看到 NameNode1 的状态为 Active：![namenode1.png](/uploads/20170327/namenode1.png)\n\n访问 NameNode2 的 Web 站点，可以看到 NameNode2 的状态为 standby：![namenode2.png](/uploads/20170327/namenode2.png)","slug":"基于QJM（Quorum-Journal-Manager）实现-HDFS-HA","published":1,"updated":"2021-07-19T16:28:00.308Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphry00g9itd35yfq0na2","content":"<h3 id=\"综述\"><a href=\"#综述\" class=\"headerlink\" title=\"综述\"></a>综述</h3><h4 id=\"集群搭建\"><a href=\"#集群搭建\" class=\"headerlink\" title=\"集群搭建\"></a>集群搭建</h4><p>Hadoop 集群搭建及服务器信息见我的另外一篇博客<a href=\"http://zhang-jc.github.io/2017/03/03/hadoop-2-7-3-%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/\">hadoop-2-7-3-集群安装</a></p>\n<h4 id=\"服务器列表\"><a href=\"#服务器列表\" class=\"headerlink\" title=\"服务器列表\"></a>服务器列表</h4><ul>\n<li>10.142.166.81：NameNode1/ JournalNode1</li>\n<li>10.142.166.119：NameNode2 / ResourceManager / JournalNode2 / MapReduce JobHistory Server</li>\n<li>10.142.165.40：DataNode / NodeManager / JournalNode3</li>\n<li>10.142.165.41 / 10.142.165.44：DataNode / NodeManager</li>\n</ul>\n<span id=\"more\"></span>\n\n<h3 id=\"架构\"><a href=\"#架构\" class=\"headerlink\" title=\"架构\"></a>架构</h3><p><img src=\"/uploads/20170327/hdfs-ha-qjm.png\" alt=\"Quorum Journal Manager\"></p>\n<p>为了防止出现“脑裂”情况， JournalNodes 同时只允许一个 NameNode 写入数据。实现 HA 后， Secondary NameNode、CheckpointNode、BackupNode 就都不需要了。</p>\n<h3 id=\"配置过程\"><a href=\"#配置过程\" class=\"headerlink\" title=\"配置过程\"></a>配置过程</h3><h4 id=\"hdfs-site-xml-配置\"><a href=\"#hdfs-site-xml-配置\" class=\"headerlink\" title=\"hdfs-site.xml 配置\"></a>hdfs-site.xml 配置</h4><h5 id=\"dfs-nameservices\"><a href=\"#dfs-nameservices\" class=\"headerlink\" title=\"dfs.nameservices\"></a>dfs.nameservices</h5><pre><code>&lt;property&gt;\n  &lt;name&gt;dfs.nameservices&lt;/name&gt;\n  &lt;value&gt;frin-cluster&lt;/value&gt;\n&lt;property&gt;\n</code></pre>\n<h5 id=\"dfs-ha-namenodes-nameservice-id\"><a href=\"#dfs-ha-namenodes-nameservice-id\" class=\"headerlink\" title=\"dfs.ha.namenodes.[nameservice id]\"></a>dfs.ha.namenodes.[nameservice id]</h5><pre><code>&lt;property&gt;\n  &lt;name&gt;dfs.ha.namenodes.frin-cluster&lt;/name&gt;\n  &lt;value&gt;frin-namenode1,frin-namenode2&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<p>目前，一个 nameservice 最多只能配置两台 NameNode。</p>\n<h5 id=\"dfs-namenode-rpc-address-nameservice-id-namenode-id\"><a href=\"#dfs-namenode-rpc-address-nameservice-id-namenode-id\" class=\"headerlink\" title=\"dfs.namenode.rpc-address.[nameservice id].[namenode id]\"></a>dfs.namenode.rpc-address.[nameservice id].[namenode id]</h5><pre><code>&lt;property&gt;\n  &lt;name&gt;dfs.namenode.rpc-address.frin-cluster.frin-namenode1&lt;/name&gt;\n  &lt;value&gt;frin-namenode1:8020&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;dfs.namenode.rpc-address.frin-cluster.frin-namenode2&lt;/name&gt;\n  &lt;value&gt;frin-namenode2:8020&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<h5 id=\"dfs-namenode-http-address-nameservice-id-namenode-id\"><a href=\"#dfs-namenode-http-address-nameservice-id-namenode-id\" class=\"headerlink\" title=\"dfs.namenode.http-address.[nameservice id].[namenode id]\"></a>dfs.namenode.http-address.[nameservice id].[namenode id]</h5><pre><code>&lt;property&gt;\n  &lt;name&gt;dfs.namenode.http-address.frin-cluster.frin-namenode1&lt;/name&gt;\n  &lt;value&gt;frin-namenode1:50070&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;dfs.namenode.http-address.frin-cluster.frin-namenode2&lt;/name&gt;\n  &lt;value&gt;frin-namenode2:50070&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<h5 id=\"dfs-namenode-shared-edits-dir\"><a href=\"#dfs-namenode-shared-edits-dir\" class=\"headerlink\" title=\"dfs.namenode.shared.edits.dir\"></a>dfs.namenode.shared.edits.dir</h5><pre><code>&lt;property&gt;\n  &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;\n  &lt;value&gt;qjournal://frin-journalnode1:8485;frin-journalnode2:8485;frin-journalnode3:8485/frin-cluster&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<h5 id=\"dfs-client-failover-proxy-provider-nameservice-id\"><a href=\"#dfs-client-failover-proxy-provider-nameservice-id\" class=\"headerlink\" title=\"dfs.client.failover.proxy.provider.[nameservice id]\"></a>dfs.client.failover.proxy.provider.[nameservice id]</h5><pre><code>&lt;property&gt;\n  &lt;name&gt;dfs.client.failover.proxy.provider.frin-cluster&lt;/name&gt;\n  &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<h5 id=\"dfs-ha-fencing-methods\"><a href=\"#dfs-ha-fencing-methods\" class=\"headerlink\" title=\"dfs.ha.fencing.methods\"></a>dfs.ha.fencing.methods</h5><pre><code>&lt;property&gt;\n  &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;\n  &lt;value&gt;sshfence&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;\n  &lt;value&gt;/home/hadoop/.ssh/id_rsa&lt;/value&gt;\n&lt;property&gt;\n&lt;property&gt;\n  &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt;\n  &lt;value&gt;30000&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<h5 id=\"dfs-journalnode-edits-dir\"><a href=\"#dfs-journalnode-edits-dir\" class=\"headerlink\" title=\"dfs.journalnode.edits.dir\"></a>dfs.journalnode.edits.dir</h5><pre><code>&lt;property&gt;\n  &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;\n  &lt;value&gt;/data/dfs/journal/edits&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<h4 id=\"core-site-xml-fs-defaultFS-配置\"><a href=\"#core-site-xml-fs-defaultFS-配置\" class=\"headerlink\" title=\"core-site.xml fs.defaultFS 配置\"></a>core-site.xml fs.defaultFS 配置</h4><pre><code>&lt;property&gt;\n  &lt;name&gt;fs.defaultFS&lt;/name&gt;\n  &lt;value&gt;hdfs://frin-namenode1:9000&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<p>改为</p>\n<pre><code>&lt;property&gt;\n  &lt;name&gt;fs.defaultFS&lt;/name&gt;\n  &lt;value&gt;hdfs://frin-cluster&lt;/value&gt;\n</code></pre>\n   </property>\n\n<h4 id=\"创建目录：\"><a href=\"#创建目录：\" class=\"headerlink\" title=\"创建目录：\"></a>创建目录：</h4><p>在journalnode节点上建立目录：/data/dfs/journal/edits</p>\n<h4 id=\"hosts-配置\"><a href=\"#hosts-配置\" class=\"headerlink\" title=\"hosts 配置\"></a>hosts 配置</h4><p>/etc/hosts 中添加如下内容：</p>\n<pre><code>10.142.166.81 frin-namenode1\n10.142.166.119 frin-namenode2\n\n10.142.166.81 frin-journalnode1\n10.142.166.119 frin-journalnode2\n10.142.165.40 frin-journalnode3\n\n10.142.166.119 frin-resourcemanager1\n10.142.166.119 frin-jobhistoryserver\n\n10.142.166.81 vm-10-142-166-81\n10.142.166.119 vm-10-142-166-119\n10.142.165.40 vm-10-142-165-40\n10.142.165.41 vm-10-142-165-41\n10.142.165.44 vm-10-142-165-44\n</code></pre>\n<h3 id=\"启动-HDFS-HA\"><a href=\"#启动-HDFS-HA\" class=\"headerlink\" title=\"启动 HDFS HA\"></a>启动 HDFS HA</h3><h4 id=\"停止-NameNode\"><a href=\"#停止-NameNode\" class=\"headerlink\" title=\"停止 NameNode\"></a>停止 NameNode</h4><pre><code>hadoop-daemon.sh stop namenode\n</code></pre>\n<h4 id=\"分发配置\"><a href=\"#分发配置\" class=\"headerlink\" title=\"分发配置\"></a>分发配置</h4><p>将以下修改后的配置文件分发到集群所有节点：</p>\n<pre><code>core-site.xml\nhdfs-site.xml\n</code></pre>\n<h4 id=\"重启DataNodes\"><a href=\"#重启DataNodes\" class=\"headerlink\" title=\"重启DataNodes\"></a>重启DataNodes</h4><pre><code>hadoop-daemon.sh stop datanode\nhadoop-daemon.sh start datanode\n</code></pre>\n<h4 id=\"启动-JournalNode\"><a href=\"#启动-JournalNode\" class=\"headerlink\" title=\"启动 JournalNode\"></a>启动 JournalNode</h4><pre><code>hadoop-daemon.sh start journalnode\n</code></pre>\n<h4 id=\"启动-NameNode1\"><a href=\"#启动-NameNode1\" class=\"headerlink\" title=\"启动 NameNode1\"></a>启动 NameNode1</h4><h5 id=\"初始化-JournalNode\"><a href=\"#初始化-JournalNode\" class=\"headerlink\" title=\"初始化 JournalNode\"></a>初始化 JournalNode</h5><p>从非 HA 改为 HA 的 NameNode 需要用 NameNode 本地 edits 目录中的数据初始化 JornalNode：</p>\n<pre><code>hdfs namenode -initializeSharedEdits\n</code></pre>\n<h5 id=\"启动-NameNode1-1\"><a href=\"#启动-NameNode1-1\" class=\"headerlink\" title=\"启动 NameNode1\"></a>启动 NameNode1</h5><pre><code>hadoop-daemon.sh start namenode\n</code></pre>\n<h5 id=\"是-NameNode1-变为-Active\"><a href=\"#是-NameNode1-变为-Active\" class=\"headerlink\" title=\"是 NameNode1 变为 Active\"></a>是 NameNode1 变为 Active</h5><p>如果 NameNode1 启动后状态为 standby，则用以下命令将 NameNode1 状态变为 active：</p>\n<pre><code>hdfs haadmin -transitionToActive frin-namenode1\n</code></pre>\n<h4 id=\"启动-NameNode2\"><a href=\"#启动-NameNode2\" class=\"headerlink\" title=\"启动 NameNode2\"></a>启动 NameNode2</h4><h5 id=\"初始化-NameNode2\"><a href=\"#初始化-NameNode2\" class=\"headerlink\" title=\"初始化 NameNode2\"></a>初始化 NameNode2</h5><p>从非 HA 改为 HA 的 NameNode，使用如下命令将 NameNode1 上的元数据信息拷贝到 NameNode2：</p>\n<pre><code>hdfs namenode -bootstrapStandby\n</code></pre>\n<h5 id=\"启动-NameNode2-1\"><a href=\"#启动-NameNode2-1\" class=\"headerlink\" title=\"启动 NameNode2\"></a>启动 NameNode2</h5><pre><code>hadoop-daemon.sh start namenode\n</code></pre>\n<h3 id=\"验证\"><a href=\"#验证\" class=\"headerlink\" title=\"验证\"></a>验证</h3><p>访问 NameNode1 的 Web 站点，可以看到 NameNode1 的状态为 Active：<img src=\"/uploads/20170327/namenode1.png\" alt=\"namenode1.png\"></p>\n<p>访问 NameNode2 的 Web 站点，可以看到 NameNode2 的状态为 standby：<img src=\"/uploads/20170327/namenode2.png\" alt=\"namenode2.png\"></p>\n","site":{"data":{}},"excerpt":"<h3 id=\"综述\"><a href=\"#综述\" class=\"headerlink\" title=\"综述\"></a>综述</h3><h4 id=\"集群搭建\"><a href=\"#集群搭建\" class=\"headerlink\" title=\"集群搭建\"></a>集群搭建</h4><p>Hadoop 集群搭建及服务器信息见我的另外一篇博客<a href=\"http://zhang-jc.github.io/2017/03/03/hadoop-2-7-3-%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/\">hadoop-2-7-3-集群安装</a></p>\n<h4 id=\"服务器列表\"><a href=\"#服务器列表\" class=\"headerlink\" title=\"服务器列表\"></a>服务器列表</h4><ul>\n<li>10.142.166.81：NameNode1/ JournalNode1</li>\n<li>10.142.166.119：NameNode2 / ResourceManager / JournalNode2 / MapReduce JobHistory Server</li>\n<li>10.142.165.40：DataNode / NodeManager / JournalNode3</li>\n<li>10.142.165.41 / 10.142.165.44：DataNode / NodeManager</li>\n</ul>","more":"<h3 id=\"架构\"><a href=\"#架构\" class=\"headerlink\" title=\"架构\"></a>架构</h3><p><img src=\"/uploads/20170327/hdfs-ha-qjm.png\" alt=\"Quorum Journal Manager\"></p>\n<p>为了防止出现“脑裂”情况， JournalNodes 同时只允许一个 NameNode 写入数据。实现 HA 后， Secondary NameNode、CheckpointNode、BackupNode 就都不需要了。</p>\n<h3 id=\"配置过程\"><a href=\"#配置过程\" class=\"headerlink\" title=\"配置过程\"></a>配置过程</h3><h4 id=\"hdfs-site-xml-配置\"><a href=\"#hdfs-site-xml-配置\" class=\"headerlink\" title=\"hdfs-site.xml 配置\"></a>hdfs-site.xml 配置</h4><h5 id=\"dfs-nameservices\"><a href=\"#dfs-nameservices\" class=\"headerlink\" title=\"dfs.nameservices\"></a>dfs.nameservices</h5><pre><code>&lt;property&gt;\n  &lt;name&gt;dfs.nameservices&lt;/name&gt;\n  &lt;value&gt;frin-cluster&lt;/value&gt;\n&lt;property&gt;\n</code></pre>\n<h5 id=\"dfs-ha-namenodes-nameservice-id\"><a href=\"#dfs-ha-namenodes-nameservice-id\" class=\"headerlink\" title=\"dfs.ha.namenodes.[nameservice id]\"></a>dfs.ha.namenodes.[nameservice id]</h5><pre><code>&lt;property&gt;\n  &lt;name&gt;dfs.ha.namenodes.frin-cluster&lt;/name&gt;\n  &lt;value&gt;frin-namenode1,frin-namenode2&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<p>目前，一个 nameservice 最多只能配置两台 NameNode。</p>\n<h5 id=\"dfs-namenode-rpc-address-nameservice-id-namenode-id\"><a href=\"#dfs-namenode-rpc-address-nameservice-id-namenode-id\" class=\"headerlink\" title=\"dfs.namenode.rpc-address.[nameservice id].[namenode id]\"></a>dfs.namenode.rpc-address.[nameservice id].[namenode id]</h5><pre><code>&lt;property&gt;\n  &lt;name&gt;dfs.namenode.rpc-address.frin-cluster.frin-namenode1&lt;/name&gt;\n  &lt;value&gt;frin-namenode1:8020&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;dfs.namenode.rpc-address.frin-cluster.frin-namenode2&lt;/name&gt;\n  &lt;value&gt;frin-namenode2:8020&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<h5 id=\"dfs-namenode-http-address-nameservice-id-namenode-id\"><a href=\"#dfs-namenode-http-address-nameservice-id-namenode-id\" class=\"headerlink\" title=\"dfs.namenode.http-address.[nameservice id].[namenode id]\"></a>dfs.namenode.http-address.[nameservice id].[namenode id]</h5><pre><code>&lt;property&gt;\n  &lt;name&gt;dfs.namenode.http-address.frin-cluster.frin-namenode1&lt;/name&gt;\n  &lt;value&gt;frin-namenode1:50070&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;dfs.namenode.http-address.frin-cluster.frin-namenode2&lt;/name&gt;\n  &lt;value&gt;frin-namenode2:50070&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<h5 id=\"dfs-namenode-shared-edits-dir\"><a href=\"#dfs-namenode-shared-edits-dir\" class=\"headerlink\" title=\"dfs.namenode.shared.edits.dir\"></a>dfs.namenode.shared.edits.dir</h5><pre><code>&lt;property&gt;\n  &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;\n  &lt;value&gt;qjournal://frin-journalnode1:8485;frin-journalnode2:8485;frin-journalnode3:8485/frin-cluster&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<h5 id=\"dfs-client-failover-proxy-provider-nameservice-id\"><a href=\"#dfs-client-failover-proxy-provider-nameservice-id\" class=\"headerlink\" title=\"dfs.client.failover.proxy.provider.[nameservice id]\"></a>dfs.client.failover.proxy.provider.[nameservice id]</h5><pre><code>&lt;property&gt;\n  &lt;name&gt;dfs.client.failover.proxy.provider.frin-cluster&lt;/name&gt;\n  &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<h5 id=\"dfs-ha-fencing-methods\"><a href=\"#dfs-ha-fencing-methods\" class=\"headerlink\" title=\"dfs.ha.fencing.methods\"></a>dfs.ha.fencing.methods</h5><pre><code>&lt;property&gt;\n  &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;\n  &lt;value&gt;sshfence&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n  &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;\n  &lt;value&gt;/home/hadoop/.ssh/id_rsa&lt;/value&gt;\n&lt;property&gt;\n&lt;property&gt;\n  &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt;\n  &lt;value&gt;30000&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<h5 id=\"dfs-journalnode-edits-dir\"><a href=\"#dfs-journalnode-edits-dir\" class=\"headerlink\" title=\"dfs.journalnode.edits.dir\"></a>dfs.journalnode.edits.dir</h5><pre><code>&lt;property&gt;\n  &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;\n  &lt;value&gt;/data/dfs/journal/edits&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<h4 id=\"core-site-xml-fs-defaultFS-配置\"><a href=\"#core-site-xml-fs-defaultFS-配置\" class=\"headerlink\" title=\"core-site.xml fs.defaultFS 配置\"></a>core-site.xml fs.defaultFS 配置</h4><pre><code>&lt;property&gt;\n  &lt;name&gt;fs.defaultFS&lt;/name&gt;\n  &lt;value&gt;hdfs://frin-namenode1:9000&lt;/value&gt;\n&lt;/property&gt;\n</code></pre>\n<p>改为</p>\n<pre><code>&lt;property&gt;\n  &lt;name&gt;fs.defaultFS&lt;/name&gt;\n  &lt;value&gt;hdfs://frin-cluster&lt;/value&gt;\n</code></pre>\n   </property>\n\n<h4 id=\"创建目录：\"><a href=\"#创建目录：\" class=\"headerlink\" title=\"创建目录：\"></a>创建目录：</h4><p>在journalnode节点上建立目录：/data/dfs/journal/edits</p>\n<h4 id=\"hosts-配置\"><a href=\"#hosts-配置\" class=\"headerlink\" title=\"hosts 配置\"></a>hosts 配置</h4><p>/etc/hosts 中添加如下内容：</p>\n<pre><code>10.142.166.81 frin-namenode1\n10.142.166.119 frin-namenode2\n\n10.142.166.81 frin-journalnode1\n10.142.166.119 frin-journalnode2\n10.142.165.40 frin-journalnode3\n\n10.142.166.119 frin-resourcemanager1\n10.142.166.119 frin-jobhistoryserver\n\n10.142.166.81 vm-10-142-166-81\n10.142.166.119 vm-10-142-166-119\n10.142.165.40 vm-10-142-165-40\n10.142.165.41 vm-10-142-165-41\n10.142.165.44 vm-10-142-165-44\n</code></pre>\n<h3 id=\"启动-HDFS-HA\"><a href=\"#启动-HDFS-HA\" class=\"headerlink\" title=\"启动 HDFS HA\"></a>启动 HDFS HA</h3><h4 id=\"停止-NameNode\"><a href=\"#停止-NameNode\" class=\"headerlink\" title=\"停止 NameNode\"></a>停止 NameNode</h4><pre><code>hadoop-daemon.sh stop namenode\n</code></pre>\n<h4 id=\"分发配置\"><a href=\"#分发配置\" class=\"headerlink\" title=\"分发配置\"></a>分发配置</h4><p>将以下修改后的配置文件分发到集群所有节点：</p>\n<pre><code>core-site.xml\nhdfs-site.xml\n</code></pre>\n<h4 id=\"重启DataNodes\"><a href=\"#重启DataNodes\" class=\"headerlink\" title=\"重启DataNodes\"></a>重启DataNodes</h4><pre><code>hadoop-daemon.sh stop datanode\nhadoop-daemon.sh start datanode\n</code></pre>\n<h4 id=\"启动-JournalNode\"><a href=\"#启动-JournalNode\" class=\"headerlink\" title=\"启动 JournalNode\"></a>启动 JournalNode</h4><pre><code>hadoop-daemon.sh start journalnode\n</code></pre>\n<h4 id=\"启动-NameNode1\"><a href=\"#启动-NameNode1\" class=\"headerlink\" title=\"启动 NameNode1\"></a>启动 NameNode1</h4><h5 id=\"初始化-JournalNode\"><a href=\"#初始化-JournalNode\" class=\"headerlink\" title=\"初始化 JournalNode\"></a>初始化 JournalNode</h5><p>从非 HA 改为 HA 的 NameNode 需要用 NameNode 本地 edits 目录中的数据初始化 JornalNode：</p>\n<pre><code>hdfs namenode -initializeSharedEdits\n</code></pre>\n<h5 id=\"启动-NameNode1-1\"><a href=\"#启动-NameNode1-1\" class=\"headerlink\" title=\"启动 NameNode1\"></a>启动 NameNode1</h5><pre><code>hadoop-daemon.sh start namenode\n</code></pre>\n<h5 id=\"是-NameNode1-变为-Active\"><a href=\"#是-NameNode1-变为-Active\" class=\"headerlink\" title=\"是 NameNode1 变为 Active\"></a>是 NameNode1 变为 Active</h5><p>如果 NameNode1 启动后状态为 standby，则用以下命令将 NameNode1 状态变为 active：</p>\n<pre><code>hdfs haadmin -transitionToActive frin-namenode1\n</code></pre>\n<h4 id=\"启动-NameNode2\"><a href=\"#启动-NameNode2\" class=\"headerlink\" title=\"启动 NameNode2\"></a>启动 NameNode2</h4><h5 id=\"初始化-NameNode2\"><a href=\"#初始化-NameNode2\" class=\"headerlink\" title=\"初始化 NameNode2\"></a>初始化 NameNode2</h5><p>从非 HA 改为 HA 的 NameNode，使用如下命令将 NameNode1 上的元数据信息拷贝到 NameNode2：</p>\n<pre><code>hdfs namenode -bootstrapStandby\n</code></pre>\n<h5 id=\"启动-NameNode2-1\"><a href=\"#启动-NameNode2-1\" class=\"headerlink\" title=\"启动 NameNode2\"></a>启动 NameNode2</h5><pre><code>hadoop-daemon.sh start namenode\n</code></pre>\n<h3 id=\"验证\"><a href=\"#验证\" class=\"headerlink\" title=\"验证\"></a>验证</h3><p>访问 NameNode1 的 Web 站点，可以看到 NameNode1 的状态为 Active：<img src=\"/uploads/20170327/namenode1.png\" alt=\"namenode1.png\"></p>\n<p>访问 NameNode2 的 Web 站点，可以看到 NameNode2 的状态为 standby：<img src=\"/uploads/20170327/namenode2.png\" alt=\"namenode2.png\"></p>"},{"title":"安装 Nginx","date":"2016-04-09T15:30:54.000Z","_content":"\n\n对于 Linux 平台，[Nginx 安装包](http://nginx.org/en/linux_packages.html) 可以从 nginx.org 下载。\n\n<!-- more -->\n\n**Ubuntu:**\n\n版本&nbsp;&nbsp;&nbsp;&nbsp;Codename&nbsp;&nbsp;&nbsp;&nbsp;支持平台  \n12.04&nbsp;&nbsp;&nbsp;precise&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x86_64, i386  \n14.04&nbsp;&nbsp;&nbsp;trusty&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x86_64, i386, aarch64/arm64  \n15.10&nbsp;&nbsp;&nbsp;wily&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x86_64, i386\n\n在 Debian/Ubuntu 系统上，为了生效 nginx 仓库的签名，并且消除 nginx 安装过程中丢失 PGP 密钥的告警，必须向 apt 程序密钥环添加用来签名 nginx 安装包和仓库的密钥。请从 nginx 网站下载[密钥](http://nginx.org/keys/nginx_signing.key)，并且用下面的命令添加到 apt 程序密钥环中：\n\n> sudo apt-key add nginx_signing.key\n\n用上表中 Ubuntu 发行版本的 codename 替换 *codename* ，然后添加下面内容到 /etc/apt/sources.list 文件的末尾：\n\n> deb http://nginx.org/packages/ubuntu/ *codename* nginx  \n> deb-src http://nginx.org/packages/ubuntu/ *codename* nginx\n\n然后执行一下命令完成安装：\n\n> apt-get update  \n> apt-get install nginx\n","source":"_posts/安装-Nginx.md","raw":"title: 安装 Nginx\ntags:\n  - Nginx\ncategories:\n  - 开发工具\n  - Nginx\ndate: 2016-04-09 23:30:54\n---\n\n\n对于 Linux 平台，[Nginx 安装包](http://nginx.org/en/linux_packages.html) 可以从 nginx.org 下载。\n\n<!-- more -->\n\n**Ubuntu:**\n\n版本&nbsp;&nbsp;&nbsp;&nbsp;Codename&nbsp;&nbsp;&nbsp;&nbsp;支持平台  \n12.04&nbsp;&nbsp;&nbsp;precise&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x86_64, i386  \n14.04&nbsp;&nbsp;&nbsp;trusty&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x86_64, i386, aarch64/arm64  \n15.10&nbsp;&nbsp;&nbsp;wily&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x86_64, i386\n\n在 Debian/Ubuntu 系统上，为了生效 nginx 仓库的签名，并且消除 nginx 安装过程中丢失 PGP 密钥的告警，必须向 apt 程序密钥环添加用来签名 nginx 安装包和仓库的密钥。请从 nginx 网站下载[密钥](http://nginx.org/keys/nginx_signing.key)，并且用下面的命令添加到 apt 程序密钥环中：\n\n> sudo apt-key add nginx_signing.key\n\n用上表中 Ubuntu 发行版本的 codename 替换 *codename* ，然后添加下面内容到 /etc/apt/sources.list 文件的末尾：\n\n> deb http://nginx.org/packages/ubuntu/ *codename* nginx  \n> deb-src http://nginx.org/packages/ubuntu/ *codename* nginx\n\n然后执行一下命令完成安装：\n\n> apt-get update  \n> apt-get install nginx\n","slug":"安装-Nginx","published":1,"updated":"2021-07-19T16:28:00.308Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphrz00gditd31mog2g5u","content":"<p>对于 Linux 平台，<a href=\"http://nginx.org/en/linux_packages.html\">Nginx 安装包</a> 可以从 nginx.org 下载。</p>\n<span id=\"more\"></span>\n\n<p><strong>Ubuntu:</strong></p>\n<p>版本&nbsp;&nbsp;&nbsp;&nbsp;Codename&nbsp;&nbsp;&nbsp;&nbsp;支持平台<br>12.04&nbsp;&nbsp;&nbsp;precise&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x86_64, i386<br>14.04&nbsp;&nbsp;&nbsp;trusty&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x86_64, i386, aarch64/arm64<br>15.10&nbsp;&nbsp;&nbsp;wily&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x86_64, i386</p>\n<p>在 Debian/Ubuntu 系统上，为了生效 nginx 仓库的签名，并且消除 nginx 安装过程中丢失 PGP 密钥的告警，必须向 apt 程序密钥环添加用来签名 nginx 安装包和仓库的密钥。请从 nginx 网站下载<a href=\"http://nginx.org/keys/nginx_signing.key\">密钥</a>，并且用下面的命令添加到 apt 程序密钥环中：</p>\n<blockquote>\n<p>sudo apt-key add nginx_signing.key</p>\n</blockquote>\n<p>用上表中 Ubuntu 发行版本的 codename 替换 <em>codename</em> ，然后添加下面内容到 /etc/apt/sources.list 文件的末尾：</p>\n<blockquote>\n<p>deb <a href=\"http://nginx.org/packages/ubuntu/\">http://nginx.org/packages/ubuntu/</a> <em>codename</em> nginx<br>deb-src <a href=\"http://nginx.org/packages/ubuntu/\">http://nginx.org/packages/ubuntu/</a> <em>codename</em> nginx</p>\n</blockquote>\n<p>然后执行一下命令完成安装：</p>\n<blockquote>\n<p>apt-get update<br>apt-get install nginx</p>\n</blockquote>\n","site":{"data":{}},"excerpt":"<p>对于 Linux 平台，<a href=\"http://nginx.org/en/linux_packages.html\">Nginx 安装包</a> 可以从 nginx.org 下载。</p>","more":"<p><strong>Ubuntu:</strong></p>\n<p>版本&nbsp;&nbsp;&nbsp;&nbsp;Codename&nbsp;&nbsp;&nbsp;&nbsp;支持平台<br>12.04&nbsp;&nbsp;&nbsp;precise&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x86_64, i386<br>14.04&nbsp;&nbsp;&nbsp;trusty&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x86_64, i386, aarch64/arm64<br>15.10&nbsp;&nbsp;&nbsp;wily&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x86_64, i386</p>\n<p>在 Debian/Ubuntu 系统上，为了生效 nginx 仓库的签名，并且消除 nginx 安装过程中丢失 PGP 密钥的告警，必须向 apt 程序密钥环添加用来签名 nginx 安装包和仓库的密钥。请从 nginx 网站下载<a href=\"http://nginx.org/keys/nginx_signing.key\">密钥</a>，并且用下面的命令添加到 apt 程序密钥环中：</p>\n<blockquote>\n<p>sudo apt-key add nginx_signing.key</p>\n</blockquote>\n<p>用上表中 Ubuntu 发行版本的 codename 替换 <em>codename</em> ，然后添加下面内容到 /etc/apt/sources.list 文件的末尾：</p>\n<blockquote>\n<p>deb <a href=\"http://nginx.org/packages/ubuntu/\">http://nginx.org/packages/ubuntu/</a> <em>codename</em> nginx<br>deb-src <a href=\"http://nginx.org/packages/ubuntu/\">http://nginx.org/packages/ubuntu/</a> <em>codename</em> nginx</p>\n</blockquote>\n<p>然后执行一下命令完成安装：</p>\n<blockquote>\n<p>apt-get update<br>apt-get install nginx</p>\n</blockquote>"},{"title":"开启 Hadoop HDFS 审计日志","date":"2017-03-03T03:32:35.000Z","_content":"\n1. etc/hadoop/log4j.properties\n\n修改 etc/hadoop/log4j.properties 中 hdfs audit logging 部分，将以下配置：\n\n    hdfs.audit.logger=INFO,NullAppender\n\n改为：\n\n    hdfs.audit.logger=INFO,RFAAUDIT\n\n<!-- more -->\n\n2. etc/hadoop/hadoop-env.sh\n\n找到 etc/hadoop/hadoop-env.sh 文件中 HADOOP_NAMENODE_OPTS 配置部分，将原配置：\n\n    export HADOOP_NAMENODE_OPTS=\"-Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,RFAS} -Dhdfs.audit.logger=${HDFS_AUDIT_LOGGER:-INFO,NullAppender} $HADOOP_NAMENODE_OPTS\"\n\n修改为：\n\n    export HADOOP_NAMENODE_OPTS=\"-Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,RFAS} $HADOOP_NAMENODE_OPTS\"\n\n删除 -Dhdfs.audit.logger=${HDFS_AUDIT_LOGGER:-INFO,NullAppender}，使用 log4j.properties 中的配置。","source":"_posts/开启-Hadoop-HDFS-审计日志.md","raw":"title: 开启 Hadoop HDFS 审计日志\ntags:\n  - Hadoop\n  - HDFS\ncategories:\n  - 大数据\n  - Hadoop\ndate: 2017-03-03 11:32:35\n---\n\n1. etc/hadoop/log4j.properties\n\n修改 etc/hadoop/log4j.properties 中 hdfs audit logging 部分，将以下配置：\n\n    hdfs.audit.logger=INFO,NullAppender\n\n改为：\n\n    hdfs.audit.logger=INFO,RFAAUDIT\n\n<!-- more -->\n\n2. etc/hadoop/hadoop-env.sh\n\n找到 etc/hadoop/hadoop-env.sh 文件中 HADOOP_NAMENODE_OPTS 配置部分，将原配置：\n\n    export HADOOP_NAMENODE_OPTS=\"-Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,RFAS} -Dhdfs.audit.logger=${HDFS_AUDIT_LOGGER:-INFO,NullAppender} $HADOOP_NAMENODE_OPTS\"\n\n修改为：\n\n    export HADOOP_NAMENODE_OPTS=\"-Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,RFAS} $HADOOP_NAMENODE_OPTS\"\n\n删除 -Dhdfs.audit.logger=${HDFS_AUDIT_LOGGER:-INFO,NullAppender}，使用 log4j.properties 中的配置。","slug":"开启-Hadoop-HDFS-审计日志","published":1,"updated":"2021-07-19T16:28:00.308Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphs200ghitd31f0sg29s","content":"<ol>\n<li>etc/hadoop/log4j.properties</li>\n</ol>\n<p>修改 etc/hadoop/log4j.properties 中 hdfs audit logging 部分，将以下配置：</p>\n<pre><code>hdfs.audit.logger=INFO,NullAppender\n</code></pre>\n<p>改为：</p>\n<pre><code>hdfs.audit.logger=INFO,RFAAUDIT\n</code></pre>\n<span id=\"more\"></span>\n\n<ol start=\"2\">\n<li>etc/hadoop/hadoop-env.sh</li>\n</ol>\n<p>找到 etc/hadoop/hadoop-env.sh 文件中 HADOOP_NAMENODE_OPTS 配置部分，将原配置：</p>\n<pre><code>export HADOOP_NAMENODE_OPTS=&quot;-Dhadoop.security.logger=$&#123;HADOOP_SECURITY_LOGGER:-INFO,RFAS&#125; -Dhdfs.audit.logger=$&#123;HDFS_AUDIT_LOGGER:-INFO,NullAppender&#125; $HADOOP_NAMENODE_OPTS&quot;\n</code></pre>\n<p>修改为：</p>\n<pre><code>export HADOOP_NAMENODE_OPTS=&quot;-Dhadoop.security.logger=$&#123;HADOOP_SECURITY_LOGGER:-INFO,RFAS&#125; $HADOOP_NAMENODE_OPTS&quot;\n</code></pre>\n<p>删除 -Dhdfs.audit.logger=${HDFS_AUDIT_LOGGER:-INFO,NullAppender}，使用 log4j.properties 中的配置。</p>\n","site":{"data":{}},"excerpt":"<ol>\n<li>etc/hadoop/log4j.properties</li>\n</ol>\n<p>修改 etc/hadoop/log4j.properties 中 hdfs audit logging 部分，将以下配置：</p>\n<pre><code>hdfs.audit.logger=INFO,NullAppender\n</code></pre>\n<p>改为：</p>\n<pre><code>hdfs.audit.logger=INFO,RFAAUDIT\n</code></pre>","more":"<ol start=\"2\">\n<li>etc/hadoop/hadoop-env.sh</li>\n</ol>\n<p>找到 etc/hadoop/hadoop-env.sh 文件中 HADOOP_NAMENODE_OPTS 配置部分，将原配置：</p>\n<pre><code>export HADOOP_NAMENODE_OPTS=&quot;-Dhadoop.security.logger=$&#123;HADOOP_SECURITY_LOGGER:-INFO,RFAS&#125; -Dhdfs.audit.logger=$&#123;HDFS_AUDIT_LOGGER:-INFO,NullAppender&#125; $HADOOP_NAMENODE_OPTS&quot;\n</code></pre>\n<p>修改为：</p>\n<pre><code>export HADOOP_NAMENODE_OPTS=&quot;-Dhadoop.security.logger=$&#123;HADOOP_SECURITY_LOGGER:-INFO,RFAS&#125; $HADOOP_NAMENODE_OPTS&quot;\n</code></pre>\n<p>删除 -Dhdfs.audit.logger=${HDFS_AUDIT_LOGGER:-INFO,NullAppender}，使用 log4j.properties 中的配置。</p>"},{"title":"搭建 Hadoop 源代码阅读环境","date":"2016-08-28T18:11:42.000Z","_content":"\n\n### 环境\n\n- Mac OS X EI Capitan 10.11.6\n- java version \"1.7.0_80\"\n- git version 2.7.4 (Apple Git-66)\n- Apache Maven 3.3.9\n\n<!-- more -->\n\n### 下载源代码\n\n从 Git 上下载最新源代码：\n\n    git clone git://git.apache.org/hadoop-common.git\n\n### 构建代码\n\n构建代码，使项目可以导入到 Eclipse 中。切换目录到 hadoop-common，执行以下命令：\n\n    $ mvn install -DskipTests\n    $ mvn eclipse:eclipse -DdownloadSources=true -DdownloadJavadocs=true\n\n> 这个过程时间比较久，最好有内部的 Nexus 服务器，不然可能会不够顺畅。\n\n### Hadoop 2.7.3 源码环境\n\n下载地址：<http://hadoop.apache.org/releases.html>。下载 2.7.3 源代码包到本地。使用以下命令解压：\n\n    $ tar xzvf hadoop-2.7.3-src.tar.gz\n\n> 解压目录下的 hadoop-2.7.3-src/BUILDING.txt 文件提供了很多信息。\n\n源码构建方式同上。\n\n### Eclipse 中的操作\n\n#### Common\n\n- File -> Import...\n- Choose \"Existing Projects into Workspace\"\n- Select the hadoop-common-project directory as the root directory\n- Select the hadoop-annotations, hadoop-auth, hadoop-auth-examples, hadoop-nfs and hadoop-common projects\n- Click \"Finish\"\n- File -> Import...\n- Choose \"Existing Projects into Workspace\"\n- Select the hadoop-assemblies directory as the root directory\n- Select the hadoop-assemblies project\n- Click \"Finish\"\n- To get the projects to build cleanly:\n- * Add target/generated-test-sources/java as a source directory for hadoop-common\n- * You may have to add then remove the JRE System Library to avoid errors due to access restrictions\n\n最后一个操作的步骤：\n\n1. Go to the Build Path settings in the project properties.\n2. Remove the JRE System Library\n3. Add it back; Select \"Add Library\" and select the JRE System Library. The default worked for me.\n\n#### HDFS\n\n- File -> Import...\n- Choose \"Existing Projects into Workspace\"\n- Select the hadoop-hdfs-project directory as the root directory\n- Select the hadoop-hdfs project\n- Click \"Finish\"\n\n#### YARN\n\n- File -> Import...\n- Choose \"Existing Projects into Workspace\"\n- Select the hadoop-yarn-project directory as the root directory\n- Select the hadoop-yarn-project project\n- Click \"Finish\"\n\n#### MapReduce\n\n- File -> Import...\n- Choose \"Existing Projects into Workspace\"\n- Select the hadoop-mapreduce-project directory as the root directory\n- Select the hadoop-mapreduce-project project\n- Click \"Finish\"\n\n### 错误\n\n#### 错误: 程序包com.sun.javadoc不存在\n\n如果使用 JDK8 执行 mvn install -DskipTests 的话会报一下错误，需要替换为 JDK7 后再执行。\n\n    [INFO] -------------------------------------------------------------\n    [ERROR] COMPILATION ERROR :\n    [INFO] -------------------------------------------------------------\n    [ERROR] /Users/ling/work/git/hadoop-common/hadoop-common-project/hadoop-annotations/src/main/java/org/apache/hadoop/classification/tools/ExcludePrivateAnnotationsStandardDoclet.java:[20,22] 错误: 程序包com.sun.javadoc不存在\n\n#### 'protoc --version' did not return a version\n\n错误信息如下：\n\n    [ERROR] Failed to execute goal org.apache.hadoop:hadoop-maven-plugins:3.0.0-SNAPSHOT:protoc (compile-protoc) on project hadoop-common: org.apache.maven.plugin.MojoExecutionException: 'protoc --version' did not return a version -> [Help 1]\n\n这是因为没有安装 protoc 的缘故。我安装了最新版的 protoc3.0.0 重新执行报错信息如下：\n\n    [ERROR] Failed to execute goal org.apache.hadoop:hadoop-maven-plugins:3.0.0-SNAPSHOT:protoc (compile-protoc) on project hadoop-common: org.apache.maven.plugin.MojoExecutionException: protoc version is 'libprotoc 3.0.0', expected version is '2.5.0' -> [Help 1]\n\n安装 protoc2.5.0 版本后重新执行成功。\n\n#### hadoop-common 编译错误：Type AvroRecord cannot be resolved to a type\n\n- 下载 [avro-tools](http://mirrors.cnnic.cn/apache/avro/avro-1.7.7/java/avro-tools-1.7.7.jar) 最新版。\n- 进入源码目录 hadoop-2.7.3-src/hadoop-common-project/hadoop-common/src/test/avro，执行以下命令：\n\n    $ java -jar <所在目录>/avro-tools-1.7.7.jar compile schema avroRecord.avsc ../java/\n\n其中 avsc 文件是 avro 的模式文件，上面命令是要通过模式文件生成相应的 .java 文件。\n- 右键单击 eclipse 中的 hadoop-common 项目，然后 refresh。\n\n> 注意，avro-tools 不要下载最新版，要下载 1.7.7 版本；最新 1.8.1 版本测试失败。\n\n#### hadoop-common 编译错误：Type EchoRequestProto cannot be resolved\n\n- 进入源码目录 hadoop-2.7.3-src/hadoop-common-project/hadoop-common/src/test/proto，执行以下命令：\n\n    $ protoc \\-\\-java_out=../java *.proto\n\n- 右键单击 eclipse 中的 hadoop-common，然后 refresh。\n","source":"_posts/搭建-Hadoop-源代码阅读环境.md","raw":"title: 搭建 Hadoop 源代码阅读环境\ntags:\n  - Hadoop\n  - Eclipse\n  - Maven\ncategories:\n  - 大数据\n  - Hadoop\ndate: 2016-08-29 02:11:42\n---\n\n\n### 环境\n\n- Mac OS X EI Capitan 10.11.6\n- java version \"1.7.0_80\"\n- git version 2.7.4 (Apple Git-66)\n- Apache Maven 3.3.9\n\n<!-- more -->\n\n### 下载源代码\n\n从 Git 上下载最新源代码：\n\n    git clone git://git.apache.org/hadoop-common.git\n\n### 构建代码\n\n构建代码，使项目可以导入到 Eclipse 中。切换目录到 hadoop-common，执行以下命令：\n\n    $ mvn install -DskipTests\n    $ mvn eclipse:eclipse -DdownloadSources=true -DdownloadJavadocs=true\n\n> 这个过程时间比较久，最好有内部的 Nexus 服务器，不然可能会不够顺畅。\n\n### Hadoop 2.7.3 源码环境\n\n下载地址：<http://hadoop.apache.org/releases.html>。下载 2.7.3 源代码包到本地。使用以下命令解压：\n\n    $ tar xzvf hadoop-2.7.3-src.tar.gz\n\n> 解压目录下的 hadoop-2.7.3-src/BUILDING.txt 文件提供了很多信息。\n\n源码构建方式同上。\n\n### Eclipse 中的操作\n\n#### Common\n\n- File -> Import...\n- Choose \"Existing Projects into Workspace\"\n- Select the hadoop-common-project directory as the root directory\n- Select the hadoop-annotations, hadoop-auth, hadoop-auth-examples, hadoop-nfs and hadoop-common projects\n- Click \"Finish\"\n- File -> Import...\n- Choose \"Existing Projects into Workspace\"\n- Select the hadoop-assemblies directory as the root directory\n- Select the hadoop-assemblies project\n- Click \"Finish\"\n- To get the projects to build cleanly:\n- * Add target/generated-test-sources/java as a source directory for hadoop-common\n- * You may have to add then remove the JRE System Library to avoid errors due to access restrictions\n\n最后一个操作的步骤：\n\n1. Go to the Build Path settings in the project properties.\n2. Remove the JRE System Library\n3. Add it back; Select \"Add Library\" and select the JRE System Library. The default worked for me.\n\n#### HDFS\n\n- File -> Import...\n- Choose \"Existing Projects into Workspace\"\n- Select the hadoop-hdfs-project directory as the root directory\n- Select the hadoop-hdfs project\n- Click \"Finish\"\n\n#### YARN\n\n- File -> Import...\n- Choose \"Existing Projects into Workspace\"\n- Select the hadoop-yarn-project directory as the root directory\n- Select the hadoop-yarn-project project\n- Click \"Finish\"\n\n#### MapReduce\n\n- File -> Import...\n- Choose \"Existing Projects into Workspace\"\n- Select the hadoop-mapreduce-project directory as the root directory\n- Select the hadoop-mapreduce-project project\n- Click \"Finish\"\n\n### 错误\n\n#### 错误: 程序包com.sun.javadoc不存在\n\n如果使用 JDK8 执行 mvn install -DskipTests 的话会报一下错误，需要替换为 JDK7 后再执行。\n\n    [INFO] -------------------------------------------------------------\n    [ERROR] COMPILATION ERROR :\n    [INFO] -------------------------------------------------------------\n    [ERROR] /Users/ling/work/git/hadoop-common/hadoop-common-project/hadoop-annotations/src/main/java/org/apache/hadoop/classification/tools/ExcludePrivateAnnotationsStandardDoclet.java:[20,22] 错误: 程序包com.sun.javadoc不存在\n\n#### 'protoc --version' did not return a version\n\n错误信息如下：\n\n    [ERROR] Failed to execute goal org.apache.hadoop:hadoop-maven-plugins:3.0.0-SNAPSHOT:protoc (compile-protoc) on project hadoop-common: org.apache.maven.plugin.MojoExecutionException: 'protoc --version' did not return a version -> [Help 1]\n\n这是因为没有安装 protoc 的缘故。我安装了最新版的 protoc3.0.0 重新执行报错信息如下：\n\n    [ERROR] Failed to execute goal org.apache.hadoop:hadoop-maven-plugins:3.0.0-SNAPSHOT:protoc (compile-protoc) on project hadoop-common: org.apache.maven.plugin.MojoExecutionException: protoc version is 'libprotoc 3.0.0', expected version is '2.5.0' -> [Help 1]\n\n安装 protoc2.5.0 版本后重新执行成功。\n\n#### hadoop-common 编译错误：Type AvroRecord cannot be resolved to a type\n\n- 下载 [avro-tools](http://mirrors.cnnic.cn/apache/avro/avro-1.7.7/java/avro-tools-1.7.7.jar) 最新版。\n- 进入源码目录 hadoop-2.7.3-src/hadoop-common-project/hadoop-common/src/test/avro，执行以下命令：\n\n    $ java -jar <所在目录>/avro-tools-1.7.7.jar compile schema avroRecord.avsc ../java/\n\n其中 avsc 文件是 avro 的模式文件，上面命令是要通过模式文件生成相应的 .java 文件。\n- 右键单击 eclipse 中的 hadoop-common 项目，然后 refresh。\n\n> 注意，avro-tools 不要下载最新版，要下载 1.7.7 版本；最新 1.8.1 版本测试失败。\n\n#### hadoop-common 编译错误：Type EchoRequestProto cannot be resolved\n\n- 进入源码目录 hadoop-2.7.3-src/hadoop-common-project/hadoop-common/src/test/proto，执行以下命令：\n\n    $ protoc \\-\\-java_out=../java *.proto\n\n- 右键单击 eclipse 中的 hadoop-common，然后 refresh。\n","slug":"搭建-Hadoop-源代码阅读环境","published":1,"updated":"2021-07-19T16:28:00.308Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphs400glitd3fhjvh7d2","content":"<h3 id=\"环境\"><a href=\"#环境\" class=\"headerlink\" title=\"环境\"></a>环境</h3><ul>\n<li>Mac OS X EI Capitan 10.11.6</li>\n<li>java version “1.7.0_80”</li>\n<li>git version 2.7.4 (Apple Git-66)</li>\n<li>Apache Maven 3.3.9</li>\n</ul>\n<span id=\"more\"></span>\n\n<h3 id=\"下载源代码\"><a href=\"#下载源代码\" class=\"headerlink\" title=\"下载源代码\"></a>下载源代码</h3><p>从 Git 上下载最新源代码：</p>\n<pre><code>git clone git://git.apache.org/hadoop-common.git\n</code></pre>\n<h3 id=\"构建代码\"><a href=\"#构建代码\" class=\"headerlink\" title=\"构建代码\"></a>构建代码</h3><p>构建代码，使项目可以导入到 Eclipse 中。切换目录到 hadoop-common，执行以下命令：</p>\n<pre><code>$ mvn install -DskipTests\n$ mvn eclipse:eclipse -DdownloadSources=true -DdownloadJavadocs=true\n</code></pre>\n<blockquote>\n<p>这个过程时间比较久，最好有内部的 Nexus 服务器，不然可能会不够顺畅。</p>\n</blockquote>\n<h3 id=\"Hadoop-2-7-3-源码环境\"><a href=\"#Hadoop-2-7-3-源码环境\" class=\"headerlink\" title=\"Hadoop 2.7.3 源码环境\"></a>Hadoop 2.7.3 源码环境</h3><p>下载地址：<a href=\"http://hadoop.apache.org/releases.html\">http://hadoop.apache.org/releases.html</a>。下载 2.7.3 源代码包到本地。使用以下命令解压：</p>\n<pre><code>$ tar xzvf hadoop-2.7.3-src.tar.gz\n</code></pre>\n<blockquote>\n<p>解压目录下的 hadoop-2.7.3-src/BUILDING.txt 文件提供了很多信息。</p>\n</blockquote>\n<p>源码构建方式同上。</p>\n<h3 id=\"Eclipse-中的操作\"><a href=\"#Eclipse-中的操作\" class=\"headerlink\" title=\"Eclipse 中的操作\"></a>Eclipse 中的操作</h3><h4 id=\"Common\"><a href=\"#Common\" class=\"headerlink\" title=\"Common\"></a>Common</h4><ul>\n<li>File -&gt; Import…</li>\n<li>Choose “Existing Projects into Workspace”</li>\n<li>Select the hadoop-common-project directory as the root directory</li>\n<li>Select the hadoop-annotations, hadoop-auth, hadoop-auth-examples, hadoop-nfs and hadoop-common projects</li>\n<li>Click “Finish”</li>\n<li>File -&gt; Import…</li>\n<li>Choose “Existing Projects into Workspace”</li>\n<li>Select the hadoop-assemblies directory as the root directory</li>\n<li>Select the hadoop-assemblies project</li>\n<li>Click “Finish”</li>\n<li>To get the projects to build cleanly:</li>\n<li><ul>\n<li>Add target/generated-test-sources/java as a source directory for hadoop-common</li>\n</ul>\n</li>\n<li><ul>\n<li>You may have to add then remove the JRE System Library to avoid errors due to access restrictions</li>\n</ul>\n</li>\n</ul>\n<p>最后一个操作的步骤：</p>\n<ol>\n<li>Go to the Build Path settings in the project properties.</li>\n<li>Remove the JRE System Library</li>\n<li>Add it back; Select “Add Library” and select the JRE System Library. The default worked for me.</li>\n</ol>\n<h4 id=\"HDFS\"><a href=\"#HDFS\" class=\"headerlink\" title=\"HDFS\"></a>HDFS</h4><ul>\n<li>File -&gt; Import…</li>\n<li>Choose “Existing Projects into Workspace”</li>\n<li>Select the hadoop-hdfs-project directory as the root directory</li>\n<li>Select the hadoop-hdfs project</li>\n<li>Click “Finish”</li>\n</ul>\n<h4 id=\"YARN\"><a href=\"#YARN\" class=\"headerlink\" title=\"YARN\"></a>YARN</h4><ul>\n<li>File -&gt; Import…</li>\n<li>Choose “Existing Projects into Workspace”</li>\n<li>Select the hadoop-yarn-project directory as the root directory</li>\n<li>Select the hadoop-yarn-project project</li>\n<li>Click “Finish”</li>\n</ul>\n<h4 id=\"MapReduce\"><a href=\"#MapReduce\" class=\"headerlink\" title=\"MapReduce\"></a>MapReduce</h4><ul>\n<li>File -&gt; Import…</li>\n<li>Choose “Existing Projects into Workspace”</li>\n<li>Select the hadoop-mapreduce-project directory as the root directory</li>\n<li>Select the hadoop-mapreduce-project project</li>\n<li>Click “Finish”</li>\n</ul>\n<h3 id=\"错误\"><a href=\"#错误\" class=\"headerlink\" title=\"错误\"></a>错误</h3><h4 id=\"错误-程序包com-sun-javadoc不存在\"><a href=\"#错误-程序包com-sun-javadoc不存在\" class=\"headerlink\" title=\"错误: 程序包com.sun.javadoc不存在\"></a>错误: 程序包com.sun.javadoc不存在</h4><p>如果使用 JDK8 执行 mvn install -DskipTests 的话会报一下错误，需要替换为 JDK7 后再执行。</p>\n<pre><code>[INFO] -------------------------------------------------------------\n[ERROR] COMPILATION ERROR :\n[INFO] -------------------------------------------------------------\n[ERROR] /Users/ling/work/git/hadoop-common/hadoop-common-project/hadoop-annotations/src/main/java/org/apache/hadoop/classification/tools/ExcludePrivateAnnotationsStandardDoclet.java:[20,22] 错误: 程序包com.sun.javadoc不存在\n</code></pre>\n<h4 id=\"‘protoc-–version’-did-not-return-a-version\"><a href=\"#‘protoc-–version’-did-not-return-a-version\" class=\"headerlink\" title=\"‘protoc –version’ did not return a version\"></a>‘protoc –version’ did not return a version</h4><p>错误信息如下：</p>\n<pre><code>[ERROR] Failed to execute goal org.apache.hadoop:hadoop-maven-plugins:3.0.0-SNAPSHOT:protoc (compile-protoc) on project hadoop-common: org.apache.maven.plugin.MojoExecutionException: &#39;protoc --version&#39; did not return a version -&gt; [Help 1]\n</code></pre>\n<p>这是因为没有安装 protoc 的缘故。我安装了最新版的 protoc3.0.0 重新执行报错信息如下：</p>\n<pre><code>[ERROR] Failed to execute goal org.apache.hadoop:hadoop-maven-plugins:3.0.0-SNAPSHOT:protoc (compile-protoc) on project hadoop-common: org.apache.maven.plugin.MojoExecutionException: protoc version is &#39;libprotoc 3.0.0&#39;, expected version is &#39;2.5.0&#39; -&gt; [Help 1]\n</code></pre>\n<p>安装 protoc2.5.0 版本后重新执行成功。</p>\n<h4 id=\"hadoop-common-编译错误：Type-AvroRecord-cannot-be-resolved-to-a-type\"><a href=\"#hadoop-common-编译错误：Type-AvroRecord-cannot-be-resolved-to-a-type\" class=\"headerlink\" title=\"hadoop-common 编译错误：Type AvroRecord cannot be resolved to a type\"></a>hadoop-common 编译错误：Type AvroRecord cannot be resolved to a type</h4><ul>\n<li><p>下载 <a href=\"http://mirrors.cnnic.cn/apache/avro/avro-1.7.7/java/avro-tools-1.7.7.jar\">avro-tools</a> 最新版。</p>\n</li>\n<li><p>进入源码目录 hadoop-2.7.3-src/hadoop-common-project/hadoop-common/src/test/avro，执行以下命令：</p>\n<p>  $ java -jar &lt;所在目录&gt;/avro-tools-1.7.7.jar compile schema avroRecord.avsc ../java/</p>\n</li>\n</ul>\n<p>其中 avsc 文件是 avro 的模式文件，上面命令是要通过模式文件生成相应的 .java 文件。</p>\n<ul>\n<li>右键单击 eclipse 中的 hadoop-common 项目，然后 refresh。</li>\n</ul>\n<blockquote>\n<p>注意，avro-tools 不要下载最新版，要下载 1.7.7 版本；最新 1.8.1 版本测试失败。</p>\n</blockquote>\n<h4 id=\"hadoop-common-编译错误：Type-EchoRequestProto-cannot-be-resolved\"><a href=\"#hadoop-common-编译错误：Type-EchoRequestProto-cannot-be-resolved\" class=\"headerlink\" title=\"hadoop-common 编译错误：Type EchoRequestProto cannot be resolved\"></a>hadoop-common 编译错误：Type EchoRequestProto cannot be resolved</h4><ul>\n<li><p>进入源码目录 hadoop-2.7.3-src/hadoop-common-project/hadoop-common/src/test/proto，执行以下命令：</p>\n<p>  $ protoc --java_out=../java *.proto</p>\n</li>\n<li><p>右键单击 eclipse 中的 hadoop-common，然后 refresh。</p>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"<h3 id=\"环境\"><a href=\"#环境\" class=\"headerlink\" title=\"环境\"></a>环境</h3><ul>\n<li>Mac OS X EI Capitan 10.11.6</li>\n<li>java version “1.7.0_80”</li>\n<li>git version 2.7.4 (Apple Git-66)</li>\n<li>Apache Maven 3.3.9</li>\n</ul>","more":"<h3 id=\"下载源代码\"><a href=\"#下载源代码\" class=\"headerlink\" title=\"下载源代码\"></a>下载源代码</h3><p>从 Git 上下载最新源代码：</p>\n<pre><code>git clone git://git.apache.org/hadoop-common.git\n</code></pre>\n<h3 id=\"构建代码\"><a href=\"#构建代码\" class=\"headerlink\" title=\"构建代码\"></a>构建代码</h3><p>构建代码，使项目可以导入到 Eclipse 中。切换目录到 hadoop-common，执行以下命令：</p>\n<pre><code>$ mvn install -DskipTests\n$ mvn eclipse:eclipse -DdownloadSources=true -DdownloadJavadocs=true\n</code></pre>\n<blockquote>\n<p>这个过程时间比较久，最好有内部的 Nexus 服务器，不然可能会不够顺畅。</p>\n</blockquote>\n<h3 id=\"Hadoop-2-7-3-源码环境\"><a href=\"#Hadoop-2-7-3-源码环境\" class=\"headerlink\" title=\"Hadoop 2.7.3 源码环境\"></a>Hadoop 2.7.3 源码环境</h3><p>下载地址：<a href=\"http://hadoop.apache.org/releases.html\">http://hadoop.apache.org/releases.html</a>。下载 2.7.3 源代码包到本地。使用以下命令解压：</p>\n<pre><code>$ tar xzvf hadoop-2.7.3-src.tar.gz\n</code></pre>\n<blockquote>\n<p>解压目录下的 hadoop-2.7.3-src/BUILDING.txt 文件提供了很多信息。</p>\n</blockquote>\n<p>源码构建方式同上。</p>\n<h3 id=\"Eclipse-中的操作\"><a href=\"#Eclipse-中的操作\" class=\"headerlink\" title=\"Eclipse 中的操作\"></a>Eclipse 中的操作</h3><h4 id=\"Common\"><a href=\"#Common\" class=\"headerlink\" title=\"Common\"></a>Common</h4><ul>\n<li>File -&gt; Import…</li>\n<li>Choose “Existing Projects into Workspace”</li>\n<li>Select the hadoop-common-project directory as the root directory</li>\n<li>Select the hadoop-annotations, hadoop-auth, hadoop-auth-examples, hadoop-nfs and hadoop-common projects</li>\n<li>Click “Finish”</li>\n<li>File -&gt; Import…</li>\n<li>Choose “Existing Projects into Workspace”</li>\n<li>Select the hadoop-assemblies directory as the root directory</li>\n<li>Select the hadoop-assemblies project</li>\n<li>Click “Finish”</li>\n<li>To get the projects to build cleanly:</li>\n<li><ul>\n<li>Add target/generated-test-sources/java as a source directory for hadoop-common</li>\n</ul>\n</li>\n<li><ul>\n<li>You may have to add then remove the JRE System Library to avoid errors due to access restrictions</li>\n</ul>\n</li>\n</ul>\n<p>最后一个操作的步骤：</p>\n<ol>\n<li>Go to the Build Path settings in the project properties.</li>\n<li>Remove the JRE System Library</li>\n<li>Add it back; Select “Add Library” and select the JRE System Library. The default worked for me.</li>\n</ol>\n<h4 id=\"HDFS\"><a href=\"#HDFS\" class=\"headerlink\" title=\"HDFS\"></a>HDFS</h4><ul>\n<li>File -&gt; Import…</li>\n<li>Choose “Existing Projects into Workspace”</li>\n<li>Select the hadoop-hdfs-project directory as the root directory</li>\n<li>Select the hadoop-hdfs project</li>\n<li>Click “Finish”</li>\n</ul>\n<h4 id=\"YARN\"><a href=\"#YARN\" class=\"headerlink\" title=\"YARN\"></a>YARN</h4><ul>\n<li>File -&gt; Import…</li>\n<li>Choose “Existing Projects into Workspace”</li>\n<li>Select the hadoop-yarn-project directory as the root directory</li>\n<li>Select the hadoop-yarn-project project</li>\n<li>Click “Finish”</li>\n</ul>\n<h4 id=\"MapReduce\"><a href=\"#MapReduce\" class=\"headerlink\" title=\"MapReduce\"></a>MapReduce</h4><ul>\n<li>File -&gt; Import…</li>\n<li>Choose “Existing Projects into Workspace”</li>\n<li>Select the hadoop-mapreduce-project directory as the root directory</li>\n<li>Select the hadoop-mapreduce-project project</li>\n<li>Click “Finish”</li>\n</ul>\n<h3 id=\"错误\"><a href=\"#错误\" class=\"headerlink\" title=\"错误\"></a>错误</h3><h4 id=\"错误-程序包com-sun-javadoc不存在\"><a href=\"#错误-程序包com-sun-javadoc不存在\" class=\"headerlink\" title=\"错误: 程序包com.sun.javadoc不存在\"></a>错误: 程序包com.sun.javadoc不存在</h4><p>如果使用 JDK8 执行 mvn install -DskipTests 的话会报一下错误，需要替换为 JDK7 后再执行。</p>\n<pre><code>[INFO] -------------------------------------------------------------\n[ERROR] COMPILATION ERROR :\n[INFO] -------------------------------------------------------------\n[ERROR] /Users/ling/work/git/hadoop-common/hadoop-common-project/hadoop-annotations/src/main/java/org/apache/hadoop/classification/tools/ExcludePrivateAnnotationsStandardDoclet.java:[20,22] 错误: 程序包com.sun.javadoc不存在\n</code></pre>\n<h4 id=\"‘protoc-–version’-did-not-return-a-version\"><a href=\"#‘protoc-–version’-did-not-return-a-version\" class=\"headerlink\" title=\"‘protoc –version’ did not return a version\"></a>‘protoc –version’ did not return a version</h4><p>错误信息如下：</p>\n<pre><code>[ERROR] Failed to execute goal org.apache.hadoop:hadoop-maven-plugins:3.0.0-SNAPSHOT:protoc (compile-protoc) on project hadoop-common: org.apache.maven.plugin.MojoExecutionException: &#39;protoc --version&#39; did not return a version -&gt; [Help 1]\n</code></pre>\n<p>这是因为没有安装 protoc 的缘故。我安装了最新版的 protoc3.0.0 重新执行报错信息如下：</p>\n<pre><code>[ERROR] Failed to execute goal org.apache.hadoop:hadoop-maven-plugins:3.0.0-SNAPSHOT:protoc (compile-protoc) on project hadoop-common: org.apache.maven.plugin.MojoExecutionException: protoc version is &#39;libprotoc 3.0.0&#39;, expected version is &#39;2.5.0&#39; -&gt; [Help 1]\n</code></pre>\n<p>安装 protoc2.5.0 版本后重新执行成功。</p>\n<h4 id=\"hadoop-common-编译错误：Type-AvroRecord-cannot-be-resolved-to-a-type\"><a href=\"#hadoop-common-编译错误：Type-AvroRecord-cannot-be-resolved-to-a-type\" class=\"headerlink\" title=\"hadoop-common 编译错误：Type AvroRecord cannot be resolved to a type\"></a>hadoop-common 编译错误：Type AvroRecord cannot be resolved to a type</h4><ul>\n<li><p>下载 <a href=\"http://mirrors.cnnic.cn/apache/avro/avro-1.7.7/java/avro-tools-1.7.7.jar\">avro-tools</a> 最新版。</p>\n</li>\n<li><p>进入源码目录 hadoop-2.7.3-src/hadoop-common-project/hadoop-common/src/test/avro，执行以下命令：</p>\n<p>  $ java -jar &lt;所在目录&gt;/avro-tools-1.7.7.jar compile schema avroRecord.avsc ../java/</p>\n</li>\n</ul>\n<p>其中 avsc 文件是 avro 的模式文件，上面命令是要通过模式文件生成相应的 .java 文件。</p>\n<ul>\n<li>右键单击 eclipse 中的 hadoop-common 项目，然后 refresh。</li>\n</ul>\n<blockquote>\n<p>注意，avro-tools 不要下载最新版，要下载 1.7.7 版本；最新 1.8.1 版本测试失败。</p>\n</blockquote>\n<h4 id=\"hadoop-common-编译错误：Type-EchoRequestProto-cannot-be-resolved\"><a href=\"#hadoop-common-编译错误：Type-EchoRequestProto-cannot-be-resolved\" class=\"headerlink\" title=\"hadoop-common 编译错误：Type EchoRequestProto cannot be resolved\"></a>hadoop-common 编译错误：Type EchoRequestProto cannot be resolved</h4><ul>\n<li><p>进入源码目录 hadoop-2.7.3-src/hadoop-common-project/hadoop-common/src/test/proto，执行以下命令：</p>\n<p>  $ protoc --java_out=../java *.proto</p>\n</li>\n<li><p>右键单击 eclipse 中的 hadoop-common，然后 refresh。</p>\n</li>\n</ul>"},{"title":"机会成本","date":"2019-12-19T04:58:50.000Z","_content":"\n### 定义\n\n机会成本（Opportunity Cost）是指在面临多方案择一决策时，被舍弃的选项中的最高价值者是本次决策的机会成本。\n\n机会成本又称为择一成本、替代性成本。机会成本对商业公司来说，可以是利用一定的时间或资源生产一种商品时，而失去的利用这些资源生产其他最佳替代品的机会就是机会成本。\n\n### 涵盖内容\n\n- 使用他人资源的机会成本，即付给资源拥有者的货币代价被称作显性成本。\n- 因为使用自有资源而放弃其他可能性中得到的最大回报的那个代价，也被称为隐性成本。\n\n### 前提条件\n\n利用机会成本概念进行经济分析的前提条件是：\n\n- 资源是稀缺的\n- 资源具有多种用途\n- 资源已经得到充分利用\n- 资源可以自由流动","source":"_posts/机会成本.md","raw":"title: 机会成本\ndate: 2019-12-19 12:58:50\ntags:\n- 成本\n- 经济学\ncategories:\n- 经济学\n- 成本\n---\n\n### 定义\n\n机会成本（Opportunity Cost）是指在面临多方案择一决策时，被舍弃的选项中的最高价值者是本次决策的机会成本。\n\n机会成本又称为择一成本、替代性成本。机会成本对商业公司来说，可以是利用一定的时间或资源生产一种商品时，而失去的利用这些资源生产其他最佳替代品的机会就是机会成本。\n\n### 涵盖内容\n\n- 使用他人资源的机会成本，即付给资源拥有者的货币代价被称作显性成本。\n- 因为使用自有资源而放弃其他可能性中得到的最大回报的那个代价，也被称为隐性成本。\n\n### 前提条件\n\n利用机会成本概念进行经济分析的前提条件是：\n\n- 资源是稀缺的\n- 资源具有多种用途\n- 资源已经得到充分利用\n- 资源可以自由流动","slug":"机会成本","published":1,"updated":"2021-07-19T16:28:00.328Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphs900gpitd30p833eap","content":"<h3 id=\"定义\"><a href=\"#定义\" class=\"headerlink\" title=\"定义\"></a>定义</h3><p>机会成本（Opportunity Cost）是指在面临多方案择一决策时，被舍弃的选项中的最高价值者是本次决策的机会成本。</p>\n<p>机会成本又称为择一成本、替代性成本。机会成本对商业公司来说，可以是利用一定的时间或资源生产一种商品时，而失去的利用这些资源生产其他最佳替代品的机会就是机会成本。</p>\n<h3 id=\"涵盖内容\"><a href=\"#涵盖内容\" class=\"headerlink\" title=\"涵盖内容\"></a>涵盖内容</h3><ul>\n<li>使用他人资源的机会成本，即付给资源拥有者的货币代价被称作显性成本。</li>\n<li>因为使用自有资源而放弃其他可能性中得到的最大回报的那个代价，也被称为隐性成本。</li>\n</ul>\n<h3 id=\"前提条件\"><a href=\"#前提条件\" class=\"headerlink\" title=\"前提条件\"></a>前提条件</h3><p>利用机会成本概念进行经济分析的前提条件是：</p>\n<ul>\n<li>资源是稀缺的</li>\n<li>资源具有多种用途</li>\n<li>资源已经得到充分利用</li>\n<li>资源可以自由流动</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"定义\"><a href=\"#定义\" class=\"headerlink\" title=\"定义\"></a>定义</h3><p>机会成本（Opportunity Cost）是指在面临多方案择一决策时，被舍弃的选项中的最高价值者是本次决策的机会成本。</p>\n<p>机会成本又称为择一成本、替代性成本。机会成本对商业公司来说，可以是利用一定的时间或资源生产一种商品时，而失去的利用这些资源生产其他最佳替代品的机会就是机会成本。</p>\n<h3 id=\"涵盖内容\"><a href=\"#涵盖内容\" class=\"headerlink\" title=\"涵盖内容\"></a>涵盖内容</h3><ul>\n<li>使用他人资源的机会成本，即付给资源拥有者的货币代价被称作显性成本。</li>\n<li>因为使用自有资源而放弃其他可能性中得到的最大回报的那个代价，也被称为隐性成本。</li>\n</ul>\n<h3 id=\"前提条件\"><a href=\"#前提条件\" class=\"headerlink\" title=\"前提条件\"></a>前提条件</h3><p>利用机会成本概念进行经济分析的前提条件是：</p>\n<ul>\n<li>资源是稀缺的</li>\n<li>资源具有多种用途</li>\n<li>资源已经得到充分利用</li>\n<li>资源可以自由流动</li>\n</ul>\n"},{"title":"沉没成本","date":"2019-12-20T05:15:12.000Z","_content":"\n沉没成本（Sunk Cost）是指以往发生的，但与当前决策无关的费用。从决策的角度看，以往发生的费用只是造成当前状态的某个因素，当前决策所要考虑的是未来可能发生的费用及所带来的收益，而不考虑以往发生的费用。\n\n人们在决定是否去做一件事情的时候，不仅是看这件事对自己有没有好处，而且也看过去是不是已经在这件事情上有过投入。我们把这些已经发生不可收回的支出，如时间、金钱、精力等称为“沉没成本”。在经济学和商业决策制定过程中会用到“沉没成本”的概念，代指已经付出且不可收回的成本。沉没成本常用来和可变成本作比较，可变成本可以被改变，而沉没成本则不能被改变。\n\n例如，买了一张电影票，不能退票的那种，那么买这张电影票的钱就属于沉没成本。电影开场后，在看电影的过程中发现电影不是自己喜欢的类型，现在有两个选择继续看完还是中间退场。不管坐哪个选择，票钱都不应该做为做出选择的依据，因为不管那种选择票钱都是没法挽回的。","source":"_posts/沉没成本.md","raw":"title: 沉没成本\ndate: 2019-12-20 13:15:12\ntags:\n- 成本\n- 经济学\ncategories:\n- 经济学\n- 成本\n---\n\n沉没成本（Sunk Cost）是指以往发生的，但与当前决策无关的费用。从决策的角度看，以往发生的费用只是造成当前状态的某个因素，当前决策所要考虑的是未来可能发生的费用及所带来的收益，而不考虑以往发生的费用。\n\n人们在决定是否去做一件事情的时候，不仅是看这件事对自己有没有好处，而且也看过去是不是已经在这件事情上有过投入。我们把这些已经发生不可收回的支出，如时间、金钱、精力等称为“沉没成本”。在经济学和商业决策制定过程中会用到“沉没成本”的概念，代指已经付出且不可收回的成本。沉没成本常用来和可变成本作比较，可变成本可以被改变，而沉没成本则不能被改变。\n\n例如，买了一张电影票，不能退票的那种，那么买这张电影票的钱就属于沉没成本。电影开场后，在看电影的过程中发现电影不是自己喜欢的类型，现在有两个选择继续看完还是中间退场。不管坐哪个选择，票钱都不应该做为做出选择的依据，因为不管那种选择票钱都是没法挽回的。","slug":"沉没成本","published":1,"updated":"2021-07-19T16:28:00.328Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphsb00gtitd33j4z7rno","content":"<p>沉没成本（Sunk Cost）是指以往发生的，但与当前决策无关的费用。从决策的角度看，以往发生的费用只是造成当前状态的某个因素，当前决策所要考虑的是未来可能发生的费用及所带来的收益，而不考虑以往发生的费用。</p>\n<p>人们在决定是否去做一件事情的时候，不仅是看这件事对自己有没有好处，而且也看过去是不是已经在这件事情上有过投入。我们把这些已经发生不可收回的支出，如时间、金钱、精力等称为“沉没成本”。在经济学和商业决策制定过程中会用到“沉没成本”的概念，代指已经付出且不可收回的成本。沉没成本常用来和可变成本作比较，可变成本可以被改变，而沉没成本则不能被改变。</p>\n<p>例如，买了一张电影票，不能退票的那种，那么买这张电影票的钱就属于沉没成本。电影开场后，在看电影的过程中发现电影不是自己喜欢的类型，现在有两个选择继续看完还是中间退场。不管坐哪个选择，票钱都不应该做为做出选择的依据，因为不管那种选择票钱都是没法挽回的。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>沉没成本（Sunk Cost）是指以往发生的，但与当前决策无关的费用。从决策的角度看，以往发生的费用只是造成当前状态的某个因素，当前决策所要考虑的是未来可能发生的费用及所带来的收益，而不考虑以往发生的费用。</p>\n<p>人们在决定是否去做一件事情的时候，不仅是看这件事对自己有没有好处，而且也看过去是不是已经在这件事情上有过投入。我们把这些已经发生不可收回的支出，如时间、金钱、精力等称为“沉没成本”。在经济学和商业决策制定过程中会用到“沉没成本”的概念，代指已经付出且不可收回的成本。沉没成本常用来和可变成本作比较，可变成本可以被改变，而沉没成本则不能被改变。</p>\n<p>例如，买了一张电影票，不能退票的那种，那么买这张电影票的钱就属于沉没成本。电影开场后，在看电影的过程中发现电影不是自己喜欢的类型，现在有两个选择继续看完还是中间退场。不管坐哪个选择，票钱都不应该做为做出选择的依据，因为不管那种选择票钱都是没法挽回的。</p>\n"},{"title":"注解（一）：综述","date":"2016-12-22T14:19:13.000Z","_content":"\n\n注解，一种元数据的形式，提供程序相关的数据，这些数据不是程序本身的一部分。注解对它们注解的代码操作没有直接的作用。\n\n注解有许多用途，其中：\n\n- **为编译器提供信息** - 注解被编译器用来发现错误或禁止告警。\n- **编译期和部署期处理** - 软件工具可以处理注解信息来生成代码、XML 文件等等。\n- **运行时处理** - 一些注解可以在运行时检查。\n\n这节课讲解哪些地方可以用注解、如何应用注解、在 Java 平台（标准版 Java SE API）中可以获取那些类型的预定义注解、类型注解如何与可插拔类型系统一起使用来写强类型检查代码、如何实现重复注解。\n","source":"_posts/注解（一）：综述.md","raw":"title: 注解（一）：综述\ntags:\n  - Java\ncategories:\n  - 语言\n  - Java\ndate: 2016-12-22 22:19:13\n---\n\n\n注解，一种元数据的形式，提供程序相关的数据，这些数据不是程序本身的一部分。注解对它们注解的代码操作没有直接的作用。\n\n注解有许多用途，其中：\n\n- **为编译器提供信息** - 注解被编译器用来发现错误或禁止告警。\n- **编译期和部署期处理** - 软件工具可以处理注解信息来生成代码、XML 文件等等。\n- **运行时处理** - 一些注解可以在运行时检查。\n\n这节课讲解哪些地方可以用注解、如何应用注解、在 Java 平台（标准版 Java SE API）中可以获取那些类型的预定义注解、类型注解如何与可插拔类型系统一起使用来写强类型检查代码、如何实现重复注解。\n","slug":"注解（一）：综述","published":1,"updated":"2021-07-19T16:28:00.308Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphsd00gxitd3fdbk3zp7","content":"<p>注解，一种元数据的形式，提供程序相关的数据，这些数据不是程序本身的一部分。注解对它们注解的代码操作没有直接的作用。</p>\n<p>注解有许多用途，其中：</p>\n<ul>\n<li><strong>为编译器提供信息</strong> - 注解被编译器用来发现错误或禁止告警。</li>\n<li><strong>编译期和部署期处理</strong> - 软件工具可以处理注解信息来生成代码、XML 文件等等。</li>\n<li><strong>运行时处理</strong> - 一些注解可以在运行时检查。</li>\n</ul>\n<p>这节课讲解哪些地方可以用注解、如何应用注解、在 Java 平台（标准版 Java SE API）中可以获取那些类型的预定义注解、类型注解如何与可插拔类型系统一起使用来写强类型检查代码、如何实现重复注解。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>注解，一种元数据的形式，提供程序相关的数据，这些数据不是程序本身的一部分。注解对它们注解的代码操作没有直接的作用。</p>\n<p>注解有许多用途，其中：</p>\n<ul>\n<li><strong>为编译器提供信息</strong> - 注解被编译器用来发现错误或禁止告警。</li>\n<li><strong>编译期和部署期处理</strong> - 软件工具可以处理注解信息来生成代码、XML 文件等等。</li>\n<li><strong>运行时处理</strong> - 一些注解可以在运行时检查。</li>\n</ul>\n<p>这节课讲解哪些地方可以用注解、如何应用注解、在 Java 平台（标准版 Java SE API）中可以获取那些类型的预定义注解、类型注解如何与可插拔类型系统一起使用来写强类型检查代码、如何实现重复注解。</p>\n"},{"title":"注解（三）：声明一个注解类型","date":"2016-12-23T05:25:20.000Z","_content":"\n很多注解替代了代码中的注释。\n\n想象一个软件团队习惯用提供重要信息的注释开始每个类体：\n\n    public class Generation3List extends Generation2List {\n\n      // Author: John Doe\n      // Date: 3/17/2002\n      // Current revision: 6\n      // Last modified: 4/12/2004\n      // By: Jane Doe\n      // Reviewers: Alice, Bill, Cindy\n\n      // class code goes here\n\n    }\n\n<!-- more -->\n\n用一个注解添加相同的元数据，首先必须要定义注解类型。语法是：\n\n    @interface ClassPreamble {\n      String author();\n      String date();\n      int currentRevision() default 1;\n      String lastModified() default \"N/A\";\n      String lastModifiedBy() default \"N/A\";\n      // Note use of array\n      String[] reviewers();\n    }\n\n注解类型定义看上去跟接口定义类似，interface 关键字前面是 @ 符号。注解类型是接口的一种形式，这在后面的课程中讲解。现在，你不需要理解接口。\n\n前面注解的定义体包含注解类型元素声明，这很像是一个方法。注意，可以定义可选的默认值。\n\n注解类型定义后，就可以使用这种类型的注解，并填写值，像这样：\n\n    @ClassPreamble (\n      author = \"John Doe\",\n      date = \"3/17/2002\",\n      currentRevision = 6,\n      lastModified = \"4/12/2004\",\n      lastModifiedBy = \"Jane Doe\",\n      // Note array notation\n      reviewers = {\"Alice\", \"Bob\", \"Cindy\"}\n    )\n    public class Generation3List extends Generation2List {\n\n      // class code goes here\n\n    }\n\n> 注：为了使 @ClassPreamble 中的信息出现在 Javadoc 生成的文档中，必须用 @Documented 注解 @ClassPreamble 的定义：\n>    // import this to use @Documented\n>    import java.lang.annotation.*;\n\n>    @Documented\n>    @interface ClassPreamble {\n\n>      // Annotation element definitions\n\n>    }\n","source":"_posts/注解（三）：声明一个注解类型.md","raw":"title: 注解（三）：声明一个注解类型\ntags:\n  - Java\ncategories:\n  - 语言\n  - Java\ndate: 2016-12-23 13:25:20\n---\n\n很多注解替代了代码中的注释。\n\n想象一个软件团队习惯用提供重要信息的注释开始每个类体：\n\n    public class Generation3List extends Generation2List {\n\n      // Author: John Doe\n      // Date: 3/17/2002\n      // Current revision: 6\n      // Last modified: 4/12/2004\n      // By: Jane Doe\n      // Reviewers: Alice, Bill, Cindy\n\n      // class code goes here\n\n    }\n\n<!-- more -->\n\n用一个注解添加相同的元数据，首先必须要定义注解类型。语法是：\n\n    @interface ClassPreamble {\n      String author();\n      String date();\n      int currentRevision() default 1;\n      String lastModified() default \"N/A\";\n      String lastModifiedBy() default \"N/A\";\n      // Note use of array\n      String[] reviewers();\n    }\n\n注解类型定义看上去跟接口定义类似，interface 关键字前面是 @ 符号。注解类型是接口的一种形式，这在后面的课程中讲解。现在，你不需要理解接口。\n\n前面注解的定义体包含注解类型元素声明，这很像是一个方法。注意，可以定义可选的默认值。\n\n注解类型定义后，就可以使用这种类型的注解，并填写值，像这样：\n\n    @ClassPreamble (\n      author = \"John Doe\",\n      date = \"3/17/2002\",\n      currentRevision = 6,\n      lastModified = \"4/12/2004\",\n      lastModifiedBy = \"Jane Doe\",\n      // Note array notation\n      reviewers = {\"Alice\", \"Bob\", \"Cindy\"}\n    )\n    public class Generation3List extends Generation2List {\n\n      // class code goes here\n\n    }\n\n> 注：为了使 @ClassPreamble 中的信息出现在 Javadoc 生成的文档中，必须用 @Documented 注解 @ClassPreamble 的定义：\n>    // import this to use @Documented\n>    import java.lang.annotation.*;\n\n>    @Documented\n>    @interface ClassPreamble {\n\n>      // Annotation element definitions\n\n>    }\n","slug":"注解（三）：声明一个注解类型","published":1,"updated":"2021-07-19T16:28:00.308Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphsg00h0itd33fiza3b3","content":"<p>很多注解替代了代码中的注释。</p>\n<p>想象一个软件团队习惯用提供重要信息的注释开始每个类体：</p>\n<pre><code>public class Generation3List extends Generation2List &#123;\n\n  // Author: John Doe\n  // Date: 3/17/2002\n  // Current revision: 6\n  // Last modified: 4/12/2004\n  // By: Jane Doe\n  // Reviewers: Alice, Bill, Cindy\n\n  // class code goes here\n\n&#125;\n</code></pre>\n<span id=\"more\"></span>\n\n<p>用一个注解添加相同的元数据，首先必须要定义注解类型。语法是：</p>\n<pre><code>@interface ClassPreamble &#123;\n  String author();\n  String date();\n  int currentRevision() default 1;\n  String lastModified() default &quot;N/A&quot;;\n  String lastModifiedBy() default &quot;N/A&quot;;\n  // Note use of array\n  String[] reviewers();\n&#125;\n</code></pre>\n<p>注解类型定义看上去跟接口定义类似，interface 关键字前面是 @ 符号。注解类型是接口的一种形式，这在后面的课程中讲解。现在，你不需要理解接口。</p>\n<p>前面注解的定义体包含注解类型元素声明，这很像是一个方法。注意，可以定义可选的默认值。</p>\n<p>注解类型定义后，就可以使用这种类型的注解，并填写值，像这样：</p>\n<pre><code>@ClassPreamble (\n  author = &quot;John Doe&quot;,\n  date = &quot;3/17/2002&quot;,\n  currentRevision = 6,\n  lastModified = &quot;4/12/2004&quot;,\n  lastModifiedBy = &quot;Jane Doe&quot;,\n  // Note array notation\n  reviewers = &#123;&quot;Alice&quot;, &quot;Bob&quot;, &quot;Cindy&quot;&#125;\n)\npublic class Generation3List extends Generation2List &#123;\n\n  // class code goes here\n\n&#125;\n</code></pre>\n<blockquote>\n<p>注：为了使 @ClassPreamble 中的信息出现在 Javadoc 生成的文档中，必须用 @Documented 注解 @ClassPreamble 的定义：<br>   // import this to use @Documented<br>   import java.lang.annotation.*;</p>\n</blockquote>\n<blockquote>\n<p>   @Documented<br>   @interface ClassPreamble {</p>\n</blockquote>\n<blockquote>\n<pre><code> // Annotation element definitions\n</code></pre>\n</blockquote>\n<blockquote>\n<p>   }</p>\n</blockquote>\n","site":{"data":{}},"excerpt":"<p>很多注解替代了代码中的注释。</p>\n<p>想象一个软件团队习惯用提供重要信息的注释开始每个类体：</p>\n<pre><code>public class Generation3List extends Generation2List &#123;\n\n  // Author: John Doe\n  // Date: 3/17/2002\n  // Current revision: 6\n  // Last modified: 4/12/2004\n  // By: Jane Doe\n  // Reviewers: Alice, Bill, Cindy\n\n  // class code goes here\n\n&#125;\n</code></pre>","more":"<p>用一个注解添加相同的元数据，首先必须要定义注解类型。语法是：</p>\n<pre><code>@interface ClassPreamble &#123;\n  String author();\n  String date();\n  int currentRevision() default 1;\n  String lastModified() default &quot;N/A&quot;;\n  String lastModifiedBy() default &quot;N/A&quot;;\n  // Note use of array\n  String[] reviewers();\n&#125;\n</code></pre>\n<p>注解类型定义看上去跟接口定义类似，interface 关键字前面是 @ 符号。注解类型是接口的一种形式，这在后面的课程中讲解。现在，你不需要理解接口。</p>\n<p>前面注解的定义体包含注解类型元素声明，这很像是一个方法。注意，可以定义可选的默认值。</p>\n<p>注解类型定义后，就可以使用这种类型的注解，并填写值，像这样：</p>\n<pre><code>@ClassPreamble (\n  author = &quot;John Doe&quot;,\n  date = &quot;3/17/2002&quot;,\n  currentRevision = 6,\n  lastModified = &quot;4/12/2004&quot;,\n  lastModifiedBy = &quot;Jane Doe&quot;,\n  // Note array notation\n  reviewers = &#123;&quot;Alice&quot;, &quot;Bob&quot;, &quot;Cindy&quot;&#125;\n)\npublic class Generation3List extends Generation2List &#123;\n\n  // class code goes here\n\n&#125;\n</code></pre>\n<blockquote>\n<p>注：为了使 @ClassPreamble 中的信息出现在 Javadoc 生成的文档中，必须用 @Documented 注解 @ClassPreamble 的定义：<br>   // import this to use @Documented<br>   import java.lang.annotation.*;</p>\n</blockquote>\n<blockquote>\n<p>   @Documented<br>   @interface ClassPreamble {</p>\n</blockquote>\n<blockquote>\n<pre><code> // Annotation element definitions\n</code></pre>\n</blockquote>\n<blockquote>\n<p>   }</p>\n</blockquote>"},{"title":"注解（二）：注解基础","date":"2016-12-22T15:13:43.000Z","_content":"\n### 注解的格式\n\n最简单的格式，一个注解看起来像下面这样：\n\n    @Entity\n\n阿特符号字母（@）向编译器表明后面的是一个注解。在下面的例子中，注解的名称是 Override：\n\n    @Override\n    void mySuperMethod() { ... }\n\n<!-- more -->\n\n注解可以包含*元素*，元素可以是命名的或匿名的，这些元素是有值的：\n\n    @Author(\n      name = \"Benjamin Franklin\",\n      date = \"3/27/2003\"\n    )\n    class MyClass() { ... }\n\n或者：\n\n    @SuppressWarnings(value = \"unchecked\")\n    void myMethod() { ... }\n\n如果只有一个名称为 value 的元素，那么名称可以被忽略，像：\n\n    @SuppressWarnings(\"unchecked\")\n    void myMethod() { ... }\n\n如果注解没有元素，那么括号可以被忽略，像前面 @Override 例子展示的。\n\n也可以在同一个声明上使用多个注解：\n\n    @Author(name = \"Jane Doe\")\n    @EBook\n    class MyClass { ... }\n\n如果注解是相同的类型，那么这被叫做重复注解：\n\n    @Author(name = \"Jane Doe\")\n    @Author(name = \"John Smith\")\n    class MyClass { ... }\n\n重复注解在 Java SE 8 中被支持。要获取更多信息，参见[重复注解](http://docs.oracle.com/javase/tutorial/java/annotations/repeating.html)。\n\n注解的类型可以是在 Java SE API 的 java.lang 或 ava.lang.annotation 包中定义的一个类型。在前面的例子中，Override 和 SuppressWarnings 是[预定义 Java 注解](http://docs.oracle.com/javase/tutorial/java/annotations/predefined.html)。也可以定义你自己的注解类型。在前面例子中的 Author 和 Ebook 注解是自定义注解类型。\n\n### 注解可以被用在哪里\n\n注解可以用在声明上：类、属性、方法和其他程序元素声明。当用在声明上，按照惯例每个注解出现在一行。\n\n像 Java SE 8 版本，注解也被应用到类型使用上。这有一些例子：\n\n- 类实例创建表达式：\n\n    new @Interned MyObject();\n\n- 强制类型转换：\n\n    myString = (@NonNull String) str;\n\n- implements 子句：\n\n    class UnmodifiableList<T> implements @Readonly List<@Readonly T> { ... }\n\n- 抛出异常声明：\n\n    void monitorTemperature() throws @Critical TemperatureException { ... }\n\n注解的这种形式叫做类型注解。获取更多信息，参见[类型注解和可插拔类型系统](http://docs.oracle.com/javase/tutorial/java/annotations/type_annotations.html)。\n","source":"_posts/注解（二）：注解基础.md","raw":"title: 注解（二）：注解基础\ntags:\n  - Java\ncategories:\n  - 语言\n  - Java\ndate: 2016-12-22 23:13:43\n---\n\n### 注解的格式\n\n最简单的格式，一个注解看起来像下面这样：\n\n    @Entity\n\n阿特符号字母（@）向编译器表明后面的是一个注解。在下面的例子中，注解的名称是 Override：\n\n    @Override\n    void mySuperMethod() { ... }\n\n<!-- more -->\n\n注解可以包含*元素*，元素可以是命名的或匿名的，这些元素是有值的：\n\n    @Author(\n      name = \"Benjamin Franklin\",\n      date = \"3/27/2003\"\n    )\n    class MyClass() { ... }\n\n或者：\n\n    @SuppressWarnings(value = \"unchecked\")\n    void myMethod() { ... }\n\n如果只有一个名称为 value 的元素，那么名称可以被忽略，像：\n\n    @SuppressWarnings(\"unchecked\")\n    void myMethod() { ... }\n\n如果注解没有元素，那么括号可以被忽略，像前面 @Override 例子展示的。\n\n也可以在同一个声明上使用多个注解：\n\n    @Author(name = \"Jane Doe\")\n    @EBook\n    class MyClass { ... }\n\n如果注解是相同的类型，那么这被叫做重复注解：\n\n    @Author(name = \"Jane Doe\")\n    @Author(name = \"John Smith\")\n    class MyClass { ... }\n\n重复注解在 Java SE 8 中被支持。要获取更多信息，参见[重复注解](http://docs.oracle.com/javase/tutorial/java/annotations/repeating.html)。\n\n注解的类型可以是在 Java SE API 的 java.lang 或 ava.lang.annotation 包中定义的一个类型。在前面的例子中，Override 和 SuppressWarnings 是[预定义 Java 注解](http://docs.oracle.com/javase/tutorial/java/annotations/predefined.html)。也可以定义你自己的注解类型。在前面例子中的 Author 和 Ebook 注解是自定义注解类型。\n\n### 注解可以被用在哪里\n\n注解可以用在声明上：类、属性、方法和其他程序元素声明。当用在声明上，按照惯例每个注解出现在一行。\n\n像 Java SE 8 版本，注解也被应用到类型使用上。这有一些例子：\n\n- 类实例创建表达式：\n\n    new @Interned MyObject();\n\n- 强制类型转换：\n\n    myString = (@NonNull String) str;\n\n- implements 子句：\n\n    class UnmodifiableList<T> implements @Readonly List<@Readonly T> { ... }\n\n- 抛出异常声明：\n\n    void monitorTemperature() throws @Critical TemperatureException { ... }\n\n注解的这种形式叫做类型注解。获取更多信息，参见[类型注解和可插拔类型系统](http://docs.oracle.com/javase/tutorial/java/annotations/type_annotations.html)。\n","slug":"注解（二）：注解基础","published":1,"updated":"2021-07-19T16:28:00.308Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphsh00h4itd308ra5l9o","content":"<h3 id=\"注解的格式\"><a href=\"#注解的格式\" class=\"headerlink\" title=\"注解的格式\"></a>注解的格式</h3><p>最简单的格式，一个注解看起来像下面这样：</p>\n<pre><code>@Entity\n</code></pre>\n<p>阿特符号字母（@）向编译器表明后面的是一个注解。在下面的例子中，注解的名称是 Override：</p>\n<pre><code>@Override\nvoid mySuperMethod() &#123; ... &#125;\n</code></pre>\n<span id=\"more\"></span>\n\n<p>注解可以包含<em>元素</em>，元素可以是命名的或匿名的，这些元素是有值的：</p>\n<pre><code>@Author(\n  name = &quot;Benjamin Franklin&quot;,\n  date = &quot;3/27/2003&quot;\n)\nclass MyClass() &#123; ... &#125;\n</code></pre>\n<p>或者：</p>\n<pre><code>@SuppressWarnings(value = &quot;unchecked&quot;)\nvoid myMethod() &#123; ... &#125;\n</code></pre>\n<p>如果只有一个名称为 value 的元素，那么名称可以被忽略，像：</p>\n<pre><code>@SuppressWarnings(&quot;unchecked&quot;)\nvoid myMethod() &#123; ... &#125;\n</code></pre>\n<p>如果注解没有元素，那么括号可以被忽略，像前面 @Override 例子展示的。</p>\n<p>也可以在同一个声明上使用多个注解：</p>\n<pre><code>@Author(name = &quot;Jane Doe&quot;)\n@EBook\nclass MyClass &#123; ... &#125;\n</code></pre>\n<p>如果注解是相同的类型，那么这被叫做重复注解：</p>\n<pre><code>@Author(name = &quot;Jane Doe&quot;)\n@Author(name = &quot;John Smith&quot;)\nclass MyClass &#123; ... &#125;\n</code></pre>\n<p>重复注解在 Java SE 8 中被支持。要获取更多信息，参见<a href=\"http://docs.oracle.com/javase/tutorial/java/annotations/repeating.html\">重复注解</a>。</p>\n<p>注解的类型可以是在 Java SE API 的 java.lang 或 ava.lang.annotation 包中定义的一个类型。在前面的例子中，Override 和 SuppressWarnings 是<a href=\"http://docs.oracle.com/javase/tutorial/java/annotations/predefined.html\">预定义 Java 注解</a>。也可以定义你自己的注解类型。在前面例子中的 Author 和 Ebook 注解是自定义注解类型。</p>\n<h3 id=\"注解可以被用在哪里\"><a href=\"#注解可以被用在哪里\" class=\"headerlink\" title=\"注解可以被用在哪里\"></a>注解可以被用在哪里</h3><p>注解可以用在声明上：类、属性、方法和其他程序元素声明。当用在声明上，按照惯例每个注解出现在一行。</p>\n<p>像 Java SE 8 版本，注解也被应用到类型使用上。这有一些例子：</p>\n<ul>\n<li><p>类实例创建表达式：</p>\n<p>  new @Interned MyObject();</p>\n</li>\n<li><p>强制类型转换：</p>\n<p>  myString = (@NonNull String) str;</p>\n</li>\n<li><p>implements 子句：</p>\n<p>  class UnmodifiableList<T> implements @Readonly List&lt;@Readonly T&gt; { … }</p>\n</li>\n<li><p>抛出异常声明：</p>\n<p>  void monitorTemperature() throws @Critical TemperatureException { … }</p>\n</li>\n</ul>\n<p>注解的这种形式叫做类型注解。获取更多信息，参见<a href=\"http://docs.oracle.com/javase/tutorial/java/annotations/type_annotations.html\">类型注解和可插拔类型系统</a>。</p>\n","site":{"data":{}},"excerpt":"<h3 id=\"注解的格式\"><a href=\"#注解的格式\" class=\"headerlink\" title=\"注解的格式\"></a>注解的格式</h3><p>最简单的格式，一个注解看起来像下面这样：</p>\n<pre><code>@Entity\n</code></pre>\n<p>阿特符号字母（@）向编译器表明后面的是一个注解。在下面的例子中，注解的名称是 Override：</p>\n<pre><code>@Override\nvoid mySuperMethod() &#123; ... &#125;\n</code></pre>","more":"<p>注解可以包含<em>元素</em>，元素可以是命名的或匿名的，这些元素是有值的：</p>\n<pre><code>@Author(\n  name = &quot;Benjamin Franklin&quot;,\n  date = &quot;3/27/2003&quot;\n)\nclass MyClass() &#123; ... &#125;\n</code></pre>\n<p>或者：</p>\n<pre><code>@SuppressWarnings(value = &quot;unchecked&quot;)\nvoid myMethod() &#123; ... &#125;\n</code></pre>\n<p>如果只有一个名称为 value 的元素，那么名称可以被忽略，像：</p>\n<pre><code>@SuppressWarnings(&quot;unchecked&quot;)\nvoid myMethod() &#123; ... &#125;\n</code></pre>\n<p>如果注解没有元素，那么括号可以被忽略，像前面 @Override 例子展示的。</p>\n<p>也可以在同一个声明上使用多个注解：</p>\n<pre><code>@Author(name = &quot;Jane Doe&quot;)\n@EBook\nclass MyClass &#123; ... &#125;\n</code></pre>\n<p>如果注解是相同的类型，那么这被叫做重复注解：</p>\n<pre><code>@Author(name = &quot;Jane Doe&quot;)\n@Author(name = &quot;John Smith&quot;)\nclass MyClass &#123; ... &#125;\n</code></pre>\n<p>重复注解在 Java SE 8 中被支持。要获取更多信息，参见<a href=\"http://docs.oracle.com/javase/tutorial/java/annotations/repeating.html\">重复注解</a>。</p>\n<p>注解的类型可以是在 Java SE API 的 java.lang 或 ava.lang.annotation 包中定义的一个类型。在前面的例子中，Override 和 SuppressWarnings 是<a href=\"http://docs.oracle.com/javase/tutorial/java/annotations/predefined.html\">预定义 Java 注解</a>。也可以定义你自己的注解类型。在前面例子中的 Author 和 Ebook 注解是自定义注解类型。</p>\n<h3 id=\"注解可以被用在哪里\"><a href=\"#注解可以被用在哪里\" class=\"headerlink\" title=\"注解可以被用在哪里\"></a>注解可以被用在哪里</h3><p>注解可以用在声明上：类、属性、方法和其他程序元素声明。当用在声明上，按照惯例每个注解出现在一行。</p>\n<p>像 Java SE 8 版本，注解也被应用到类型使用上。这有一些例子：</p>\n<ul>\n<li><p>类实例创建表达式：</p>\n<p>  new @Interned MyObject();</p>\n</li>\n<li><p>强制类型转换：</p>\n<p>  myString = (@NonNull String) str;</p>\n</li>\n<li><p>implements 子句：</p>\n<p>  class UnmodifiableList<T> implements @Readonly List&lt;@Readonly T&gt; { … }</p>\n</li>\n<li><p>抛出异常声明：</p>\n<p>  void monitorTemperature() throws @Critical TemperatureException { … }</p>\n</li>\n</ul>\n<p>注解的这种形式叫做类型注解。获取更多信息，参见<a href=\"http://docs.oracle.com/javase/tutorial/java/annotations/type_annotations.html\">类型注解和可插拔类型系统</a>。</p>"},{"title":"注解（五）：类型注解和可插拔类型系统","date":"2016-12-23T13:33:50.000Z","_content":"\nJava SE 8 发布之前，注解只能应用于声明。随着 Java SE 8 发布，注解可以被应用于任何类型使用。这意味着注解可以被用于任何使用类型的地方。类型在什么地方使用的几个例子是类实例创建表达式（new）、强制类型转换、implements 子句和 throws 子句。这种形式的注解称为类型注解，在[注解基础](http://docs.oracle.com/javase/tutorial/java/annotations/basics.html)中提供了几个例子。\n\n<!-- more -->\n\n类型注解被创建来增强确保 Java 程序强类型检查的分析方法。Java SE 8 发布版不提供类型检查框架，但允许编写（或下载）类型检查框架，这些框架的实现是作为一个或多个用来与 Java 编译器结合使用的可插拔模块。\n\n例如，想确保程序中的一个常规变量永远不被赋值为 null；避免触发 NullPointerException。可以写一个定制的插件来检查这个。然后修改代码注解那个常规变量，表明它永远不能被赋值为 null。变量声明可能看上去像这样：\n\n    @NonNull String str;\n\n当你编译这代码时，在命令行包含 NonNull 模块，如果编译器检测到一个潜在的问题则会打印一个警告，允许你修改代码避免这个错误。改正代码移除所有告警后，这个常规错误不会在程序运行时发生。\n\n可以使用多个类型检查模块，每个模块检查一个不同类型的错误。以这种方式，可以在 Java 类型系统的上层在需要的时候和地方添加指定的检查。\n\n随着明智使用类型注解和可插入类型检查，可以写出更健壮和更少出错的代码。\n\n在很多场景中，没必须写自己的类型检查模块。有第三方已经为你完成了这项工作。例如，你可能想利用华盛顿大学创建的检查框架。这个框架包含一个 NonNull 模块，此外还有一个正则表达式模块，以及一个互斥锁模块。更多信息，参见[检查框架](http://types.cs.washington.edu/checker-framework/)。\n","source":"_posts/注解（五）：类型注解和可插拔类型系统.md","raw":"title: 注解（五）：类型注解和可插拔类型系统\ntags:\n  - Java\ncategories:\n  - 语言\n  - Java\ndate: 2016-12-23 21:33:50\n---\n\nJava SE 8 发布之前，注解只能应用于声明。随着 Java SE 8 发布，注解可以被应用于任何类型使用。这意味着注解可以被用于任何使用类型的地方。类型在什么地方使用的几个例子是类实例创建表达式（new）、强制类型转换、implements 子句和 throws 子句。这种形式的注解称为类型注解，在[注解基础](http://docs.oracle.com/javase/tutorial/java/annotations/basics.html)中提供了几个例子。\n\n<!-- more -->\n\n类型注解被创建来增强确保 Java 程序强类型检查的分析方法。Java SE 8 发布版不提供类型检查框架，但允许编写（或下载）类型检查框架，这些框架的实现是作为一个或多个用来与 Java 编译器结合使用的可插拔模块。\n\n例如，想确保程序中的一个常规变量永远不被赋值为 null；避免触发 NullPointerException。可以写一个定制的插件来检查这个。然后修改代码注解那个常规变量，表明它永远不能被赋值为 null。变量声明可能看上去像这样：\n\n    @NonNull String str;\n\n当你编译这代码时，在命令行包含 NonNull 模块，如果编译器检测到一个潜在的问题则会打印一个警告，允许你修改代码避免这个错误。改正代码移除所有告警后，这个常规错误不会在程序运行时发生。\n\n可以使用多个类型检查模块，每个模块检查一个不同类型的错误。以这种方式，可以在 Java 类型系统的上层在需要的时候和地方添加指定的检查。\n\n随着明智使用类型注解和可插入类型检查，可以写出更健壮和更少出错的代码。\n\n在很多场景中，没必须写自己的类型检查模块。有第三方已经为你完成了这项工作。例如，你可能想利用华盛顿大学创建的检查框架。这个框架包含一个 NonNull 模块，此外还有一个正则表达式模块，以及一个互斥锁模块。更多信息，参见[检查框架](http://types.cs.washington.edu/checker-framework/)。\n","slug":"注解（五）：类型注解和可插拔类型系统","published":1,"updated":"2021-07-19T16:28:00.312Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphsj00h8itd35r71hnak","content":"<p>Java SE 8 发布之前，注解只能应用于声明。随着 Java SE 8 发布，注解可以被应用于任何类型使用。这意味着注解可以被用于任何使用类型的地方。类型在什么地方使用的几个例子是类实例创建表达式（new）、强制类型转换、implements 子句和 throws 子句。这种形式的注解称为类型注解，在<a href=\"http://docs.oracle.com/javase/tutorial/java/annotations/basics.html\">注解基础</a>中提供了几个例子。</p>\n<span id=\"more\"></span>\n\n<p>类型注解被创建来增强确保 Java 程序强类型检查的分析方法。Java SE 8 发布版不提供类型检查框架，但允许编写（或下载）类型检查框架，这些框架的实现是作为一个或多个用来与 Java 编译器结合使用的可插拔模块。</p>\n<p>例如，想确保程序中的一个常规变量永远不被赋值为 null；避免触发 NullPointerException。可以写一个定制的插件来检查这个。然后修改代码注解那个常规变量，表明它永远不能被赋值为 null。变量声明可能看上去像这样：</p>\n<pre><code>@NonNull String str;\n</code></pre>\n<p>当你编译这代码时，在命令行包含 NonNull 模块，如果编译器检测到一个潜在的问题则会打印一个警告，允许你修改代码避免这个错误。改正代码移除所有告警后，这个常规错误不会在程序运行时发生。</p>\n<p>可以使用多个类型检查模块，每个模块检查一个不同类型的错误。以这种方式，可以在 Java 类型系统的上层在需要的时候和地方添加指定的检查。</p>\n<p>随着明智使用类型注解和可插入类型检查，可以写出更健壮和更少出错的代码。</p>\n<p>在很多场景中，没必须写自己的类型检查模块。有第三方已经为你完成了这项工作。例如，你可能想利用华盛顿大学创建的检查框架。这个框架包含一个 NonNull 模块，此外还有一个正则表达式模块，以及一个互斥锁模块。更多信息，参见<a href=\"http://types.cs.washington.edu/checker-framework/\">检查框架</a>。</p>\n","site":{"data":{}},"excerpt":"<p>Java SE 8 发布之前，注解只能应用于声明。随着 Java SE 8 发布，注解可以被应用于任何类型使用。这意味着注解可以被用于任何使用类型的地方。类型在什么地方使用的几个例子是类实例创建表达式（new）、强制类型转换、implements 子句和 throws 子句。这种形式的注解称为类型注解，在<a href=\"http://docs.oracle.com/javase/tutorial/java/annotations/basics.html\">注解基础</a>中提供了几个例子。</p>","more":"<p>类型注解被创建来增强确保 Java 程序强类型检查的分析方法。Java SE 8 发布版不提供类型检查框架，但允许编写（或下载）类型检查框架，这些框架的实现是作为一个或多个用来与 Java 编译器结合使用的可插拔模块。</p>\n<p>例如，想确保程序中的一个常规变量永远不被赋值为 null；避免触发 NullPointerException。可以写一个定制的插件来检查这个。然后修改代码注解那个常规变量，表明它永远不能被赋值为 null。变量声明可能看上去像这样：</p>\n<pre><code>@NonNull String str;\n</code></pre>\n<p>当你编译这代码时，在命令行包含 NonNull 模块，如果编译器检测到一个潜在的问题则会打印一个警告，允许你修改代码避免这个错误。改正代码移除所有告警后，这个常规错误不会在程序运行时发生。</p>\n<p>可以使用多个类型检查模块，每个模块检查一个不同类型的错误。以这种方式，可以在 Java 类型系统的上层在需要的时候和地方添加指定的检查。</p>\n<p>随着明智使用类型注解和可插入类型检查，可以写出更健壮和更少出错的代码。</p>\n<p>在很多场景中，没必须写自己的类型检查模块。有第三方已经为你完成了这项工作。例如，你可能想利用华盛顿大学创建的检查框架。这个框架包含一个 NonNull 模块，此外还有一个正则表达式模块，以及一个互斥锁模块。更多信息，参见<a href=\"http://types.cs.washington.edu/checker-framework/\">检查框架</a>。</p>"},{"title":"注解（六）：重复注解","date":"2016-12-23T17:24:12.000Z","_content":"\n存在一些情况，想对一个声明或类型使用应用相同的注解。随着 Java SE 8 版发布，重复注解允许你这样做。\n\n<!-- more -->\n\n例如，要编写代码使用计时器服务在一个给定的时间或以确定的调度运行一个方法，类似于 UNIX cron 服务。现在想设置一个计时器来运行一个方法（doPeriodicCleanup），在每个月的最后一天及在每个周五下午 11:00 点。为了设置计时器来运行，创建一个 @Schedule 注解并应用到 doPeriodicCleanup 方法两次。第一个使用指定每个月最后一天，第二个指定周五下午 11:00 点，像下面代码示例展示的：\n\n    @Schedule(dayOfMonth=\"last\")\n    @Schedule(dayOfWeek=\"Fri\", hour=\"23\")\n    public void doPeriodicCleanup() { ... }\n\n前面的例子应用一个注解到一个方法。可以在任何使用标准注解的地方重复一个注解。例如，有一个处理未授权异常的类。为管理者用一个 @Alert 注解且为管理员用另外一个 @Alert 注解：\n\n    @Alert(role=\"Manager\")\n    @Alert(role=\"Administrator\")\n    public class UnauthorizedAccessException extends SecurityException { ... }\n\n为了兼容性原因，重复注解存储在一个容器注解中，容器注解是由 Java 编译器自动生成的。为了编译器做这个，在代码中需要两个声明。\n\n**步骤 1：声明一个可重复的注解类型**\n\n这个注解类型必须用 @Repeatable 元注解标记。下面的例子定义了一个定制的 @Schedule 重复注解类型：\n\n    import java.lang.annotation.Repeatable;\n\n    @Repeatable(Schedules.class)\n    public @interface Schedule {\n      String dayOfMonth() default \"first\";\n      String dayOfWeek() default \"Mon\";\n      int hour() default 12;\n    }\n\n@Repeatable 元注解的值（在括号中的）是 Java 编译器生成来存储重复注解的容器注解的类型。在这个例子中，容器注解的类型是 Schedules，因此重复 @Schedule 注解存储在一个 @Schedules 注解中。\n\n如果不首先声明注解是可重复的就应用同一个注解到一个声明将引起一个编译器错误。\n\n**步骤 2：声明容器注解类型**\n\n容器注解类型必须有一个数组类型的 value 元素。数组中的元素类型必须是可重复注解的类型。Schedules 容器注解类型的声明如下：\n\n    public @interface Schedules {\n      Schedule[] value();\n    }\n\n### 检索注解\n\n在反射 API 中有几种方法可以用来检索注解。这些方法的行为返回一个注解，例如 [AnnotatedElement.getAnnotationByType(Class<T>)](https://docs.oracle.com/javase/8/docs/api/java/lang/reflect/AnnotatedElement.html#getAnnotationByType-java.lang.Class-)，如果一个需要类型的注解出现这些方法不会改变只返回单个注解。如果多于一个需要类型的注解出现，可以首先获取它们的容器注解来获取它们。以这种方式，继承的代码可以继续工作。Java SE 8 中引入了其他方法，可以通过扫描容器注解一次返回多个注解，像 [AnnotatedElement.getAnnotations(Class<T>)](https://docs.oracle.com/javase/8/docs/api/java/lang/reflect/AnnotatedElement.html#getAnnotations-java.lang.Class-)。参见 [AnnotatedElement](https://docs.oracle.com/javase/8/docs/api/java/lang/reflect/AnnotatedElement.html) 类定义获取关于所有可获得方法的信息。\n\n### 设计考量\n\n当设计一个注解类型时，必须考虑这种类型注解的基。可能使用一个注解零次、一次，或者多次（如果注解类型被标记为 @Repeatable）。也可能通过 @Target 元注解来限制一个注解哪里可以使用。例如，可以创建一个可重复的注解类型只能用在方法和属性上。仔细地设计注解类型很重要以确保程序员尽可能使用适合的和功能强大的注解。\n","source":"_posts/注解（六）：重复注解.md","raw":"title: 注解（六）：重复注解\ntags:\n  - Java\ncategories:\n  - 语言\n  - Java\ndate: 2016-12-24 01:24:12\n---\n\n存在一些情况，想对一个声明或类型使用应用相同的注解。随着 Java SE 8 版发布，重复注解允许你这样做。\n\n<!-- more -->\n\n例如，要编写代码使用计时器服务在一个给定的时间或以确定的调度运行一个方法，类似于 UNIX cron 服务。现在想设置一个计时器来运行一个方法（doPeriodicCleanup），在每个月的最后一天及在每个周五下午 11:00 点。为了设置计时器来运行，创建一个 @Schedule 注解并应用到 doPeriodicCleanup 方法两次。第一个使用指定每个月最后一天，第二个指定周五下午 11:00 点，像下面代码示例展示的：\n\n    @Schedule(dayOfMonth=\"last\")\n    @Schedule(dayOfWeek=\"Fri\", hour=\"23\")\n    public void doPeriodicCleanup() { ... }\n\n前面的例子应用一个注解到一个方法。可以在任何使用标准注解的地方重复一个注解。例如，有一个处理未授权异常的类。为管理者用一个 @Alert 注解且为管理员用另外一个 @Alert 注解：\n\n    @Alert(role=\"Manager\")\n    @Alert(role=\"Administrator\")\n    public class UnauthorizedAccessException extends SecurityException { ... }\n\n为了兼容性原因，重复注解存储在一个容器注解中，容器注解是由 Java 编译器自动生成的。为了编译器做这个，在代码中需要两个声明。\n\n**步骤 1：声明一个可重复的注解类型**\n\n这个注解类型必须用 @Repeatable 元注解标记。下面的例子定义了一个定制的 @Schedule 重复注解类型：\n\n    import java.lang.annotation.Repeatable;\n\n    @Repeatable(Schedules.class)\n    public @interface Schedule {\n      String dayOfMonth() default \"first\";\n      String dayOfWeek() default \"Mon\";\n      int hour() default 12;\n    }\n\n@Repeatable 元注解的值（在括号中的）是 Java 编译器生成来存储重复注解的容器注解的类型。在这个例子中，容器注解的类型是 Schedules，因此重复 @Schedule 注解存储在一个 @Schedules 注解中。\n\n如果不首先声明注解是可重复的就应用同一个注解到一个声明将引起一个编译器错误。\n\n**步骤 2：声明容器注解类型**\n\n容器注解类型必须有一个数组类型的 value 元素。数组中的元素类型必须是可重复注解的类型。Schedules 容器注解类型的声明如下：\n\n    public @interface Schedules {\n      Schedule[] value();\n    }\n\n### 检索注解\n\n在反射 API 中有几种方法可以用来检索注解。这些方法的行为返回一个注解，例如 [AnnotatedElement.getAnnotationByType(Class<T>)](https://docs.oracle.com/javase/8/docs/api/java/lang/reflect/AnnotatedElement.html#getAnnotationByType-java.lang.Class-)，如果一个需要类型的注解出现这些方法不会改变只返回单个注解。如果多于一个需要类型的注解出现，可以首先获取它们的容器注解来获取它们。以这种方式，继承的代码可以继续工作。Java SE 8 中引入了其他方法，可以通过扫描容器注解一次返回多个注解，像 [AnnotatedElement.getAnnotations(Class<T>)](https://docs.oracle.com/javase/8/docs/api/java/lang/reflect/AnnotatedElement.html#getAnnotations-java.lang.Class-)。参见 [AnnotatedElement](https://docs.oracle.com/javase/8/docs/api/java/lang/reflect/AnnotatedElement.html) 类定义获取关于所有可获得方法的信息。\n\n### 设计考量\n\n当设计一个注解类型时，必须考虑这种类型注解的基。可能使用一个注解零次、一次，或者多次（如果注解类型被标记为 @Repeatable）。也可能通过 @Target 元注解来限制一个注解哪里可以使用。例如，可以创建一个可重复的注解类型只能用在方法和属性上。仔细地设计注解类型很重要以确保程序员尽可能使用适合的和功能强大的注解。\n","slug":"注解（六）：重复注解","published":1,"updated":"2021-07-19T16:28:00.312Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphsk00hcitd3grlx4qol","content":"<p>存在一些情况，想对一个声明或类型使用应用相同的注解。随着 Java SE 8 版发布，重复注解允许你这样做。</p>\n<span id=\"more\"></span>\n\n<p>例如，要编写代码使用计时器服务在一个给定的时间或以确定的调度运行一个方法，类似于 UNIX cron 服务。现在想设置一个计时器来运行一个方法（doPeriodicCleanup），在每个月的最后一天及在每个周五下午 11:00 点。为了设置计时器来运行，创建一个 @Schedule 注解并应用到 doPeriodicCleanup 方法两次。第一个使用指定每个月最后一天，第二个指定周五下午 11:00 点，像下面代码示例展示的：</p>\n<pre><code>@Schedule(dayOfMonth=&quot;last&quot;)\n@Schedule(dayOfWeek=&quot;Fri&quot;, hour=&quot;23&quot;)\npublic void doPeriodicCleanup() &#123; ... &#125;\n</code></pre>\n<p>前面的例子应用一个注解到一个方法。可以在任何使用标准注解的地方重复一个注解。例如，有一个处理未授权异常的类。为管理者用一个 @Alert 注解且为管理员用另外一个 @Alert 注解：</p>\n<pre><code>@Alert(role=&quot;Manager&quot;)\n@Alert(role=&quot;Administrator&quot;)\npublic class UnauthorizedAccessException extends SecurityException &#123; ... &#125;\n</code></pre>\n<p>为了兼容性原因，重复注解存储在一个容器注解中，容器注解是由 Java 编译器自动生成的。为了编译器做这个，在代码中需要两个声明。</p>\n<p><strong>步骤 1：声明一个可重复的注解类型</strong></p>\n<p>这个注解类型必须用 @Repeatable 元注解标记。下面的例子定义了一个定制的 @Schedule 重复注解类型：</p>\n<pre><code>import java.lang.annotation.Repeatable;\n\n@Repeatable(Schedules.class)\npublic @interface Schedule &#123;\n  String dayOfMonth() default &quot;first&quot;;\n  String dayOfWeek() default &quot;Mon&quot;;\n  int hour() default 12;\n&#125;\n</code></pre>\n<p>@Repeatable 元注解的值（在括号中的）是 Java 编译器生成来存储重复注解的容器注解的类型。在这个例子中，容器注解的类型是 Schedules，因此重复 @Schedule 注解存储在一个 @Schedules 注解中。</p>\n<p>如果不首先声明注解是可重复的就应用同一个注解到一个声明将引起一个编译器错误。</p>\n<p><strong>步骤 2：声明容器注解类型</strong></p>\n<p>容器注解类型必须有一个数组类型的 value 元素。数组中的元素类型必须是可重复注解的类型。Schedules 容器注解类型的声明如下：</p>\n<pre><code>public @interface Schedules &#123;\n  Schedule[] value();\n&#125;\n</code></pre>\n<h3 id=\"检索注解\"><a href=\"#检索注解\" class=\"headerlink\" title=\"检索注解\"></a>检索注解</h3><p>在反射 API 中有几种方法可以用来检索注解。这些方法的行为返回一个注解，例如 <a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/reflect/AnnotatedElement.html#getAnnotationByType-java.lang.Class-\">AnnotatedElement.getAnnotationByType(Class<T>)</a>，如果一个需要类型的注解出现这些方法不会改变只返回单个注解。如果多于一个需要类型的注解出现，可以首先获取它们的容器注解来获取它们。以这种方式，继承的代码可以继续工作。Java SE 8 中引入了其他方法，可以通过扫描容器注解一次返回多个注解，像 <a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/reflect/AnnotatedElement.html#getAnnotations-java.lang.Class-\">AnnotatedElement.getAnnotations(Class<T>)</a>。参见 <a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/reflect/AnnotatedElement.html\">AnnotatedElement</a> 类定义获取关于所有可获得方法的信息。</p>\n<h3 id=\"设计考量\"><a href=\"#设计考量\" class=\"headerlink\" title=\"设计考量\"></a>设计考量</h3><p>当设计一个注解类型时，必须考虑这种类型注解的基。可能使用一个注解零次、一次，或者多次（如果注解类型被标记为 @Repeatable）。也可能通过 @Target 元注解来限制一个注解哪里可以使用。例如，可以创建一个可重复的注解类型只能用在方法和属性上。仔细地设计注解类型很重要以确保程序员尽可能使用适合的和功能强大的注解。</p>\n","site":{"data":{}},"excerpt":"<p>存在一些情况，想对一个声明或类型使用应用相同的注解。随着 Java SE 8 版发布，重复注解允许你这样做。</p>","more":"<p>例如，要编写代码使用计时器服务在一个给定的时间或以确定的调度运行一个方法，类似于 UNIX cron 服务。现在想设置一个计时器来运行一个方法（doPeriodicCleanup），在每个月的最后一天及在每个周五下午 11:00 点。为了设置计时器来运行，创建一个 @Schedule 注解并应用到 doPeriodicCleanup 方法两次。第一个使用指定每个月最后一天，第二个指定周五下午 11:00 点，像下面代码示例展示的：</p>\n<pre><code>@Schedule(dayOfMonth=&quot;last&quot;)\n@Schedule(dayOfWeek=&quot;Fri&quot;, hour=&quot;23&quot;)\npublic void doPeriodicCleanup() &#123; ... &#125;\n</code></pre>\n<p>前面的例子应用一个注解到一个方法。可以在任何使用标准注解的地方重复一个注解。例如，有一个处理未授权异常的类。为管理者用一个 @Alert 注解且为管理员用另外一个 @Alert 注解：</p>\n<pre><code>@Alert(role=&quot;Manager&quot;)\n@Alert(role=&quot;Administrator&quot;)\npublic class UnauthorizedAccessException extends SecurityException &#123; ... &#125;\n</code></pre>\n<p>为了兼容性原因，重复注解存储在一个容器注解中，容器注解是由 Java 编译器自动生成的。为了编译器做这个，在代码中需要两个声明。</p>\n<p><strong>步骤 1：声明一个可重复的注解类型</strong></p>\n<p>这个注解类型必须用 @Repeatable 元注解标记。下面的例子定义了一个定制的 @Schedule 重复注解类型：</p>\n<pre><code>import java.lang.annotation.Repeatable;\n\n@Repeatable(Schedules.class)\npublic @interface Schedule &#123;\n  String dayOfMonth() default &quot;first&quot;;\n  String dayOfWeek() default &quot;Mon&quot;;\n  int hour() default 12;\n&#125;\n</code></pre>\n<p>@Repeatable 元注解的值（在括号中的）是 Java 编译器生成来存储重复注解的容器注解的类型。在这个例子中，容器注解的类型是 Schedules，因此重复 @Schedule 注解存储在一个 @Schedules 注解中。</p>\n<p>如果不首先声明注解是可重复的就应用同一个注解到一个声明将引起一个编译器错误。</p>\n<p><strong>步骤 2：声明容器注解类型</strong></p>\n<p>容器注解类型必须有一个数组类型的 value 元素。数组中的元素类型必须是可重复注解的类型。Schedules 容器注解类型的声明如下：</p>\n<pre><code>public @interface Schedules &#123;\n  Schedule[] value();\n&#125;\n</code></pre>\n<h3 id=\"检索注解\"><a href=\"#检索注解\" class=\"headerlink\" title=\"检索注解\"></a>检索注解</h3><p>在反射 API 中有几种方法可以用来检索注解。这些方法的行为返回一个注解，例如 <a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/reflect/AnnotatedElement.html#getAnnotationByType-java.lang.Class-\">AnnotatedElement.getAnnotationByType(Class<T>)</a>，如果一个需要类型的注解出现这些方法不会改变只返回单个注解。如果多于一个需要类型的注解出现，可以首先获取它们的容器注解来获取它们。以这种方式，继承的代码可以继续工作。Java SE 8 中引入了其他方法，可以通过扫描容器注解一次返回多个注解，像 <a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/reflect/AnnotatedElement.html#getAnnotations-java.lang.Class-\">AnnotatedElement.getAnnotations(Class<T>)</a>。参见 <a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/reflect/AnnotatedElement.html\">AnnotatedElement</a> 类定义获取关于所有可获得方法的信息。</p>\n<h3 id=\"设计考量\"><a href=\"#设计考量\" class=\"headerlink\" title=\"设计考量\"></a>设计考量</h3><p>当设计一个注解类型时，必须考虑这种类型注解的基。可能使用一个注解零次、一次，或者多次（如果注解类型被标记为 @Repeatable）。也可能通过 @Target 元注解来限制一个注解哪里可以使用。例如，可以创建一个可重复的注解类型只能用在方法和属性上。仔细地设计注解类型很重要以确保程序员尽可能使用适合的和功能强大的注解。</p>"},{"title":"注解（四）：预定义注解类型","date":"2016-12-23T09:51:38.000Z","_content":"\n在 Java SE API 中预定义了一系列注解类型。一些注解类型被 Java 编译器使用，一些应用到其他注解上。\n\n<!-- more -->\n\n### Java 语言使用的注解类型\n\n在 java.lang 中预定义的注解类型是 @Deprecated、@Override 和 @SuppressWarnings。\n\n**@Deprecated** [@Deprecated](https://docs.oracle.com/javase/8/docs/api/java/lang/Deprecated.html) 注解表明被标记的元素是过时的且不应该继续使用。不管什么时候程序使用了一个有 @Deprecated 注解的方法、类或属性，编译期都会生成一个告警。当一个元素是过时的，应该使用 Javadoc @deprecated 标签记入文档，像下面的例子展示的。在 Javadoc 注释和在注解中使用 @ 符号不是同时发生的：从概念上它们是有关系的。并且，注意 Javadoc 标签以小写 d 开始，注解以大写 D 开始。\n\n    // Javadoc comment follows\n    /**\n     * @deprecated\n     * explanation of why it was deprecated\n     */\n    @Deprecated\n    static void deprecatedMethod() { }\n\n**@Override** [@Override](https://docs.oracle.com/javase/8/docs/api/java/lang/Override.html) 注解告诉编译器这个元素是从超类重载的一个元素。重载方法将在[接口和继承](http://docs.oracle.com/javase/tutorial/java/IandI/index.html)中讨论。\n\n    // mark method as a superclass method\n    // that has been overridden\n    @Override\n    int overriddenMethod() { }\n\n因为当重载一个方法的时候使用这个注解不是必须的，但它有助于防止错误。如果用 @Override 标记的一个方法不能正确的重载一个它的超类中的方法，编译器会生成一个错误。\n\n**@SuppressWarnings** [@SuppressWarnings](https://docs.oracle.com/javase/8/docs/api/java/lang/SuppressWarnings.html) 注解告诉编译器禁止指定的告警，否则会生成该告警。在下面的例子中，使用了一个过时的方法，编译器通常会生成一个告警。然而，在这个例子中，注解使告警被禁止。\n\n    // use a deprecated method and tell\n    // compiler not to generate a warning\n    @SuppressWarnings(\"deprecation\")\n    void useDeprecatedMethod() {\n      // deprecation warning\n      // - suppressed\n      objectOne.deprecatedMethod();\n    }\n\n每一个编译告警属于一种分类。Java 语言规范列出了两种分类：deprecation 和 unchecked。unchecked 告警发生在[范型](http://docs.oracle.com/javase/tutorial/java/generics/index.html)出现之前结合遗传代码编写时。禁止多种类型警告，使用下面的语法：\n\n    @SuppressWarnings({\"unchecked\", \"deprecation\"})\n\n**@SafeVarargs** [@SafeVarargs](https://docs.oracle.com/javase/8/docs/api/java/lang/SafeVarargs.html) 注解，当应用到一个方法或构造方法时，代码不会对可变参数执行潜在的不安全的操作。当这种注解类型被使用时，跟可变参数相关的未检查告警被禁止。\n\n**@FunctionalInterface** [@FunctionalInterface](https://docs.oracle.com/javase/8/docs/api/java/lang/FunctionalInterface.html) 注解，在 Java SE 8 中被引进，表明这个类型声明是一个功能性接口，像 Java 语言规范中定义的。\n\n### 应用到其他注解上的注解\n\n应用到其他注解上的注解被叫做*元注解*。在 java.lang.annotation 中定义了几种元注解类型。\n\n**@Retention** [@Retention](https://docs.oracle.com/javase/8/docs/api/java/lang/annotation/Retention.html) 注解指定被标记的注解如何存储：\n\n- RetentionPolicy.SOURCE - 被标记的注解只在源码级别被保留且被编译器忽略。\n- RetentionPolicy.CLASS - 被标记的注解在编译期被编译器保留，但被 Java 虚拟机（JVM）忽略。\n- RetentionPolicy.RUNTIME - 被标记的注解被 JVM 保留，因此可以被运行时环境使用。\n\n**@Documented** [@Documented](https://docs.oracle.com/javase/8/docs/api/java/lang/annotation/Documented.html) 注解表明不论何时使用该注解的元素会被 Javadoc 工具文档化。（默认的，注解不包含在 Javadoc 中。）获取更多信息，参见[Javadoc 工具页面](https://docs.oracle.com/javase/8/docs/technotes/guides/javadoc/index.html)。\n\n**@Target** [@Target](https://docs.oracle.com/javase/8/docs/api/java/lang/annotation/Target.html) 注解标记另外一个注解来限制它可以应用到什么类型的 Java 元素上。一个目标注解指定一个下面元素的类型作为它的值：\n\n- ElementType.ANNOTATION_TYPE 可以应用到一个注解类型。\n- ElementType.CONSTRUCTOR 可以应用到一个构造方法。\n- ElementType.FIELD 可以应用到一个域或属性。\n- ElementType.LOCAL_VARIABLE 可以应用到一个本地变量。\n- ElementType.METHOD 可以应用到方法级别的注解。\n- ElementType.PACKAGE 可以应用到包声明。\n- ElementType.PARAMETER 可以应用到方法的参数。\n- ElementType.TYPE 可以应用到一个累的任何元素。\n\n**@Inherited** [@Inherited](https://docs.oracle.com/javase/8/docs/api/java/lang/annotation/Inherited.html) 注解表明注解类型可以从超类继承。（默认是否。）当用户查询注解类型且类没有这种类型的注解时，会查询这个类的超类获取这个注解类型。这个注解只应用于类声明。\n\n**@Repeatable** [@Repeatable](https://docs.oracle.com/javase/8/docs/api/java/lang/annotation/Repeatable.html) 注解，在 Java SE 8 中被引入，表明被标记的注解可以对同一个声明或类型使用被应用多次。获取更多信息，参见[重复注解](http://docs.oracle.com/javase/tutorial/java/annotations/repeating.html)。\n","source":"_posts/注解（四）：预定义注解类型.md","raw":"title: 注解（四）：预定义注解类型\ntags:\n  - Java\ncategories:\n  - 语言\n  - Java\ndate: 2016-12-23 17:51:38\n---\n\n在 Java SE API 中预定义了一系列注解类型。一些注解类型被 Java 编译器使用，一些应用到其他注解上。\n\n<!-- more -->\n\n### Java 语言使用的注解类型\n\n在 java.lang 中预定义的注解类型是 @Deprecated、@Override 和 @SuppressWarnings。\n\n**@Deprecated** [@Deprecated](https://docs.oracle.com/javase/8/docs/api/java/lang/Deprecated.html) 注解表明被标记的元素是过时的且不应该继续使用。不管什么时候程序使用了一个有 @Deprecated 注解的方法、类或属性，编译期都会生成一个告警。当一个元素是过时的，应该使用 Javadoc @deprecated 标签记入文档，像下面的例子展示的。在 Javadoc 注释和在注解中使用 @ 符号不是同时发生的：从概念上它们是有关系的。并且，注意 Javadoc 标签以小写 d 开始，注解以大写 D 开始。\n\n    // Javadoc comment follows\n    /**\n     * @deprecated\n     * explanation of why it was deprecated\n     */\n    @Deprecated\n    static void deprecatedMethod() { }\n\n**@Override** [@Override](https://docs.oracle.com/javase/8/docs/api/java/lang/Override.html) 注解告诉编译器这个元素是从超类重载的一个元素。重载方法将在[接口和继承](http://docs.oracle.com/javase/tutorial/java/IandI/index.html)中讨论。\n\n    // mark method as a superclass method\n    // that has been overridden\n    @Override\n    int overriddenMethod() { }\n\n因为当重载一个方法的时候使用这个注解不是必须的，但它有助于防止错误。如果用 @Override 标记的一个方法不能正确的重载一个它的超类中的方法，编译器会生成一个错误。\n\n**@SuppressWarnings** [@SuppressWarnings](https://docs.oracle.com/javase/8/docs/api/java/lang/SuppressWarnings.html) 注解告诉编译器禁止指定的告警，否则会生成该告警。在下面的例子中，使用了一个过时的方法，编译器通常会生成一个告警。然而，在这个例子中，注解使告警被禁止。\n\n    // use a deprecated method and tell\n    // compiler not to generate a warning\n    @SuppressWarnings(\"deprecation\")\n    void useDeprecatedMethod() {\n      // deprecation warning\n      // - suppressed\n      objectOne.deprecatedMethod();\n    }\n\n每一个编译告警属于一种分类。Java 语言规范列出了两种分类：deprecation 和 unchecked。unchecked 告警发生在[范型](http://docs.oracle.com/javase/tutorial/java/generics/index.html)出现之前结合遗传代码编写时。禁止多种类型警告，使用下面的语法：\n\n    @SuppressWarnings({\"unchecked\", \"deprecation\"})\n\n**@SafeVarargs** [@SafeVarargs](https://docs.oracle.com/javase/8/docs/api/java/lang/SafeVarargs.html) 注解，当应用到一个方法或构造方法时，代码不会对可变参数执行潜在的不安全的操作。当这种注解类型被使用时，跟可变参数相关的未检查告警被禁止。\n\n**@FunctionalInterface** [@FunctionalInterface](https://docs.oracle.com/javase/8/docs/api/java/lang/FunctionalInterface.html) 注解，在 Java SE 8 中被引进，表明这个类型声明是一个功能性接口，像 Java 语言规范中定义的。\n\n### 应用到其他注解上的注解\n\n应用到其他注解上的注解被叫做*元注解*。在 java.lang.annotation 中定义了几种元注解类型。\n\n**@Retention** [@Retention](https://docs.oracle.com/javase/8/docs/api/java/lang/annotation/Retention.html) 注解指定被标记的注解如何存储：\n\n- RetentionPolicy.SOURCE - 被标记的注解只在源码级别被保留且被编译器忽略。\n- RetentionPolicy.CLASS - 被标记的注解在编译期被编译器保留，但被 Java 虚拟机（JVM）忽略。\n- RetentionPolicy.RUNTIME - 被标记的注解被 JVM 保留，因此可以被运行时环境使用。\n\n**@Documented** [@Documented](https://docs.oracle.com/javase/8/docs/api/java/lang/annotation/Documented.html) 注解表明不论何时使用该注解的元素会被 Javadoc 工具文档化。（默认的，注解不包含在 Javadoc 中。）获取更多信息，参见[Javadoc 工具页面](https://docs.oracle.com/javase/8/docs/technotes/guides/javadoc/index.html)。\n\n**@Target** [@Target](https://docs.oracle.com/javase/8/docs/api/java/lang/annotation/Target.html) 注解标记另外一个注解来限制它可以应用到什么类型的 Java 元素上。一个目标注解指定一个下面元素的类型作为它的值：\n\n- ElementType.ANNOTATION_TYPE 可以应用到一个注解类型。\n- ElementType.CONSTRUCTOR 可以应用到一个构造方法。\n- ElementType.FIELD 可以应用到一个域或属性。\n- ElementType.LOCAL_VARIABLE 可以应用到一个本地变量。\n- ElementType.METHOD 可以应用到方法级别的注解。\n- ElementType.PACKAGE 可以应用到包声明。\n- ElementType.PARAMETER 可以应用到方法的参数。\n- ElementType.TYPE 可以应用到一个累的任何元素。\n\n**@Inherited** [@Inherited](https://docs.oracle.com/javase/8/docs/api/java/lang/annotation/Inherited.html) 注解表明注解类型可以从超类继承。（默认是否。）当用户查询注解类型且类没有这种类型的注解时，会查询这个类的超类获取这个注解类型。这个注解只应用于类声明。\n\n**@Repeatable** [@Repeatable](https://docs.oracle.com/javase/8/docs/api/java/lang/annotation/Repeatable.html) 注解，在 Java SE 8 中被引入，表明被标记的注解可以对同一个声明或类型使用被应用多次。获取更多信息，参见[重复注解](http://docs.oracle.com/javase/tutorial/java/annotations/repeating.html)。\n","slug":"注解（四）：预定义注解类型","published":1,"updated":"2021-07-19T16:28:00.312Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphsl00hgitd33d4vbfsx","content":"<p>在 Java SE API 中预定义了一系列注解类型。一些注解类型被 Java 编译器使用，一些应用到其他注解上。</p>\n<span id=\"more\"></span>\n\n<h3 id=\"Java-语言使用的注解类型\"><a href=\"#Java-语言使用的注解类型\" class=\"headerlink\" title=\"Java 语言使用的注解类型\"></a>Java 语言使用的注解类型</h3><p>在 java.lang 中预定义的注解类型是 @Deprecated、@Override 和 @SuppressWarnings。</p>\n<p><strong>@Deprecated</strong> <a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/Deprecated.html\">@Deprecated</a> 注解表明被标记的元素是过时的且不应该继续使用。不管什么时候程序使用了一个有 @Deprecated 注解的方法、类或属性，编译期都会生成一个告警。当一个元素是过时的，应该使用 Javadoc @deprecated 标签记入文档，像下面的例子展示的。在 Javadoc 注释和在注解中使用 @ 符号不是同时发生的：从概念上它们是有关系的。并且，注意 Javadoc 标签以小写 d 开始，注解以大写 D 开始。</p>\n<pre><code>// Javadoc comment follows\n/**\n * @deprecated\n * explanation of why it was deprecated\n */\n@Deprecated\nstatic void deprecatedMethod() &#123; &#125;\n</code></pre>\n<p><strong>@Override</strong> <a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/Override.html\">@Override</a> 注解告诉编译器这个元素是从超类重载的一个元素。重载方法将在<a href=\"http://docs.oracle.com/javase/tutorial/java/IandI/index.html\">接口和继承</a>中讨论。</p>\n<pre><code>// mark method as a superclass method\n// that has been overridden\n@Override\nint overriddenMethod() &#123; &#125;\n</code></pre>\n<p>因为当重载一个方法的时候使用这个注解不是必须的，但它有助于防止错误。如果用 @Override 标记的一个方法不能正确的重载一个它的超类中的方法，编译器会生成一个错误。</p>\n<p><strong>@SuppressWarnings</strong> <a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/SuppressWarnings.html\">@SuppressWarnings</a> 注解告诉编译器禁止指定的告警，否则会生成该告警。在下面的例子中，使用了一个过时的方法，编译器通常会生成一个告警。然而，在这个例子中，注解使告警被禁止。</p>\n<pre><code>// use a deprecated method and tell\n// compiler not to generate a warning\n@SuppressWarnings(&quot;deprecation&quot;)\nvoid useDeprecatedMethod() &#123;\n  // deprecation warning\n  // - suppressed\n  objectOne.deprecatedMethod();\n&#125;\n</code></pre>\n<p>每一个编译告警属于一种分类。Java 语言规范列出了两种分类：deprecation 和 unchecked。unchecked 告警发生在<a href=\"http://docs.oracle.com/javase/tutorial/java/generics/index.html\">范型</a>出现之前结合遗传代码编写时。禁止多种类型警告，使用下面的语法：</p>\n<pre><code>@SuppressWarnings(&#123;&quot;unchecked&quot;, &quot;deprecation&quot;&#125;)\n</code></pre>\n<p><strong>@SafeVarargs</strong> <a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/SafeVarargs.html\">@SafeVarargs</a> 注解，当应用到一个方法或构造方法时，代码不会对可变参数执行潜在的不安全的操作。当这种注解类型被使用时，跟可变参数相关的未检查告警被禁止。</p>\n<p><strong>@FunctionalInterface</strong> <a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/FunctionalInterface.html\">@FunctionalInterface</a> 注解，在 Java SE 8 中被引进，表明这个类型声明是一个功能性接口，像 Java 语言规范中定义的。</p>\n<h3 id=\"应用到其他注解上的注解\"><a href=\"#应用到其他注解上的注解\" class=\"headerlink\" title=\"应用到其他注解上的注解\"></a>应用到其他注解上的注解</h3><p>应用到其他注解上的注解被叫做<em>元注解</em>。在 java.lang.annotation 中定义了几种元注解类型。</p>\n<p><strong>@Retention</strong> <a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/annotation/Retention.html\">@Retention</a> 注解指定被标记的注解如何存储：</p>\n<ul>\n<li>RetentionPolicy.SOURCE - 被标记的注解只在源码级别被保留且被编译器忽略。</li>\n<li>RetentionPolicy.CLASS - 被标记的注解在编译期被编译器保留，但被 Java 虚拟机（JVM）忽略。</li>\n<li>RetentionPolicy.RUNTIME - 被标记的注解被 JVM 保留，因此可以被运行时环境使用。</li>\n</ul>\n<p><strong>@Documented</strong> <a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/annotation/Documented.html\">@Documented</a> 注解表明不论何时使用该注解的元素会被 Javadoc 工具文档化。（默认的，注解不包含在 Javadoc 中。）获取更多信息，参见<a href=\"https://docs.oracle.com/javase/8/docs/technotes/guides/javadoc/index.html\">Javadoc 工具页面</a>。</p>\n<p><strong>@Target</strong> <a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/annotation/Target.html\">@Target</a> 注解标记另外一个注解来限制它可以应用到什么类型的 Java 元素上。一个目标注解指定一个下面元素的类型作为它的值：</p>\n<ul>\n<li>ElementType.ANNOTATION_TYPE 可以应用到一个注解类型。</li>\n<li>ElementType.CONSTRUCTOR 可以应用到一个构造方法。</li>\n<li>ElementType.FIELD 可以应用到一个域或属性。</li>\n<li>ElementType.LOCAL_VARIABLE 可以应用到一个本地变量。</li>\n<li>ElementType.METHOD 可以应用到方法级别的注解。</li>\n<li>ElementType.PACKAGE 可以应用到包声明。</li>\n<li>ElementType.PARAMETER 可以应用到方法的参数。</li>\n<li>ElementType.TYPE 可以应用到一个累的任何元素。</li>\n</ul>\n<p><strong>@Inherited</strong> <a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/annotation/Inherited.html\">@Inherited</a> 注解表明注解类型可以从超类继承。（默认是否。）当用户查询注解类型且类没有这种类型的注解时，会查询这个类的超类获取这个注解类型。这个注解只应用于类声明。</p>\n<p><strong>@Repeatable</strong> <a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/annotation/Repeatable.html\">@Repeatable</a> 注解，在 Java SE 8 中被引入，表明被标记的注解可以对同一个声明或类型使用被应用多次。获取更多信息，参见<a href=\"http://docs.oracle.com/javase/tutorial/java/annotations/repeating.html\">重复注解</a>。</p>\n","site":{"data":{}},"excerpt":"<p>在 Java SE API 中预定义了一系列注解类型。一些注解类型被 Java 编译器使用，一些应用到其他注解上。</p>","more":"<h3 id=\"Java-语言使用的注解类型\"><a href=\"#Java-语言使用的注解类型\" class=\"headerlink\" title=\"Java 语言使用的注解类型\"></a>Java 语言使用的注解类型</h3><p>在 java.lang 中预定义的注解类型是 @Deprecated、@Override 和 @SuppressWarnings。</p>\n<p><strong>@Deprecated</strong> <a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/Deprecated.html\">@Deprecated</a> 注解表明被标记的元素是过时的且不应该继续使用。不管什么时候程序使用了一个有 @Deprecated 注解的方法、类或属性，编译期都会生成一个告警。当一个元素是过时的，应该使用 Javadoc @deprecated 标签记入文档，像下面的例子展示的。在 Javadoc 注释和在注解中使用 @ 符号不是同时发生的：从概念上它们是有关系的。并且，注意 Javadoc 标签以小写 d 开始，注解以大写 D 开始。</p>\n<pre><code>// Javadoc comment follows\n/**\n * @deprecated\n * explanation of why it was deprecated\n */\n@Deprecated\nstatic void deprecatedMethod() &#123; &#125;\n</code></pre>\n<p><strong>@Override</strong> <a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/Override.html\">@Override</a> 注解告诉编译器这个元素是从超类重载的一个元素。重载方法将在<a href=\"http://docs.oracle.com/javase/tutorial/java/IandI/index.html\">接口和继承</a>中讨论。</p>\n<pre><code>// mark method as a superclass method\n// that has been overridden\n@Override\nint overriddenMethod() &#123; &#125;\n</code></pre>\n<p>因为当重载一个方法的时候使用这个注解不是必须的，但它有助于防止错误。如果用 @Override 标记的一个方法不能正确的重载一个它的超类中的方法，编译器会生成一个错误。</p>\n<p><strong>@SuppressWarnings</strong> <a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/SuppressWarnings.html\">@SuppressWarnings</a> 注解告诉编译器禁止指定的告警，否则会生成该告警。在下面的例子中，使用了一个过时的方法，编译器通常会生成一个告警。然而，在这个例子中，注解使告警被禁止。</p>\n<pre><code>// use a deprecated method and tell\n// compiler not to generate a warning\n@SuppressWarnings(&quot;deprecation&quot;)\nvoid useDeprecatedMethod() &#123;\n  // deprecation warning\n  // - suppressed\n  objectOne.deprecatedMethod();\n&#125;\n</code></pre>\n<p>每一个编译告警属于一种分类。Java 语言规范列出了两种分类：deprecation 和 unchecked。unchecked 告警发生在<a href=\"http://docs.oracle.com/javase/tutorial/java/generics/index.html\">范型</a>出现之前结合遗传代码编写时。禁止多种类型警告，使用下面的语法：</p>\n<pre><code>@SuppressWarnings(&#123;&quot;unchecked&quot;, &quot;deprecation&quot;&#125;)\n</code></pre>\n<p><strong>@SafeVarargs</strong> <a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/SafeVarargs.html\">@SafeVarargs</a> 注解，当应用到一个方法或构造方法时，代码不会对可变参数执行潜在的不安全的操作。当这种注解类型被使用时，跟可变参数相关的未检查告警被禁止。</p>\n<p><strong>@FunctionalInterface</strong> <a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/FunctionalInterface.html\">@FunctionalInterface</a> 注解，在 Java SE 8 中被引进，表明这个类型声明是一个功能性接口，像 Java 语言规范中定义的。</p>\n<h3 id=\"应用到其他注解上的注解\"><a href=\"#应用到其他注解上的注解\" class=\"headerlink\" title=\"应用到其他注解上的注解\"></a>应用到其他注解上的注解</h3><p>应用到其他注解上的注解被叫做<em>元注解</em>。在 java.lang.annotation 中定义了几种元注解类型。</p>\n<p><strong>@Retention</strong> <a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/annotation/Retention.html\">@Retention</a> 注解指定被标记的注解如何存储：</p>\n<ul>\n<li>RetentionPolicy.SOURCE - 被标记的注解只在源码级别被保留且被编译器忽略。</li>\n<li>RetentionPolicy.CLASS - 被标记的注解在编译期被编译器保留，但被 Java 虚拟机（JVM）忽略。</li>\n<li>RetentionPolicy.RUNTIME - 被标记的注解被 JVM 保留，因此可以被运行时环境使用。</li>\n</ul>\n<p><strong>@Documented</strong> <a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/annotation/Documented.html\">@Documented</a> 注解表明不论何时使用该注解的元素会被 Javadoc 工具文档化。（默认的，注解不包含在 Javadoc 中。）获取更多信息，参见<a href=\"https://docs.oracle.com/javase/8/docs/technotes/guides/javadoc/index.html\">Javadoc 工具页面</a>。</p>\n<p><strong>@Target</strong> <a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/annotation/Target.html\">@Target</a> 注解标记另外一个注解来限制它可以应用到什么类型的 Java 元素上。一个目标注解指定一个下面元素的类型作为它的值：</p>\n<ul>\n<li>ElementType.ANNOTATION_TYPE 可以应用到一个注解类型。</li>\n<li>ElementType.CONSTRUCTOR 可以应用到一个构造方法。</li>\n<li>ElementType.FIELD 可以应用到一个域或属性。</li>\n<li>ElementType.LOCAL_VARIABLE 可以应用到一个本地变量。</li>\n<li>ElementType.METHOD 可以应用到方法级别的注解。</li>\n<li>ElementType.PACKAGE 可以应用到包声明。</li>\n<li>ElementType.PARAMETER 可以应用到方法的参数。</li>\n<li>ElementType.TYPE 可以应用到一个累的任何元素。</li>\n</ul>\n<p><strong>@Inherited</strong> <a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/annotation/Inherited.html\">@Inherited</a> 注解表明注解类型可以从超类继承。（默认是否。）当用户查询注解类型且类没有这种类型的注解时，会查询这个类的超类获取这个注解类型。这个注解只应用于类声明。</p>\n<p><strong>@Repeatable</strong> <a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/annotation/Repeatable.html\">@Repeatable</a> 注解，在 Java SE 8 中被引入，表明被标记的注解可以对同一个声明或类型使用被应用多次。获取更多信息，参见<a href=\"http://docs.oracle.com/javase/tutorial/java/annotations/repeating.html\">重复注解</a>。</p>"},{"title":"浅谈 Hadoop NameNode、SecondaryNameNode、CheckPoint Node、BackupNode","date":"2017-01-08T13:40:34.000Z","_content":"\n### NameNode\n\nHadoop NameNode 管理着文件系统的 Namespace，它维护着整个文件系统树（FileSystem Tree）以及文件树中所有的文件和文件夹元数据（Metadata）。\n\nNameNode Metadata 主要是两个文件：edits 和 fsimage。fsimage 是 HDFS 的最新状态（截止到 fsimage 文件创建时间的最新状态）文件，而 edits 是自 fsimage 创建后的 Namespace 操作日志。NameNode 每次启动的时候，都要合并两个文件，按照 edits 的记录，把 fsimage 文件更新到最新。\n\n<!-- more -->\n\n### Secondary NameNode\n\nHadoop SecondaryNameNode 并不是 Hadoop 第二个 NameNode，它不提供 NameNode 服务，而仅仅是 NameNode 的一个工具，帮助 NameNode 管理 Metadata 数据。\n\n一般情况下，当  NameNode 重启的时候，会合并硬盘上的 fsimage 文件和 edits 文件，得到完整的 Metadata 信息。但是，如果集群规模十分庞大，操作频繁，那么 edits 文件就会非常大，这个合并过程就会非常慢，导致 HDFS 长时间无法启动。如果定时将 edits 文件合并到 fsimage，那么重启 NameNode 就可以非常快，而 Secondary NameNode 就做这个合并的工作。\n\nSecondary NameNode 定期地从 NameNode 上获取元数据。当它准备获取元数据的时候，就通知 NameNode 暂停写入 edits 文件。NameNode收到请求后停止写入 edits 文件，之后的 log 记录写入一个名为 edits.new 的文件。Secondary NameNode 获取到元数据以后，把 edits 文件和 fsimage 文件在本机进行合并，创建出一个新的 fsimage 文件，然后把新的 fsimage 文件发送回 NameNode。NameNode 收到 Secondary NameNode 发回的 fsimage 后，就拿它覆盖掉原来的 fsimage 文件，并删除 edits 文件，把 edits.new 重命名为 edits。\n\n通过这样一番操作，就避免了 NameNode 的 edits 日志的无限增长，加速 Namenode 的启动过程。\n\n![Secondary NameNode](/uploads/20170108/secondarynamenode.png)\n\n### CheckPoint Node\n\n可能是由于 Secondary NameNode 容易对人产生误导，因此 Hadoop 1.0.4 之后建议不要使用 Secondary NameNode，而使用 CheckPoint Node。Checkpoint Node 和 Secondary NameNode 的作用以及配置完全相同，只是启动命令不同 bin/hdfs namenode -checkpoint。\n\n### Backup Node\n\nBackup Node 在内存中维护了一份从 NameNode 同步过来的 fsimage，同时它还从 NameNode 接收 edits 文件的日志流，并把它们持久化硬盘，Backup Node 把收到的这些 edits 文件和内存中的 fsimage 文件进行合并，创建一份元数据备份。虽然 BackupNode 是一个备份的 NameNode 节点，不过 Backup Node 目前还无法直接接替 NameNode 提供服务。因此当前版本的 Backup Node 还不具有热备功能，也就是说，当 NameNode 发生故障，目前还只能通过重启 NameNode 的方式来恢复服务。\n\n不过在 Hadoop 2.x 中提出了 Hadoop HA 的一些策略，实现了 Hadoop NameNode 的 failover。","source":"_posts/浅谈-Hadoop-NameNode、SecondaryNameNode、CheckPoint-Node、BackupNode.md","raw":"title: 浅谈 Hadoop NameNode、SecondaryNameNode、CheckPoint Node、BackupNode\ntags:\n  - Hadoop\n  - HDFS\ncategories:\n  - 大数据\n  - Hadoop\ndate: 2017-01-08 21:40:34\n---\n\n### NameNode\n\nHadoop NameNode 管理着文件系统的 Namespace，它维护着整个文件系统树（FileSystem Tree）以及文件树中所有的文件和文件夹元数据（Metadata）。\n\nNameNode Metadata 主要是两个文件：edits 和 fsimage。fsimage 是 HDFS 的最新状态（截止到 fsimage 文件创建时间的最新状态）文件，而 edits 是自 fsimage 创建后的 Namespace 操作日志。NameNode 每次启动的时候，都要合并两个文件，按照 edits 的记录，把 fsimage 文件更新到最新。\n\n<!-- more -->\n\n### Secondary NameNode\n\nHadoop SecondaryNameNode 并不是 Hadoop 第二个 NameNode，它不提供 NameNode 服务，而仅仅是 NameNode 的一个工具，帮助 NameNode 管理 Metadata 数据。\n\n一般情况下，当  NameNode 重启的时候，会合并硬盘上的 fsimage 文件和 edits 文件，得到完整的 Metadata 信息。但是，如果集群规模十分庞大，操作频繁，那么 edits 文件就会非常大，这个合并过程就会非常慢，导致 HDFS 长时间无法启动。如果定时将 edits 文件合并到 fsimage，那么重启 NameNode 就可以非常快，而 Secondary NameNode 就做这个合并的工作。\n\nSecondary NameNode 定期地从 NameNode 上获取元数据。当它准备获取元数据的时候，就通知 NameNode 暂停写入 edits 文件。NameNode收到请求后停止写入 edits 文件，之后的 log 记录写入一个名为 edits.new 的文件。Secondary NameNode 获取到元数据以后，把 edits 文件和 fsimage 文件在本机进行合并，创建出一个新的 fsimage 文件，然后把新的 fsimage 文件发送回 NameNode。NameNode 收到 Secondary NameNode 发回的 fsimage 后，就拿它覆盖掉原来的 fsimage 文件，并删除 edits 文件，把 edits.new 重命名为 edits。\n\n通过这样一番操作，就避免了 NameNode 的 edits 日志的无限增长，加速 Namenode 的启动过程。\n\n![Secondary NameNode](/uploads/20170108/secondarynamenode.png)\n\n### CheckPoint Node\n\n可能是由于 Secondary NameNode 容易对人产生误导，因此 Hadoop 1.0.4 之后建议不要使用 Secondary NameNode，而使用 CheckPoint Node。Checkpoint Node 和 Secondary NameNode 的作用以及配置完全相同，只是启动命令不同 bin/hdfs namenode -checkpoint。\n\n### Backup Node\n\nBackup Node 在内存中维护了一份从 NameNode 同步过来的 fsimage，同时它还从 NameNode 接收 edits 文件的日志流，并把它们持久化硬盘，Backup Node 把收到的这些 edits 文件和内存中的 fsimage 文件进行合并，创建一份元数据备份。虽然 BackupNode 是一个备份的 NameNode 节点，不过 Backup Node 目前还无法直接接替 NameNode 提供服务。因此当前版本的 Backup Node 还不具有热备功能，也就是说，当 NameNode 发生故障，目前还只能通过重启 NameNode 的方式来恢复服务。\n\n不过在 Hadoop 2.x 中提出了 Hadoop HA 的一些策略，实现了 Hadoop NameNode 的 failover。","slug":"浅谈-Hadoop-NameNode、SecondaryNameNode、CheckPoint-Node、BackupNode","published":1,"updated":"2021-07-19T16:28:00.312Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphsn00hkitd35bis77se","content":"<h3 id=\"NameNode\"><a href=\"#NameNode\" class=\"headerlink\" title=\"NameNode\"></a>NameNode</h3><p>Hadoop NameNode 管理着文件系统的 Namespace，它维护着整个文件系统树（FileSystem Tree）以及文件树中所有的文件和文件夹元数据（Metadata）。</p>\n<p>NameNode Metadata 主要是两个文件：edits 和 fsimage。fsimage 是 HDFS 的最新状态（截止到 fsimage 文件创建时间的最新状态）文件，而 edits 是自 fsimage 创建后的 Namespace 操作日志。NameNode 每次启动的时候，都要合并两个文件，按照 edits 的记录，把 fsimage 文件更新到最新。</p>\n<span id=\"more\"></span>\n\n<h3 id=\"Secondary-NameNode\"><a href=\"#Secondary-NameNode\" class=\"headerlink\" title=\"Secondary NameNode\"></a>Secondary NameNode</h3><p>Hadoop SecondaryNameNode 并不是 Hadoop 第二个 NameNode，它不提供 NameNode 服务，而仅仅是 NameNode 的一个工具，帮助 NameNode 管理 Metadata 数据。</p>\n<p>一般情况下，当  NameNode 重启的时候，会合并硬盘上的 fsimage 文件和 edits 文件，得到完整的 Metadata 信息。但是，如果集群规模十分庞大，操作频繁，那么 edits 文件就会非常大，这个合并过程就会非常慢，导致 HDFS 长时间无法启动。如果定时将 edits 文件合并到 fsimage，那么重启 NameNode 就可以非常快，而 Secondary NameNode 就做这个合并的工作。</p>\n<p>Secondary NameNode 定期地从 NameNode 上获取元数据。当它准备获取元数据的时候，就通知 NameNode 暂停写入 edits 文件。NameNode收到请求后停止写入 edits 文件，之后的 log 记录写入一个名为 edits.new 的文件。Secondary NameNode 获取到元数据以后，把 edits 文件和 fsimage 文件在本机进行合并，创建出一个新的 fsimage 文件，然后把新的 fsimage 文件发送回 NameNode。NameNode 收到 Secondary NameNode 发回的 fsimage 后，就拿它覆盖掉原来的 fsimage 文件，并删除 edits 文件，把 edits.new 重命名为 edits。</p>\n<p>通过这样一番操作，就避免了 NameNode 的 edits 日志的无限增长，加速 Namenode 的启动过程。</p>\n<p><img src=\"/uploads/20170108/secondarynamenode.png\" alt=\"Secondary NameNode\"></p>\n<h3 id=\"CheckPoint-Node\"><a href=\"#CheckPoint-Node\" class=\"headerlink\" title=\"CheckPoint Node\"></a>CheckPoint Node</h3><p>可能是由于 Secondary NameNode 容易对人产生误导，因此 Hadoop 1.0.4 之后建议不要使用 Secondary NameNode，而使用 CheckPoint Node。Checkpoint Node 和 Secondary NameNode 的作用以及配置完全相同，只是启动命令不同 bin/hdfs namenode -checkpoint。</p>\n<h3 id=\"Backup-Node\"><a href=\"#Backup-Node\" class=\"headerlink\" title=\"Backup Node\"></a>Backup Node</h3><p>Backup Node 在内存中维护了一份从 NameNode 同步过来的 fsimage，同时它还从 NameNode 接收 edits 文件的日志流，并把它们持久化硬盘，Backup Node 把收到的这些 edits 文件和内存中的 fsimage 文件进行合并，创建一份元数据备份。虽然 BackupNode 是一个备份的 NameNode 节点，不过 Backup Node 目前还无法直接接替 NameNode 提供服务。因此当前版本的 Backup Node 还不具有热备功能，也就是说，当 NameNode 发生故障，目前还只能通过重启 NameNode 的方式来恢复服务。</p>\n<p>不过在 Hadoop 2.x 中提出了 Hadoop HA 的一些策略，实现了 Hadoop NameNode 的 failover。</p>\n","site":{"data":{}},"excerpt":"<h3 id=\"NameNode\"><a href=\"#NameNode\" class=\"headerlink\" title=\"NameNode\"></a>NameNode</h3><p>Hadoop NameNode 管理着文件系统的 Namespace，它维护着整个文件系统树（FileSystem Tree）以及文件树中所有的文件和文件夹元数据（Metadata）。</p>\n<p>NameNode Metadata 主要是两个文件：edits 和 fsimage。fsimage 是 HDFS 的最新状态（截止到 fsimage 文件创建时间的最新状态）文件，而 edits 是自 fsimage 创建后的 Namespace 操作日志。NameNode 每次启动的时候，都要合并两个文件，按照 edits 的记录，把 fsimage 文件更新到最新。</p>","more":"<h3 id=\"Secondary-NameNode\"><a href=\"#Secondary-NameNode\" class=\"headerlink\" title=\"Secondary NameNode\"></a>Secondary NameNode</h3><p>Hadoop SecondaryNameNode 并不是 Hadoop 第二个 NameNode，它不提供 NameNode 服务，而仅仅是 NameNode 的一个工具，帮助 NameNode 管理 Metadata 数据。</p>\n<p>一般情况下，当  NameNode 重启的时候，会合并硬盘上的 fsimage 文件和 edits 文件，得到完整的 Metadata 信息。但是，如果集群规模十分庞大，操作频繁，那么 edits 文件就会非常大，这个合并过程就会非常慢，导致 HDFS 长时间无法启动。如果定时将 edits 文件合并到 fsimage，那么重启 NameNode 就可以非常快，而 Secondary NameNode 就做这个合并的工作。</p>\n<p>Secondary NameNode 定期地从 NameNode 上获取元数据。当它准备获取元数据的时候，就通知 NameNode 暂停写入 edits 文件。NameNode收到请求后停止写入 edits 文件，之后的 log 记录写入一个名为 edits.new 的文件。Secondary NameNode 获取到元数据以后，把 edits 文件和 fsimage 文件在本机进行合并，创建出一个新的 fsimage 文件，然后把新的 fsimage 文件发送回 NameNode。NameNode 收到 Secondary NameNode 发回的 fsimage 后，就拿它覆盖掉原来的 fsimage 文件，并删除 edits 文件，把 edits.new 重命名为 edits。</p>\n<p>通过这样一番操作，就避免了 NameNode 的 edits 日志的无限增长，加速 Namenode 的启动过程。</p>\n<p><img src=\"/uploads/20170108/secondarynamenode.png\" alt=\"Secondary NameNode\"></p>\n<h3 id=\"CheckPoint-Node\"><a href=\"#CheckPoint-Node\" class=\"headerlink\" title=\"CheckPoint Node\"></a>CheckPoint Node</h3><p>可能是由于 Secondary NameNode 容易对人产生误导，因此 Hadoop 1.0.4 之后建议不要使用 Secondary NameNode，而使用 CheckPoint Node。Checkpoint Node 和 Secondary NameNode 的作用以及配置完全相同，只是启动命令不同 bin/hdfs namenode -checkpoint。</p>\n<h3 id=\"Backup-Node\"><a href=\"#Backup-Node\" class=\"headerlink\" title=\"Backup Node\"></a>Backup Node</h3><p>Backup Node 在内存中维护了一份从 NameNode 同步过来的 fsimage，同时它还从 NameNode 接收 edits 文件的日志流，并把它们持久化硬盘，Backup Node 把收到的这些 edits 文件和内存中的 fsimage 文件进行合并，创建一份元数据备份。虽然 BackupNode 是一个备份的 NameNode 节点，不过 Backup Node 目前还无法直接接替 NameNode 提供服务。因此当前版本的 Backup Node 还不具有热备功能，也就是说，当 NameNode 发生故障，目前还只能通过重启 NameNode 的方式来恢复服务。</p>\n<p>不过在 Hadoop 2.x 中提出了 Hadoop HA 的一些策略，实现了 Hadoop NameNode 的 failover。</p>"},{"title":"编译 Sqoop","date":"2018-01-16T06:19:47.000Z","_content":"\n\n### 基本要求\n\n编译 Sqoop 需要以下工具：\n\n- Sqoop 1.4.6 版本源码\n- Apache ant (1.7.1)\n- Java JDK 1.6\n\n另外，构建文档还需要下面的工具：\n\n- asciidoc\n- make\n- python 2.5+\n- xmlto\n- tar\n- gzip\n\n<!-- more -->\n\n此外，可以用以下工具测试 Sqoop 的构建：\n\n- findbugs (1.3.9)：检查代码质量\n- cobertura (1.9.4.1)：检查代码覆盖率\n- checkstyle (5.x)：检查代码格式\n\n### 编译过程\n\nSqoop 是用 ant 编译的。输入“ant -p”查看可以获得的构建目标列表。\n\n数据“ant”编译所有的 java 源码。然后可以用“bin/sqoop”运行 Sqoop。\n\n如果想构建所有的东西（包含文档），运行“ant package”。结果将出现在“build/sqoop-(version)/”目录。\n","source":"_posts/编译-Sqoop.md","raw":"title: 编译 Sqoop\ntags:\n  - Sqoop\n  - 大数据\ncategories:\n  - 大数据\n  - Sqoop\ndate: 2018-01-16 14:19:47\n---\n\n\n### 基本要求\n\n编译 Sqoop 需要以下工具：\n\n- Sqoop 1.4.6 版本源码\n- Apache ant (1.7.1)\n- Java JDK 1.6\n\n另外，构建文档还需要下面的工具：\n\n- asciidoc\n- make\n- python 2.5+\n- xmlto\n- tar\n- gzip\n\n<!-- more -->\n\n此外，可以用以下工具测试 Sqoop 的构建：\n\n- findbugs (1.3.9)：检查代码质量\n- cobertura (1.9.4.1)：检查代码覆盖率\n- checkstyle (5.x)：检查代码格式\n\n### 编译过程\n\nSqoop 是用 ant 编译的。输入“ant -p”查看可以获得的构建目标列表。\n\n数据“ant”编译所有的 java 源码。然后可以用“bin/sqoop”运行 Sqoop。\n\n如果想构建所有的东西（包含文档），运行“ant package”。结果将出现在“build/sqoop-(version)/”目录。\n","slug":"编译-Sqoop","published":1,"updated":"2021-07-19T16:28:00.312Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphso00hoitd3gt7d1hvy","content":"<h3 id=\"基本要求\"><a href=\"#基本要求\" class=\"headerlink\" title=\"基本要求\"></a>基本要求</h3><p>编译 Sqoop 需要以下工具：</p>\n<ul>\n<li>Sqoop 1.4.6 版本源码</li>\n<li>Apache ant (1.7.1)</li>\n<li>Java JDK 1.6</li>\n</ul>\n<p>另外，构建文档还需要下面的工具：</p>\n<ul>\n<li>asciidoc</li>\n<li>make</li>\n<li>python 2.5+</li>\n<li>xmlto</li>\n<li>tar</li>\n<li>gzip</li>\n</ul>\n<span id=\"more\"></span>\n\n<p>此外，可以用以下工具测试 Sqoop 的构建：</p>\n<ul>\n<li>findbugs (1.3.9)：检查代码质量</li>\n<li>cobertura (1.9.4.1)：检查代码覆盖率</li>\n<li>checkstyle (5.x)：检查代码格式</li>\n</ul>\n<h3 id=\"编译过程\"><a href=\"#编译过程\" class=\"headerlink\" title=\"编译过程\"></a>编译过程</h3><p>Sqoop 是用 ant 编译的。输入“ant -p”查看可以获得的构建目标列表。</p>\n<p>数据“ant”编译所有的 java 源码。然后可以用“bin/sqoop”运行 Sqoop。</p>\n<p>如果想构建所有的东西（包含文档），运行“ant package”。结果将出现在“build/sqoop-(version)/”目录。</p>\n","site":{"data":{}},"excerpt":"<h3 id=\"基本要求\"><a href=\"#基本要求\" class=\"headerlink\" title=\"基本要求\"></a>基本要求</h3><p>编译 Sqoop 需要以下工具：</p>\n<ul>\n<li>Sqoop 1.4.6 版本源码</li>\n<li>Apache ant (1.7.1)</li>\n<li>Java JDK 1.6</li>\n</ul>\n<p>另外，构建文档还需要下面的工具：</p>\n<ul>\n<li>asciidoc</li>\n<li>make</li>\n<li>python 2.5+</li>\n<li>xmlto</li>\n<li>tar</li>\n<li>gzip</li>\n</ul>","more":"<p>此外，可以用以下工具测试 Sqoop 的构建：</p>\n<ul>\n<li>findbugs (1.3.9)：检查代码质量</li>\n<li>cobertura (1.9.4.1)：检查代码覆盖率</li>\n<li>checkstyle (5.x)：检查代码格式</li>\n</ul>\n<h3 id=\"编译过程\"><a href=\"#编译过程\" class=\"headerlink\" title=\"编译过程\"></a>编译过程</h3><p>Sqoop 是用 ant 编译的。输入“ant -p”查看可以获得的构建目标列表。</p>\n<p>数据“ant”编译所有的 java 源码。然后可以用“bin/sqoop”运行 Sqoop。</p>\n<p>如果想构建所有的东西（包含文档），运行“ant package”。结果将出现在“build/sqoop-(version)/”目录。</p>"},{"title":"范型（一）：综述","date":"2016-11-19T13:34:37.000Z","_content":"\n在所有不寻常的软件项目中，Bug 只是一个生活的事实。精心的策划、编程、测试可以减少它普遍出现，但是在某种程度上、在某个地方，它们总会找到某种方式进入你的代码。伴随新功能的引进和你代码库的规模和复杂性的增长，这变得特别明显。\n\n<!-- more -->\n\n幸运的是，一些 Bug 比其他 Bug 更容易发现。例如，编译期 Bug 可以在早起被发现；你可以使用编译器的错误信息找出问题是什么并当场修复它。然而，运行时 Bug 会有更多的问题；它们总是不立即表现出来，当它们发生时，可能在程序中离问题发生原因很远的位置。\n\n范型通过使更多的 Bug 在编译期被发现增加代码的稳定性。完成这个课程后，你可能想跟进Gilad Bracha的[泛型](http://docs.oracle.com/javase/tutorial/extra/generics/index.html)教程。\n","source":"_posts/范型（一）：综述.md","raw":"title: 范型（一）：综述\ntags:\n  - Java\ncategories:\n  - 语言\n  - Java\ndate: 2016-11-19 21:34:37\n---\n\n在所有不寻常的软件项目中，Bug 只是一个生活的事实。精心的策划、编程、测试可以减少它普遍出现，但是在某种程度上、在某个地方，它们总会找到某种方式进入你的代码。伴随新功能的引进和你代码库的规模和复杂性的增长，这变得特别明显。\n\n<!-- more -->\n\n幸运的是，一些 Bug 比其他 Bug 更容易发现。例如，编译期 Bug 可以在早起被发现；你可以使用编译器的错误信息找出问题是什么并当场修复它。然而，运行时 Bug 会有更多的问题；它们总是不立即表现出来，当它们发生时，可能在程序中离问题发生原因很远的位置。\n\n范型通过使更多的 Bug 在编译期被发现增加代码的稳定性。完成这个课程后，你可能想跟进Gilad Bracha的[泛型](http://docs.oracle.com/javase/tutorial/extra/generics/index.html)教程。\n","slug":"范型（一）：综述","published":1,"updated":"2021-07-19T16:28:00.312Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphsp00hsitd3ae9calvd","content":"<p>在所有不寻常的软件项目中，Bug 只是一个生活的事实。精心的策划、编程、测试可以减少它普遍出现，但是在某种程度上、在某个地方，它们总会找到某种方式进入你的代码。伴随新功能的引进和你代码库的规模和复杂性的增长，这变得特别明显。</p>\n<span id=\"more\"></span>\n\n<p>幸运的是，一些 Bug 比其他 Bug 更容易发现。例如，编译期 Bug 可以在早起被发现；你可以使用编译器的错误信息找出问题是什么并当场修复它。然而，运行时 Bug 会有更多的问题；它们总是不立即表现出来，当它们发生时，可能在程序中离问题发生原因很远的位置。</p>\n<p>范型通过使更多的 Bug 在编译期被发现增加代码的稳定性。完成这个课程后，你可能想跟进Gilad Bracha的<a href=\"http://docs.oracle.com/javase/tutorial/extra/generics/index.html\">泛型</a>教程。</p>\n","site":{"data":{}},"excerpt":"<p>在所有不寻常的软件项目中，Bug 只是一个生活的事实。精心的策划、编程、测试可以减少它普遍出现，但是在某种程度上、在某个地方，它们总会找到某种方式进入你的代码。伴随新功能的引进和你代码库的规模和复杂性的增长，这变得特别明显。</p>","more":"<p>幸运的是，一些 Bug 比其他 Bug 更容易发现。例如，编译期 Bug 可以在早起被发现；你可以使用编译器的错误信息找出问题是什么并当场修复它。然而，运行时 Bug 会有更多的问题；它们总是不立即表现出来，当它们发生时，可能在程序中离问题发生原因很远的位置。</p>\n<p>范型通过使更多的 Bug 在编译期被发现增加代码的稳定性。完成这个课程后，你可能想跟进Gilad Bracha的<a href=\"http://docs.oracle.com/javase/tutorial/extra/generics/index.html\">泛型</a>教程。</p>"},{"title":"范型（七）：限定类型参数之范型方法和限定类型参数","date":"2016-11-27T07:41:54.000Z","_content":"\n\n限定类型参数是实现范型算法的关键。考虑下面的方法，计算数组 T[] 中大于指定元素 elem 的元素个数：\n\n    public static <T> int countGreaterThan(T[] anArray, T elem) {\n      int count = 0;\n      for (T e : anArray)\n        if (e > elem)  // compiler error\n          ++count;\n      return count;\n    }\n\n<!-- more -->\n\n方法的实现功能很明确，但却不能编译，因为大于操作符（>）只能应用于原始类型，如 short、int、double、long、float、byte 和 char。不能用 > 比较对象。为了解决这个问题，通过 Comparable<T> 接口使用类型参数限定：\n\n    public interface Comparable<T> {\n      public int compareTo(T o);\n    }\n\n结果代码将变为：\n\n    public static <T extends Comparable<T>> int countGreaterThan(T[] anArray, T elem) {\n      int count = 0;\n      for (T e : anArray)\n        if (e.compareTo(elem) > 0)\n          ++count;\n      return count;\n    }\n","source":"_posts/范型（七）：限定类型参数之范型方法和限定类型参数.md","raw":"title: 范型（七）：限定类型参数之范型方法和限定类型参数\ntags:\n  - Java\ncategories:\n  - 语言\n  - Java\ndate: 2016-11-27 15:41:54\n---\n\n\n限定类型参数是实现范型算法的关键。考虑下面的方法，计算数组 T[] 中大于指定元素 elem 的元素个数：\n\n    public static <T> int countGreaterThan(T[] anArray, T elem) {\n      int count = 0;\n      for (T e : anArray)\n        if (e > elem)  // compiler error\n          ++count;\n      return count;\n    }\n\n<!-- more -->\n\n方法的实现功能很明确，但却不能编译，因为大于操作符（>）只能应用于原始类型，如 short、int、double、long、float、byte 和 char。不能用 > 比较对象。为了解决这个问题，通过 Comparable<T> 接口使用类型参数限定：\n\n    public interface Comparable<T> {\n      public int compareTo(T o);\n    }\n\n结果代码将变为：\n\n    public static <T extends Comparable<T>> int countGreaterThan(T[] anArray, T elem) {\n      int count = 0;\n      for (T e : anArray)\n        if (e.compareTo(elem) > 0)\n          ++count;\n      return count;\n    }\n","slug":"范型（七）：限定类型参数之范型方法和限定类型参数","published":1,"updated":"2021-07-19T16:28:00.312Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphsr00hwitd3gw5i5kpl","content":"<p>限定类型参数是实现范型算法的关键。考虑下面的方法，计算数组 T[] 中大于指定元素 elem 的元素个数：</p>\n<pre><code>public static &lt;T&gt; int countGreaterThan(T[] anArray, T elem) &#123;\n  int count = 0;\n  for (T e : anArray)\n    if (e &gt; elem)  // compiler error\n      ++count;\n  return count;\n&#125;\n</code></pre>\n<span id=\"more\"></span>\n\n<p>方法的实现功能很明确，但却不能编译，因为大于操作符（&gt;）只能应用于原始类型，如 short、int、double、long、float、byte 和 char。不能用 &gt; 比较对象。为了解决这个问题，通过 Comparable<T> 接口使用类型参数限定：</p>\n<pre><code>public interface Comparable&lt;T&gt; &#123;\n  public int compareTo(T o);\n&#125;\n</code></pre>\n<p>结果代码将变为：</p>\n<pre><code>public static &lt;T extends Comparable&lt;T&gt;&gt; int countGreaterThan(T[] anArray, T elem) &#123;\n  int count = 0;\n  for (T e : anArray)\n    if (e.compareTo(elem) &gt; 0)\n      ++count;\n  return count;\n&#125;\n</code></pre>\n","site":{"data":{}},"excerpt":"<p>限定类型参数是实现范型算法的关键。考虑下面的方法，计算数组 T[] 中大于指定元素 elem 的元素个数：</p>\n<pre><code>public static &lt;T&gt; int countGreaterThan(T[] anArray, T elem) &#123;\n  int count = 0;\n  for (T e : anArray)\n    if (e &gt; elem)  // compiler error\n      ++count;\n  return count;\n&#125;\n</code></pre>","more":"<p>方法的实现功能很明确，但却不能编译，因为大于操作符（&gt;）只能应用于原始类型，如 short、int、double、long、float、byte 和 char。不能用 &gt; 比较对象。为了解决这个问题，通过 Comparable<T> 接口使用类型参数限定：</p>\n<pre><code>public interface Comparable&lt;T&gt; &#123;\n  public int compareTo(T o);\n&#125;\n</code></pre>\n<p>结果代码将变为：</p>\n<pre><code>public static &lt;T extends Comparable&lt;T&gt;&gt; int countGreaterThan(T[] anArray, T elem) &#123;\n  int count = 0;\n  for (T e : anArray)\n    if (e.compareTo(elem) &gt; 0)\n      ++count;\n  return count;\n&#125;\n</code></pre>"},{"title":"范型（三）：范型类型","date":"2016-11-19T15:51:56.000Z","_content":"\n范型类型是一个范型类或者接口，是参数化的类型。下面的 Box 类将被修改以展示这个概念。\n\n<!-- more -->\n\n### 简单的 Box 类\n\n首先检查一个可以操作任何类型对象的非范型 Box 类。它只需要提供两个方法：set，给 box 添加一个对象；get，获取这个对象。\n\n    public class Box {\n      private Object object;\n\n      public void set(Object object) { this.object = object; }\n      public Object get() { return object; }\n    }\n\n因为它的方法接受或返回一个 Object，你可以自由的输入任何你想要的，如果它不是原始类型之一。在编译期，没有方法确认这个类是如何被使用的。代码的一部分可能放置一个 Integer 在 box 中并且希望取出 Integer，而另一部分的代码可能错误的输入一个 String，导致一个运行时错误。\n\n### 范型版本的 Box 类\n\n用下面的格式定义一个范型类：\n\n    class name<T1, T2, ..., Tn> { /* ... */ }\n\n类型参数部分，被尖括号分隔的（<>），在类名称的后面。它指明类型参数（也叫类型变量）T1、T2 ... 和 Tn。\n\n更新 Box 类使用范型，通过改变”public class Box”为“public class Box<T>”创建一个范型类型声明。这引出了类型变量 T，它可以在类内部任何地方使用。\n\n变化后，Box 类变为：\n\n    /**\n     * Generic version of the Box class.\n     * @param <T> the type of the value being boxed\n     */\n    public class Box<T> {\n      // T stands for \"Type\"\n      private T t;\n\n      public void set(T t) { this.t = t; }\n      public T get() { return t; }\n    }\n\n像你看到的，所有出现的 Object 都被 T 取代了。类型变量可以是你指定的任意**非基本**类型：任意类类型、任意接口类型、任意数组类型，或者甚至是另外一个类型变量。\n\n同样的技术可以应用来创建通用接口。\n\n### 类型参数命名约定\n\n按照约定，类型参数名称是单个大写字母。这与你已经知道的变量命名约定形成了鲜明对比，并且有很好的理由：如果没有这个约定，就很难知道一个类型变量和一个普通的类或接口名称之间的区别。\n\n最常用的类型参数名称是：\n\n- E - Element（被 Java 集合框架广泛使用）\n- K - Key\n- N - Number\n- T - Type\n- V - Value\n- S、U、V 等 - 第二个、第三个、第四个类型\n\n你会在 Java SE API 和这节课剩下的内容看到这些名称的使用。\n\n### 调用和实例化范型类型\n\n在你的代码中引用范型 Box 类，你必须执行范型类型调用，用一些具体的值代替 T，例如 Integer：\n\n    Box<Integer> integerBox;\n\n你可以理解一个范型类型调用类似与一个普通方法调用，只是不是传递一个参数给一个方法，而是你传递一个类型参数-在这个例子中是 Integer -给 Box 类自己。\n\n> **Type Parameter 和 Type Argument 术语**：许多开发人员可互换的使用术语“type parameter”和“type argument”，但是这些术语不是相同的。当编码时，提供“type argument”为了创建一个参数化的类型。因此，T 在 Foo<T> 中是一个“type parameter”，String 在 Foo<String> 中是一个“type argument”。在这节课中当使用这些术语时遵守这个定义。\n\n像任意其他变量声明，这个代码不会实际创建一个 Box 对象。它简单的声明 integerBox 将保持一个指向“Integer 的 Box”的引用，这是 Box<Integer> 是如何读的。\n\n一个范型类型调用通常被称为参数化类型。\n\n要实例化这个类，通常使用 new 关键字，但是在类名和圆括号中间放置 <Integer>：\n\n    Box<Integer> integerBox = new Box<Integer>();\n\n### 钻石语法\n\n在 Java SE 7 之后，你可以用一个空类型参数组（<>）替换调用一个范型类构造方法的类型参数，主要编译器可以从上下文中确定或推断类型参数。这对尖括号（<>）被通俗的叫做钻石语法。例如，你可以使用下面的表达式创建一个 Box<Integer> 的实例：\n\n    Box<Integer> integerBox = new Box<>();\n\n有关钻石符号和类型推断的更多信息，请参见[类型推断](http://docs.oracle.com/javase/tutorial/java/generics/genTypeInference.html)。\n\n### 多类型参数\n\n如前所述，一个范型类可以有多个类型参数。例如，OrderedPair 范型类，实现了 Pair 范型接口：\n\n    public interface Pair<K, V> {\n      public K getKey();\n      public V getValue();\n    }\n\n    public class OrderedPair<K, V> implements Pair<K, V> {\n\n      private K key;\n      private V value;\n\n      public OrderedPair(K key, V value) {\n\tthis.key = key;\n\tthis.value = value;\n      }\n\n      public K getKey()\t{ return key; }\n      public V getValue() { return value; }\n    }\n\n下面的表达式创建了 OrderedPair 类的两个实例：\n\n    Pair<String, Integer> p1 = new OrderedPair<String, Integer>(\"Even\", 8);\n    Pair<String, String>  p2 = new OrderedPair<String, String>(\"hello\", \"world\");\n\n代码 new OrderedPair<String, Integer> 实例化 K 为一个 String 且 V 为一个 Integer。因此，OrderedPair 的构造函数的各个参数化类型是 String 和 Integer。由于[自动装箱](http://docs.oracle.com/javase/tutorial/java/data/autoboxing.html)，传递一个 String 和一个 int 给这个类是有效的。\n\n如[钻石语法](http://docs.oracle.com/javase/tutorial/java/generics/types.html#diamond)所述，因为 Java 编译器可以从 OrderedPair<String, Integer> 声明推测 K 和 V 的类型，这些表达式可以使用钻石符号简写：\n\n    OrderedPair<String, Integer> p1 = new OrderedPair<>(\"Even\", 8);\n    OrderedPair<String, String>  p2 = new OrderedPair<>(\"hello\", \"world\");\n\n要创建一个范型接口，遵循创建范型类的相同的约定。\n\n### 参数化类型\n\n你也可以用参数化类型（即 List<String>）来代替类型参数（即 K 或 V）。例如，使用 OrderedPair<K, V> 的例子：\n\n    OrderedPair<String, Box<Integer>> p = new OrderedPair<>(\"primes\", new Box<Integer>(...));\n","source":"_posts/范型（三）：范型类型.md","raw":"title: 范型（三）：范型类型\ntags:\n  - Java\ncategories:\n  - 语言\n  - Java\ndate: 2016-11-19 23:51:56\n---\n\n范型类型是一个范型类或者接口，是参数化的类型。下面的 Box 类将被修改以展示这个概念。\n\n<!-- more -->\n\n### 简单的 Box 类\n\n首先检查一个可以操作任何类型对象的非范型 Box 类。它只需要提供两个方法：set，给 box 添加一个对象；get，获取这个对象。\n\n    public class Box {\n      private Object object;\n\n      public void set(Object object) { this.object = object; }\n      public Object get() { return object; }\n    }\n\n因为它的方法接受或返回一个 Object，你可以自由的输入任何你想要的，如果它不是原始类型之一。在编译期，没有方法确认这个类是如何被使用的。代码的一部分可能放置一个 Integer 在 box 中并且希望取出 Integer，而另一部分的代码可能错误的输入一个 String，导致一个运行时错误。\n\n### 范型版本的 Box 类\n\n用下面的格式定义一个范型类：\n\n    class name<T1, T2, ..., Tn> { /* ... */ }\n\n类型参数部分，被尖括号分隔的（<>），在类名称的后面。它指明类型参数（也叫类型变量）T1、T2 ... 和 Tn。\n\n更新 Box 类使用范型，通过改变”public class Box”为“public class Box<T>”创建一个范型类型声明。这引出了类型变量 T，它可以在类内部任何地方使用。\n\n变化后，Box 类变为：\n\n    /**\n     * Generic version of the Box class.\n     * @param <T> the type of the value being boxed\n     */\n    public class Box<T> {\n      // T stands for \"Type\"\n      private T t;\n\n      public void set(T t) { this.t = t; }\n      public T get() { return t; }\n    }\n\n像你看到的，所有出现的 Object 都被 T 取代了。类型变量可以是你指定的任意**非基本**类型：任意类类型、任意接口类型、任意数组类型，或者甚至是另外一个类型变量。\n\n同样的技术可以应用来创建通用接口。\n\n### 类型参数命名约定\n\n按照约定，类型参数名称是单个大写字母。这与你已经知道的变量命名约定形成了鲜明对比，并且有很好的理由：如果没有这个约定，就很难知道一个类型变量和一个普通的类或接口名称之间的区别。\n\n最常用的类型参数名称是：\n\n- E - Element（被 Java 集合框架广泛使用）\n- K - Key\n- N - Number\n- T - Type\n- V - Value\n- S、U、V 等 - 第二个、第三个、第四个类型\n\n你会在 Java SE API 和这节课剩下的内容看到这些名称的使用。\n\n### 调用和实例化范型类型\n\n在你的代码中引用范型 Box 类，你必须执行范型类型调用，用一些具体的值代替 T，例如 Integer：\n\n    Box<Integer> integerBox;\n\n你可以理解一个范型类型调用类似与一个普通方法调用，只是不是传递一个参数给一个方法，而是你传递一个类型参数-在这个例子中是 Integer -给 Box 类自己。\n\n> **Type Parameter 和 Type Argument 术语**：许多开发人员可互换的使用术语“type parameter”和“type argument”，但是这些术语不是相同的。当编码时，提供“type argument”为了创建一个参数化的类型。因此，T 在 Foo<T> 中是一个“type parameter”，String 在 Foo<String> 中是一个“type argument”。在这节课中当使用这些术语时遵守这个定义。\n\n像任意其他变量声明，这个代码不会实际创建一个 Box 对象。它简单的声明 integerBox 将保持一个指向“Integer 的 Box”的引用，这是 Box<Integer> 是如何读的。\n\n一个范型类型调用通常被称为参数化类型。\n\n要实例化这个类，通常使用 new 关键字，但是在类名和圆括号中间放置 <Integer>：\n\n    Box<Integer> integerBox = new Box<Integer>();\n\n### 钻石语法\n\n在 Java SE 7 之后，你可以用一个空类型参数组（<>）替换调用一个范型类构造方法的类型参数，主要编译器可以从上下文中确定或推断类型参数。这对尖括号（<>）被通俗的叫做钻石语法。例如，你可以使用下面的表达式创建一个 Box<Integer> 的实例：\n\n    Box<Integer> integerBox = new Box<>();\n\n有关钻石符号和类型推断的更多信息，请参见[类型推断](http://docs.oracle.com/javase/tutorial/java/generics/genTypeInference.html)。\n\n### 多类型参数\n\n如前所述，一个范型类可以有多个类型参数。例如，OrderedPair 范型类，实现了 Pair 范型接口：\n\n    public interface Pair<K, V> {\n      public K getKey();\n      public V getValue();\n    }\n\n    public class OrderedPair<K, V> implements Pair<K, V> {\n\n      private K key;\n      private V value;\n\n      public OrderedPair(K key, V value) {\n\tthis.key = key;\n\tthis.value = value;\n      }\n\n      public K getKey()\t{ return key; }\n      public V getValue() { return value; }\n    }\n\n下面的表达式创建了 OrderedPair 类的两个实例：\n\n    Pair<String, Integer> p1 = new OrderedPair<String, Integer>(\"Even\", 8);\n    Pair<String, String>  p2 = new OrderedPair<String, String>(\"hello\", \"world\");\n\n代码 new OrderedPair<String, Integer> 实例化 K 为一个 String 且 V 为一个 Integer。因此，OrderedPair 的构造函数的各个参数化类型是 String 和 Integer。由于[自动装箱](http://docs.oracle.com/javase/tutorial/java/data/autoboxing.html)，传递一个 String 和一个 int 给这个类是有效的。\n\n如[钻石语法](http://docs.oracle.com/javase/tutorial/java/generics/types.html#diamond)所述，因为 Java 编译器可以从 OrderedPair<String, Integer> 声明推测 K 和 V 的类型，这些表达式可以使用钻石符号简写：\n\n    OrderedPair<String, Integer> p1 = new OrderedPair<>(\"Even\", 8);\n    OrderedPair<String, String>  p2 = new OrderedPair<>(\"hello\", \"world\");\n\n要创建一个范型接口，遵循创建范型类的相同的约定。\n\n### 参数化类型\n\n你也可以用参数化类型（即 List<String>）来代替类型参数（即 K 或 V）。例如，使用 OrderedPair<K, V> 的例子：\n\n    OrderedPair<String, Box<Integer>> p = new OrderedPair<>(\"primes\", new Box<Integer>(...));\n","slug":"范型（三）：范型类型","published":1,"updated":"2021-07-19T16:28:00.312Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphss00i0itd35tib9uqe","content":"<p>范型类型是一个范型类或者接口，是参数化的类型。下面的 Box 类将被修改以展示这个概念。</p>\n<span id=\"more\"></span>\n\n<h3 id=\"简单的-Box-类\"><a href=\"#简单的-Box-类\" class=\"headerlink\" title=\"简单的 Box 类\"></a>简单的 Box 类</h3><p>首先检查一个可以操作任何类型对象的非范型 Box 类。它只需要提供两个方法：set，给 box 添加一个对象；get，获取这个对象。</p>\n<pre><code>public class Box &#123;\n  private Object object;\n\n  public void set(Object object) &#123; this.object = object; &#125;\n  public Object get() &#123; return object; &#125;\n&#125;\n</code></pre>\n<p>因为它的方法接受或返回一个 Object，你可以自由的输入任何你想要的，如果它不是原始类型之一。在编译期，没有方法确认这个类是如何被使用的。代码的一部分可能放置一个 Integer 在 box 中并且希望取出 Integer，而另一部分的代码可能错误的输入一个 String，导致一个运行时错误。</p>\n<h3 id=\"范型版本的-Box-类\"><a href=\"#范型版本的-Box-类\" class=\"headerlink\" title=\"范型版本的 Box 类\"></a>范型版本的 Box 类</h3><p>用下面的格式定义一个范型类：</p>\n<pre><code>class name&lt;T1, T2, ..., Tn&gt; &#123; /* ... */ &#125;\n</code></pre>\n<p>类型参数部分，被尖括号分隔的（&lt;&gt;），在类名称的后面。它指明类型参数（也叫类型变量）T1、T2 … 和 Tn。</p>\n<p>更新 Box 类使用范型，通过改变”public class Box”为“public class Box<T>”创建一个范型类型声明。这引出了类型变量 T，它可以在类内部任何地方使用。</p>\n<p>变化后，Box 类变为：</p>\n<pre><code>/**\n * Generic version of the Box class.\n * @param &lt;T&gt; the type of the value being boxed\n */\npublic class Box&lt;T&gt; &#123;\n  // T stands for &quot;Type&quot;\n  private T t;\n\n  public void set(T t) &#123; this.t = t; &#125;\n  public T get() &#123; return t; &#125;\n&#125;\n</code></pre>\n<p>像你看到的，所有出现的 Object 都被 T 取代了。类型变量可以是你指定的任意<strong>非基本</strong>类型：任意类类型、任意接口类型、任意数组类型，或者甚至是另外一个类型变量。</p>\n<p>同样的技术可以应用来创建通用接口。</p>\n<h3 id=\"类型参数命名约定\"><a href=\"#类型参数命名约定\" class=\"headerlink\" title=\"类型参数命名约定\"></a>类型参数命名约定</h3><p>按照约定，类型参数名称是单个大写字母。这与你已经知道的变量命名约定形成了鲜明对比，并且有很好的理由：如果没有这个约定，就很难知道一个类型变量和一个普通的类或接口名称之间的区别。</p>\n<p>最常用的类型参数名称是：</p>\n<ul>\n<li>E - Element（被 Java 集合框架广泛使用）</li>\n<li>K - Key</li>\n<li>N - Number</li>\n<li>T - Type</li>\n<li>V - Value</li>\n<li>S、U、V 等 - 第二个、第三个、第四个类型</li>\n</ul>\n<p>你会在 Java SE API 和这节课剩下的内容看到这些名称的使用。</p>\n<h3 id=\"调用和实例化范型类型\"><a href=\"#调用和实例化范型类型\" class=\"headerlink\" title=\"调用和实例化范型类型\"></a>调用和实例化范型类型</h3><p>在你的代码中引用范型 Box 类，你必须执行范型类型调用，用一些具体的值代替 T，例如 Integer：</p>\n<pre><code>Box&lt;Integer&gt; integerBox;\n</code></pre>\n<p>你可以理解一个范型类型调用类似与一个普通方法调用，只是不是传递一个参数给一个方法，而是你传递一个类型参数-在这个例子中是 Integer -给 Box 类自己。</p>\n<blockquote>\n<p><strong>Type Parameter 和 Type Argument 术语</strong>：许多开发人员可互换的使用术语“type parameter”和“type argument”，但是这些术语不是相同的。当编码时，提供“type argument”为了创建一个参数化的类型。因此，T 在 Foo<T> 中是一个“type parameter”，String 在 Foo<String> 中是一个“type argument”。在这节课中当使用这些术语时遵守这个定义。</p>\n</blockquote>\n<p>像任意其他变量声明，这个代码不会实际创建一个 Box 对象。它简单的声明 integerBox 将保持一个指向“Integer 的 Box”的引用，这是 Box<Integer> 是如何读的。</p>\n<p>一个范型类型调用通常被称为参数化类型。</p>\n<p>要实例化这个类，通常使用 new 关键字，但是在类名和圆括号中间放置 <Integer>：</p>\n<pre><code>Box&lt;Integer&gt; integerBox = new Box&lt;Integer&gt;();\n</code></pre>\n<h3 id=\"钻石语法\"><a href=\"#钻石语法\" class=\"headerlink\" title=\"钻石语法\"></a>钻石语法</h3><p>在 Java SE 7 之后，你可以用一个空类型参数组（&lt;&gt;）替换调用一个范型类构造方法的类型参数，主要编译器可以从上下文中确定或推断类型参数。这对尖括号（&lt;&gt;）被通俗的叫做钻石语法。例如，你可以使用下面的表达式创建一个 Box<Integer> 的实例：</p>\n<pre><code>Box&lt;Integer&gt; integerBox = new Box&lt;&gt;();\n</code></pre>\n<p>有关钻石符号和类型推断的更多信息，请参见<a href=\"http://docs.oracle.com/javase/tutorial/java/generics/genTypeInference.html\">类型推断</a>。</p>\n<h3 id=\"多类型参数\"><a href=\"#多类型参数\" class=\"headerlink\" title=\"多类型参数\"></a>多类型参数</h3><p>如前所述，一个范型类可以有多个类型参数。例如，OrderedPair 范型类，实现了 Pair 范型接口：</p>\n<pre><code>public interface Pair&lt;K, V&gt; &#123;\n  public K getKey();\n  public V getValue();\n&#125;\n\npublic class OrderedPair&lt;K, V&gt; implements Pair&lt;K, V&gt; &#123;\n\n  private K key;\n  private V value;\n\n  public OrderedPair(K key, V value) &#123;\nthis.key = key;\nthis.value = value;\n  &#125;\n\n  public K getKey()    &#123; return key; &#125;\n  public V getValue() &#123; return value; &#125;\n&#125;\n</code></pre>\n<p>下面的表达式创建了 OrderedPair 类的两个实例：</p>\n<pre><code>Pair&lt;String, Integer&gt; p1 = new OrderedPair&lt;String, Integer&gt;(&quot;Even&quot;, 8);\nPair&lt;String, String&gt;  p2 = new OrderedPair&lt;String, String&gt;(&quot;hello&quot;, &quot;world&quot;);\n</code></pre>\n<p>代码 new OrderedPair&lt;String, Integer&gt; 实例化 K 为一个 String 且 V 为一个 Integer。因此，OrderedPair 的构造函数的各个参数化类型是 String 和 Integer。由于<a href=\"http://docs.oracle.com/javase/tutorial/java/data/autoboxing.html\">自动装箱</a>，传递一个 String 和一个 int 给这个类是有效的。</p>\n<p>如<a href=\"http://docs.oracle.com/javase/tutorial/java/generics/types.html#diamond\">钻石语法</a>所述，因为 Java 编译器可以从 OrderedPair&lt;String, Integer&gt; 声明推测 K 和 V 的类型，这些表达式可以使用钻石符号简写：</p>\n<pre><code>OrderedPair&lt;String, Integer&gt; p1 = new OrderedPair&lt;&gt;(&quot;Even&quot;, 8);\nOrderedPair&lt;String, String&gt;  p2 = new OrderedPair&lt;&gt;(&quot;hello&quot;, &quot;world&quot;);\n</code></pre>\n<p>要创建一个范型接口，遵循创建范型类的相同的约定。</p>\n<h3 id=\"参数化类型\"><a href=\"#参数化类型\" class=\"headerlink\" title=\"参数化类型\"></a>参数化类型</h3><p>你也可以用参数化类型（即 List<String>）来代替类型参数（即 K 或 V）。例如，使用 OrderedPair&lt;K, V&gt; 的例子：</p>\n<pre><code>OrderedPair&lt;String, Box&lt;Integer&gt;&gt; p = new OrderedPair&lt;&gt;(&quot;primes&quot;, new Box&lt;Integer&gt;(...));\n</code></pre>\n","site":{"data":{}},"excerpt":"<p>范型类型是一个范型类或者接口，是参数化的类型。下面的 Box 类将被修改以展示这个概念。</p>","more":"<h3 id=\"简单的-Box-类\"><a href=\"#简单的-Box-类\" class=\"headerlink\" title=\"简单的 Box 类\"></a>简单的 Box 类</h3><p>首先检查一个可以操作任何类型对象的非范型 Box 类。它只需要提供两个方法：set，给 box 添加一个对象；get，获取这个对象。</p>\n<pre><code>public class Box &#123;\n  private Object object;\n\n  public void set(Object object) &#123; this.object = object; &#125;\n  public Object get() &#123; return object; &#125;\n&#125;\n</code></pre>\n<p>因为它的方法接受或返回一个 Object，你可以自由的输入任何你想要的，如果它不是原始类型之一。在编译期，没有方法确认这个类是如何被使用的。代码的一部分可能放置一个 Integer 在 box 中并且希望取出 Integer，而另一部分的代码可能错误的输入一个 String，导致一个运行时错误。</p>\n<h3 id=\"范型版本的-Box-类\"><a href=\"#范型版本的-Box-类\" class=\"headerlink\" title=\"范型版本的 Box 类\"></a>范型版本的 Box 类</h3><p>用下面的格式定义一个范型类：</p>\n<pre><code>class name&lt;T1, T2, ..., Tn&gt; &#123; /* ... */ &#125;\n</code></pre>\n<p>类型参数部分，被尖括号分隔的（&lt;&gt;），在类名称的后面。它指明类型参数（也叫类型变量）T1、T2 … 和 Tn。</p>\n<p>更新 Box 类使用范型，通过改变”public class Box”为“public class Box<T>”创建一个范型类型声明。这引出了类型变量 T，它可以在类内部任何地方使用。</p>\n<p>变化后，Box 类变为：</p>\n<pre><code>/**\n * Generic version of the Box class.\n * @param &lt;T&gt; the type of the value being boxed\n */\npublic class Box&lt;T&gt; &#123;\n  // T stands for &quot;Type&quot;\n  private T t;\n\n  public void set(T t) &#123; this.t = t; &#125;\n  public T get() &#123; return t; &#125;\n&#125;\n</code></pre>\n<p>像你看到的，所有出现的 Object 都被 T 取代了。类型变量可以是你指定的任意<strong>非基本</strong>类型：任意类类型、任意接口类型、任意数组类型，或者甚至是另外一个类型变量。</p>\n<p>同样的技术可以应用来创建通用接口。</p>\n<h3 id=\"类型参数命名约定\"><a href=\"#类型参数命名约定\" class=\"headerlink\" title=\"类型参数命名约定\"></a>类型参数命名约定</h3><p>按照约定，类型参数名称是单个大写字母。这与你已经知道的变量命名约定形成了鲜明对比，并且有很好的理由：如果没有这个约定，就很难知道一个类型变量和一个普通的类或接口名称之间的区别。</p>\n<p>最常用的类型参数名称是：</p>\n<ul>\n<li>E - Element（被 Java 集合框架广泛使用）</li>\n<li>K - Key</li>\n<li>N - Number</li>\n<li>T - Type</li>\n<li>V - Value</li>\n<li>S、U、V 等 - 第二个、第三个、第四个类型</li>\n</ul>\n<p>你会在 Java SE API 和这节课剩下的内容看到这些名称的使用。</p>\n<h3 id=\"调用和实例化范型类型\"><a href=\"#调用和实例化范型类型\" class=\"headerlink\" title=\"调用和实例化范型类型\"></a>调用和实例化范型类型</h3><p>在你的代码中引用范型 Box 类，你必须执行范型类型调用，用一些具体的值代替 T，例如 Integer：</p>\n<pre><code>Box&lt;Integer&gt; integerBox;\n</code></pre>\n<p>你可以理解一个范型类型调用类似与一个普通方法调用，只是不是传递一个参数给一个方法，而是你传递一个类型参数-在这个例子中是 Integer -给 Box 类自己。</p>\n<blockquote>\n<p><strong>Type Parameter 和 Type Argument 术语</strong>：许多开发人员可互换的使用术语“type parameter”和“type argument”，但是这些术语不是相同的。当编码时，提供“type argument”为了创建一个参数化的类型。因此，T 在 Foo<T> 中是一个“type parameter”，String 在 Foo<String> 中是一个“type argument”。在这节课中当使用这些术语时遵守这个定义。</p>\n</blockquote>\n<p>像任意其他变量声明，这个代码不会实际创建一个 Box 对象。它简单的声明 integerBox 将保持一个指向“Integer 的 Box”的引用，这是 Box<Integer> 是如何读的。</p>\n<p>一个范型类型调用通常被称为参数化类型。</p>\n<p>要实例化这个类，通常使用 new 关键字，但是在类名和圆括号中间放置 <Integer>：</p>\n<pre><code>Box&lt;Integer&gt; integerBox = new Box&lt;Integer&gt;();\n</code></pre>\n<h3 id=\"钻石语法\"><a href=\"#钻石语法\" class=\"headerlink\" title=\"钻石语法\"></a>钻石语法</h3><p>在 Java SE 7 之后，你可以用一个空类型参数组（&lt;&gt;）替换调用一个范型类构造方法的类型参数，主要编译器可以从上下文中确定或推断类型参数。这对尖括号（&lt;&gt;）被通俗的叫做钻石语法。例如，你可以使用下面的表达式创建一个 Box<Integer> 的实例：</p>\n<pre><code>Box&lt;Integer&gt; integerBox = new Box&lt;&gt;();\n</code></pre>\n<p>有关钻石符号和类型推断的更多信息，请参见<a href=\"http://docs.oracle.com/javase/tutorial/java/generics/genTypeInference.html\">类型推断</a>。</p>\n<h3 id=\"多类型参数\"><a href=\"#多类型参数\" class=\"headerlink\" title=\"多类型参数\"></a>多类型参数</h3><p>如前所述，一个范型类可以有多个类型参数。例如，OrderedPair 范型类，实现了 Pair 范型接口：</p>\n<pre><code>public interface Pair&lt;K, V&gt; &#123;\n  public K getKey();\n  public V getValue();\n&#125;\n\npublic class OrderedPair&lt;K, V&gt; implements Pair&lt;K, V&gt; &#123;\n\n  private K key;\n  private V value;\n\n  public OrderedPair(K key, V value) &#123;\nthis.key = key;\nthis.value = value;\n  &#125;\n\n  public K getKey()    &#123; return key; &#125;\n  public V getValue() &#123; return value; &#125;\n&#125;\n</code></pre>\n<p>下面的表达式创建了 OrderedPair 类的两个实例：</p>\n<pre><code>Pair&lt;String, Integer&gt; p1 = new OrderedPair&lt;String, Integer&gt;(&quot;Even&quot;, 8);\nPair&lt;String, String&gt;  p2 = new OrderedPair&lt;String, String&gt;(&quot;hello&quot;, &quot;world&quot;);\n</code></pre>\n<p>代码 new OrderedPair&lt;String, Integer&gt; 实例化 K 为一个 String 且 V 为一个 Integer。因此，OrderedPair 的构造函数的各个参数化类型是 String 和 Integer。由于<a href=\"http://docs.oracle.com/javase/tutorial/java/data/autoboxing.html\">自动装箱</a>，传递一个 String 和一个 int 给这个类是有效的。</p>\n<p>如<a href=\"http://docs.oracle.com/javase/tutorial/java/generics/types.html#diamond\">钻石语法</a>所述，因为 Java 编译器可以从 OrderedPair&lt;String, Integer&gt; 声明推测 K 和 V 的类型，这些表达式可以使用钻石符号简写：</p>\n<pre><code>OrderedPair&lt;String, Integer&gt; p1 = new OrderedPair&lt;&gt;(&quot;Even&quot;, 8);\nOrderedPair&lt;String, String&gt;  p2 = new OrderedPair&lt;&gt;(&quot;hello&quot;, &quot;world&quot;);\n</code></pre>\n<p>要创建一个范型接口，遵循创建范型类的相同的约定。</p>\n<h3 id=\"参数化类型\"><a href=\"#参数化类型\" class=\"headerlink\" title=\"参数化类型\"></a>参数化类型</h3><p>你也可以用参数化类型（即 List<String>）来代替类型参数（即 K 或 V）。例如，使用 OrderedPair&lt;K, V&gt; 的例子：</p>\n<pre><code>OrderedPair&lt;String, Box&lt;Integer&gt;&gt; p = new OrderedPair&lt;&gt;(&quot;primes&quot;, new Box&lt;Integer&gt;(...));\n</code></pre>"},{"title":"范型（九）：类型推断","date":"2016-11-30T06:22:37.000Z","_content":"\n\n类型推断是 Java 编译器的能力，通过查看每个方法调用和相应的声明以确定类型参数（或多个类型参数）使调用是适当的。推断算法确定参数的类型，并且如果可用的话，这个类型会赋值给结果或者返回。最终，推断算法尝试找出对所有参数都适用的最明确的类型。\n\n<!-- more -->\n\n为了说明这最后一点，在下面的例子中，类型推断确定传递给 pick 方法的参数的类型是 Serializable：\n\n    static <T> T pick(T a1, T a2) { return a2; }\n    Serializable s = pick(\"d\", new ArrayList<String>());\n\n#### 类型推断和范型方法\n\n[范型方法](http://docs.oracle.com/javase/tutorial/java/generics/methods.html)介绍的类型推断，这使你像调用普通方法一样调用一个范型方法，不需要指定尖括号间的类型。看下面的例子，[BoxDemo](http://docs.oracle.com/javase/tutorial/displayCode.html?code=http://docs.oracle.com/javase/tutorial/java/generics/examples/BoxDemo.java)，需要 [Box](http://docs.oracle.com/javase/tutorial/displayCode.html?code=http://docs.oracle.com/javase/tutorial/java/generics/examples/Box.java) 类。\n\n    public class BoxDemo {\n\n      public static <U> void addBox(U u,\n          java.util.List<Box<U>> boxes) {\n        Box<U> box = new Box<>();\n        box.set(u);\n        boxes.add(box);\n      }\n\n      public static <U> void outputBoxes(java.util.List<Box<U>> boxes) {\n        int counter = 0;\n        for (Box<U> box: boxes) {\n          U boxContents = box.get();\n          System.out.println(\"Box #\" + counter + \" contains [\" + boxContents.toString() + \"]\");\n          counter++;\n        }\n      }\n\n      public static void main(String[] args) {\n        java.util.ArrayList<Box<Integer>> listOfIntegerBoxes = new java.util.ArrayList<>();\n        BoxDemo.<Integer>addBox(Integer.valueOf(10), listOfIntegerBoxes);\n        BoxDemo.addBox(Integer.valueOf(20), listOfIntegerBoxes);\n        BoxDemo.addBox(Integer.valueOf(30), listOfIntegerBoxes);\n        BoxDemo.outputBoxes(listOfIntegerBoxes);\n      }\n    }\n\n下面是这个例子的输出：\n\n    Box #0 contains [10]\n    Box #1 contains [20]\n    Box #2 contains [30]\n\n范型方法 addBox 定义了一个命名为 U 的类型参数。通常，Java 编译器可以推断出范型方法调用的类型参数。因此，大多数场景下，不需要指定它们。例如，调用范型方法 addBox，你可以指定类型参数如下：\n\n    BoxDemo.**<Integer>**addBox(Integer.valueOf(10), listOfIntegerBoxes);\n\n或者，忽略类型指定，Java 编译器自动推断（从方法的参数）类型参数是 Integer：\n\n    BoxDemo.addBox(Integer.valueOf(20), listOfIntegerBoxes);\n\n#### 类型推断和实例化范型类\n\n可以用一个空的类型参数集（<>）代替调用范型类构造方法需要的类型参数，只要编译器可以从上下文类型参数。这对尖括号被通俗的叫做[钻石语法](http://docs.oracle.com/javase/tutorial/java/generics/types.html#diamond)。\n\n例如，看下面的变量声明：\n\n    Map<String, List<String>> myMap = new HashMap<String, List<String>>();\n\n可以用一个空类型参数集（<>）替换参数化类型：\n\n    Map<String, List<String>> myMap = new HashMap<>();\n\n注意，为了在范型类实例化过程中利用类型推断，必须使用钻石语法。在下面的例子中，编译器生成一个未检查的类型转换告警，因为 HashMap() 构造方法引用的是 HashMap 原始类型，而不是 Map<String, List<String>> 类型：\n\n    Map<String, List<String>> myMap = new HashMap(); // unchecked conversion warning\n\n#### 类型推断和范型类及非范型类的范型构造方法\n\n注意在范型类及非范型类中构造方法都可以是范型方法（或者说，声明自己的正式类型参数）。考虑下面的例子：\n\n    class MyClass<X> {\n      <T> MyClass(T t) {\n        // ...\n      }\n    }\n\n考虑下面 MyClass 类的实例化：\n\n    new MyClass<Integer>(\"\")\n\n这个表达式创建了参数化类型 MyClass<Integer> 的一个实例；表达式明确为范型类 MyClass<X> 的类型参数 X 指定了 Integer 类型。注意，这个范型类的构造方法包含一个正式的类型参数 T。编译器推断类型 String 为这个范型类构造方法的正式类型参数 T 的类型（因为这个构造方法的实参是一个 String 对象）。\n\nJava SE 7 之前的编译器可以推断范型构造方法的实际类型参数，就像范型方法。但是，Java SE 7 和之后的编译器可以推断被实例化范型类的实际类型参数，如果使用钻石语法。考虑下面的例子：\n\n    MyClass<Integer> myObject = new MyClass<>(\"\");\n\n在这个例子中，编译器推断出范型类 MyClass<X> 的正式类型参数 X 的类型为 Integer。推断出这个范型类构造方法的正式类型参数 T 的类型为 String。\n\n> 注：特别注意，推断算法只用调用参数、目标类型和可能一个明显的希望的返回类型来推断类型。推断算法不使用程序中后面的结果。\n\n#### 目标类型\n\nJava 编译器使用目标类型来推断一个范型方法调用的类型参数。一个表达式的目标类型是基于表达式 Java 编译器期望的数据类型。考虑 Collections.emptyList 方法，声明如下：\n\n    static <T> List<T> emptyList();\n\n考虑下面的赋值表达式：\n\n    List<String> listOne = Collections.emptyList();\n\n这个表达式期望一个 List<String> 的实例；这个数据类型就是目标类型。因为 emptyList 方法返回类型 List<T> 的值，编译器推断类型参数 T 必须是值 String。这在 Java SE 7 和 8 中都可行。可选择的，可以使用类型依据并指定 T 的值，如下：\n\n    List<String> listOne = Collections.<String>emptyList();\n\n然而，在这个上下文中这不是必须的。而在别的上下文中这是必须的。考虑下面的例子：\n\n    void processStringList(List<String> stringList) {\n      // process stringList\n    }\n\n假设你想用一个空列表调用方法 processStringList。在 Java SE 7 中，下面的表达式不会编译：\n\n    processStringList(Collections.emptyList());\n\nJava SE 7 编译器生成一个像下面一样的错误的信息：\n\n    List<Object> cannot be converted to List<String>\n\n编译器需要类型参数 T 的一个值，所以从 Object 开始。因此，Collections.emptyList 调用返回一个 List<Object> 的值，这跟 processStringList 方法不兼容。因此，在 Java SE 7 中，必须指定类型参数的值，如下：\n\n    processStringList(Collections.<String>emptyList());\n\n这在 Java SE 8 中不再是必须的。什么是目标类型的概念已经被延伸到包含方法参数，像方法 processStringList 的参数。在这个案例中，processStringList 需要一个类型为 List<String> 的参数。方法 Collections.emptyList 返回一个 List<T> 的值，所以使用目标类型 List<String>，编译器推断类型参数 T 的值为 String。因此，Java SE 8 中，下面的语句可以编译：\n\n    processStringList(Collections.emptyList());\n\n查看 [Lambda 表达式](http://docs.oracle.com/javase/tutorial/java/javaOO/lambdaexpressions.html) 中的 [目标类型](http://docs.oracle.com/javase/tutorial/java/javaOO/lambdaexpressions.html#target-typing) 获取更多信息。\n","source":"_posts/范型（九）：类型推断.md","raw":"title: 范型（九）：类型推断\ntags:\n  - Java\ncategories:\n  - 语言\n  - Java\ndate: 2016-11-30 14:22:37\n---\n\n\n类型推断是 Java 编译器的能力，通过查看每个方法调用和相应的声明以确定类型参数（或多个类型参数）使调用是适当的。推断算法确定参数的类型，并且如果可用的话，这个类型会赋值给结果或者返回。最终，推断算法尝试找出对所有参数都适用的最明确的类型。\n\n<!-- more -->\n\n为了说明这最后一点，在下面的例子中，类型推断确定传递给 pick 方法的参数的类型是 Serializable：\n\n    static <T> T pick(T a1, T a2) { return a2; }\n    Serializable s = pick(\"d\", new ArrayList<String>());\n\n#### 类型推断和范型方法\n\n[范型方法](http://docs.oracle.com/javase/tutorial/java/generics/methods.html)介绍的类型推断，这使你像调用普通方法一样调用一个范型方法，不需要指定尖括号间的类型。看下面的例子，[BoxDemo](http://docs.oracle.com/javase/tutorial/displayCode.html?code=http://docs.oracle.com/javase/tutorial/java/generics/examples/BoxDemo.java)，需要 [Box](http://docs.oracle.com/javase/tutorial/displayCode.html?code=http://docs.oracle.com/javase/tutorial/java/generics/examples/Box.java) 类。\n\n    public class BoxDemo {\n\n      public static <U> void addBox(U u,\n          java.util.List<Box<U>> boxes) {\n        Box<U> box = new Box<>();\n        box.set(u);\n        boxes.add(box);\n      }\n\n      public static <U> void outputBoxes(java.util.List<Box<U>> boxes) {\n        int counter = 0;\n        for (Box<U> box: boxes) {\n          U boxContents = box.get();\n          System.out.println(\"Box #\" + counter + \" contains [\" + boxContents.toString() + \"]\");\n          counter++;\n        }\n      }\n\n      public static void main(String[] args) {\n        java.util.ArrayList<Box<Integer>> listOfIntegerBoxes = new java.util.ArrayList<>();\n        BoxDemo.<Integer>addBox(Integer.valueOf(10), listOfIntegerBoxes);\n        BoxDemo.addBox(Integer.valueOf(20), listOfIntegerBoxes);\n        BoxDemo.addBox(Integer.valueOf(30), listOfIntegerBoxes);\n        BoxDemo.outputBoxes(listOfIntegerBoxes);\n      }\n    }\n\n下面是这个例子的输出：\n\n    Box #0 contains [10]\n    Box #1 contains [20]\n    Box #2 contains [30]\n\n范型方法 addBox 定义了一个命名为 U 的类型参数。通常，Java 编译器可以推断出范型方法调用的类型参数。因此，大多数场景下，不需要指定它们。例如，调用范型方法 addBox，你可以指定类型参数如下：\n\n    BoxDemo.**<Integer>**addBox(Integer.valueOf(10), listOfIntegerBoxes);\n\n或者，忽略类型指定，Java 编译器自动推断（从方法的参数）类型参数是 Integer：\n\n    BoxDemo.addBox(Integer.valueOf(20), listOfIntegerBoxes);\n\n#### 类型推断和实例化范型类\n\n可以用一个空的类型参数集（<>）代替调用范型类构造方法需要的类型参数，只要编译器可以从上下文类型参数。这对尖括号被通俗的叫做[钻石语法](http://docs.oracle.com/javase/tutorial/java/generics/types.html#diamond)。\n\n例如，看下面的变量声明：\n\n    Map<String, List<String>> myMap = new HashMap<String, List<String>>();\n\n可以用一个空类型参数集（<>）替换参数化类型：\n\n    Map<String, List<String>> myMap = new HashMap<>();\n\n注意，为了在范型类实例化过程中利用类型推断，必须使用钻石语法。在下面的例子中，编译器生成一个未检查的类型转换告警，因为 HashMap() 构造方法引用的是 HashMap 原始类型，而不是 Map<String, List<String>> 类型：\n\n    Map<String, List<String>> myMap = new HashMap(); // unchecked conversion warning\n\n#### 类型推断和范型类及非范型类的范型构造方法\n\n注意在范型类及非范型类中构造方法都可以是范型方法（或者说，声明自己的正式类型参数）。考虑下面的例子：\n\n    class MyClass<X> {\n      <T> MyClass(T t) {\n        // ...\n      }\n    }\n\n考虑下面 MyClass 类的实例化：\n\n    new MyClass<Integer>(\"\")\n\n这个表达式创建了参数化类型 MyClass<Integer> 的一个实例；表达式明确为范型类 MyClass<X> 的类型参数 X 指定了 Integer 类型。注意，这个范型类的构造方法包含一个正式的类型参数 T。编译器推断类型 String 为这个范型类构造方法的正式类型参数 T 的类型（因为这个构造方法的实参是一个 String 对象）。\n\nJava SE 7 之前的编译器可以推断范型构造方法的实际类型参数，就像范型方法。但是，Java SE 7 和之后的编译器可以推断被实例化范型类的实际类型参数，如果使用钻石语法。考虑下面的例子：\n\n    MyClass<Integer> myObject = new MyClass<>(\"\");\n\n在这个例子中，编译器推断出范型类 MyClass<X> 的正式类型参数 X 的类型为 Integer。推断出这个范型类构造方法的正式类型参数 T 的类型为 String。\n\n> 注：特别注意，推断算法只用调用参数、目标类型和可能一个明显的希望的返回类型来推断类型。推断算法不使用程序中后面的结果。\n\n#### 目标类型\n\nJava 编译器使用目标类型来推断一个范型方法调用的类型参数。一个表达式的目标类型是基于表达式 Java 编译器期望的数据类型。考虑 Collections.emptyList 方法，声明如下：\n\n    static <T> List<T> emptyList();\n\n考虑下面的赋值表达式：\n\n    List<String> listOne = Collections.emptyList();\n\n这个表达式期望一个 List<String> 的实例；这个数据类型就是目标类型。因为 emptyList 方法返回类型 List<T> 的值，编译器推断类型参数 T 必须是值 String。这在 Java SE 7 和 8 中都可行。可选择的，可以使用类型依据并指定 T 的值，如下：\n\n    List<String> listOne = Collections.<String>emptyList();\n\n然而，在这个上下文中这不是必须的。而在别的上下文中这是必须的。考虑下面的例子：\n\n    void processStringList(List<String> stringList) {\n      // process stringList\n    }\n\n假设你想用一个空列表调用方法 processStringList。在 Java SE 7 中，下面的表达式不会编译：\n\n    processStringList(Collections.emptyList());\n\nJava SE 7 编译器生成一个像下面一样的错误的信息：\n\n    List<Object> cannot be converted to List<String>\n\n编译器需要类型参数 T 的一个值，所以从 Object 开始。因此，Collections.emptyList 调用返回一个 List<Object> 的值，这跟 processStringList 方法不兼容。因此，在 Java SE 7 中，必须指定类型参数的值，如下：\n\n    processStringList(Collections.<String>emptyList());\n\n这在 Java SE 8 中不再是必须的。什么是目标类型的概念已经被延伸到包含方法参数，像方法 processStringList 的参数。在这个案例中，processStringList 需要一个类型为 List<String> 的参数。方法 Collections.emptyList 返回一个 List<T> 的值，所以使用目标类型 List<String>，编译器推断类型参数 T 的值为 String。因此，Java SE 8 中，下面的语句可以编译：\n\n    processStringList(Collections.emptyList());\n\n查看 [Lambda 表达式](http://docs.oracle.com/javase/tutorial/java/javaOO/lambdaexpressions.html) 中的 [目标类型](http://docs.oracle.com/javase/tutorial/java/javaOO/lambdaexpressions.html#target-typing) 获取更多信息。\n","slug":"范型（九）：类型推断","published":1,"updated":"2021-07-19T16:28:00.312Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphsu00i3itd311pp3sk2","content":"<p>类型推断是 Java 编译器的能力，通过查看每个方法调用和相应的声明以确定类型参数（或多个类型参数）使调用是适当的。推断算法确定参数的类型，并且如果可用的话，这个类型会赋值给结果或者返回。最终，推断算法尝试找出对所有参数都适用的最明确的类型。</p>\n<span id=\"more\"></span>\n\n<p>为了说明这最后一点，在下面的例子中，类型推断确定传递给 pick 方法的参数的类型是 Serializable：</p>\n<pre><code>static &lt;T&gt; T pick(T a1, T a2) &#123; return a2; &#125;\nSerializable s = pick(&quot;d&quot;, new ArrayList&lt;String&gt;());\n</code></pre>\n<h4 id=\"类型推断和范型方法\"><a href=\"#类型推断和范型方法\" class=\"headerlink\" title=\"类型推断和范型方法\"></a>类型推断和范型方法</h4><p><a href=\"http://docs.oracle.com/javase/tutorial/java/generics/methods.html\">范型方法</a>介绍的类型推断，这使你像调用普通方法一样调用一个范型方法，不需要指定尖括号间的类型。看下面的例子，<a href=\"http://docs.oracle.com/javase/tutorial/displayCode.html?code=http://docs.oracle.com/javase/tutorial/java/generics/examples/BoxDemo.java\">BoxDemo</a>，需要 <a href=\"http://docs.oracle.com/javase/tutorial/displayCode.html?code=http://docs.oracle.com/javase/tutorial/java/generics/examples/Box.java\">Box</a> 类。</p>\n<pre><code>public class BoxDemo &#123;\n\n  public static &lt;U&gt; void addBox(U u,\n      java.util.List&lt;Box&lt;U&gt;&gt; boxes) &#123;\n    Box&lt;U&gt; box = new Box&lt;&gt;();\n    box.set(u);\n    boxes.add(box);\n  &#125;\n\n  public static &lt;U&gt; void outputBoxes(java.util.List&lt;Box&lt;U&gt;&gt; boxes) &#123;\n    int counter = 0;\n    for (Box&lt;U&gt; box: boxes) &#123;\n      U boxContents = box.get();\n      System.out.println(&quot;Box #&quot; + counter + &quot; contains [&quot; + boxContents.toString() + &quot;]&quot;);\n      counter++;\n    &#125;\n  &#125;\n\n  public static void main(String[] args) &#123;\n    java.util.ArrayList&lt;Box&lt;Integer&gt;&gt; listOfIntegerBoxes = new java.util.ArrayList&lt;&gt;();\n    BoxDemo.&lt;Integer&gt;addBox(Integer.valueOf(10), listOfIntegerBoxes);\n    BoxDemo.addBox(Integer.valueOf(20), listOfIntegerBoxes);\n    BoxDemo.addBox(Integer.valueOf(30), listOfIntegerBoxes);\n    BoxDemo.outputBoxes(listOfIntegerBoxes);\n  &#125;\n&#125;\n</code></pre>\n<p>下面是这个例子的输出：</p>\n<pre><code>Box #0 contains [10]\nBox #1 contains [20]\nBox #2 contains [30]\n</code></pre>\n<p>范型方法 addBox 定义了一个命名为 U 的类型参数。通常，Java 编译器可以推断出范型方法调用的类型参数。因此，大多数场景下，不需要指定它们。例如，调用范型方法 addBox，你可以指定类型参数如下：</p>\n<pre><code>BoxDemo.**&lt;Integer&gt;**addBox(Integer.valueOf(10), listOfIntegerBoxes);\n</code></pre>\n<p>或者，忽略类型指定，Java 编译器自动推断（从方法的参数）类型参数是 Integer：</p>\n<pre><code>BoxDemo.addBox(Integer.valueOf(20), listOfIntegerBoxes);\n</code></pre>\n<h4 id=\"类型推断和实例化范型类\"><a href=\"#类型推断和实例化范型类\" class=\"headerlink\" title=\"类型推断和实例化范型类\"></a>类型推断和实例化范型类</h4><p>可以用一个空的类型参数集（&lt;&gt;）代替调用范型类构造方法需要的类型参数，只要编译器可以从上下文类型参数。这对尖括号被通俗的叫做<a href=\"http://docs.oracle.com/javase/tutorial/java/generics/types.html#diamond\">钻石语法</a>。</p>\n<p>例如，看下面的变量声明：</p>\n<pre><code>Map&lt;String, List&lt;String&gt;&gt; myMap = new HashMap&lt;String, List&lt;String&gt;&gt;();\n</code></pre>\n<p>可以用一个空类型参数集（&lt;&gt;）替换参数化类型：</p>\n<pre><code>Map&lt;String, List&lt;String&gt;&gt; myMap = new HashMap&lt;&gt;();\n</code></pre>\n<p>注意，为了在范型类实例化过程中利用类型推断，必须使用钻石语法。在下面的例子中，编译器生成一个未检查的类型转换告警，因为 HashMap() 构造方法引用的是 HashMap 原始类型，而不是 Map&lt;String, List<String>&gt; 类型：</p>\n<pre><code>Map&lt;String, List&lt;String&gt;&gt; myMap = new HashMap(); // unchecked conversion warning\n</code></pre>\n<h4 id=\"类型推断和范型类及非范型类的范型构造方法\"><a href=\"#类型推断和范型类及非范型类的范型构造方法\" class=\"headerlink\" title=\"类型推断和范型类及非范型类的范型构造方法\"></a>类型推断和范型类及非范型类的范型构造方法</h4><p>注意在范型类及非范型类中构造方法都可以是范型方法（或者说，声明自己的正式类型参数）。考虑下面的例子：</p>\n<pre><code>class MyClass&lt;X&gt; &#123;\n  &lt;T&gt; MyClass(T t) &#123;\n    // ...\n  &#125;\n&#125;\n</code></pre>\n<p>考虑下面 MyClass 类的实例化：</p>\n<pre><code>new MyClass&lt;Integer&gt;(&quot;&quot;)\n</code></pre>\n<p>这个表达式创建了参数化类型 MyClass<Integer> 的一个实例；表达式明确为范型类 MyClass<X> 的类型参数 X 指定了 Integer 类型。注意，这个范型类的构造方法包含一个正式的类型参数 T。编译器推断类型 String 为这个范型类构造方法的正式类型参数 T 的类型（因为这个构造方法的实参是一个 String 对象）。</p>\n<p>Java SE 7 之前的编译器可以推断范型构造方法的实际类型参数，就像范型方法。但是，Java SE 7 和之后的编译器可以推断被实例化范型类的实际类型参数，如果使用钻石语法。考虑下面的例子：</p>\n<pre><code>MyClass&lt;Integer&gt; myObject = new MyClass&lt;&gt;(&quot;&quot;);\n</code></pre>\n<p>在这个例子中，编译器推断出范型类 MyClass<X> 的正式类型参数 X 的类型为 Integer。推断出这个范型类构造方法的正式类型参数 T 的类型为 String。</p>\n<blockquote>\n<p>注：特别注意，推断算法只用调用参数、目标类型和可能一个明显的希望的返回类型来推断类型。推断算法不使用程序中后面的结果。</p>\n</blockquote>\n<h4 id=\"目标类型\"><a href=\"#目标类型\" class=\"headerlink\" title=\"目标类型\"></a>目标类型</h4><p>Java 编译器使用目标类型来推断一个范型方法调用的类型参数。一个表达式的目标类型是基于表达式 Java 编译器期望的数据类型。考虑 Collections.emptyList 方法，声明如下：</p>\n<pre><code>static &lt;T&gt; List&lt;T&gt; emptyList();\n</code></pre>\n<p>考虑下面的赋值表达式：</p>\n<pre><code>List&lt;String&gt; listOne = Collections.emptyList();\n</code></pre>\n<p>这个表达式期望一个 List<String> 的实例；这个数据类型就是目标类型。因为 emptyList 方法返回类型 List<T> 的值，编译器推断类型参数 T 必须是值 String。这在 Java SE 7 和 8 中都可行。可选择的，可以使用类型依据并指定 T 的值，如下：</p>\n<pre><code>List&lt;String&gt; listOne = Collections.&lt;String&gt;emptyList();\n</code></pre>\n<p>然而，在这个上下文中这不是必须的。而在别的上下文中这是必须的。考虑下面的例子：</p>\n<pre><code>void processStringList(List&lt;String&gt; stringList) &#123;\n  // process stringList\n&#125;\n</code></pre>\n<p>假设你想用一个空列表调用方法 processStringList。在 Java SE 7 中，下面的表达式不会编译：</p>\n<pre><code>processStringList(Collections.emptyList());\n</code></pre>\n<p>Java SE 7 编译器生成一个像下面一样的错误的信息：</p>\n<pre><code>List&lt;Object&gt; cannot be converted to List&lt;String&gt;\n</code></pre>\n<p>编译器需要类型参数 T 的一个值，所以从 Object 开始。因此，Collections.emptyList 调用返回一个 List<Object> 的值，这跟 processStringList 方法不兼容。因此，在 Java SE 7 中，必须指定类型参数的值，如下：</p>\n<pre><code>processStringList(Collections.&lt;String&gt;emptyList());\n</code></pre>\n<p>这在 Java SE 8 中不再是必须的。什么是目标类型的概念已经被延伸到包含方法参数，像方法 processStringList 的参数。在这个案例中，processStringList 需要一个类型为 List<String> 的参数。方法 Collections.emptyList 返回一个 List<T> 的值，所以使用目标类型 List<String>，编译器推断类型参数 T 的值为 String。因此，Java SE 8 中，下面的语句可以编译：</p>\n<pre><code>processStringList(Collections.emptyList());\n</code></pre>\n<p>查看 <a href=\"http://docs.oracle.com/javase/tutorial/java/javaOO/lambdaexpressions.html\">Lambda 表达式</a> 中的 <a href=\"http://docs.oracle.com/javase/tutorial/java/javaOO/lambdaexpressions.html#target-typing\">目标类型</a> 获取更多信息。</p>\n","site":{"data":{}},"excerpt":"<p>类型推断是 Java 编译器的能力，通过查看每个方法调用和相应的声明以确定类型参数（或多个类型参数）使调用是适当的。推断算法确定参数的类型，并且如果可用的话，这个类型会赋值给结果或者返回。最终，推断算法尝试找出对所有参数都适用的最明确的类型。</p>","more":"<p>为了说明这最后一点，在下面的例子中，类型推断确定传递给 pick 方法的参数的类型是 Serializable：</p>\n<pre><code>static &lt;T&gt; T pick(T a1, T a2) &#123; return a2; &#125;\nSerializable s = pick(&quot;d&quot;, new ArrayList&lt;String&gt;());\n</code></pre>\n<h4 id=\"类型推断和范型方法\"><a href=\"#类型推断和范型方法\" class=\"headerlink\" title=\"类型推断和范型方法\"></a>类型推断和范型方法</h4><p><a href=\"http://docs.oracle.com/javase/tutorial/java/generics/methods.html\">范型方法</a>介绍的类型推断，这使你像调用普通方法一样调用一个范型方法，不需要指定尖括号间的类型。看下面的例子，<a href=\"http://docs.oracle.com/javase/tutorial/displayCode.html?code=http://docs.oracle.com/javase/tutorial/java/generics/examples/BoxDemo.java\">BoxDemo</a>，需要 <a href=\"http://docs.oracle.com/javase/tutorial/displayCode.html?code=http://docs.oracle.com/javase/tutorial/java/generics/examples/Box.java\">Box</a> 类。</p>\n<pre><code>public class BoxDemo &#123;\n\n  public static &lt;U&gt; void addBox(U u,\n      java.util.List&lt;Box&lt;U&gt;&gt; boxes) &#123;\n    Box&lt;U&gt; box = new Box&lt;&gt;();\n    box.set(u);\n    boxes.add(box);\n  &#125;\n\n  public static &lt;U&gt; void outputBoxes(java.util.List&lt;Box&lt;U&gt;&gt; boxes) &#123;\n    int counter = 0;\n    for (Box&lt;U&gt; box: boxes) &#123;\n      U boxContents = box.get();\n      System.out.println(&quot;Box #&quot; + counter + &quot; contains [&quot; + boxContents.toString() + &quot;]&quot;);\n      counter++;\n    &#125;\n  &#125;\n\n  public static void main(String[] args) &#123;\n    java.util.ArrayList&lt;Box&lt;Integer&gt;&gt; listOfIntegerBoxes = new java.util.ArrayList&lt;&gt;();\n    BoxDemo.&lt;Integer&gt;addBox(Integer.valueOf(10), listOfIntegerBoxes);\n    BoxDemo.addBox(Integer.valueOf(20), listOfIntegerBoxes);\n    BoxDemo.addBox(Integer.valueOf(30), listOfIntegerBoxes);\n    BoxDemo.outputBoxes(listOfIntegerBoxes);\n  &#125;\n&#125;\n</code></pre>\n<p>下面是这个例子的输出：</p>\n<pre><code>Box #0 contains [10]\nBox #1 contains [20]\nBox #2 contains [30]\n</code></pre>\n<p>范型方法 addBox 定义了一个命名为 U 的类型参数。通常，Java 编译器可以推断出范型方法调用的类型参数。因此，大多数场景下，不需要指定它们。例如，调用范型方法 addBox，你可以指定类型参数如下：</p>\n<pre><code>BoxDemo.**&lt;Integer&gt;**addBox(Integer.valueOf(10), listOfIntegerBoxes);\n</code></pre>\n<p>或者，忽略类型指定，Java 编译器自动推断（从方法的参数）类型参数是 Integer：</p>\n<pre><code>BoxDemo.addBox(Integer.valueOf(20), listOfIntegerBoxes);\n</code></pre>\n<h4 id=\"类型推断和实例化范型类\"><a href=\"#类型推断和实例化范型类\" class=\"headerlink\" title=\"类型推断和实例化范型类\"></a>类型推断和实例化范型类</h4><p>可以用一个空的类型参数集（&lt;&gt;）代替调用范型类构造方法需要的类型参数，只要编译器可以从上下文类型参数。这对尖括号被通俗的叫做<a href=\"http://docs.oracle.com/javase/tutorial/java/generics/types.html#diamond\">钻石语法</a>。</p>\n<p>例如，看下面的变量声明：</p>\n<pre><code>Map&lt;String, List&lt;String&gt;&gt; myMap = new HashMap&lt;String, List&lt;String&gt;&gt;();\n</code></pre>\n<p>可以用一个空类型参数集（&lt;&gt;）替换参数化类型：</p>\n<pre><code>Map&lt;String, List&lt;String&gt;&gt; myMap = new HashMap&lt;&gt;();\n</code></pre>\n<p>注意，为了在范型类实例化过程中利用类型推断，必须使用钻石语法。在下面的例子中，编译器生成一个未检查的类型转换告警，因为 HashMap() 构造方法引用的是 HashMap 原始类型，而不是 Map&lt;String, List<String>&gt; 类型：</p>\n<pre><code>Map&lt;String, List&lt;String&gt;&gt; myMap = new HashMap(); // unchecked conversion warning\n</code></pre>\n<h4 id=\"类型推断和范型类及非范型类的范型构造方法\"><a href=\"#类型推断和范型类及非范型类的范型构造方法\" class=\"headerlink\" title=\"类型推断和范型类及非范型类的范型构造方法\"></a>类型推断和范型类及非范型类的范型构造方法</h4><p>注意在范型类及非范型类中构造方法都可以是范型方法（或者说，声明自己的正式类型参数）。考虑下面的例子：</p>\n<pre><code>class MyClass&lt;X&gt; &#123;\n  &lt;T&gt; MyClass(T t) &#123;\n    // ...\n  &#125;\n&#125;\n</code></pre>\n<p>考虑下面 MyClass 类的实例化：</p>\n<pre><code>new MyClass&lt;Integer&gt;(&quot;&quot;)\n</code></pre>\n<p>这个表达式创建了参数化类型 MyClass<Integer> 的一个实例；表达式明确为范型类 MyClass<X> 的类型参数 X 指定了 Integer 类型。注意，这个范型类的构造方法包含一个正式的类型参数 T。编译器推断类型 String 为这个范型类构造方法的正式类型参数 T 的类型（因为这个构造方法的实参是一个 String 对象）。</p>\n<p>Java SE 7 之前的编译器可以推断范型构造方法的实际类型参数，就像范型方法。但是，Java SE 7 和之后的编译器可以推断被实例化范型类的实际类型参数，如果使用钻石语法。考虑下面的例子：</p>\n<pre><code>MyClass&lt;Integer&gt; myObject = new MyClass&lt;&gt;(&quot;&quot;);\n</code></pre>\n<p>在这个例子中，编译器推断出范型类 MyClass<X> 的正式类型参数 X 的类型为 Integer。推断出这个范型类构造方法的正式类型参数 T 的类型为 String。</p>\n<blockquote>\n<p>注：特别注意，推断算法只用调用参数、目标类型和可能一个明显的希望的返回类型来推断类型。推断算法不使用程序中后面的结果。</p>\n</blockquote>\n<h4 id=\"目标类型\"><a href=\"#目标类型\" class=\"headerlink\" title=\"目标类型\"></a>目标类型</h4><p>Java 编译器使用目标类型来推断一个范型方法调用的类型参数。一个表达式的目标类型是基于表达式 Java 编译器期望的数据类型。考虑 Collections.emptyList 方法，声明如下：</p>\n<pre><code>static &lt;T&gt; List&lt;T&gt; emptyList();\n</code></pre>\n<p>考虑下面的赋值表达式：</p>\n<pre><code>List&lt;String&gt; listOne = Collections.emptyList();\n</code></pre>\n<p>这个表达式期望一个 List<String> 的实例；这个数据类型就是目标类型。因为 emptyList 方法返回类型 List<T> 的值，编译器推断类型参数 T 必须是值 String。这在 Java SE 7 和 8 中都可行。可选择的，可以使用类型依据并指定 T 的值，如下：</p>\n<pre><code>List&lt;String&gt; listOne = Collections.&lt;String&gt;emptyList();\n</code></pre>\n<p>然而，在这个上下文中这不是必须的。而在别的上下文中这是必须的。考虑下面的例子：</p>\n<pre><code>void processStringList(List&lt;String&gt; stringList) &#123;\n  // process stringList\n&#125;\n</code></pre>\n<p>假设你想用一个空列表调用方法 processStringList。在 Java SE 7 中，下面的表达式不会编译：</p>\n<pre><code>processStringList(Collections.emptyList());\n</code></pre>\n<p>Java SE 7 编译器生成一个像下面一样的错误的信息：</p>\n<pre><code>List&lt;Object&gt; cannot be converted to List&lt;String&gt;\n</code></pre>\n<p>编译器需要类型参数 T 的一个值，所以从 Object 开始。因此，Collections.emptyList 调用返回一个 List<Object> 的值，这跟 processStringList 方法不兼容。因此，在 Java SE 7 中，必须指定类型参数的值，如下：</p>\n<pre><code>processStringList(Collections.&lt;String&gt;emptyList());\n</code></pre>\n<p>这在 Java SE 8 中不再是必须的。什么是目标类型的概念已经被延伸到包含方法参数，像方法 processStringList 的参数。在这个案例中，processStringList 需要一个类型为 List<String> 的参数。方法 Collections.emptyList 返回一个 List<T> 的值，所以使用目标类型 List<String>，编译器推断类型参数 T 的值为 String。因此，Java SE 8 中，下面的语句可以编译：</p>\n<pre><code>processStringList(Collections.emptyList());\n</code></pre>\n<p>查看 <a href=\"http://docs.oracle.com/javase/tutorial/java/javaOO/lambdaexpressions.html\">Lambda 表达式</a> 中的 <a href=\"http://docs.oracle.com/javase/tutorial/java/javaOO/lambdaexpressions.html#target-typing\">目标类型</a> 获取更多信息。</p>"},{"title":"范型（二）：为什么使用范型？","date":"2016-11-19T13:39:50.000Z","_content":"\n概括地说，当定义类、接口和方法时，范型可以使类型（类和接口）成为参数。很像在方法声明中使用的更常见的形式参数，类型参数提供了一种方法，用于重新使用不同输入的相同代码。不同的是，输入到正式参数的是值，而输入到类型参数的是类型。\n\n<!-- more -->\n\n使用范型的代码比不使用范型的代码有很多好处：\n\n- 编译期更健壮的类型检查。\n\nJava编译器对范型代码使用强类型检查，如果代码违反类型安全则提示错误。解决编译期错误比解决运行时错误要简单，运行时错误很难被发现。\n\n- 消除了类型转换。\n\n下面没有使用范型的代码片段需要进行类型转换：\n\n    List list = new ArrayList();\n    list.add(\"hello\");\n    String s = (String) list.get(0);\n\n使用范型重写上面的代码则不不需要类型转换：\n\n    List<String> list = new ArrayList<String>();\n    list.add(\"hello\");\n    String s = list.get(0);   // no cast\n\n- 程序员可以实现范型算法。\n\n通过使用范型，程序员可以实现处理不同类型数据的、可定制的、类型安全且容易阅读的范型集合程序。\n","source":"_posts/范型（二）：为什么使用范型？.md","raw":"title: 范型（二）：为什么使用范型？\ntags:\n  - Java\ncategories:\n  - 语言\n  - Java\ndate: 2016-11-19 21:39:50\n---\n\n概括地说，当定义类、接口和方法时，范型可以使类型（类和接口）成为参数。很像在方法声明中使用的更常见的形式参数，类型参数提供了一种方法，用于重新使用不同输入的相同代码。不同的是，输入到正式参数的是值，而输入到类型参数的是类型。\n\n<!-- more -->\n\n使用范型的代码比不使用范型的代码有很多好处：\n\n- 编译期更健壮的类型检查。\n\nJava编译器对范型代码使用强类型检查，如果代码违反类型安全则提示错误。解决编译期错误比解决运行时错误要简单，运行时错误很难被发现。\n\n- 消除了类型转换。\n\n下面没有使用范型的代码片段需要进行类型转换：\n\n    List list = new ArrayList();\n    list.add(\"hello\");\n    String s = (String) list.get(0);\n\n使用范型重写上面的代码则不不需要类型转换：\n\n    List<String> list = new ArrayList<String>();\n    list.add(\"hello\");\n    String s = list.get(0);   // no cast\n\n- 程序员可以实现范型算法。\n\n通过使用范型，程序员可以实现处理不同类型数据的、可定制的、类型安全且容易阅读的范型集合程序。\n","slug":"范型（二）：为什么使用范型？","published":1,"updated":"2021-07-19T16:28:00.312Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphsw00i8itd34gdk87qc","content":"<p>概括地说，当定义类、接口和方法时，范型可以使类型（类和接口）成为参数。很像在方法声明中使用的更常见的形式参数，类型参数提供了一种方法，用于重新使用不同输入的相同代码。不同的是，输入到正式参数的是值，而输入到类型参数的是类型。</p>\n<span id=\"more\"></span>\n\n<p>使用范型的代码比不使用范型的代码有很多好处：</p>\n<ul>\n<li>编译期更健壮的类型检查。</li>\n</ul>\n<p>Java编译器对范型代码使用强类型检查，如果代码违反类型安全则提示错误。解决编译期错误比解决运行时错误要简单，运行时错误很难被发现。</p>\n<ul>\n<li>消除了类型转换。</li>\n</ul>\n<p>下面没有使用范型的代码片段需要进行类型转换：</p>\n<pre><code>List list = new ArrayList();\nlist.add(&quot;hello&quot;);\nString s = (String) list.get(0);\n</code></pre>\n<p>使用范型重写上面的代码则不不需要类型转换：</p>\n<pre><code>List&lt;String&gt; list = new ArrayList&lt;String&gt;();\nlist.add(&quot;hello&quot;);\nString s = list.get(0);   // no cast\n</code></pre>\n<ul>\n<li>程序员可以实现范型算法。</li>\n</ul>\n<p>通过使用范型，程序员可以实现处理不同类型数据的、可定制的、类型安全且容易阅读的范型集合程序。</p>\n","site":{"data":{}},"excerpt":"<p>概括地说，当定义类、接口和方法时，范型可以使类型（类和接口）成为参数。很像在方法声明中使用的更常见的形式参数，类型参数提供了一种方法，用于重新使用不同输入的相同代码。不同的是，输入到正式参数的是值，而输入到类型参数的是类型。</p>","more":"<p>使用范型的代码比不使用范型的代码有很多好处：</p>\n<ul>\n<li>编译期更健壮的类型检查。</li>\n</ul>\n<p>Java编译器对范型代码使用强类型检查，如果代码违反类型安全则提示错误。解决编译期错误比解决运行时错误要简单，运行时错误很难被发现。</p>\n<ul>\n<li>消除了类型转换。</li>\n</ul>\n<p>下面没有使用范型的代码片段需要进行类型转换：</p>\n<pre><code>List list = new ArrayList();\nlist.add(&quot;hello&quot;);\nString s = (String) list.get(0);\n</code></pre>\n<p>使用范型重写上面的代码则不不需要类型转换：</p>\n<pre><code>List&lt;String&gt; list = new ArrayList&lt;String&gt;();\nlist.add(&quot;hello&quot;);\nString s = list.get(0);   // no cast\n</code></pre>\n<ul>\n<li>程序员可以实现范型算法。</li>\n</ul>\n<p>通过使用范型，程序员可以实现处理不同类型数据的、可定制的、类型安全且容易阅读的范型集合程序。</p>"},{"title":"范型（五）：范型方法","date":"2016-11-26T15:30:28.000Z","_content":"\n泛型方法是引入他们自己的类型参数的方法。这类似于声明一个泛型类型，但类型参数的范围仅限于声明它的方法。静态和非静态泛型方法是允许的，以及泛型类构造函数。\n\n泛型方法的语法包括尖括号内的类型参数，且出现在方法的返回类型前。对于静态范型方法，类型参数必须出现在方法返回类型之前。\n\n<!-- more -->\n\nUtil 类包含一个 compare 范型方法，它比较两个 Pair 对象：\n\n    public class Util {\n      public static <K, V> boolean compare(Pair<K, V> p1, Pair<K, V> p2) {\n        return p1.getKey().equals(p2.getKey()) &&\n               p1.getValue().equals(p2.getValue());\n      }\n    }\n\n    public class Pair<K, V> {\n\n      private K key;\n      private V value;\n\n      public Pair(K key, V value) {\n        this.key = key;\n        this.value = value;\n      }\n\n      public void setKey(K key) { this.key = key; }\n      public void setValue(V value) { this.value = value; }\n      public K getKey()   { return key; }\n      public V getValue() { return value; }\n    }\n\n调用这个方法完整的语法应该是：\n\n    Pair<Integer, String> p1 = new Pair<>(1, \"apple\");\n    Pair<Integer, String> p2 = new Pair<>(2, \"pear\");\n    boolean same = Util.<**Integer**, **String**>compare(p1, p2);\n\n已经像粗体展示的明确地提供了类型。一般来说，这可以被排除在外，编译器将推断所需要的类型：\n\n    Pair<Integer, String> p1 = new Pair<>(1, \"apple\");\n    Pair<Integer, String> p2 = new Pair<>(2, \"pear\");\n    boolean same = Util.compare(p1, p2);\n\n这个特性，被称为类型推断，允许像普通方法一样调用范型方法，不用指定尖括号间的类型。这个主题在下面的章节中被进一步讨论，[类型推断](http://docs.oracle.com/javase/tutorial/java/generics/genTypeInference.html)。\n","source":"_posts/范型（五）：范型方法.md","raw":"title: 范型（五）：范型方法\ntags:\n  - Java\ncategories:\n  - 语言\n  - Java\ndate: 2016-11-26 23:30:28\n---\n\n泛型方法是引入他们自己的类型参数的方法。这类似于声明一个泛型类型，但类型参数的范围仅限于声明它的方法。静态和非静态泛型方法是允许的，以及泛型类构造函数。\n\n泛型方法的语法包括尖括号内的类型参数，且出现在方法的返回类型前。对于静态范型方法，类型参数必须出现在方法返回类型之前。\n\n<!-- more -->\n\nUtil 类包含一个 compare 范型方法，它比较两个 Pair 对象：\n\n    public class Util {\n      public static <K, V> boolean compare(Pair<K, V> p1, Pair<K, V> p2) {\n        return p1.getKey().equals(p2.getKey()) &&\n               p1.getValue().equals(p2.getValue());\n      }\n    }\n\n    public class Pair<K, V> {\n\n      private K key;\n      private V value;\n\n      public Pair(K key, V value) {\n        this.key = key;\n        this.value = value;\n      }\n\n      public void setKey(K key) { this.key = key; }\n      public void setValue(V value) { this.value = value; }\n      public K getKey()   { return key; }\n      public V getValue() { return value; }\n    }\n\n调用这个方法完整的语法应该是：\n\n    Pair<Integer, String> p1 = new Pair<>(1, \"apple\");\n    Pair<Integer, String> p2 = new Pair<>(2, \"pear\");\n    boolean same = Util.<**Integer**, **String**>compare(p1, p2);\n\n已经像粗体展示的明确地提供了类型。一般来说，这可以被排除在外，编译器将推断所需要的类型：\n\n    Pair<Integer, String> p1 = new Pair<>(1, \"apple\");\n    Pair<Integer, String> p2 = new Pair<>(2, \"pear\");\n    boolean same = Util.compare(p1, p2);\n\n这个特性，被称为类型推断，允许像普通方法一样调用范型方法，不用指定尖括号间的类型。这个主题在下面的章节中被进一步讨论，[类型推断](http://docs.oracle.com/javase/tutorial/java/generics/genTypeInference.html)。\n","slug":"范型（五）：范型方法","published":1,"updated":"2021-07-19T16:28:00.312Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphsx00ibitd3h5rk63w7","content":"<p>泛型方法是引入他们自己的类型参数的方法。这类似于声明一个泛型类型，但类型参数的范围仅限于声明它的方法。静态和非静态泛型方法是允许的，以及泛型类构造函数。</p>\n<p>泛型方法的语法包括尖括号内的类型参数，且出现在方法的返回类型前。对于静态范型方法，类型参数必须出现在方法返回类型之前。</p>\n<span id=\"more\"></span>\n\n<p>Util 类包含一个 compare 范型方法，它比较两个 Pair 对象：</p>\n<pre><code>public class Util &#123;\n  public static &lt;K, V&gt; boolean compare(Pair&lt;K, V&gt; p1, Pair&lt;K, V&gt; p2) &#123;\n    return p1.getKey().equals(p2.getKey()) &amp;&amp;\n           p1.getValue().equals(p2.getValue());\n  &#125;\n&#125;\n\npublic class Pair&lt;K, V&gt; &#123;\n\n  private K key;\n  private V value;\n\n  public Pair(K key, V value) &#123;\n    this.key = key;\n    this.value = value;\n  &#125;\n\n  public void setKey(K key) &#123; this.key = key; &#125;\n  public void setValue(V value) &#123; this.value = value; &#125;\n  public K getKey()   &#123; return key; &#125;\n  public V getValue() &#123; return value; &#125;\n&#125;\n</code></pre>\n<p>调用这个方法完整的语法应该是：</p>\n<pre><code>Pair&lt;Integer, String&gt; p1 = new Pair&lt;&gt;(1, &quot;apple&quot;);\nPair&lt;Integer, String&gt; p2 = new Pair&lt;&gt;(2, &quot;pear&quot;);\nboolean same = Util.&lt;**Integer**, **String**&gt;compare(p1, p2);\n</code></pre>\n<p>已经像粗体展示的明确地提供了类型。一般来说，这可以被排除在外，编译器将推断所需要的类型：</p>\n<pre><code>Pair&lt;Integer, String&gt; p1 = new Pair&lt;&gt;(1, &quot;apple&quot;);\nPair&lt;Integer, String&gt; p2 = new Pair&lt;&gt;(2, &quot;pear&quot;);\nboolean same = Util.compare(p1, p2);\n</code></pre>\n<p>这个特性，被称为类型推断，允许像普通方法一样调用范型方法，不用指定尖括号间的类型。这个主题在下面的章节中被进一步讨论，<a href=\"http://docs.oracle.com/javase/tutorial/java/generics/genTypeInference.html\">类型推断</a>。</p>\n","site":{"data":{}},"excerpt":"<p>泛型方法是引入他们自己的类型参数的方法。这类似于声明一个泛型类型，但类型参数的范围仅限于声明它的方法。静态和非静态泛型方法是允许的，以及泛型类构造函数。</p>\n<p>泛型方法的语法包括尖括号内的类型参数，且出现在方法的返回类型前。对于静态范型方法，类型参数必须出现在方法返回类型之前。</p>","more":"<p>Util 类包含一个 compare 范型方法，它比较两个 Pair 对象：</p>\n<pre><code>public class Util &#123;\n  public static &lt;K, V&gt; boolean compare(Pair&lt;K, V&gt; p1, Pair&lt;K, V&gt; p2) &#123;\n    return p1.getKey().equals(p2.getKey()) &amp;&amp;\n           p1.getValue().equals(p2.getValue());\n  &#125;\n&#125;\n\npublic class Pair&lt;K, V&gt; &#123;\n\n  private K key;\n  private V value;\n\n  public Pair(K key, V value) &#123;\n    this.key = key;\n    this.value = value;\n  &#125;\n\n  public void setKey(K key) &#123; this.key = key; &#125;\n  public void setValue(V value) &#123; this.value = value; &#125;\n  public K getKey()   &#123; return key; &#125;\n  public V getValue() &#123; return value; &#125;\n&#125;\n</code></pre>\n<p>调用这个方法完整的语法应该是：</p>\n<pre><code>Pair&lt;Integer, String&gt; p1 = new Pair&lt;&gt;(1, &quot;apple&quot;);\nPair&lt;Integer, String&gt; p2 = new Pair&lt;&gt;(2, &quot;pear&quot;);\nboolean same = Util.&lt;**Integer**, **String**&gt;compare(p1, p2);\n</code></pre>\n<p>已经像粗体展示的明确地提供了类型。一般来说，这可以被排除在外，编译器将推断所需要的类型：</p>\n<pre><code>Pair&lt;Integer, String&gt; p1 = new Pair&lt;&gt;(1, &quot;apple&quot;);\nPair&lt;Integer, String&gt; p2 = new Pair&lt;&gt;(2, &quot;pear&quot;);\nboolean same = Util.compare(p1, p2);\n</code></pre>\n<p>这个特性，被称为类型推断，允许像普通方法一样调用范型方法，不用指定尖括号间的类型。这个主题在下面的章节中被进一步讨论，<a href=\"http://docs.oracle.com/javase/tutorial/java/generics/genTypeInference.html\">类型推断</a>。</p>"},{"title":"范型（八）：范型、继承和子类型","date":"2016-11-27T14:47:18.000Z","_content":"\n\n像你已经知道的，赋值一种类型的一个对象给另外一种类型的一个对象可能是被支持的，只要这些类型是兼容的。例如，可以赋值一个 Integer 给一个 Object，因为 Object 是 Integer 的一个超类型：\n\n    Object someObject = new Object();\n    Integer someInteger = new Integer(10);\n    someObject = someInteger;   // OK\n\n<!-- more -->\n\n用面向对象的术语，这被叫做“是一个”的关系。因为 Integer *是一种* Object，赋值是被允许的。但是 Integer 也是一种 Number，所以下面的代码也是有效的：\n\n    public void someMethod(Number n) { /* ... */ }\n\n    someMethod(new Integer(10));   // OK\n    someMethod(new Double(10.1));   // OK\n\n相同的情况对范型也是成立的。可以执行一个范型类型调用，传入 Number 作为它的类型参数，如果参数与 Number 兼容则后面 add 的调用也是允许的：\n\n    Box<Number> box = new Box<Number>();\n    box.add(new Integer(10));   // OK\n    box.add(new Double(10.1));  // OK\n\n考虑下面的方法：\n\n    public void boxTest(Box<Number> n) { /* ... */ }\n\n它接受什么类型的参数？通过查看它的特征，可以看到它接受单个类型为 Box<Number> 类型的参数。但这是什么意思呢？像你可能期望的，允许传入 Box<Integer> 或 Box<Double> 吗？答案是“不”，因为 Box<Integer> 和 Box<Double> 不是 Box<Number> 的子类。\n\n当用范型编程时这是一个常见的误解，但它是需要学习的一个重要概念。\n\nBox<Integer> 不是 Box<Number> 的子类，即使 Integer 是 Number 的子类。\n![generics-subtypeRelationship](/uploads/20161127/generics-subtypeRelationship.gif)\n\n> 备注：给两个具体的类型 A 和 B（例如，Number 和 Integer），MyClass<A> 与 MyClass<B> 没有关系，不管 A 和 B 是否有关系。MyClass<A> 和 MyClass<B> 共同的父类是 Object。\n\n> 获取关于当类型参数有关系时如何创建两个范型类间子类关系的信息，参见[通配符和子类型](http://docs.oracle.com/javase/tutorial/java/generics/subtyping.html)。\n\n#### 范型类及子类型\n\n可以通过继承或实现创建范型类的子类。一个类或接口的类型参数和另外一个类或接口的类型参数之间的关系通过 extends 和 implements 从句被确定。\n\n用集合类作为例子，ArrayList<E> 实现了 List<E>，且 List<E> 继承 Collection<E>。因此 ArrayList<String> 是 List<String> 的子类型，List<String> 是 Collection<String> 的子类型。只要不改变类型参数，子类型关系在这些类型间是被保持的。\n\n集合层次例子：\n![generics-sampleHierarchy](/uploads/20161127/generics-sampleHierarchy.gif)\n\n现在，想象我们想定义我们拥有的列表接口 PayloadList，它使一个可选的范型类型 P 与每个元素产生联系。它的声明可能看起来像这样：\n\n    interface PayloadList<E,P> extends List<E> {\n      void setPayload(int index, P val);\n      ...\n    }\n\n下面 PayloadList 的参数化是 List<String> 的子类型：\n\n- PayloadList<String,String>\n- PayloadList<String,Integer>\n- PayloadList<String,Exception>\n\n![generics-payloadListHierarchy](/uploads/20161127/generics-payloadListHierarchy.gif)\n","source":"_posts/范型（八）：范型、继承和子类型.md","raw":"title: 范型（八）：范型、继承和子类型\ntags:\n  - Java\ncategories:\n  - 语言\n  - Java\ndate: 2016-11-27 22:47:18\n---\n\n\n像你已经知道的，赋值一种类型的一个对象给另外一种类型的一个对象可能是被支持的，只要这些类型是兼容的。例如，可以赋值一个 Integer 给一个 Object，因为 Object 是 Integer 的一个超类型：\n\n    Object someObject = new Object();\n    Integer someInteger = new Integer(10);\n    someObject = someInteger;   // OK\n\n<!-- more -->\n\n用面向对象的术语，这被叫做“是一个”的关系。因为 Integer *是一种* Object，赋值是被允许的。但是 Integer 也是一种 Number，所以下面的代码也是有效的：\n\n    public void someMethod(Number n) { /* ... */ }\n\n    someMethod(new Integer(10));   // OK\n    someMethod(new Double(10.1));   // OK\n\n相同的情况对范型也是成立的。可以执行一个范型类型调用，传入 Number 作为它的类型参数，如果参数与 Number 兼容则后面 add 的调用也是允许的：\n\n    Box<Number> box = new Box<Number>();\n    box.add(new Integer(10));   // OK\n    box.add(new Double(10.1));  // OK\n\n考虑下面的方法：\n\n    public void boxTest(Box<Number> n) { /* ... */ }\n\n它接受什么类型的参数？通过查看它的特征，可以看到它接受单个类型为 Box<Number> 类型的参数。但这是什么意思呢？像你可能期望的，允许传入 Box<Integer> 或 Box<Double> 吗？答案是“不”，因为 Box<Integer> 和 Box<Double> 不是 Box<Number> 的子类。\n\n当用范型编程时这是一个常见的误解，但它是需要学习的一个重要概念。\n\nBox<Integer> 不是 Box<Number> 的子类，即使 Integer 是 Number 的子类。\n![generics-subtypeRelationship](/uploads/20161127/generics-subtypeRelationship.gif)\n\n> 备注：给两个具体的类型 A 和 B（例如，Number 和 Integer），MyClass<A> 与 MyClass<B> 没有关系，不管 A 和 B 是否有关系。MyClass<A> 和 MyClass<B> 共同的父类是 Object。\n\n> 获取关于当类型参数有关系时如何创建两个范型类间子类关系的信息，参见[通配符和子类型](http://docs.oracle.com/javase/tutorial/java/generics/subtyping.html)。\n\n#### 范型类及子类型\n\n可以通过继承或实现创建范型类的子类。一个类或接口的类型参数和另外一个类或接口的类型参数之间的关系通过 extends 和 implements 从句被确定。\n\n用集合类作为例子，ArrayList<E> 实现了 List<E>，且 List<E> 继承 Collection<E>。因此 ArrayList<String> 是 List<String> 的子类型，List<String> 是 Collection<String> 的子类型。只要不改变类型参数，子类型关系在这些类型间是被保持的。\n\n集合层次例子：\n![generics-sampleHierarchy](/uploads/20161127/generics-sampleHierarchy.gif)\n\n现在，想象我们想定义我们拥有的列表接口 PayloadList，它使一个可选的范型类型 P 与每个元素产生联系。它的声明可能看起来像这样：\n\n    interface PayloadList<E,P> extends List<E> {\n      void setPayload(int index, P val);\n      ...\n    }\n\n下面 PayloadList 的参数化是 List<String> 的子类型：\n\n- PayloadList<String,String>\n- PayloadList<String,Integer>\n- PayloadList<String,Exception>\n\n![generics-payloadListHierarchy](/uploads/20161127/generics-payloadListHierarchy.gif)\n","slug":"范型（八）：范型、继承和子类型","published":1,"updated":"2021-07-19T16:28:00.316Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphsz00igitd3hmgk9f2h","content":"<p>像你已经知道的，赋值一种类型的一个对象给另外一种类型的一个对象可能是被支持的，只要这些类型是兼容的。例如，可以赋值一个 Integer 给一个 Object，因为 Object 是 Integer 的一个超类型：</p>\n<pre><code>Object someObject = new Object();\nInteger someInteger = new Integer(10);\nsomeObject = someInteger;   // OK\n</code></pre>\n<span id=\"more\"></span>\n\n<p>用面向对象的术语，这被叫做“是一个”的关系。因为 Integer <em>是一种</em> Object，赋值是被允许的。但是 Integer 也是一种 Number，所以下面的代码也是有效的：</p>\n<pre><code>public void someMethod(Number n) &#123; /* ... */ &#125;\n\nsomeMethod(new Integer(10));   // OK\nsomeMethod(new Double(10.1));   // OK\n</code></pre>\n<p>相同的情况对范型也是成立的。可以执行一个范型类型调用，传入 Number 作为它的类型参数，如果参数与 Number 兼容则后面 add 的调用也是允许的：</p>\n<pre><code>Box&lt;Number&gt; box = new Box&lt;Number&gt;();\nbox.add(new Integer(10));   // OK\nbox.add(new Double(10.1));  // OK\n</code></pre>\n<p>考虑下面的方法：</p>\n<pre><code>public void boxTest(Box&lt;Number&gt; n) &#123; /* ... */ &#125;\n</code></pre>\n<p>它接受什么类型的参数？通过查看它的特征，可以看到它接受单个类型为 Box<Number> 类型的参数。但这是什么意思呢？像你可能期望的，允许传入 Box<Integer> 或 Box<Double> 吗？答案是“不”，因为 Box<Integer> 和 Box<Double> 不是 Box<Number> 的子类。</p>\n<p>当用范型编程时这是一个常见的误解，但它是需要学习的一个重要概念。</p>\n<p>Box<Integer> 不是 Box<Number> 的子类，即使 Integer 是 Number 的子类。<br><img src=\"/uploads/20161127/generics-subtypeRelationship.gif\" alt=\"generics-subtypeRelationship\"></p>\n<blockquote>\n<p>备注：给两个具体的类型 A 和 B（例如，Number 和 Integer），MyClass<A> 与 MyClass<B> 没有关系，不管 A 和 B 是否有关系。MyClass<A> 和 MyClass<B> 共同的父类是 Object。</p>\n</blockquote>\n<blockquote>\n<p>获取关于当类型参数有关系时如何创建两个范型类间子类关系的信息，参见<a href=\"http://docs.oracle.com/javase/tutorial/java/generics/subtyping.html\">通配符和子类型</a>。</p>\n</blockquote>\n<h4 id=\"范型类及子类型\"><a href=\"#范型类及子类型\" class=\"headerlink\" title=\"范型类及子类型\"></a>范型类及子类型</h4><p>可以通过继承或实现创建范型类的子类。一个类或接口的类型参数和另外一个类或接口的类型参数之间的关系通过 extends 和 implements 从句被确定。</p>\n<p>用集合类作为例子，ArrayList<E> 实现了 List<E>，且 List<E> 继承 Collection<E>。因此 ArrayList<String> 是 List<String> 的子类型，List<String> 是 Collection<String> 的子类型。只要不改变类型参数，子类型关系在这些类型间是被保持的。</p>\n<p>集合层次例子：<br><img src=\"/uploads/20161127/generics-sampleHierarchy.gif\" alt=\"generics-sampleHierarchy\"></p>\n<p>现在，想象我们想定义我们拥有的列表接口 PayloadList，它使一个可选的范型类型 P 与每个元素产生联系。它的声明可能看起来像这样：</p>\n<pre><code>interface PayloadList&lt;E,P&gt; extends List&lt;E&gt; &#123;\n  void setPayload(int index, P val);\n  ...\n&#125;\n</code></pre>\n<p>下面 PayloadList 的参数化是 List<String> 的子类型：</p>\n<ul>\n<li>PayloadList&lt;String,String&gt;</li>\n<li>PayloadList&lt;String,Integer&gt;</li>\n<li>PayloadList&lt;String,Exception&gt;</li>\n</ul>\n<p><img src=\"/uploads/20161127/generics-payloadListHierarchy.gif\" alt=\"generics-payloadListHierarchy\"></p>\n","site":{"data":{}},"excerpt":"<p>像你已经知道的，赋值一种类型的一个对象给另外一种类型的一个对象可能是被支持的，只要这些类型是兼容的。例如，可以赋值一个 Integer 给一个 Object，因为 Object 是 Integer 的一个超类型：</p>\n<pre><code>Object someObject = new Object();\nInteger someInteger = new Integer(10);\nsomeObject = someInteger;   // OK\n</code></pre>","more":"<p>用面向对象的术语，这被叫做“是一个”的关系。因为 Integer <em>是一种</em> Object，赋值是被允许的。但是 Integer 也是一种 Number，所以下面的代码也是有效的：</p>\n<pre><code>public void someMethod(Number n) &#123; /* ... */ &#125;\n\nsomeMethod(new Integer(10));   // OK\nsomeMethod(new Double(10.1));   // OK\n</code></pre>\n<p>相同的情况对范型也是成立的。可以执行一个范型类型调用，传入 Number 作为它的类型参数，如果参数与 Number 兼容则后面 add 的调用也是允许的：</p>\n<pre><code>Box&lt;Number&gt; box = new Box&lt;Number&gt;();\nbox.add(new Integer(10));   // OK\nbox.add(new Double(10.1));  // OK\n</code></pre>\n<p>考虑下面的方法：</p>\n<pre><code>public void boxTest(Box&lt;Number&gt; n) &#123; /* ... */ &#125;\n</code></pre>\n<p>它接受什么类型的参数？通过查看它的特征，可以看到它接受单个类型为 Box<Number> 类型的参数。但这是什么意思呢？像你可能期望的，允许传入 Box<Integer> 或 Box<Double> 吗？答案是“不”，因为 Box<Integer> 和 Box<Double> 不是 Box<Number> 的子类。</p>\n<p>当用范型编程时这是一个常见的误解，但它是需要学习的一个重要概念。</p>\n<p>Box<Integer> 不是 Box<Number> 的子类，即使 Integer 是 Number 的子类。<br><img src=\"/uploads/20161127/generics-subtypeRelationship.gif\" alt=\"generics-subtypeRelationship\"></p>\n<blockquote>\n<p>备注：给两个具体的类型 A 和 B（例如，Number 和 Integer），MyClass<A> 与 MyClass<B> 没有关系，不管 A 和 B 是否有关系。MyClass<A> 和 MyClass<B> 共同的父类是 Object。</p>\n</blockquote>\n<blockquote>\n<p>获取关于当类型参数有关系时如何创建两个范型类间子类关系的信息，参见<a href=\"http://docs.oracle.com/javase/tutorial/java/generics/subtyping.html\">通配符和子类型</a>。</p>\n</blockquote>\n<h4 id=\"范型类及子类型\"><a href=\"#范型类及子类型\" class=\"headerlink\" title=\"范型类及子类型\"></a>范型类及子类型</h4><p>可以通过继承或实现创建范型类的子类。一个类或接口的类型参数和另外一个类或接口的类型参数之间的关系通过 extends 和 implements 从句被确定。</p>\n<p>用集合类作为例子，ArrayList<E> 实现了 List<E>，且 List<E> 继承 Collection<E>。因此 ArrayList<String> 是 List<String> 的子类型，List<String> 是 Collection<String> 的子类型。只要不改变类型参数，子类型关系在这些类型间是被保持的。</p>\n<p>集合层次例子：<br><img src=\"/uploads/20161127/generics-sampleHierarchy.gif\" alt=\"generics-sampleHierarchy\"></p>\n<p>现在，想象我们想定义我们拥有的列表接口 PayloadList，它使一个可选的范型类型 P 与每个元素产生联系。它的声明可能看起来像这样：</p>\n<pre><code>interface PayloadList&lt;E,P&gt; extends List&lt;E&gt; &#123;\n  void setPayload(int index, P val);\n  ...\n&#125;\n</code></pre>\n<p>下面 PayloadList 的参数化是 List<String> 的子类型：</p>\n<ul>\n<li>PayloadList&lt;String,String&gt;</li>\n<li>PayloadList&lt;String,Integer&gt;</li>\n<li>PayloadList&lt;String,Exception&gt;</li>\n</ul>\n<p><img src=\"/uploads/20161127/generics-payloadListHierarchy.gif\" alt=\"generics-payloadListHierarchy\"></p>"},{"title":"范型（六）：限定类型参数","date":"2016-11-26T17:00:01.000Z","_content":"\n有时，你想限制可以在参数化类型中作为类型参数使用的类型。例如，一个操作数字的方法可能只想接受 Number 或它的子类的实例。这就是限定类型参数。\n\n<!-- more -->\n\n声明一个限定类型参数，列出类型参数的名称，后面跟着 extends 关键字，跟着它的父边界，在这个例子中时 Number。请注意，在这个上下文中，extends 用来作为一般含义表示“extends”（作为类中使用）或“implements”（作为接口中使用）。\n\n    public class Box<T> {\n\n      private T t;          \n\n      public void set(T t) {\n        this.t = t;\n      }\n\n      public T get() {\n        return t;\n      }\n\n      public <U extends Number> void inspect(U u){\n        System.out.println(\"T: \" + t.getClass().getName());\n        System.out.println(\"U: \" + u.getClass().getName());\n      }\n\n      public static void main(String[] args) {\n        Box<Integer> integerBox = new Box<Integer>();\n        integerBox.set(new Integer(10));\n        integerBox.inspect(\"some text\"); // error: this is still String!\n      }\n    }\n\n通过修改范型方法包含限定类型参数，现在编译将失败，因为调用 inspect 仍然包含一个 String：\n\n    Box.java:21: <U>inspect(U) in Box<java.lang.Integer> cannot\n      be applied to (java.lang.String)\n                            integerBox.inspect(\"10\");\n                                      ^\n    1 error\n\n除了可以限定用来实例化范型类型的类型外，限定类型参数允许调用定义在限定中的方法：\n\n    public class NaturalNumber<T extends Integer> {\n\n      private T n;\n\n      public NaturalNumber(T n)  { this.n = n; }\n\n      public boolean isEven() {\n        return n.intValue() % 2 == 0;\n      }\n\n      // ...\n    }\n\nisEven 方法通过 n 调用的 intValue 方法在 Integer 中被定义。\n\n#### 多重限定\n\n前面的例子展示了单个限定的类型参数的用法，但是类型参数可以有多重限定：\n\n    <T extends B1 & B2 & B3>\n\n多重限定的参数变量是限定列表中所有类型的子类型。如果限定中的一个是一个类，必须第一个指明它。例如：\n\n    Class A { /* ... */ }\n    interface B { /* ... */ }\n    interface C { /* ... */ }\n\n    class D <T extends A & B & C> { /* ... */ }\n\n如果限定 A 不是第一个被指明，你将得到一个编译期错误：\n\n    class D <T extends B & A & C> { /* ... */ }  // compile-time error\n","source":"_posts/范型（六）：限定类型参数.md","raw":"title: 范型（六）：限定类型参数\ntags:\n  - Java\ncategories:\n  - 语言\n  - Java\ndate: 2016-11-27 01:00:01\n---\n\n有时，你想限制可以在参数化类型中作为类型参数使用的类型。例如，一个操作数字的方法可能只想接受 Number 或它的子类的实例。这就是限定类型参数。\n\n<!-- more -->\n\n声明一个限定类型参数，列出类型参数的名称，后面跟着 extends 关键字，跟着它的父边界，在这个例子中时 Number。请注意，在这个上下文中，extends 用来作为一般含义表示“extends”（作为类中使用）或“implements”（作为接口中使用）。\n\n    public class Box<T> {\n\n      private T t;          \n\n      public void set(T t) {\n        this.t = t;\n      }\n\n      public T get() {\n        return t;\n      }\n\n      public <U extends Number> void inspect(U u){\n        System.out.println(\"T: \" + t.getClass().getName());\n        System.out.println(\"U: \" + u.getClass().getName());\n      }\n\n      public static void main(String[] args) {\n        Box<Integer> integerBox = new Box<Integer>();\n        integerBox.set(new Integer(10));\n        integerBox.inspect(\"some text\"); // error: this is still String!\n      }\n    }\n\n通过修改范型方法包含限定类型参数，现在编译将失败，因为调用 inspect 仍然包含一个 String：\n\n    Box.java:21: <U>inspect(U) in Box<java.lang.Integer> cannot\n      be applied to (java.lang.String)\n                            integerBox.inspect(\"10\");\n                                      ^\n    1 error\n\n除了可以限定用来实例化范型类型的类型外，限定类型参数允许调用定义在限定中的方法：\n\n    public class NaturalNumber<T extends Integer> {\n\n      private T n;\n\n      public NaturalNumber(T n)  { this.n = n; }\n\n      public boolean isEven() {\n        return n.intValue() % 2 == 0;\n      }\n\n      // ...\n    }\n\nisEven 方法通过 n 调用的 intValue 方法在 Integer 中被定义。\n\n#### 多重限定\n\n前面的例子展示了单个限定的类型参数的用法，但是类型参数可以有多重限定：\n\n    <T extends B1 & B2 & B3>\n\n多重限定的参数变量是限定列表中所有类型的子类型。如果限定中的一个是一个类，必须第一个指明它。例如：\n\n    Class A { /* ... */ }\n    interface B { /* ... */ }\n    interface C { /* ... */ }\n\n    class D <T extends A & B & C> { /* ... */ }\n\n如果限定 A 不是第一个被指明，你将得到一个编译期错误：\n\n    class D <T extends B & A & C> { /* ... */ }  // compile-time error\n","slug":"范型（六）：限定类型参数","published":1,"updated":"2021-07-19T16:28:00.316Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmpht100ijitd32lnygdm6","content":"<p>有时，你想限制可以在参数化类型中作为类型参数使用的类型。例如，一个操作数字的方法可能只想接受 Number 或它的子类的实例。这就是限定类型参数。</p>\n<span id=\"more\"></span>\n\n<p>声明一个限定类型参数，列出类型参数的名称，后面跟着 extends 关键字，跟着它的父边界，在这个例子中时 Number。请注意，在这个上下文中，extends 用来作为一般含义表示“extends”（作为类中使用）或“implements”（作为接口中使用）。</p>\n<pre><code>public class Box&lt;T&gt; &#123;\n\n  private T t;          \n\n  public void set(T t) &#123;\n    this.t = t;\n  &#125;\n\n  public T get() &#123;\n    return t;\n  &#125;\n\n  public &lt;U extends Number&gt; void inspect(U u)&#123;\n    System.out.println(&quot;T: &quot; + t.getClass().getName());\n    System.out.println(&quot;U: &quot; + u.getClass().getName());\n  &#125;\n\n  public static void main(String[] args) &#123;\n    Box&lt;Integer&gt; integerBox = new Box&lt;Integer&gt;();\n    integerBox.set(new Integer(10));\n    integerBox.inspect(&quot;some text&quot;); // error: this is still String!\n  &#125;\n&#125;\n</code></pre>\n<p>通过修改范型方法包含限定类型参数，现在编译将失败，因为调用 inspect 仍然包含一个 String：</p>\n<pre><code>Box.java:21: &lt;U&gt;inspect(U) in Box&lt;java.lang.Integer&gt; cannot\n  be applied to (java.lang.String)\n                        integerBox.inspect(&quot;10&quot;);\n                                  ^\n1 error\n</code></pre>\n<p>除了可以限定用来实例化范型类型的类型外，限定类型参数允许调用定义在限定中的方法：</p>\n<pre><code>public class NaturalNumber&lt;T extends Integer&gt; &#123;\n\n  private T n;\n\n  public NaturalNumber(T n)  &#123; this.n = n; &#125;\n\n  public boolean isEven() &#123;\n    return n.intValue() % 2 == 0;\n  &#125;\n\n  // ...\n&#125;\n</code></pre>\n<p>isEven 方法通过 n 调用的 intValue 方法在 Integer 中被定义。</p>\n<h4 id=\"多重限定\"><a href=\"#多重限定\" class=\"headerlink\" title=\"多重限定\"></a>多重限定</h4><p>前面的例子展示了单个限定的类型参数的用法，但是类型参数可以有多重限定：</p>\n<pre><code>&lt;T extends B1 &amp; B2 &amp; B3&gt;\n</code></pre>\n<p>多重限定的参数变量是限定列表中所有类型的子类型。如果限定中的一个是一个类，必须第一个指明它。例如：</p>\n<pre><code>Class A &#123; /* ... */ &#125;\ninterface B &#123; /* ... */ &#125;\ninterface C &#123; /* ... */ &#125;\n\nclass D &lt;T extends A &amp; B &amp; C&gt; &#123; /* ... */ &#125;\n</code></pre>\n<p>如果限定 A 不是第一个被指明，你将得到一个编译期错误：</p>\n<pre><code>class D &lt;T extends B &amp; A &amp; C&gt; &#123; /* ... */ &#125;  // compile-time error\n</code></pre>\n","site":{"data":{}},"excerpt":"<p>有时，你想限制可以在参数化类型中作为类型参数使用的类型。例如，一个操作数字的方法可能只想接受 Number 或它的子类的实例。这就是限定类型参数。</p>","more":"<p>声明一个限定类型参数，列出类型参数的名称，后面跟着 extends 关键字，跟着它的父边界，在这个例子中时 Number。请注意，在这个上下文中，extends 用来作为一般含义表示“extends”（作为类中使用）或“implements”（作为接口中使用）。</p>\n<pre><code>public class Box&lt;T&gt; &#123;\n\n  private T t;          \n\n  public void set(T t) &#123;\n    this.t = t;\n  &#125;\n\n  public T get() &#123;\n    return t;\n  &#125;\n\n  public &lt;U extends Number&gt; void inspect(U u)&#123;\n    System.out.println(&quot;T: &quot; + t.getClass().getName());\n    System.out.println(&quot;U: &quot; + u.getClass().getName());\n  &#125;\n\n  public static void main(String[] args) &#123;\n    Box&lt;Integer&gt; integerBox = new Box&lt;Integer&gt;();\n    integerBox.set(new Integer(10));\n    integerBox.inspect(&quot;some text&quot;); // error: this is still String!\n  &#125;\n&#125;\n</code></pre>\n<p>通过修改范型方法包含限定类型参数，现在编译将失败，因为调用 inspect 仍然包含一个 String：</p>\n<pre><code>Box.java:21: &lt;U&gt;inspect(U) in Box&lt;java.lang.Integer&gt; cannot\n  be applied to (java.lang.String)\n                        integerBox.inspect(&quot;10&quot;);\n                                  ^\n1 error\n</code></pre>\n<p>除了可以限定用来实例化范型类型的类型外，限定类型参数允许调用定义在限定中的方法：</p>\n<pre><code>public class NaturalNumber&lt;T extends Integer&gt; &#123;\n\n  private T n;\n\n  public NaturalNumber(T n)  &#123; this.n = n; &#125;\n\n  public boolean isEven() &#123;\n    return n.intValue() % 2 == 0;\n  &#125;\n\n  // ...\n&#125;\n</code></pre>\n<p>isEven 方法通过 n 调用的 intValue 方法在 Integer 中被定义。</p>\n<h4 id=\"多重限定\"><a href=\"#多重限定\" class=\"headerlink\" title=\"多重限定\"></a>多重限定</h4><p>前面的例子展示了单个限定的类型参数的用法，但是类型参数可以有多重限定：</p>\n<pre><code>&lt;T extends B1 &amp; B2 &amp; B3&gt;\n</code></pre>\n<p>多重限定的参数变量是限定列表中所有类型的子类型。如果限定中的一个是一个类，必须第一个指明它。例如：</p>\n<pre><code>Class A &#123; /* ... */ &#125;\ninterface B &#123; /* ... */ &#125;\ninterface C &#123; /* ... */ &#125;\n\nclass D &lt;T extends A &amp; B &amp; C&gt; &#123; /* ... */ &#125;\n</code></pre>\n<p>如果限定 A 不是第一个被指明，你将得到一个编译期错误：</p>\n<pre><code>class D &lt;T extends B &amp; A &amp; C&gt; &#123; /* ... */ &#125;  // compile-time error\n</code></pre>"},{"title":"范型（十一）：类型擦除","date":"2016-12-14T05:53:00.000Z","_content":"\n\nJava 语言引入范型是为了在编译期提供严谨的类型检查和支持范型编程。为了实现范型，Java 编译器应用类型擦除：\n\n- 替换所有范型类型中的类型参数，用它们的边界或者 Object（如果类型参数是无界的）。因此，生成的字节码只包含普通的类、接口和方法。\n- 如果为了保护类型安全不要则插入类型转换。\n- 生成桥接方法来保护继承范型类型中的多态。\n\n<!-- more -->\n\n类型擦除确保对于参数化类型没有新类被创建；因此，范型不会引起运行时消耗。\n\n### 范型类型的擦除\n\n在类型擦除的过程中，Java 编译器擦除所有类型参数，如果类型参数是有界的则用它的第一边界替换每个类型参数，或者用 Object替换，如果类型参数是无界的。\n\n考虑下面的范型类，展示了单向列表中的一个节点：\n\n    public class Node<T> {\n\n      private T data;\n      private Node<T> next;\n\n      public Node(T data, Node<T> next) }\n        this.data = data;\n        this.next = next;\n      }\n\n      public T getData() { return data; }\n      // ...\n    }\n\n因为类型参数 T 是无界的，Java 编译器用 Object 替换它：\n\n    public class Node {\n\n      private Object data;\n      private Node next;\n\n      public Node(Object data, Node next) {\n        this.data = data;\n        this.next = next;\n      }\n\n      public Object getData() { return data; }\n      // ...\n    }\n\n在下面的例子中，范型类 Node 使用一个有界类型参数：\n\n    public class Node<T extends Comparable<T>> {\n\n      private T data;\n      private Node<T> next;\n\n      public Node(T data, Node<T> next) {\n        this.data = data;\n        this.next = next;\n      }\n\n      public T getData() { return data; }\n      // ...\n    }\n\nJava 编译器用第一边界类 Comparable 替换有界类型参数 T：\n\n    public class Node {\n\n      private Comparable data;\n      private Node next;\n\n      public Node(Comparable data, Node next) {\n        this.data = data;\n        this.next = next;\n      }\n\n      public Comparable getData() { return data; }\n      // ...\n    }\n\n### 范型方法的擦除\n\nJava 编译器也擦除范型方法参数中的类型参数。考虑下面的范型方法：\n\n    // Counts the number of occurrences of elem in anArray.\n    //\n    public static <T> int count(T[] anArray, T elem) {\n      int cnt = 0;\n      for (T e : anArray)\n        if (e.equals(elem))\n          ++cnt;\n        return cnt;\n    }\n\n因为 T 是无界的，Java 编译器用 Object 替换它：\n\n    public static int count(Object[] anArray, Object elem) {\n      int cnt = 0;\n      for (Object e : anArray)\n        if (e.equals(elem))\n            ++cnt;\n        return cnt;\n    }\n\n假设定义了下面的类：\n\n    class Shape { /* ... */ }\n    class Circle extends Shape { /* ... */ }\n    class Rectangle extends Shape { /* ... */ }\n\n可以写一个范型方法来画不同的形状：\n\n    public static <T extends Shape> void draw(T shape) { /* ... */ }\n\nJava 编译器用 Shape 替换 T：\n\n    public static void draw(Shape shape) { /* ... */ }\n\n### 类型擦除和桥接方法的作用\n\n有时类型擦除会引起一种不可预期的情况。下面的例子展示了这种情况是如何发生的。这个例子（在[桥接方法](http://docs.oracle.com/javase/tutorial/java/generics/bridgeMethods.html#bridgeMethods)中描述的）展示了编译器如何创建一个作为类型擦除过程中一部分的合成方法（叫做桥接方法）。\n\n给出下面两个类：\n\n    public class Node<T> {\n\n      public T data;\n\n      public Node(T data) { this.data = data; }\n\n      public void setData(T data) {\n        System.out.println(\"Node.setData\");\n        this.data = data;\n      }\n    }\n\n    public class MyNode extends Node<Integer> {\n      public MyNode(Integer data) { super(data); }\n\n      public void setData(Integer data) {\n        System.out.println(\"MyNode.setData\");\n        super.setData(data);\n      }\n    }\n\n考虑下面的代码：\n\n    MyNode mn = new MyNode(5);\n    Node n = mn;            // A raw type - compiler throws an unchecked warning\n    n.setData(\"Hello\");     \n    Integer x = mn.data;    // Causes a ClassCastException to be thrown.\n\n类型擦除后，代码变为：\n\n    MyNode mn = new MyNode(5);\n    Node n = (MyNode)mn;         // A raw type - compiler throws an unchecked warning\n    n.setData(\"Hello\");\n    Integer x = (String)mn.data; // Causes a ClassCastException to be thrown.\n\n代码被执行时会发生这些：\n\n- n.setData(\"Hello\"); 引起 MyNode 类对象的 setData(Object) 方法被执行。（MyNode 类从 Node 继承 setData(Object)。）\n- setData(Object) 的方法体中，n 指向的对象的 data 属性被赋值了一个 String。\n- 同一个对象的 data 属性（mn 指向的对象）可以被访问且希望是一个 integer（因为 mn 是 MyNode 的对象，MyNode 是 Node<Integer>）。\n- 尝试赋值一个 String 给一个 Integer 引起一个 ClassCastException（来自被 Java 编译器在赋值中插入的类型转化）。\n\n#### 桥接方法\n\n当编译一个继承参数化类或实现参数化接口的类或接口时，编译器可能需要创建一个作为类型擦除过程中一部分的合成方法（叫做桥接方法）。通常不需要担心桥接方法，但在跟踪栈中出现时可能会使你困惑。\n\n类型擦除之后，Node 和 MyNode 类变为：\n\n    public class Node {\n\n      public Object data;\n\n      public Node(Object data) { this.data = data; }\n\n      public void setData(Object data) {\n        System.out.println(\"Node.setData\");\n        this.data = data;\n      }\n    }\n\n    public class MyNode extends Node {\n\n      public MyNode(Integer data) { super(data); }\n\n      public void setData(Integer data) {\n        System.out.println(\"MyNode.setData\");\n        super.setData(data);\n      }\n    }\n\n类型擦除之后，方法特征并不匹配。Node 方法变为 setData(Object)，MyNode 方法变为 setData(Integer)。因此，MyNode setData 方法不是重载 Node setData 方法。\n\n为了解决这个问题，并保持类型擦除后范型类型的[多态性](http://docs.oracle.com/javase/tutorial/java/IandI/polymorphism.html)，Java 编译器生成一个桥接方法来保证子类型化像期望的一样工作。对于 MyNode 类，编译器为 setData 生成下面的桥接方法：\n\n    class MyNode extends Node {\n\n      // Bridge method generated by the compiler\n      //\n      public void setData(Object data) {\n        setData((Integer) data);\n      }\n\n      public void setData(Integer data) {\n        System.out.println(\"MyNode.setData\");\n        super.setData(data);\n      }\n\n      // ...\n    }\n\n像你看到的，桥接方法与 Node 类的 setData 方法类型擦除后有相同的方法特征，代表原来的 setData 方法。\n\n### 非范型具体化类型\n\n[类型擦除](http://docs.oracle.com/javase/tutorial/java/generics/erasure.html)一节讨论了编译器移除类型参数相关的过程。类型擦除关于变量参数（也被称为可变参数）方法有重要地位，变量参数方法的可变的形式参数有一个非范型具体化类型。参见[将信息传递给方法或构造函数](http://docs.oracle.com/javase/tutorial/java/javaOO/arguments.html)中的[任意数量参数](http://docs.oracle.com/javase/tutorial/java/javaOO/arguments.html#varargs)一节获取关于可变参数方法的更多信息。\n\n这页内容覆盖下面的主题：\n\n- [非范型具体化类型](http://docs.oracle.com/javase/tutorial/java/generics/nonReifiableVarargsType.html#non-reifiable-types)\n- [堆污染](http://docs.oracle.com/javase/tutorial/java/generics/nonReifiableVarargsType.html#heap_pollution)\n- [有非范型具体化形式参数的可变参数方法的潜在漏洞](http://docs.oracle.com/javase/tutorial/java/generics/nonReifiableVarargsType.html#vulnerabilities)\n- [从非范型具体化形式参数的可变参数方法预防警告](http://docs.oracle.com/javase/tutorial/java/generics/nonReifiableVarargsType.html#suppressing)\n\n#### 非范型具体化类型\n\n一个可具体化的类型是在运行时类型信息完全可用的类型。这包含原生类型、非范型类型、原始类型和无界通配符的调用。\n\n非范型具体化类型是通过类型擦除在编译期被移除信息的类型 - 非无界通配符范型类型的调用。非范型具体化类型不能在运行时获取它的所有信息。非范型具体化类型的例子是 List<String> 和 List<Number>；JVM 在运行时不知道这些类型的差别。如在[范型限制](http://docs.oracle.com/javase/tutorial/java/generics/restrictions.html)中展示的，不能使用非范型具体化类型的情况：例如，在 instanceof 表达式中或者作为数组中的元素。\n\n#### 堆污染\n\n堆污染发生在一个参数化类型变量指向一个不是这个参数化类型的对象。如果程序执行一些在编译期产生一个未检查告警的操作，这种情况就会发生。如果涉及参数化类型操作（例如，类型转换或方法调用）的正确性不能被核查，不管是在编译期（在编译期类型检查规则范围内）或是在运行时，会生成一个未检查告警。例如，堆污染会发生在原始类型和参数化类型混合使用的时候，或者执行未检查类型转换的时候。\n\n常规情况下，所有代码同时被编译时，编译器放出一个未检查告警来引起你对潜在堆污染的注意。如果分别单独编译你的各部分代码，发现潜在的堆污染风险就很困难。如果确保代码没有编译警告，那么就没有堆污染发生。\n\n#### 有非范型具体化形式参数的可变参数方法的潜在漏洞\n\n包含可变输入参数的范型方法可能引起堆污染。\n\n考虑下面的 ArrayBuilder 类：\n\n    public class ArrayBuilder {\n\n      public static <T> void addToList (List<T> listArg, T... elements) {\n        for (T x : elements) {\n          listArg.add(x);\n        }\n      }\n\n      public static void faultyMethod(List<String>... l) {\n        Object[] objectArray = l;     // Valid\n        objectArray[0] = Arrays.asList(42);\n        String s = l[0].get(0);       // ClassCastException thrown here\n      }\n\n    }\n\n下面的例子，HeapPollutionExample 使用 ArrayBuiler 类：\n\n    public class HeapPollutionExample {\n\n      public static void main(String[] args) {\n\n        List<String> stringListA = new ArrayList<String>();\n        List<String> stringListB = new ArrayList<String>();\n\n        ArrayBuilder.addToList(stringListA, \"Seven\", \"Eight\", \"Nine\");\n        ArrayBuilder.addToList(stringListB, \"Ten\", \"Eleven\", \"Twelve\");\n        List<List<String>> listOfStringLists = new ArrayList<List<String>>();\n        ArrayBuilder.addToList(listOfStringLists, stringListA, stringListB);\n\n        ArrayBuilder.faultyMethod(Arrays.asList(\"Hello!\"), Arrays.asList(\"World!\"));\n      }\n    }\n\n当编译时，ArrayBuilder.addToList 方法定义会产生下面的告警：\n\n    warning: [varargs] Possible heap pollution from parameterized vararg type T\n\n当编译器遇到一个可变参数的方法时，它转化可变常规参数为一个数组。然而，Java 编程语言不保证参数化类型数组的创建。在 ArrayBuilder.addToList 方法中，编译器转化可变参数 T... elements 为一个常规数组参数 T[] elements。然而，因为类型擦除，编译器转换可变常规参数为 Object[] elements。因此，存在堆污染的可能性。\n\n下面的表达式赋值可变常规参数 l 给 Object 数组 objectArgs：\n\n    Object[] objectArray = l;\n\n这个表达式会产生潜在的堆污染。匹配可变常规参数 l 的参数化类型的值可以被赋值给 objectArray 变量，因此可以被赋给 l。但是，编译器不会在这个表达式上生成未检查告警。编译器已经在转化可变常规参数 List<String>... l 到常规参数 List[] l 时生成了告警。这个表达式是有效的；变量 l 的类型是 List[]（Object[] 的子类型）。\n\n因此，如果通过下面展示的表达式赋值任何类型的 List 对象给 objectArray 数组的任何数组元素，编译器不会显示告警或错误：\n\n    objectArray[0] = Arrays.asList(42);\n\n这个表达式给 objectArray 数组的第一个元素赋值一个包含一个 Integer 类型对象的 List 对象。\n\n假设用下面的表达式调用 ArrayBuilder.faultyMethod：\n\n    ArrayBuilder.faultyMethod(Arrays.asList(\"Hello!\"), Arrays.asList(\"World!\"));\n\n在运行时，JVM 在下面的表达式中抛出一个 ClassCastException：\n\n    // ClassCastException thrown here\n    String s = l[0].get(0);\n\n存储在变量 l 数组的第一个元素是 List<Integer> 类型，但是这个表达式希望得到一个 List<String> 类型的对象。\n\n#### 从非范型具体化形式参数的可变参数方法预防警告\n\n如果声明一个有参数化类型参数的可变参数方法，你需要确认方法体不会抛出 ClassCastException 或因为不适当的处理可变常规参数引起的其他类似异常，可以通过给静态或非构造方法声明添加下面的注解防止编译器针对可变参数方法生成的这些类型的告警：\n\n    @SafeVarargs\n\n@SafeVarargs 注解是方法协议文档化的一部分；这个注解断言方法的实现会恰当处理可变常规参数。\n\n添加下面的内容到方法的声明来防止这样的告警也是勉强可取的：\n\n    @SuppressWarnings({\"unchecked\", \"varargs\"})\n\n然而，这种方法不能防止从方法调用端生成的告警。如果对 @SuppressWarnings 语法不熟悉，参见[注解](http://docs.oracle.com/javase/tutorial/java/annotations/index.html)。\n","source":"_posts/范型（十一）：类型擦除.md","raw":"title: 范型（十一）：类型擦除\ntags:\n  - Java\ncategories:\n  - 语言\n  - Java\ndate: 2016-12-14 13:53:00\n---\n\n\nJava 语言引入范型是为了在编译期提供严谨的类型检查和支持范型编程。为了实现范型，Java 编译器应用类型擦除：\n\n- 替换所有范型类型中的类型参数，用它们的边界或者 Object（如果类型参数是无界的）。因此，生成的字节码只包含普通的类、接口和方法。\n- 如果为了保护类型安全不要则插入类型转换。\n- 生成桥接方法来保护继承范型类型中的多态。\n\n<!-- more -->\n\n类型擦除确保对于参数化类型没有新类被创建；因此，范型不会引起运行时消耗。\n\n### 范型类型的擦除\n\n在类型擦除的过程中，Java 编译器擦除所有类型参数，如果类型参数是有界的则用它的第一边界替换每个类型参数，或者用 Object替换，如果类型参数是无界的。\n\n考虑下面的范型类，展示了单向列表中的一个节点：\n\n    public class Node<T> {\n\n      private T data;\n      private Node<T> next;\n\n      public Node(T data, Node<T> next) }\n        this.data = data;\n        this.next = next;\n      }\n\n      public T getData() { return data; }\n      // ...\n    }\n\n因为类型参数 T 是无界的，Java 编译器用 Object 替换它：\n\n    public class Node {\n\n      private Object data;\n      private Node next;\n\n      public Node(Object data, Node next) {\n        this.data = data;\n        this.next = next;\n      }\n\n      public Object getData() { return data; }\n      // ...\n    }\n\n在下面的例子中，范型类 Node 使用一个有界类型参数：\n\n    public class Node<T extends Comparable<T>> {\n\n      private T data;\n      private Node<T> next;\n\n      public Node(T data, Node<T> next) {\n        this.data = data;\n        this.next = next;\n      }\n\n      public T getData() { return data; }\n      // ...\n    }\n\nJava 编译器用第一边界类 Comparable 替换有界类型参数 T：\n\n    public class Node {\n\n      private Comparable data;\n      private Node next;\n\n      public Node(Comparable data, Node next) {\n        this.data = data;\n        this.next = next;\n      }\n\n      public Comparable getData() { return data; }\n      // ...\n    }\n\n### 范型方法的擦除\n\nJava 编译器也擦除范型方法参数中的类型参数。考虑下面的范型方法：\n\n    // Counts the number of occurrences of elem in anArray.\n    //\n    public static <T> int count(T[] anArray, T elem) {\n      int cnt = 0;\n      for (T e : anArray)\n        if (e.equals(elem))\n          ++cnt;\n        return cnt;\n    }\n\n因为 T 是无界的，Java 编译器用 Object 替换它：\n\n    public static int count(Object[] anArray, Object elem) {\n      int cnt = 0;\n      for (Object e : anArray)\n        if (e.equals(elem))\n            ++cnt;\n        return cnt;\n    }\n\n假设定义了下面的类：\n\n    class Shape { /* ... */ }\n    class Circle extends Shape { /* ... */ }\n    class Rectangle extends Shape { /* ... */ }\n\n可以写一个范型方法来画不同的形状：\n\n    public static <T extends Shape> void draw(T shape) { /* ... */ }\n\nJava 编译器用 Shape 替换 T：\n\n    public static void draw(Shape shape) { /* ... */ }\n\n### 类型擦除和桥接方法的作用\n\n有时类型擦除会引起一种不可预期的情况。下面的例子展示了这种情况是如何发生的。这个例子（在[桥接方法](http://docs.oracle.com/javase/tutorial/java/generics/bridgeMethods.html#bridgeMethods)中描述的）展示了编译器如何创建一个作为类型擦除过程中一部分的合成方法（叫做桥接方法）。\n\n给出下面两个类：\n\n    public class Node<T> {\n\n      public T data;\n\n      public Node(T data) { this.data = data; }\n\n      public void setData(T data) {\n        System.out.println(\"Node.setData\");\n        this.data = data;\n      }\n    }\n\n    public class MyNode extends Node<Integer> {\n      public MyNode(Integer data) { super(data); }\n\n      public void setData(Integer data) {\n        System.out.println(\"MyNode.setData\");\n        super.setData(data);\n      }\n    }\n\n考虑下面的代码：\n\n    MyNode mn = new MyNode(5);\n    Node n = mn;            // A raw type - compiler throws an unchecked warning\n    n.setData(\"Hello\");     \n    Integer x = mn.data;    // Causes a ClassCastException to be thrown.\n\n类型擦除后，代码变为：\n\n    MyNode mn = new MyNode(5);\n    Node n = (MyNode)mn;         // A raw type - compiler throws an unchecked warning\n    n.setData(\"Hello\");\n    Integer x = (String)mn.data; // Causes a ClassCastException to be thrown.\n\n代码被执行时会发生这些：\n\n- n.setData(\"Hello\"); 引起 MyNode 类对象的 setData(Object) 方法被执行。（MyNode 类从 Node 继承 setData(Object)。）\n- setData(Object) 的方法体中，n 指向的对象的 data 属性被赋值了一个 String。\n- 同一个对象的 data 属性（mn 指向的对象）可以被访问且希望是一个 integer（因为 mn 是 MyNode 的对象，MyNode 是 Node<Integer>）。\n- 尝试赋值一个 String 给一个 Integer 引起一个 ClassCastException（来自被 Java 编译器在赋值中插入的类型转化）。\n\n#### 桥接方法\n\n当编译一个继承参数化类或实现参数化接口的类或接口时，编译器可能需要创建一个作为类型擦除过程中一部分的合成方法（叫做桥接方法）。通常不需要担心桥接方法，但在跟踪栈中出现时可能会使你困惑。\n\n类型擦除之后，Node 和 MyNode 类变为：\n\n    public class Node {\n\n      public Object data;\n\n      public Node(Object data) { this.data = data; }\n\n      public void setData(Object data) {\n        System.out.println(\"Node.setData\");\n        this.data = data;\n      }\n    }\n\n    public class MyNode extends Node {\n\n      public MyNode(Integer data) { super(data); }\n\n      public void setData(Integer data) {\n        System.out.println(\"MyNode.setData\");\n        super.setData(data);\n      }\n    }\n\n类型擦除之后，方法特征并不匹配。Node 方法变为 setData(Object)，MyNode 方法变为 setData(Integer)。因此，MyNode setData 方法不是重载 Node setData 方法。\n\n为了解决这个问题，并保持类型擦除后范型类型的[多态性](http://docs.oracle.com/javase/tutorial/java/IandI/polymorphism.html)，Java 编译器生成一个桥接方法来保证子类型化像期望的一样工作。对于 MyNode 类，编译器为 setData 生成下面的桥接方法：\n\n    class MyNode extends Node {\n\n      // Bridge method generated by the compiler\n      //\n      public void setData(Object data) {\n        setData((Integer) data);\n      }\n\n      public void setData(Integer data) {\n        System.out.println(\"MyNode.setData\");\n        super.setData(data);\n      }\n\n      // ...\n    }\n\n像你看到的，桥接方法与 Node 类的 setData 方法类型擦除后有相同的方法特征，代表原来的 setData 方法。\n\n### 非范型具体化类型\n\n[类型擦除](http://docs.oracle.com/javase/tutorial/java/generics/erasure.html)一节讨论了编译器移除类型参数相关的过程。类型擦除关于变量参数（也被称为可变参数）方法有重要地位，变量参数方法的可变的形式参数有一个非范型具体化类型。参见[将信息传递给方法或构造函数](http://docs.oracle.com/javase/tutorial/java/javaOO/arguments.html)中的[任意数量参数](http://docs.oracle.com/javase/tutorial/java/javaOO/arguments.html#varargs)一节获取关于可变参数方法的更多信息。\n\n这页内容覆盖下面的主题：\n\n- [非范型具体化类型](http://docs.oracle.com/javase/tutorial/java/generics/nonReifiableVarargsType.html#non-reifiable-types)\n- [堆污染](http://docs.oracle.com/javase/tutorial/java/generics/nonReifiableVarargsType.html#heap_pollution)\n- [有非范型具体化形式参数的可变参数方法的潜在漏洞](http://docs.oracle.com/javase/tutorial/java/generics/nonReifiableVarargsType.html#vulnerabilities)\n- [从非范型具体化形式参数的可变参数方法预防警告](http://docs.oracle.com/javase/tutorial/java/generics/nonReifiableVarargsType.html#suppressing)\n\n#### 非范型具体化类型\n\n一个可具体化的类型是在运行时类型信息完全可用的类型。这包含原生类型、非范型类型、原始类型和无界通配符的调用。\n\n非范型具体化类型是通过类型擦除在编译期被移除信息的类型 - 非无界通配符范型类型的调用。非范型具体化类型不能在运行时获取它的所有信息。非范型具体化类型的例子是 List<String> 和 List<Number>；JVM 在运行时不知道这些类型的差别。如在[范型限制](http://docs.oracle.com/javase/tutorial/java/generics/restrictions.html)中展示的，不能使用非范型具体化类型的情况：例如，在 instanceof 表达式中或者作为数组中的元素。\n\n#### 堆污染\n\n堆污染发生在一个参数化类型变量指向一个不是这个参数化类型的对象。如果程序执行一些在编译期产生一个未检查告警的操作，这种情况就会发生。如果涉及参数化类型操作（例如，类型转换或方法调用）的正确性不能被核查，不管是在编译期（在编译期类型检查规则范围内）或是在运行时，会生成一个未检查告警。例如，堆污染会发生在原始类型和参数化类型混合使用的时候，或者执行未检查类型转换的时候。\n\n常规情况下，所有代码同时被编译时，编译器放出一个未检查告警来引起你对潜在堆污染的注意。如果分别单独编译你的各部分代码，发现潜在的堆污染风险就很困难。如果确保代码没有编译警告，那么就没有堆污染发生。\n\n#### 有非范型具体化形式参数的可变参数方法的潜在漏洞\n\n包含可变输入参数的范型方法可能引起堆污染。\n\n考虑下面的 ArrayBuilder 类：\n\n    public class ArrayBuilder {\n\n      public static <T> void addToList (List<T> listArg, T... elements) {\n        for (T x : elements) {\n          listArg.add(x);\n        }\n      }\n\n      public static void faultyMethod(List<String>... l) {\n        Object[] objectArray = l;     // Valid\n        objectArray[0] = Arrays.asList(42);\n        String s = l[0].get(0);       // ClassCastException thrown here\n      }\n\n    }\n\n下面的例子，HeapPollutionExample 使用 ArrayBuiler 类：\n\n    public class HeapPollutionExample {\n\n      public static void main(String[] args) {\n\n        List<String> stringListA = new ArrayList<String>();\n        List<String> stringListB = new ArrayList<String>();\n\n        ArrayBuilder.addToList(stringListA, \"Seven\", \"Eight\", \"Nine\");\n        ArrayBuilder.addToList(stringListB, \"Ten\", \"Eleven\", \"Twelve\");\n        List<List<String>> listOfStringLists = new ArrayList<List<String>>();\n        ArrayBuilder.addToList(listOfStringLists, stringListA, stringListB);\n\n        ArrayBuilder.faultyMethod(Arrays.asList(\"Hello!\"), Arrays.asList(\"World!\"));\n      }\n    }\n\n当编译时，ArrayBuilder.addToList 方法定义会产生下面的告警：\n\n    warning: [varargs] Possible heap pollution from parameterized vararg type T\n\n当编译器遇到一个可变参数的方法时，它转化可变常规参数为一个数组。然而，Java 编程语言不保证参数化类型数组的创建。在 ArrayBuilder.addToList 方法中，编译器转化可变参数 T... elements 为一个常规数组参数 T[] elements。然而，因为类型擦除，编译器转换可变常规参数为 Object[] elements。因此，存在堆污染的可能性。\n\n下面的表达式赋值可变常规参数 l 给 Object 数组 objectArgs：\n\n    Object[] objectArray = l;\n\n这个表达式会产生潜在的堆污染。匹配可变常规参数 l 的参数化类型的值可以被赋值给 objectArray 变量，因此可以被赋给 l。但是，编译器不会在这个表达式上生成未检查告警。编译器已经在转化可变常规参数 List<String>... l 到常规参数 List[] l 时生成了告警。这个表达式是有效的；变量 l 的类型是 List[]（Object[] 的子类型）。\n\n因此，如果通过下面展示的表达式赋值任何类型的 List 对象给 objectArray 数组的任何数组元素，编译器不会显示告警或错误：\n\n    objectArray[0] = Arrays.asList(42);\n\n这个表达式给 objectArray 数组的第一个元素赋值一个包含一个 Integer 类型对象的 List 对象。\n\n假设用下面的表达式调用 ArrayBuilder.faultyMethod：\n\n    ArrayBuilder.faultyMethod(Arrays.asList(\"Hello!\"), Arrays.asList(\"World!\"));\n\n在运行时，JVM 在下面的表达式中抛出一个 ClassCastException：\n\n    // ClassCastException thrown here\n    String s = l[0].get(0);\n\n存储在变量 l 数组的第一个元素是 List<Integer> 类型，但是这个表达式希望得到一个 List<String> 类型的对象。\n\n#### 从非范型具体化形式参数的可变参数方法预防警告\n\n如果声明一个有参数化类型参数的可变参数方法，你需要确认方法体不会抛出 ClassCastException 或因为不适当的处理可变常规参数引起的其他类似异常，可以通过给静态或非构造方法声明添加下面的注解防止编译器针对可变参数方法生成的这些类型的告警：\n\n    @SafeVarargs\n\n@SafeVarargs 注解是方法协议文档化的一部分；这个注解断言方法的实现会恰当处理可变常规参数。\n\n添加下面的内容到方法的声明来防止这样的告警也是勉强可取的：\n\n    @SuppressWarnings({\"unchecked\", \"varargs\"})\n\n然而，这种方法不能防止从方法调用端生成的告警。如果对 @SuppressWarnings 语法不熟悉，参见[注解](http://docs.oracle.com/javase/tutorial/java/annotations/index.html)。\n","slug":"范型（十一）：类型擦除","published":1,"updated":"2021-07-19T16:28:00.316Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmpht300ioitd3auuh762o","content":"<p>Java 语言引入范型是为了在编译期提供严谨的类型检查和支持范型编程。为了实现范型，Java 编译器应用类型擦除：</p>\n<ul>\n<li>替换所有范型类型中的类型参数，用它们的边界或者 Object（如果类型参数是无界的）。因此，生成的字节码只包含普通的类、接口和方法。</li>\n<li>如果为了保护类型安全不要则插入类型转换。</li>\n<li>生成桥接方法来保护继承范型类型中的多态。</li>\n</ul>\n<span id=\"more\"></span>\n\n<p>类型擦除确保对于参数化类型没有新类被创建；因此，范型不会引起运行时消耗。</p>\n<h3 id=\"范型类型的擦除\"><a href=\"#范型类型的擦除\" class=\"headerlink\" title=\"范型类型的擦除\"></a>范型类型的擦除</h3><p>在类型擦除的过程中，Java 编译器擦除所有类型参数，如果类型参数是有界的则用它的第一边界替换每个类型参数，或者用 Object替换，如果类型参数是无界的。</p>\n<p>考虑下面的范型类，展示了单向列表中的一个节点：</p>\n<pre><code>public class Node&lt;T&gt; &#123;\n\n  private T data;\n  private Node&lt;T&gt; next;\n\n  public Node(T data, Node&lt;T&gt; next) &#125;\n    this.data = data;\n    this.next = next;\n  &#125;\n\n  public T getData() &#123; return data; &#125;\n  // ...\n&#125;\n</code></pre>\n<p>因为类型参数 T 是无界的，Java 编译器用 Object 替换它：</p>\n<pre><code>public class Node &#123;\n\n  private Object data;\n  private Node next;\n\n  public Node(Object data, Node next) &#123;\n    this.data = data;\n    this.next = next;\n  &#125;\n\n  public Object getData() &#123; return data; &#125;\n  // ...\n&#125;\n</code></pre>\n<p>在下面的例子中，范型类 Node 使用一个有界类型参数：</p>\n<pre><code>public class Node&lt;T extends Comparable&lt;T&gt;&gt; &#123;\n\n  private T data;\n  private Node&lt;T&gt; next;\n\n  public Node(T data, Node&lt;T&gt; next) &#123;\n    this.data = data;\n    this.next = next;\n  &#125;\n\n  public T getData() &#123; return data; &#125;\n  // ...\n&#125;\n</code></pre>\n<p>Java 编译器用第一边界类 Comparable 替换有界类型参数 T：</p>\n<pre><code>public class Node &#123;\n\n  private Comparable data;\n  private Node next;\n\n  public Node(Comparable data, Node next) &#123;\n    this.data = data;\n    this.next = next;\n  &#125;\n\n  public Comparable getData() &#123; return data; &#125;\n  // ...\n&#125;\n</code></pre>\n<h3 id=\"范型方法的擦除\"><a href=\"#范型方法的擦除\" class=\"headerlink\" title=\"范型方法的擦除\"></a>范型方法的擦除</h3><p>Java 编译器也擦除范型方法参数中的类型参数。考虑下面的范型方法：</p>\n<pre><code>// Counts the number of occurrences of elem in anArray.\n//\npublic static &lt;T&gt; int count(T[] anArray, T elem) &#123;\n  int cnt = 0;\n  for (T e : anArray)\n    if (e.equals(elem))\n      ++cnt;\n    return cnt;\n&#125;\n</code></pre>\n<p>因为 T 是无界的，Java 编译器用 Object 替换它：</p>\n<pre><code>public static int count(Object[] anArray, Object elem) &#123;\n  int cnt = 0;\n  for (Object e : anArray)\n    if (e.equals(elem))\n        ++cnt;\n    return cnt;\n&#125;\n</code></pre>\n<p>假设定义了下面的类：</p>\n<pre><code>class Shape &#123; /* ... */ &#125;\nclass Circle extends Shape &#123; /* ... */ &#125;\nclass Rectangle extends Shape &#123; /* ... */ &#125;\n</code></pre>\n<p>可以写一个范型方法来画不同的形状：</p>\n<pre><code>public static &lt;T extends Shape&gt; void draw(T shape) &#123; /* ... */ &#125;\n</code></pre>\n<p>Java 编译器用 Shape 替换 T：</p>\n<pre><code>public static void draw(Shape shape) &#123; /* ... */ &#125;\n</code></pre>\n<h3 id=\"类型擦除和桥接方法的作用\"><a href=\"#类型擦除和桥接方法的作用\" class=\"headerlink\" title=\"类型擦除和桥接方法的作用\"></a>类型擦除和桥接方法的作用</h3><p>有时类型擦除会引起一种不可预期的情况。下面的例子展示了这种情况是如何发生的。这个例子（在<a href=\"http://docs.oracle.com/javase/tutorial/java/generics/bridgeMethods.html#bridgeMethods\">桥接方法</a>中描述的）展示了编译器如何创建一个作为类型擦除过程中一部分的合成方法（叫做桥接方法）。</p>\n<p>给出下面两个类：</p>\n<pre><code>public class Node&lt;T&gt; &#123;\n\n  public T data;\n\n  public Node(T data) &#123; this.data = data; &#125;\n\n  public void setData(T data) &#123;\n    System.out.println(&quot;Node.setData&quot;);\n    this.data = data;\n  &#125;\n&#125;\n\npublic class MyNode extends Node&lt;Integer&gt; &#123;\n  public MyNode(Integer data) &#123; super(data); &#125;\n\n  public void setData(Integer data) &#123;\n    System.out.println(&quot;MyNode.setData&quot;);\n    super.setData(data);\n  &#125;\n&#125;\n</code></pre>\n<p>考虑下面的代码：</p>\n<pre><code>MyNode mn = new MyNode(5);\nNode n = mn;            // A raw type - compiler throws an unchecked warning\nn.setData(&quot;Hello&quot;);     \nInteger x = mn.data;    // Causes a ClassCastException to be thrown.\n</code></pre>\n<p>类型擦除后，代码变为：</p>\n<pre><code>MyNode mn = new MyNode(5);\nNode n = (MyNode)mn;         // A raw type - compiler throws an unchecked warning\nn.setData(&quot;Hello&quot;);\nInteger x = (String)mn.data; // Causes a ClassCastException to be thrown.\n</code></pre>\n<p>代码被执行时会发生这些：</p>\n<ul>\n<li>n.setData(“Hello”); 引起 MyNode 类对象的 setData(Object) 方法被执行。（MyNode 类从 Node 继承 setData(Object)。）</li>\n<li>setData(Object) 的方法体中，n 指向的对象的 data 属性被赋值了一个 String。</li>\n<li>同一个对象的 data 属性（mn 指向的对象）可以被访问且希望是一个 integer（因为 mn 是 MyNode 的对象，MyNode 是 Node<Integer>）。</li>\n<li>尝试赋值一个 String 给一个 Integer 引起一个 ClassCastException（来自被 Java 编译器在赋值中插入的类型转化）。</li>\n</ul>\n<h4 id=\"桥接方法\"><a href=\"#桥接方法\" class=\"headerlink\" title=\"桥接方法\"></a>桥接方法</h4><p>当编译一个继承参数化类或实现参数化接口的类或接口时，编译器可能需要创建一个作为类型擦除过程中一部分的合成方法（叫做桥接方法）。通常不需要担心桥接方法，但在跟踪栈中出现时可能会使你困惑。</p>\n<p>类型擦除之后，Node 和 MyNode 类变为：</p>\n<pre><code>public class Node &#123;\n\n  public Object data;\n\n  public Node(Object data) &#123; this.data = data; &#125;\n\n  public void setData(Object data) &#123;\n    System.out.println(&quot;Node.setData&quot;);\n    this.data = data;\n  &#125;\n&#125;\n\npublic class MyNode extends Node &#123;\n\n  public MyNode(Integer data) &#123; super(data); &#125;\n\n  public void setData(Integer data) &#123;\n    System.out.println(&quot;MyNode.setData&quot;);\n    super.setData(data);\n  &#125;\n&#125;\n</code></pre>\n<p>类型擦除之后，方法特征并不匹配。Node 方法变为 setData(Object)，MyNode 方法变为 setData(Integer)。因此，MyNode setData 方法不是重载 Node setData 方法。</p>\n<p>为了解决这个问题，并保持类型擦除后范型类型的<a href=\"http://docs.oracle.com/javase/tutorial/java/IandI/polymorphism.html\">多态性</a>，Java 编译器生成一个桥接方法来保证子类型化像期望的一样工作。对于 MyNode 类，编译器为 setData 生成下面的桥接方法：</p>\n<pre><code>class MyNode extends Node &#123;\n\n  // Bridge method generated by the compiler\n  //\n  public void setData(Object data) &#123;\n    setData((Integer) data);\n  &#125;\n\n  public void setData(Integer data) &#123;\n    System.out.println(&quot;MyNode.setData&quot;);\n    super.setData(data);\n  &#125;\n\n  // ...\n&#125;\n</code></pre>\n<p>像你看到的，桥接方法与 Node 类的 setData 方法类型擦除后有相同的方法特征，代表原来的 setData 方法。</p>\n<h3 id=\"非范型具体化类型\"><a href=\"#非范型具体化类型\" class=\"headerlink\" title=\"非范型具体化类型\"></a>非范型具体化类型</h3><p><a href=\"http://docs.oracle.com/javase/tutorial/java/generics/erasure.html\">类型擦除</a>一节讨论了编译器移除类型参数相关的过程。类型擦除关于变量参数（也被称为可变参数）方法有重要地位，变量参数方法的可变的形式参数有一个非范型具体化类型。参见<a href=\"http://docs.oracle.com/javase/tutorial/java/javaOO/arguments.html\">将信息传递给方法或构造函数</a>中的<a href=\"http://docs.oracle.com/javase/tutorial/java/javaOO/arguments.html#varargs\">任意数量参数</a>一节获取关于可变参数方法的更多信息。</p>\n<p>这页内容覆盖下面的主题：</p>\n<ul>\n<li><a href=\"http://docs.oracle.com/javase/tutorial/java/generics/nonReifiableVarargsType.html#non-reifiable-types\">非范型具体化类型</a></li>\n<li><a href=\"http://docs.oracle.com/javase/tutorial/java/generics/nonReifiableVarargsType.html#heap_pollution\">堆污染</a></li>\n<li><a href=\"http://docs.oracle.com/javase/tutorial/java/generics/nonReifiableVarargsType.html#vulnerabilities\">有非范型具体化形式参数的可变参数方法的潜在漏洞</a></li>\n<li><a href=\"http://docs.oracle.com/javase/tutorial/java/generics/nonReifiableVarargsType.html#suppressing\">从非范型具体化形式参数的可变参数方法预防警告</a></li>\n</ul>\n<h4 id=\"非范型具体化类型-1\"><a href=\"#非范型具体化类型-1\" class=\"headerlink\" title=\"非范型具体化类型\"></a>非范型具体化类型</h4><p>一个可具体化的类型是在运行时类型信息完全可用的类型。这包含原生类型、非范型类型、原始类型和无界通配符的调用。</p>\n<p>非范型具体化类型是通过类型擦除在编译期被移除信息的类型 - 非无界通配符范型类型的调用。非范型具体化类型不能在运行时获取它的所有信息。非范型具体化类型的例子是 List<String> 和 List<Number>；JVM 在运行时不知道这些类型的差别。如在<a href=\"http://docs.oracle.com/javase/tutorial/java/generics/restrictions.html\">范型限制</a>中展示的，不能使用非范型具体化类型的情况：例如，在 instanceof 表达式中或者作为数组中的元素。</p>\n<h4 id=\"堆污染\"><a href=\"#堆污染\" class=\"headerlink\" title=\"堆污染\"></a>堆污染</h4><p>堆污染发生在一个参数化类型变量指向一个不是这个参数化类型的对象。如果程序执行一些在编译期产生一个未检查告警的操作，这种情况就会发生。如果涉及参数化类型操作（例如，类型转换或方法调用）的正确性不能被核查，不管是在编译期（在编译期类型检查规则范围内）或是在运行时，会生成一个未检查告警。例如，堆污染会发生在原始类型和参数化类型混合使用的时候，或者执行未检查类型转换的时候。</p>\n<p>常规情况下，所有代码同时被编译时，编译器放出一个未检查告警来引起你对潜在堆污染的注意。如果分别单独编译你的各部分代码，发现潜在的堆污染风险就很困难。如果确保代码没有编译警告，那么就没有堆污染发生。</p>\n<h4 id=\"有非范型具体化形式参数的可变参数方法的潜在漏洞\"><a href=\"#有非范型具体化形式参数的可变参数方法的潜在漏洞\" class=\"headerlink\" title=\"有非范型具体化形式参数的可变参数方法的潜在漏洞\"></a>有非范型具体化形式参数的可变参数方法的潜在漏洞</h4><p>包含可变输入参数的范型方法可能引起堆污染。</p>\n<p>考虑下面的 ArrayBuilder 类：</p>\n<pre><code>public class ArrayBuilder &#123;\n\n  public static &lt;T&gt; void addToList (List&lt;T&gt; listArg, T... elements) &#123;\n    for (T x : elements) &#123;\n      listArg.add(x);\n    &#125;\n  &#125;\n\n  public static void faultyMethod(List&lt;String&gt;... l) &#123;\n    Object[] objectArray = l;     // Valid\n    objectArray[0] = Arrays.asList(42);\n    String s = l[0].get(0);       // ClassCastException thrown here\n  &#125;\n\n&#125;\n</code></pre>\n<p>下面的例子，HeapPollutionExample 使用 ArrayBuiler 类：</p>\n<pre><code>public class HeapPollutionExample &#123;\n\n  public static void main(String[] args) &#123;\n\n    List&lt;String&gt; stringListA = new ArrayList&lt;String&gt;();\n    List&lt;String&gt; stringListB = new ArrayList&lt;String&gt;();\n\n    ArrayBuilder.addToList(stringListA, &quot;Seven&quot;, &quot;Eight&quot;, &quot;Nine&quot;);\n    ArrayBuilder.addToList(stringListB, &quot;Ten&quot;, &quot;Eleven&quot;, &quot;Twelve&quot;);\n    List&lt;List&lt;String&gt;&gt; listOfStringLists = new ArrayList&lt;List&lt;String&gt;&gt;();\n    ArrayBuilder.addToList(listOfStringLists, stringListA, stringListB);\n\n    ArrayBuilder.faultyMethod(Arrays.asList(&quot;Hello!&quot;), Arrays.asList(&quot;World!&quot;));\n  &#125;\n&#125;\n</code></pre>\n<p>当编译时，ArrayBuilder.addToList 方法定义会产生下面的告警：</p>\n<pre><code>warning: [varargs] Possible heap pollution from parameterized vararg type T\n</code></pre>\n<p>当编译器遇到一个可变参数的方法时，它转化可变常规参数为一个数组。然而，Java 编程语言不保证参数化类型数组的创建。在 ArrayBuilder.addToList 方法中，编译器转化可变参数 T… elements 为一个常规数组参数 T[] elements。然而，因为类型擦除，编译器转换可变常规参数为 Object[] elements。因此，存在堆污染的可能性。</p>\n<p>下面的表达式赋值可变常规参数 l 给 Object 数组 objectArgs：</p>\n<pre><code>Object[] objectArray = l;\n</code></pre>\n<p>这个表达式会产生潜在的堆污染。匹配可变常规参数 l 的参数化类型的值可以被赋值给 objectArray 变量，因此可以被赋给 l。但是，编译器不会在这个表达式上生成未检查告警。编译器已经在转化可变常规参数 List<String>… l 到常规参数 List[] l 时生成了告警。这个表达式是有效的；变量 l 的类型是 List[]（Object[] 的子类型）。</p>\n<p>因此，如果通过下面展示的表达式赋值任何类型的 List 对象给 objectArray 数组的任何数组元素，编译器不会显示告警或错误：</p>\n<pre><code>objectArray[0] = Arrays.asList(42);\n</code></pre>\n<p>这个表达式给 objectArray 数组的第一个元素赋值一个包含一个 Integer 类型对象的 List 对象。</p>\n<p>假设用下面的表达式调用 ArrayBuilder.faultyMethod：</p>\n<pre><code>ArrayBuilder.faultyMethod(Arrays.asList(&quot;Hello!&quot;), Arrays.asList(&quot;World!&quot;));\n</code></pre>\n<p>在运行时，JVM 在下面的表达式中抛出一个 ClassCastException：</p>\n<pre><code>// ClassCastException thrown here\nString s = l[0].get(0);\n</code></pre>\n<p>存储在变量 l 数组的第一个元素是 List<Integer> 类型，但是这个表达式希望得到一个 List<String> 类型的对象。</p>\n<h4 id=\"从非范型具体化形式参数的可变参数方法预防警告\"><a href=\"#从非范型具体化形式参数的可变参数方法预防警告\" class=\"headerlink\" title=\"从非范型具体化形式参数的可变参数方法预防警告\"></a>从非范型具体化形式参数的可变参数方法预防警告</h4><p>如果声明一个有参数化类型参数的可变参数方法，你需要确认方法体不会抛出 ClassCastException 或因为不适当的处理可变常规参数引起的其他类似异常，可以通过给静态或非构造方法声明添加下面的注解防止编译器针对可变参数方法生成的这些类型的告警：</p>\n<pre><code>@SafeVarargs\n</code></pre>\n<p>@SafeVarargs 注解是方法协议文档化的一部分；这个注解断言方法的实现会恰当处理可变常规参数。</p>\n<p>添加下面的内容到方法的声明来防止这样的告警也是勉强可取的：</p>\n<pre><code>@SuppressWarnings(&#123;&quot;unchecked&quot;, &quot;varargs&quot;&#125;)\n</code></pre>\n<p>然而，这种方法不能防止从方法调用端生成的告警。如果对 @SuppressWarnings 语法不熟悉，参见<a href=\"http://docs.oracle.com/javase/tutorial/java/annotations/index.html\">注解</a>。</p>\n","site":{"data":{}},"excerpt":"<p>Java 语言引入范型是为了在编译期提供严谨的类型检查和支持范型编程。为了实现范型，Java 编译器应用类型擦除：</p>\n<ul>\n<li>替换所有范型类型中的类型参数，用它们的边界或者 Object（如果类型参数是无界的）。因此，生成的字节码只包含普通的类、接口和方法。</li>\n<li>如果为了保护类型安全不要则插入类型转换。</li>\n<li>生成桥接方法来保护继承范型类型中的多态。</li>\n</ul>","more":"<p>类型擦除确保对于参数化类型没有新类被创建；因此，范型不会引起运行时消耗。</p>\n<h3 id=\"范型类型的擦除\"><a href=\"#范型类型的擦除\" class=\"headerlink\" title=\"范型类型的擦除\"></a>范型类型的擦除</h3><p>在类型擦除的过程中，Java 编译器擦除所有类型参数，如果类型参数是有界的则用它的第一边界替换每个类型参数，或者用 Object替换，如果类型参数是无界的。</p>\n<p>考虑下面的范型类，展示了单向列表中的一个节点：</p>\n<pre><code>public class Node&lt;T&gt; &#123;\n\n  private T data;\n  private Node&lt;T&gt; next;\n\n  public Node(T data, Node&lt;T&gt; next) &#125;\n    this.data = data;\n    this.next = next;\n  &#125;\n\n  public T getData() &#123; return data; &#125;\n  // ...\n&#125;\n</code></pre>\n<p>因为类型参数 T 是无界的，Java 编译器用 Object 替换它：</p>\n<pre><code>public class Node &#123;\n\n  private Object data;\n  private Node next;\n\n  public Node(Object data, Node next) &#123;\n    this.data = data;\n    this.next = next;\n  &#125;\n\n  public Object getData() &#123; return data; &#125;\n  // ...\n&#125;\n</code></pre>\n<p>在下面的例子中，范型类 Node 使用一个有界类型参数：</p>\n<pre><code>public class Node&lt;T extends Comparable&lt;T&gt;&gt; &#123;\n\n  private T data;\n  private Node&lt;T&gt; next;\n\n  public Node(T data, Node&lt;T&gt; next) &#123;\n    this.data = data;\n    this.next = next;\n  &#125;\n\n  public T getData() &#123; return data; &#125;\n  // ...\n&#125;\n</code></pre>\n<p>Java 编译器用第一边界类 Comparable 替换有界类型参数 T：</p>\n<pre><code>public class Node &#123;\n\n  private Comparable data;\n  private Node next;\n\n  public Node(Comparable data, Node next) &#123;\n    this.data = data;\n    this.next = next;\n  &#125;\n\n  public Comparable getData() &#123; return data; &#125;\n  // ...\n&#125;\n</code></pre>\n<h3 id=\"范型方法的擦除\"><a href=\"#范型方法的擦除\" class=\"headerlink\" title=\"范型方法的擦除\"></a>范型方法的擦除</h3><p>Java 编译器也擦除范型方法参数中的类型参数。考虑下面的范型方法：</p>\n<pre><code>// Counts the number of occurrences of elem in anArray.\n//\npublic static &lt;T&gt; int count(T[] anArray, T elem) &#123;\n  int cnt = 0;\n  for (T e : anArray)\n    if (e.equals(elem))\n      ++cnt;\n    return cnt;\n&#125;\n</code></pre>\n<p>因为 T 是无界的，Java 编译器用 Object 替换它：</p>\n<pre><code>public static int count(Object[] anArray, Object elem) &#123;\n  int cnt = 0;\n  for (Object e : anArray)\n    if (e.equals(elem))\n        ++cnt;\n    return cnt;\n&#125;\n</code></pre>\n<p>假设定义了下面的类：</p>\n<pre><code>class Shape &#123; /* ... */ &#125;\nclass Circle extends Shape &#123; /* ... */ &#125;\nclass Rectangle extends Shape &#123; /* ... */ &#125;\n</code></pre>\n<p>可以写一个范型方法来画不同的形状：</p>\n<pre><code>public static &lt;T extends Shape&gt; void draw(T shape) &#123; /* ... */ &#125;\n</code></pre>\n<p>Java 编译器用 Shape 替换 T：</p>\n<pre><code>public static void draw(Shape shape) &#123; /* ... */ &#125;\n</code></pre>\n<h3 id=\"类型擦除和桥接方法的作用\"><a href=\"#类型擦除和桥接方法的作用\" class=\"headerlink\" title=\"类型擦除和桥接方法的作用\"></a>类型擦除和桥接方法的作用</h3><p>有时类型擦除会引起一种不可预期的情况。下面的例子展示了这种情况是如何发生的。这个例子（在<a href=\"http://docs.oracle.com/javase/tutorial/java/generics/bridgeMethods.html#bridgeMethods\">桥接方法</a>中描述的）展示了编译器如何创建一个作为类型擦除过程中一部分的合成方法（叫做桥接方法）。</p>\n<p>给出下面两个类：</p>\n<pre><code>public class Node&lt;T&gt; &#123;\n\n  public T data;\n\n  public Node(T data) &#123; this.data = data; &#125;\n\n  public void setData(T data) &#123;\n    System.out.println(&quot;Node.setData&quot;);\n    this.data = data;\n  &#125;\n&#125;\n\npublic class MyNode extends Node&lt;Integer&gt; &#123;\n  public MyNode(Integer data) &#123; super(data); &#125;\n\n  public void setData(Integer data) &#123;\n    System.out.println(&quot;MyNode.setData&quot;);\n    super.setData(data);\n  &#125;\n&#125;\n</code></pre>\n<p>考虑下面的代码：</p>\n<pre><code>MyNode mn = new MyNode(5);\nNode n = mn;            // A raw type - compiler throws an unchecked warning\nn.setData(&quot;Hello&quot;);     \nInteger x = mn.data;    // Causes a ClassCastException to be thrown.\n</code></pre>\n<p>类型擦除后，代码变为：</p>\n<pre><code>MyNode mn = new MyNode(5);\nNode n = (MyNode)mn;         // A raw type - compiler throws an unchecked warning\nn.setData(&quot;Hello&quot;);\nInteger x = (String)mn.data; // Causes a ClassCastException to be thrown.\n</code></pre>\n<p>代码被执行时会发生这些：</p>\n<ul>\n<li>n.setData(“Hello”); 引起 MyNode 类对象的 setData(Object) 方法被执行。（MyNode 类从 Node 继承 setData(Object)。）</li>\n<li>setData(Object) 的方法体中，n 指向的对象的 data 属性被赋值了一个 String。</li>\n<li>同一个对象的 data 属性（mn 指向的对象）可以被访问且希望是一个 integer（因为 mn 是 MyNode 的对象，MyNode 是 Node<Integer>）。</li>\n<li>尝试赋值一个 String 给一个 Integer 引起一个 ClassCastException（来自被 Java 编译器在赋值中插入的类型转化）。</li>\n</ul>\n<h4 id=\"桥接方法\"><a href=\"#桥接方法\" class=\"headerlink\" title=\"桥接方法\"></a>桥接方法</h4><p>当编译一个继承参数化类或实现参数化接口的类或接口时，编译器可能需要创建一个作为类型擦除过程中一部分的合成方法（叫做桥接方法）。通常不需要担心桥接方法，但在跟踪栈中出现时可能会使你困惑。</p>\n<p>类型擦除之后，Node 和 MyNode 类变为：</p>\n<pre><code>public class Node &#123;\n\n  public Object data;\n\n  public Node(Object data) &#123; this.data = data; &#125;\n\n  public void setData(Object data) &#123;\n    System.out.println(&quot;Node.setData&quot;);\n    this.data = data;\n  &#125;\n&#125;\n\npublic class MyNode extends Node &#123;\n\n  public MyNode(Integer data) &#123; super(data); &#125;\n\n  public void setData(Integer data) &#123;\n    System.out.println(&quot;MyNode.setData&quot;);\n    super.setData(data);\n  &#125;\n&#125;\n</code></pre>\n<p>类型擦除之后，方法特征并不匹配。Node 方法变为 setData(Object)，MyNode 方法变为 setData(Integer)。因此，MyNode setData 方法不是重载 Node setData 方法。</p>\n<p>为了解决这个问题，并保持类型擦除后范型类型的<a href=\"http://docs.oracle.com/javase/tutorial/java/IandI/polymorphism.html\">多态性</a>，Java 编译器生成一个桥接方法来保证子类型化像期望的一样工作。对于 MyNode 类，编译器为 setData 生成下面的桥接方法：</p>\n<pre><code>class MyNode extends Node &#123;\n\n  // Bridge method generated by the compiler\n  //\n  public void setData(Object data) &#123;\n    setData((Integer) data);\n  &#125;\n\n  public void setData(Integer data) &#123;\n    System.out.println(&quot;MyNode.setData&quot;);\n    super.setData(data);\n  &#125;\n\n  // ...\n&#125;\n</code></pre>\n<p>像你看到的，桥接方法与 Node 类的 setData 方法类型擦除后有相同的方法特征，代表原来的 setData 方法。</p>\n<h3 id=\"非范型具体化类型\"><a href=\"#非范型具体化类型\" class=\"headerlink\" title=\"非范型具体化类型\"></a>非范型具体化类型</h3><p><a href=\"http://docs.oracle.com/javase/tutorial/java/generics/erasure.html\">类型擦除</a>一节讨论了编译器移除类型参数相关的过程。类型擦除关于变量参数（也被称为可变参数）方法有重要地位，变量参数方法的可变的形式参数有一个非范型具体化类型。参见<a href=\"http://docs.oracle.com/javase/tutorial/java/javaOO/arguments.html\">将信息传递给方法或构造函数</a>中的<a href=\"http://docs.oracle.com/javase/tutorial/java/javaOO/arguments.html#varargs\">任意数量参数</a>一节获取关于可变参数方法的更多信息。</p>\n<p>这页内容覆盖下面的主题：</p>\n<ul>\n<li><a href=\"http://docs.oracle.com/javase/tutorial/java/generics/nonReifiableVarargsType.html#non-reifiable-types\">非范型具体化类型</a></li>\n<li><a href=\"http://docs.oracle.com/javase/tutorial/java/generics/nonReifiableVarargsType.html#heap_pollution\">堆污染</a></li>\n<li><a href=\"http://docs.oracle.com/javase/tutorial/java/generics/nonReifiableVarargsType.html#vulnerabilities\">有非范型具体化形式参数的可变参数方法的潜在漏洞</a></li>\n<li><a href=\"http://docs.oracle.com/javase/tutorial/java/generics/nonReifiableVarargsType.html#suppressing\">从非范型具体化形式参数的可变参数方法预防警告</a></li>\n</ul>\n<h4 id=\"非范型具体化类型-1\"><a href=\"#非范型具体化类型-1\" class=\"headerlink\" title=\"非范型具体化类型\"></a>非范型具体化类型</h4><p>一个可具体化的类型是在运行时类型信息完全可用的类型。这包含原生类型、非范型类型、原始类型和无界通配符的调用。</p>\n<p>非范型具体化类型是通过类型擦除在编译期被移除信息的类型 - 非无界通配符范型类型的调用。非范型具体化类型不能在运行时获取它的所有信息。非范型具体化类型的例子是 List<String> 和 List<Number>；JVM 在运行时不知道这些类型的差别。如在<a href=\"http://docs.oracle.com/javase/tutorial/java/generics/restrictions.html\">范型限制</a>中展示的，不能使用非范型具体化类型的情况：例如，在 instanceof 表达式中或者作为数组中的元素。</p>\n<h4 id=\"堆污染\"><a href=\"#堆污染\" class=\"headerlink\" title=\"堆污染\"></a>堆污染</h4><p>堆污染发生在一个参数化类型变量指向一个不是这个参数化类型的对象。如果程序执行一些在编译期产生一个未检查告警的操作，这种情况就会发生。如果涉及参数化类型操作（例如，类型转换或方法调用）的正确性不能被核查，不管是在编译期（在编译期类型检查规则范围内）或是在运行时，会生成一个未检查告警。例如，堆污染会发生在原始类型和参数化类型混合使用的时候，或者执行未检查类型转换的时候。</p>\n<p>常规情况下，所有代码同时被编译时，编译器放出一个未检查告警来引起你对潜在堆污染的注意。如果分别单独编译你的各部分代码，发现潜在的堆污染风险就很困难。如果确保代码没有编译警告，那么就没有堆污染发生。</p>\n<h4 id=\"有非范型具体化形式参数的可变参数方法的潜在漏洞\"><a href=\"#有非范型具体化形式参数的可变参数方法的潜在漏洞\" class=\"headerlink\" title=\"有非范型具体化形式参数的可变参数方法的潜在漏洞\"></a>有非范型具体化形式参数的可变参数方法的潜在漏洞</h4><p>包含可变输入参数的范型方法可能引起堆污染。</p>\n<p>考虑下面的 ArrayBuilder 类：</p>\n<pre><code>public class ArrayBuilder &#123;\n\n  public static &lt;T&gt; void addToList (List&lt;T&gt; listArg, T... elements) &#123;\n    for (T x : elements) &#123;\n      listArg.add(x);\n    &#125;\n  &#125;\n\n  public static void faultyMethod(List&lt;String&gt;... l) &#123;\n    Object[] objectArray = l;     // Valid\n    objectArray[0] = Arrays.asList(42);\n    String s = l[0].get(0);       // ClassCastException thrown here\n  &#125;\n\n&#125;\n</code></pre>\n<p>下面的例子，HeapPollutionExample 使用 ArrayBuiler 类：</p>\n<pre><code>public class HeapPollutionExample &#123;\n\n  public static void main(String[] args) &#123;\n\n    List&lt;String&gt; stringListA = new ArrayList&lt;String&gt;();\n    List&lt;String&gt; stringListB = new ArrayList&lt;String&gt;();\n\n    ArrayBuilder.addToList(stringListA, &quot;Seven&quot;, &quot;Eight&quot;, &quot;Nine&quot;);\n    ArrayBuilder.addToList(stringListB, &quot;Ten&quot;, &quot;Eleven&quot;, &quot;Twelve&quot;);\n    List&lt;List&lt;String&gt;&gt; listOfStringLists = new ArrayList&lt;List&lt;String&gt;&gt;();\n    ArrayBuilder.addToList(listOfStringLists, stringListA, stringListB);\n\n    ArrayBuilder.faultyMethod(Arrays.asList(&quot;Hello!&quot;), Arrays.asList(&quot;World!&quot;));\n  &#125;\n&#125;\n</code></pre>\n<p>当编译时，ArrayBuilder.addToList 方法定义会产生下面的告警：</p>\n<pre><code>warning: [varargs] Possible heap pollution from parameterized vararg type T\n</code></pre>\n<p>当编译器遇到一个可变参数的方法时，它转化可变常规参数为一个数组。然而，Java 编程语言不保证参数化类型数组的创建。在 ArrayBuilder.addToList 方法中，编译器转化可变参数 T… elements 为一个常规数组参数 T[] elements。然而，因为类型擦除，编译器转换可变常规参数为 Object[] elements。因此，存在堆污染的可能性。</p>\n<p>下面的表达式赋值可变常规参数 l 给 Object 数组 objectArgs：</p>\n<pre><code>Object[] objectArray = l;\n</code></pre>\n<p>这个表达式会产生潜在的堆污染。匹配可变常规参数 l 的参数化类型的值可以被赋值给 objectArray 变量，因此可以被赋给 l。但是，编译器不会在这个表达式上生成未检查告警。编译器已经在转化可变常规参数 List<String>… l 到常规参数 List[] l 时生成了告警。这个表达式是有效的；变量 l 的类型是 List[]（Object[] 的子类型）。</p>\n<p>因此，如果通过下面展示的表达式赋值任何类型的 List 对象给 objectArray 数组的任何数组元素，编译器不会显示告警或错误：</p>\n<pre><code>objectArray[0] = Arrays.asList(42);\n</code></pre>\n<p>这个表达式给 objectArray 数组的第一个元素赋值一个包含一个 Integer 类型对象的 List 对象。</p>\n<p>假设用下面的表达式调用 ArrayBuilder.faultyMethod：</p>\n<pre><code>ArrayBuilder.faultyMethod(Arrays.asList(&quot;Hello!&quot;), Arrays.asList(&quot;World!&quot;));\n</code></pre>\n<p>在运行时，JVM 在下面的表达式中抛出一个 ClassCastException：</p>\n<pre><code>// ClassCastException thrown here\nString s = l[0].get(0);\n</code></pre>\n<p>存储在变量 l 数组的第一个元素是 List<Integer> 类型，但是这个表达式希望得到一个 List<String> 类型的对象。</p>\n<h4 id=\"从非范型具体化形式参数的可变参数方法预防警告\"><a href=\"#从非范型具体化形式参数的可变参数方法预防警告\" class=\"headerlink\" title=\"从非范型具体化形式参数的可变参数方法预防警告\"></a>从非范型具体化形式参数的可变参数方法预防警告</h4><p>如果声明一个有参数化类型参数的可变参数方法，你需要确认方法体不会抛出 ClassCastException 或因为不适当的处理可变常规参数引起的其他类似异常，可以通过给静态或非构造方法声明添加下面的注解防止编译器针对可变参数方法生成的这些类型的告警：</p>\n<pre><code>@SafeVarargs\n</code></pre>\n<p>@SafeVarargs 注解是方法协议文档化的一部分；这个注解断言方法的实现会恰当处理可变常规参数。</p>\n<p>添加下面的内容到方法的声明来防止这样的告警也是勉强可取的：</p>\n<pre><code>@SuppressWarnings(&#123;&quot;unchecked&quot;, &quot;varargs&quot;&#125;)\n</code></pre>\n<p>然而，这种方法不能防止从方法调用端生成的告警。如果对 @SuppressWarnings 语法不熟悉，参见<a href=\"http://docs.oracle.com/javase/tutorial/java/annotations/index.html\">注解</a>。</p>"},{"title":"范型（十二）：范型限制","date":"2016-12-17T04:18:01.000Z","_content":"\n\n为了有效地使用 Java 范型，必须考虑下面的限制：\n\n- [不能用原始类型实例化范型类型](http://docs.oracle.com/javase/tutorial/java/generics/restrictions.html#instantiate)\n- [不能创建类型参数的实例](http://docs.oracle.com/javase/tutorial/java/generics/restrictions.html#createObjects)\n- [不能声明类型为类型参数的的静态属性](http://docs.oracle.com/javase/tutorial/java/generics/restrictions.html#createStatic)\n- [不能对参数化类型使用类型转换或者 instanceof 操作](http://docs.oracle.com/javase/tutorial/java/generics/restrictions.html#cannotCast)\n- [不能创建参数化类型的数组](http://docs.oracle.com/javase/tutorial/java/generics/restrictions.html#createArrays)\n- [不能创建、捕获或抛出参数化类型的对象](http://docs.oracle.com/javase/tutorial/java/generics/restrictions.html#cannotCatch)\n- [不能重载一个重载参数类型擦除后是原始类型相同的方法](http://docs.oracle.com/javase/tutorial/java/generics/restrictions.html#cannotOverload)\n\n<!-- more -->\n\n### 不能用原始类型实例化范型类型\n\n考虑下面的参数化类型：\n\n    class Pair<K, V> {\n\n      private K key;\n      private V value;\n\n      public Pair(K key, V value) {\n        this.key = key;\n        this.value = value;\n      }\n\n      // ...\n    }\n\n当创建一个 Pair 对象时，不能用原始类型替换类型参数 K 或 V：\n\n    Pair<int, char> p = new Pair<>(8, 'a');  // compile-time error\n\n只能用非原始类型替换类型参数 K 和 V：\n\n    Pair<Integer, Character> p = new Pair<>(8, 'a');\n\n注意，Java 编译器会自动装箱 8 为 Integer.valueOf(8) 及 ‘a’ 为 Character('a')：\n\n    Pair<Integer, Character> p = new Pair<>(Integer.valueOf(8), new Character('a'));\n\n要获取关于自动装箱的更多信息，参见[数字和字符串](http://docs.oracle.com/javase/tutorial/java/data/index.html)一节中的[自动装箱和拆箱](http://docs.oracle.com/javase/tutorial/java/data/autoboxing.html)。\n\n### 不能创建类型参数的实例\n\n不能创建类型参数的实例。例如，下面的代码导致一个编译器错误：\n\n    public static <E> void append(List<E> list) {\n      E elem = new E();  // compile-time error\n      list.add(elem);\n    }\n\n作为一种变通方法，可以通过反射创建类型参数的一个对象：\n\n    public static <E> void append(List<E> list, Class<E> cls) throws Exception {\n      E elem = cls.newInstance();   // OK\n      list.add(elem);\n    }\n\n像下面这样调用 append 方法：\n\n    List<String> ls = new ArrayList<>();\n    append(ls, String.class);\n\n### 不能声明类型为类型参数的的静态属性\n\n类的静态属性是类级别的被类非静态对象共享的变量。因此，类型参数的静态属性是不允许的。考虑下面的类：\n\n    public class MobileDevice<T> {\n      private static T os;\n\n      // ...\n    }\n\n如果允许类型参数的静态属性，那么下面的代码会引起混乱：\n\n    MobileDevice<Smartphone> phone = new MobileDevice<>();\n    MobileDevice<Pager> pager = new MobileDevice<>();\n    MobileDevice<TabletPC> pc = new MobileDevice<>();\n\n因为静态属性 os 被 phone、pager 和 pc 共享，os 的实际类型是什么？它不能同时是 Smartphone、Pager 和 TabletPC。因此，不能创建类型参数的静态属性。\n\n### 不能对参数化类型使用类型转换或者 instanceof 操作\n\n因为在范型代码中 Java 编译器擦除所有类型参数，在运行时不能确定范型类型的哪个参数化类型被使用：\n\n    public static <E> void rtti(List<E> list) {\n      if (list instanceof ArrayList<Integer>) {  // compile-time error\n        // ...\n      }\n    }\n\n参数化类型集合传递给 rtti 方法：\n\n    S = { ArrayList<Integer>, ArrayList<String> LinkedList<Character>, ... }\n\n运行时不会跟踪类型参数，所以它不能说出 ArrayList<Integer> 和 ArrayList<String> 之间的区别。你可以做的是用一个无界通配符来确定 list 是一个 ArrayList：\n\n    public static void rtti(List<?> list) {\n      if (list instanceof ArrayList<?>) {  // OK; instanceof requires a reifiable type\n        // ...\n      }\n    }\n\n通常，不能类型转换一个参数化类型，除非它是被无界通配符参数化的。例如：\n\n    List<Integer> li = new ArrayList<>();\n    List<Number>  ln = (List<Number>) li;  // compile-time error\n\n然而，在一些场景下编译器知道类型参数总是有效的且允许类型转换。例如：\n\n    List<String> l1 = ...;\n    ArrayList<String> l2 = (ArrayList<String>)l1;  // OK\n\n### 不能创建参数化类型的数组\n\n不能创建参数化类型的数组。例如，下面的代码不能编译：\n\n    List<Integer>[] arrayOfLists = new List<Integer>[2];  // compile-time error\n\n下面的代码展示了当不同类型插入一个数组时会发生什么：\n\n    Object[] strings = new String[2];\n    strings[0] = \"hi\";   // OK\n    strings[1] = 100;    // An ArrayStoreException is thrown.\n\n如果对范型列表做同样的事情，将会有一个问题：\n\n    Object[] stringLists = new List<String>[];  // compiler error, but pretend it's allowed\n    stringLists[0] = new ArrayList<String>();   // OK\n    stringLists[1] = new ArrayList<Integer>();  // An ArrayStoreException should be thrown,\n                                                // but the runtime can't detect it.\n如果参数化列表数组被允许，前面的代码将不能抛出期望的 ArrayStoreException。\n\n### 不能创建、捕获或抛出参数化类型的对象\n\n范型类不能直接或间接地继承 Throwable 类。例如，下面的类不能编译：\n\n    // Extends Throwable indirectly\n    class MathException<T> extends Exception { /* ... */ }    // compile-time error\n\n    // Extends Throwable directly\n    class QueueFullException<T> extends Throwable { /* ... */ // compile-time error\n\n一个方法不能捕获类型参数的一个实例：\n\n    public static <T extends Exception, J> void execute(List<J> jobs) {\n      try {\n        for (J job : jobs)\n            // ...\n      } catch (T e) {   // compile-time error\n        // ...\n      }\n    }\n\n然而，可以在 throws 语句中使用一个类型参数：\n\n    class Parser<T extends Exception> {\n      public void parse(File file) throws T {     // OK\n        // ...\n      }\n    }\n\n### 不能重载一个重载参数类型擦除后是原始类型相同的方法\n\n一个类不能有两个类型擦除后有相同特征的重载方法。\n\n    public class Example {\n      public void print(Set<String> strSet) { }\n      public void print(Set<Integer> intSet) { }\n    }\n\n重载将共享相同的类文件表现且生成一个编译时错误。\n","source":"_posts/范型（十二）：范型限制.md","raw":"title: 范型（十二）：范型限制\ntags:\n  - Java\ncategories:\n  - 语言\n  - Java\ndate: 2016-12-17 12:18:01\n---\n\n\n为了有效地使用 Java 范型，必须考虑下面的限制：\n\n- [不能用原始类型实例化范型类型](http://docs.oracle.com/javase/tutorial/java/generics/restrictions.html#instantiate)\n- [不能创建类型参数的实例](http://docs.oracle.com/javase/tutorial/java/generics/restrictions.html#createObjects)\n- [不能声明类型为类型参数的的静态属性](http://docs.oracle.com/javase/tutorial/java/generics/restrictions.html#createStatic)\n- [不能对参数化类型使用类型转换或者 instanceof 操作](http://docs.oracle.com/javase/tutorial/java/generics/restrictions.html#cannotCast)\n- [不能创建参数化类型的数组](http://docs.oracle.com/javase/tutorial/java/generics/restrictions.html#createArrays)\n- [不能创建、捕获或抛出参数化类型的对象](http://docs.oracle.com/javase/tutorial/java/generics/restrictions.html#cannotCatch)\n- [不能重载一个重载参数类型擦除后是原始类型相同的方法](http://docs.oracle.com/javase/tutorial/java/generics/restrictions.html#cannotOverload)\n\n<!-- more -->\n\n### 不能用原始类型实例化范型类型\n\n考虑下面的参数化类型：\n\n    class Pair<K, V> {\n\n      private K key;\n      private V value;\n\n      public Pair(K key, V value) {\n        this.key = key;\n        this.value = value;\n      }\n\n      // ...\n    }\n\n当创建一个 Pair 对象时，不能用原始类型替换类型参数 K 或 V：\n\n    Pair<int, char> p = new Pair<>(8, 'a');  // compile-time error\n\n只能用非原始类型替换类型参数 K 和 V：\n\n    Pair<Integer, Character> p = new Pair<>(8, 'a');\n\n注意，Java 编译器会自动装箱 8 为 Integer.valueOf(8) 及 ‘a’ 为 Character('a')：\n\n    Pair<Integer, Character> p = new Pair<>(Integer.valueOf(8), new Character('a'));\n\n要获取关于自动装箱的更多信息，参见[数字和字符串](http://docs.oracle.com/javase/tutorial/java/data/index.html)一节中的[自动装箱和拆箱](http://docs.oracle.com/javase/tutorial/java/data/autoboxing.html)。\n\n### 不能创建类型参数的实例\n\n不能创建类型参数的实例。例如，下面的代码导致一个编译器错误：\n\n    public static <E> void append(List<E> list) {\n      E elem = new E();  // compile-time error\n      list.add(elem);\n    }\n\n作为一种变通方法，可以通过反射创建类型参数的一个对象：\n\n    public static <E> void append(List<E> list, Class<E> cls) throws Exception {\n      E elem = cls.newInstance();   // OK\n      list.add(elem);\n    }\n\n像下面这样调用 append 方法：\n\n    List<String> ls = new ArrayList<>();\n    append(ls, String.class);\n\n### 不能声明类型为类型参数的的静态属性\n\n类的静态属性是类级别的被类非静态对象共享的变量。因此，类型参数的静态属性是不允许的。考虑下面的类：\n\n    public class MobileDevice<T> {\n      private static T os;\n\n      // ...\n    }\n\n如果允许类型参数的静态属性，那么下面的代码会引起混乱：\n\n    MobileDevice<Smartphone> phone = new MobileDevice<>();\n    MobileDevice<Pager> pager = new MobileDevice<>();\n    MobileDevice<TabletPC> pc = new MobileDevice<>();\n\n因为静态属性 os 被 phone、pager 和 pc 共享，os 的实际类型是什么？它不能同时是 Smartphone、Pager 和 TabletPC。因此，不能创建类型参数的静态属性。\n\n### 不能对参数化类型使用类型转换或者 instanceof 操作\n\n因为在范型代码中 Java 编译器擦除所有类型参数，在运行时不能确定范型类型的哪个参数化类型被使用：\n\n    public static <E> void rtti(List<E> list) {\n      if (list instanceof ArrayList<Integer>) {  // compile-time error\n        // ...\n      }\n    }\n\n参数化类型集合传递给 rtti 方法：\n\n    S = { ArrayList<Integer>, ArrayList<String> LinkedList<Character>, ... }\n\n运行时不会跟踪类型参数，所以它不能说出 ArrayList<Integer> 和 ArrayList<String> 之间的区别。你可以做的是用一个无界通配符来确定 list 是一个 ArrayList：\n\n    public static void rtti(List<?> list) {\n      if (list instanceof ArrayList<?>) {  // OK; instanceof requires a reifiable type\n        // ...\n      }\n    }\n\n通常，不能类型转换一个参数化类型，除非它是被无界通配符参数化的。例如：\n\n    List<Integer> li = new ArrayList<>();\n    List<Number>  ln = (List<Number>) li;  // compile-time error\n\n然而，在一些场景下编译器知道类型参数总是有效的且允许类型转换。例如：\n\n    List<String> l1 = ...;\n    ArrayList<String> l2 = (ArrayList<String>)l1;  // OK\n\n### 不能创建参数化类型的数组\n\n不能创建参数化类型的数组。例如，下面的代码不能编译：\n\n    List<Integer>[] arrayOfLists = new List<Integer>[2];  // compile-time error\n\n下面的代码展示了当不同类型插入一个数组时会发生什么：\n\n    Object[] strings = new String[2];\n    strings[0] = \"hi\";   // OK\n    strings[1] = 100;    // An ArrayStoreException is thrown.\n\n如果对范型列表做同样的事情，将会有一个问题：\n\n    Object[] stringLists = new List<String>[];  // compiler error, but pretend it's allowed\n    stringLists[0] = new ArrayList<String>();   // OK\n    stringLists[1] = new ArrayList<Integer>();  // An ArrayStoreException should be thrown,\n                                                // but the runtime can't detect it.\n如果参数化列表数组被允许，前面的代码将不能抛出期望的 ArrayStoreException。\n\n### 不能创建、捕获或抛出参数化类型的对象\n\n范型类不能直接或间接地继承 Throwable 类。例如，下面的类不能编译：\n\n    // Extends Throwable indirectly\n    class MathException<T> extends Exception { /* ... */ }    // compile-time error\n\n    // Extends Throwable directly\n    class QueueFullException<T> extends Throwable { /* ... */ // compile-time error\n\n一个方法不能捕获类型参数的一个实例：\n\n    public static <T extends Exception, J> void execute(List<J> jobs) {\n      try {\n        for (J job : jobs)\n            // ...\n      } catch (T e) {   // compile-time error\n        // ...\n      }\n    }\n\n然而，可以在 throws 语句中使用一个类型参数：\n\n    class Parser<T extends Exception> {\n      public void parse(File file) throws T {     // OK\n        // ...\n      }\n    }\n\n### 不能重载一个重载参数类型擦除后是原始类型相同的方法\n\n一个类不能有两个类型擦除后有相同特征的重载方法。\n\n    public class Example {\n      public void print(Set<String> strSet) { }\n      public void print(Set<Integer> intSet) { }\n    }\n\n重载将共享相同的类文件表现且生成一个编译时错误。\n","slug":"范型（十二）：范型限制","published":1,"updated":"2021-07-19T16:28:00.316Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmpht400iritd3f3gk3iai","content":"<p>为了有效地使用 Java 范型，必须考虑下面的限制：</p>\n<ul>\n<li><a href=\"http://docs.oracle.com/javase/tutorial/java/generics/restrictions.html#instantiate\">不能用原始类型实例化范型类型</a></li>\n<li><a href=\"http://docs.oracle.com/javase/tutorial/java/generics/restrictions.html#createObjects\">不能创建类型参数的实例</a></li>\n<li><a href=\"http://docs.oracle.com/javase/tutorial/java/generics/restrictions.html#createStatic\">不能声明类型为类型参数的的静态属性</a></li>\n<li><a href=\"http://docs.oracle.com/javase/tutorial/java/generics/restrictions.html#cannotCast\">不能对参数化类型使用类型转换或者 instanceof 操作</a></li>\n<li><a href=\"http://docs.oracle.com/javase/tutorial/java/generics/restrictions.html#createArrays\">不能创建参数化类型的数组</a></li>\n<li><a href=\"http://docs.oracle.com/javase/tutorial/java/generics/restrictions.html#cannotCatch\">不能创建、捕获或抛出参数化类型的对象</a></li>\n<li><a href=\"http://docs.oracle.com/javase/tutorial/java/generics/restrictions.html#cannotOverload\">不能重载一个重载参数类型擦除后是原始类型相同的方法</a></li>\n</ul>\n<span id=\"more\"></span>\n\n<h3 id=\"不能用原始类型实例化范型类型\"><a href=\"#不能用原始类型实例化范型类型\" class=\"headerlink\" title=\"不能用原始类型实例化范型类型\"></a>不能用原始类型实例化范型类型</h3><p>考虑下面的参数化类型：</p>\n<pre><code>class Pair&lt;K, V&gt; &#123;\n\n  private K key;\n  private V value;\n\n  public Pair(K key, V value) &#123;\n    this.key = key;\n    this.value = value;\n  &#125;\n\n  // ...\n&#125;\n</code></pre>\n<p>当创建一个 Pair 对象时，不能用原始类型替换类型参数 K 或 V：</p>\n<pre><code>Pair&lt;int, char&gt; p = new Pair&lt;&gt;(8, &#39;a&#39;);  // compile-time error\n</code></pre>\n<p>只能用非原始类型替换类型参数 K 和 V：</p>\n<pre><code>Pair&lt;Integer, Character&gt; p = new Pair&lt;&gt;(8, &#39;a&#39;);\n</code></pre>\n<p>注意，Java 编译器会自动装箱 8 为 Integer.valueOf(8) 及 ‘a’ 为 Character(‘a’)：</p>\n<pre><code>Pair&lt;Integer, Character&gt; p = new Pair&lt;&gt;(Integer.valueOf(8), new Character(&#39;a&#39;));\n</code></pre>\n<p>要获取关于自动装箱的更多信息，参见<a href=\"http://docs.oracle.com/javase/tutorial/java/data/index.html\">数字和字符串</a>一节中的<a href=\"http://docs.oracle.com/javase/tutorial/java/data/autoboxing.html\">自动装箱和拆箱</a>。</p>\n<h3 id=\"不能创建类型参数的实例\"><a href=\"#不能创建类型参数的实例\" class=\"headerlink\" title=\"不能创建类型参数的实例\"></a>不能创建类型参数的实例</h3><p>不能创建类型参数的实例。例如，下面的代码导致一个编译器错误：</p>\n<pre><code>public static &lt;E&gt; void append(List&lt;E&gt; list) &#123;\n  E elem = new E();  // compile-time error\n  list.add(elem);\n&#125;\n</code></pre>\n<p>作为一种变通方法，可以通过反射创建类型参数的一个对象：</p>\n<pre><code>public static &lt;E&gt; void append(List&lt;E&gt; list, Class&lt;E&gt; cls) throws Exception &#123;\n  E elem = cls.newInstance();   // OK\n  list.add(elem);\n&#125;\n</code></pre>\n<p>像下面这样调用 append 方法：</p>\n<pre><code>List&lt;String&gt; ls = new ArrayList&lt;&gt;();\nappend(ls, String.class);\n</code></pre>\n<h3 id=\"不能声明类型为类型参数的的静态属性\"><a href=\"#不能声明类型为类型参数的的静态属性\" class=\"headerlink\" title=\"不能声明类型为类型参数的的静态属性\"></a>不能声明类型为类型参数的的静态属性</h3><p>类的静态属性是类级别的被类非静态对象共享的变量。因此，类型参数的静态属性是不允许的。考虑下面的类：</p>\n<pre><code>public class MobileDevice&lt;T&gt; &#123;\n  private static T os;\n\n  // ...\n&#125;\n</code></pre>\n<p>如果允许类型参数的静态属性，那么下面的代码会引起混乱：</p>\n<pre><code>MobileDevice&lt;Smartphone&gt; phone = new MobileDevice&lt;&gt;();\nMobileDevice&lt;Pager&gt; pager = new MobileDevice&lt;&gt;();\nMobileDevice&lt;TabletPC&gt; pc = new MobileDevice&lt;&gt;();\n</code></pre>\n<p>因为静态属性 os 被 phone、pager 和 pc 共享，os 的实际类型是什么？它不能同时是 Smartphone、Pager 和 TabletPC。因此，不能创建类型参数的静态属性。</p>\n<h3 id=\"不能对参数化类型使用类型转换或者-instanceof-操作\"><a href=\"#不能对参数化类型使用类型转换或者-instanceof-操作\" class=\"headerlink\" title=\"不能对参数化类型使用类型转换或者 instanceof 操作\"></a>不能对参数化类型使用类型转换或者 instanceof 操作</h3><p>因为在范型代码中 Java 编译器擦除所有类型参数，在运行时不能确定范型类型的哪个参数化类型被使用：</p>\n<pre><code>public static &lt;E&gt; void rtti(List&lt;E&gt; list) &#123;\n  if (list instanceof ArrayList&lt;Integer&gt;) &#123;  // compile-time error\n    // ...\n  &#125;\n&#125;\n</code></pre>\n<p>参数化类型集合传递给 rtti 方法：</p>\n<pre><code>S = &#123; ArrayList&lt;Integer&gt;, ArrayList&lt;String&gt; LinkedList&lt;Character&gt;, ... &#125;\n</code></pre>\n<p>运行时不会跟踪类型参数，所以它不能说出 ArrayList<Integer> 和 ArrayList<String> 之间的区别。你可以做的是用一个无界通配符来确定 list 是一个 ArrayList：</p>\n<pre><code>public static void rtti(List&lt;?&gt; list) &#123;\n  if (list instanceof ArrayList&lt;?&gt;) &#123;  // OK; instanceof requires a reifiable type\n    // ...\n  &#125;\n&#125;\n</code></pre>\n<p>通常，不能类型转换一个参数化类型，除非它是被无界通配符参数化的。例如：</p>\n<pre><code>List&lt;Integer&gt; li = new ArrayList&lt;&gt;();\nList&lt;Number&gt;  ln = (List&lt;Number&gt;) li;  // compile-time error\n</code></pre>\n<p>然而，在一些场景下编译器知道类型参数总是有效的且允许类型转换。例如：</p>\n<pre><code>List&lt;String&gt; l1 = ...;\nArrayList&lt;String&gt; l2 = (ArrayList&lt;String&gt;)l1;  // OK\n</code></pre>\n<h3 id=\"不能创建参数化类型的数组\"><a href=\"#不能创建参数化类型的数组\" class=\"headerlink\" title=\"不能创建参数化类型的数组\"></a>不能创建参数化类型的数组</h3><p>不能创建参数化类型的数组。例如，下面的代码不能编译：</p>\n<pre><code>List&lt;Integer&gt;[] arrayOfLists = new List&lt;Integer&gt;[2];  // compile-time error\n</code></pre>\n<p>下面的代码展示了当不同类型插入一个数组时会发生什么：</p>\n<pre><code>Object[] strings = new String[2];\nstrings[0] = &quot;hi&quot;;   // OK\nstrings[1] = 100;    // An ArrayStoreException is thrown.\n</code></pre>\n<p>如果对范型列表做同样的事情，将会有一个问题：</p>\n<pre><code>Object[] stringLists = new List&lt;String&gt;[];  // compiler error, but pretend it&#39;s allowed\nstringLists[0] = new ArrayList&lt;String&gt;();   // OK\nstringLists[1] = new ArrayList&lt;Integer&gt;();  // An ArrayStoreException should be thrown,\n                                            // but the runtime can&#39;t detect it.\n</code></pre>\n<p>如果参数化列表数组被允许，前面的代码将不能抛出期望的 ArrayStoreException。</p>\n<h3 id=\"不能创建、捕获或抛出参数化类型的对象\"><a href=\"#不能创建、捕获或抛出参数化类型的对象\" class=\"headerlink\" title=\"不能创建、捕获或抛出参数化类型的对象\"></a>不能创建、捕获或抛出参数化类型的对象</h3><p>范型类不能直接或间接地继承 Throwable 类。例如，下面的类不能编译：</p>\n<pre><code>// Extends Throwable indirectly\nclass MathException&lt;T&gt; extends Exception &#123; /* ... */ &#125;    // compile-time error\n\n// Extends Throwable directly\nclass QueueFullException&lt;T&gt; extends Throwable &#123; /* ... */ // compile-time error\n</code></pre>\n<p>一个方法不能捕获类型参数的一个实例：</p>\n<pre><code>public static &lt;T extends Exception, J&gt; void execute(List&lt;J&gt; jobs) &#123;\n  try &#123;\n    for (J job : jobs)\n        // ...\n  &#125; catch (T e) &#123;   // compile-time error\n    // ...\n  &#125;\n&#125;\n</code></pre>\n<p>然而，可以在 throws 语句中使用一个类型参数：</p>\n<pre><code>class Parser&lt;T extends Exception&gt; &#123;\n  public void parse(File file) throws T &#123;     // OK\n    // ...\n  &#125;\n&#125;\n</code></pre>\n<h3 id=\"不能重载一个重载参数类型擦除后是原始类型相同的方法\"><a href=\"#不能重载一个重载参数类型擦除后是原始类型相同的方法\" class=\"headerlink\" title=\"不能重载一个重载参数类型擦除后是原始类型相同的方法\"></a>不能重载一个重载参数类型擦除后是原始类型相同的方法</h3><p>一个类不能有两个类型擦除后有相同特征的重载方法。</p>\n<pre><code>public class Example &#123;\n  public void print(Set&lt;String&gt; strSet) &#123; &#125;\n  public void print(Set&lt;Integer&gt; intSet) &#123; &#125;\n&#125;\n</code></pre>\n<p>重载将共享相同的类文件表现且生成一个编译时错误。</p>\n","site":{"data":{}},"excerpt":"<p>为了有效地使用 Java 范型，必须考虑下面的限制：</p>\n<ul>\n<li><a href=\"http://docs.oracle.com/javase/tutorial/java/generics/restrictions.html#instantiate\">不能用原始类型实例化范型类型</a></li>\n<li><a href=\"http://docs.oracle.com/javase/tutorial/java/generics/restrictions.html#createObjects\">不能创建类型参数的实例</a></li>\n<li><a href=\"http://docs.oracle.com/javase/tutorial/java/generics/restrictions.html#createStatic\">不能声明类型为类型参数的的静态属性</a></li>\n<li><a href=\"http://docs.oracle.com/javase/tutorial/java/generics/restrictions.html#cannotCast\">不能对参数化类型使用类型转换或者 instanceof 操作</a></li>\n<li><a href=\"http://docs.oracle.com/javase/tutorial/java/generics/restrictions.html#createArrays\">不能创建参数化类型的数组</a></li>\n<li><a href=\"http://docs.oracle.com/javase/tutorial/java/generics/restrictions.html#cannotCatch\">不能创建、捕获或抛出参数化类型的对象</a></li>\n<li><a href=\"http://docs.oracle.com/javase/tutorial/java/generics/restrictions.html#cannotOverload\">不能重载一个重载参数类型擦除后是原始类型相同的方法</a></li>\n</ul>","more":"<h3 id=\"不能用原始类型实例化范型类型\"><a href=\"#不能用原始类型实例化范型类型\" class=\"headerlink\" title=\"不能用原始类型实例化范型类型\"></a>不能用原始类型实例化范型类型</h3><p>考虑下面的参数化类型：</p>\n<pre><code>class Pair&lt;K, V&gt; &#123;\n\n  private K key;\n  private V value;\n\n  public Pair(K key, V value) &#123;\n    this.key = key;\n    this.value = value;\n  &#125;\n\n  // ...\n&#125;\n</code></pre>\n<p>当创建一个 Pair 对象时，不能用原始类型替换类型参数 K 或 V：</p>\n<pre><code>Pair&lt;int, char&gt; p = new Pair&lt;&gt;(8, &#39;a&#39;);  // compile-time error\n</code></pre>\n<p>只能用非原始类型替换类型参数 K 和 V：</p>\n<pre><code>Pair&lt;Integer, Character&gt; p = new Pair&lt;&gt;(8, &#39;a&#39;);\n</code></pre>\n<p>注意，Java 编译器会自动装箱 8 为 Integer.valueOf(8) 及 ‘a’ 为 Character(‘a’)：</p>\n<pre><code>Pair&lt;Integer, Character&gt; p = new Pair&lt;&gt;(Integer.valueOf(8), new Character(&#39;a&#39;));\n</code></pre>\n<p>要获取关于自动装箱的更多信息，参见<a href=\"http://docs.oracle.com/javase/tutorial/java/data/index.html\">数字和字符串</a>一节中的<a href=\"http://docs.oracle.com/javase/tutorial/java/data/autoboxing.html\">自动装箱和拆箱</a>。</p>\n<h3 id=\"不能创建类型参数的实例\"><a href=\"#不能创建类型参数的实例\" class=\"headerlink\" title=\"不能创建类型参数的实例\"></a>不能创建类型参数的实例</h3><p>不能创建类型参数的实例。例如，下面的代码导致一个编译器错误：</p>\n<pre><code>public static &lt;E&gt; void append(List&lt;E&gt; list) &#123;\n  E elem = new E();  // compile-time error\n  list.add(elem);\n&#125;\n</code></pre>\n<p>作为一种变通方法，可以通过反射创建类型参数的一个对象：</p>\n<pre><code>public static &lt;E&gt; void append(List&lt;E&gt; list, Class&lt;E&gt; cls) throws Exception &#123;\n  E elem = cls.newInstance();   // OK\n  list.add(elem);\n&#125;\n</code></pre>\n<p>像下面这样调用 append 方法：</p>\n<pre><code>List&lt;String&gt; ls = new ArrayList&lt;&gt;();\nappend(ls, String.class);\n</code></pre>\n<h3 id=\"不能声明类型为类型参数的的静态属性\"><a href=\"#不能声明类型为类型参数的的静态属性\" class=\"headerlink\" title=\"不能声明类型为类型参数的的静态属性\"></a>不能声明类型为类型参数的的静态属性</h3><p>类的静态属性是类级别的被类非静态对象共享的变量。因此，类型参数的静态属性是不允许的。考虑下面的类：</p>\n<pre><code>public class MobileDevice&lt;T&gt; &#123;\n  private static T os;\n\n  // ...\n&#125;\n</code></pre>\n<p>如果允许类型参数的静态属性，那么下面的代码会引起混乱：</p>\n<pre><code>MobileDevice&lt;Smartphone&gt; phone = new MobileDevice&lt;&gt;();\nMobileDevice&lt;Pager&gt; pager = new MobileDevice&lt;&gt;();\nMobileDevice&lt;TabletPC&gt; pc = new MobileDevice&lt;&gt;();\n</code></pre>\n<p>因为静态属性 os 被 phone、pager 和 pc 共享，os 的实际类型是什么？它不能同时是 Smartphone、Pager 和 TabletPC。因此，不能创建类型参数的静态属性。</p>\n<h3 id=\"不能对参数化类型使用类型转换或者-instanceof-操作\"><a href=\"#不能对参数化类型使用类型转换或者-instanceof-操作\" class=\"headerlink\" title=\"不能对参数化类型使用类型转换或者 instanceof 操作\"></a>不能对参数化类型使用类型转换或者 instanceof 操作</h3><p>因为在范型代码中 Java 编译器擦除所有类型参数，在运行时不能确定范型类型的哪个参数化类型被使用：</p>\n<pre><code>public static &lt;E&gt; void rtti(List&lt;E&gt; list) &#123;\n  if (list instanceof ArrayList&lt;Integer&gt;) &#123;  // compile-time error\n    // ...\n  &#125;\n&#125;\n</code></pre>\n<p>参数化类型集合传递给 rtti 方法：</p>\n<pre><code>S = &#123; ArrayList&lt;Integer&gt;, ArrayList&lt;String&gt; LinkedList&lt;Character&gt;, ... &#125;\n</code></pre>\n<p>运行时不会跟踪类型参数，所以它不能说出 ArrayList<Integer> 和 ArrayList<String> 之间的区别。你可以做的是用一个无界通配符来确定 list 是一个 ArrayList：</p>\n<pre><code>public static void rtti(List&lt;?&gt; list) &#123;\n  if (list instanceof ArrayList&lt;?&gt;) &#123;  // OK; instanceof requires a reifiable type\n    // ...\n  &#125;\n&#125;\n</code></pre>\n<p>通常，不能类型转换一个参数化类型，除非它是被无界通配符参数化的。例如：</p>\n<pre><code>List&lt;Integer&gt; li = new ArrayList&lt;&gt;();\nList&lt;Number&gt;  ln = (List&lt;Number&gt;) li;  // compile-time error\n</code></pre>\n<p>然而，在一些场景下编译器知道类型参数总是有效的且允许类型转换。例如：</p>\n<pre><code>List&lt;String&gt; l1 = ...;\nArrayList&lt;String&gt; l2 = (ArrayList&lt;String&gt;)l1;  // OK\n</code></pre>\n<h3 id=\"不能创建参数化类型的数组\"><a href=\"#不能创建参数化类型的数组\" class=\"headerlink\" title=\"不能创建参数化类型的数组\"></a>不能创建参数化类型的数组</h3><p>不能创建参数化类型的数组。例如，下面的代码不能编译：</p>\n<pre><code>List&lt;Integer&gt;[] arrayOfLists = new List&lt;Integer&gt;[2];  // compile-time error\n</code></pre>\n<p>下面的代码展示了当不同类型插入一个数组时会发生什么：</p>\n<pre><code>Object[] strings = new String[2];\nstrings[0] = &quot;hi&quot;;   // OK\nstrings[1] = 100;    // An ArrayStoreException is thrown.\n</code></pre>\n<p>如果对范型列表做同样的事情，将会有一个问题：</p>\n<pre><code>Object[] stringLists = new List&lt;String&gt;[];  // compiler error, but pretend it&#39;s allowed\nstringLists[0] = new ArrayList&lt;String&gt;();   // OK\nstringLists[1] = new ArrayList&lt;Integer&gt;();  // An ArrayStoreException should be thrown,\n                                            // but the runtime can&#39;t detect it.\n</code></pre>\n<p>如果参数化列表数组被允许，前面的代码将不能抛出期望的 ArrayStoreException。</p>\n<h3 id=\"不能创建、捕获或抛出参数化类型的对象\"><a href=\"#不能创建、捕获或抛出参数化类型的对象\" class=\"headerlink\" title=\"不能创建、捕获或抛出参数化类型的对象\"></a>不能创建、捕获或抛出参数化类型的对象</h3><p>范型类不能直接或间接地继承 Throwable 类。例如，下面的类不能编译：</p>\n<pre><code>// Extends Throwable indirectly\nclass MathException&lt;T&gt; extends Exception &#123; /* ... */ &#125;    // compile-time error\n\n// Extends Throwable directly\nclass QueueFullException&lt;T&gt; extends Throwable &#123; /* ... */ // compile-time error\n</code></pre>\n<p>一个方法不能捕获类型参数的一个实例：</p>\n<pre><code>public static &lt;T extends Exception, J&gt; void execute(List&lt;J&gt; jobs) &#123;\n  try &#123;\n    for (J job : jobs)\n        // ...\n  &#125; catch (T e) &#123;   // compile-time error\n    // ...\n  &#125;\n&#125;\n</code></pre>\n<p>然而，可以在 throws 语句中使用一个类型参数：</p>\n<pre><code>class Parser&lt;T extends Exception&gt; &#123;\n  public void parse(File file) throws T &#123;     // OK\n    // ...\n  &#125;\n&#125;\n</code></pre>\n<h3 id=\"不能重载一个重载参数类型擦除后是原始类型相同的方法\"><a href=\"#不能重载一个重载参数类型擦除后是原始类型相同的方法\" class=\"headerlink\" title=\"不能重载一个重载参数类型擦除后是原始类型相同的方法\"></a>不能重载一个重载参数类型擦除后是原始类型相同的方法</h3><p>一个类不能有两个类型擦除后有相同特征的重载方法。</p>\n<pre><code>public class Example &#123;\n  public void print(Set&lt;String&gt; strSet) &#123; &#125;\n  public void print(Set&lt;Integer&gt; intSet) &#123; &#125;\n&#125;\n</code></pre>\n<p>重载将共享相同的类文件表现且生成一个编译时错误。</p>"},{"title":"范型（十）：通配符","date":"2016-12-04T10:03:45.000Z","_content":"\n在范型代码中，问好（?），被叫做通配符，表示一个未知的类型。通配符可以被用在多种情况：作为一个参数的类型、域或本地变量；有时作为返回类型（虽然编程的最佳实践是更明确）。通配符永远不会被用作范型方法调用的类型参数、范型类实例化的创建或者超类型。\n\n<!-- more -->\n\n下面的章节更细致的讨论通配符，包括上限通配符、下限通配符和通配符匹配。\n\n### 上限通配符\n\n可以用上限通配符可以放宽对变量的限制。例如，你想写一个工作于 List<Integer>、List<Double> 和 List<Number> 的方法；你可以通过使用上限通配符实现。\n\n声明一个上限通配符，使用通配符字符（‘?‘），后面跟着 extends 关键字，随后是它的上限。注意，关于这点，extends 被用作一般意义上的“extends”（对类而言）或者“implements“（对接口而言）。\n\n为了写工作与 Number 和 Number 子类型（例如：Integer、Double 和 Float）列表的方法，你需要指定 List<? extends Number>。List<Number> 比 List<? extends Number> 更加严格，因为前者只匹配 Number 类型的列表，然而后者匹配 Number 或者它的子类的列表。\n\n考虑下面的 process 方法：\n\n    public static void process(List<? extends Foo> list) { /* ... */ }\n\n上限通配符，<? extends Foo>，这里 Foo 是任意匹配 Foo 和任意 Foo 的子类型。process 方法可以作为类型 Foo 访问列表元素：\n\n    public static void process(List<? extends Foo> list) {\n      for (Foo elem : list) {\n        // ...\n      }\n    }\n\n在 foreach 语句中，elem 变量遍历列表中的每个元素。在 Foo 类中定义的所有方法可用于 elem。\n\nsumOfList 方法返回在列表中数字的和：\n\n    public static double sumOfList(List<? extends Number> list) {\n      double s = 0.0;\n      for (Number n : list)\n        s += n.doubleValue();\n      return s;\n    }\n\n下面的代码，使用 Integer 对象的列表，打印 sum = 6.0：\n\n    List<Integer> li = Arrays.asList(1, 2, 3);\n    System.out.println(\"sum = \" + sumOfList(li));\n\nDouble 数值的列表可以使用同样的 sumOfList 方法。下面的代码打印 sum = 7.0：\n\nList<Double> ld = Arrays.asList(1.2, 2.3, 3.5);\nSystem.out.println(\"sum = \" + sumOfList(ld));\n\n### 无界通配符\n\n无界通配符类型用通配符字符（?）被指定，例如，List<?>。这被叫做一个未知类型的列表。有两种情况无界通配符是有用的方法：\n\n- 如果您正在编写一个可以使用 Object 类中提供的功能来实现的方法。\n- 当代码使用不依赖类型参数的泛型类中的方法时。例如，List.size 或 List.clear。事实上，Class<?> 如此频繁的被使用，因为 Class<T> 中的大部分方法不依赖 T。\n\n考虑下面的方法，printList：\n\n    public static void printList(List<Object> list) {\n      for (Object elem : list)\n        System.out.println(elem + \" \");\n      System.out.println();\n    }\n\nprintList 的目标是打印任何类型的列表，但是这个目标却不能实现 - 它只能打印 Object 的列表实例；它不能打印 List<Integer>、List<String>、List<Double> 等，因为它们不是 List<Object> 的子类型。为了写一个范型 printList 方法，使用 List<?>：\n\n    public static void printList(List<?> list) {\n      for (Object elem: list)\n        System.out.print(elem + \" \");\n      System.out.println();\n    }\n\n因为对于任何具体类型 A，List<A> 是 List<?> 的一个子类型，你可以使用 printList 打印任何类型的列表：\n\n    List<Integer> li = Arrays.asList(1, 2, 3);\n    List<String>  ls = Arrays.asList(\"one\", \"two\", \"three\");\n    printList(li);\n    printList(ls);\n\n> 备注：[Arrays.asList](https://docs.oracle.com/javase/8/docs/api/java/util/Arrays.html#asList-T...-) 在这节课中的例子中被用到。这个静态工厂方法转变指定的数组并返回一个固定大小的列表。\n\n注意到 List<Object> 和 List<?> 不是一样的很重要。你可以插入一个 Object，或 Object 的任意子类型，到 List<Object>。但是你只能插入 null 到 List<?>。[通配符使用指南](http://docs.oracle.com/javase/tutorial/java/generics/wildcardGuidelines.html) 一节有更多信息关于如何确定在一个给定的情况下应该用什么样的通配符，如果有的话。\n\n### 下限通配符\n\n[上限通配符](http://docs.oracle.com/javase/tutorial/java/generics/upperBounded.html)一节展示了上限通配符限制一个未知类型到一个指定类型或这个类型的子类型，并且使用 extends 关键字来表示。同样的方法，下限通配符限制未知类型为一个指定的类型或这个类型的超类型。\n\n下限通配符用通配符字符（?）表示，跟着 super 关键字，后面跟着它的下限：<? super A>。\n\n> 注意：你可以为一个通配符指定上限，或者你可以下限，但是不能同时指定。\n\n你想写一个将 Integer 对象放入列表的方法。为了最大化灵活性，你会希望对 List<Integer>、List<Number> 和 List<Object> 都可用 - 任何可保存 Integer 值的。\n\n为了写可用于 Integer 和 Integer 超类型（如：Integer、Number 和 Object）的列表的方法，你应该指定 List<? super Integer>。List<Integer> 比 List<? super Integer> 限制更严格，因为前者只匹配类型 Integer 的列表，然而后者匹配任何 Integer 超类的列表。\n\n下面的代码添加数字 1 到 10 到一个列表的末尾：\n\n    public static void addNumbers(List<? super Integer> list) {\n      for (int i = 1; i <= 10; i++) {\n        list.add(i);\n      }\n    }\n\n[通配符使用指南](http://docs.oracle.com/javase/tutorial/java/generics/wildcardGuidelines.html) 一节提供关于什么时候使用上限通配符和什么时候使用下限通配符的指南。\n\n### 通配符和子类型\n\n像[范型、继承和子类型](http://docs.oracle.com/javase/tutorial/java/generics/inheritance.html)中描述的，范型类或接口不相关，仅仅因为它们的类型间有关系。然而，可以使用通配符来创建范型类或接口间的关系。\n\n给出下面两个常规（非范型）类：\n\n    class A { /* ... */ }\n    class B extends A { /* ... */ }\n\n有理由写下面的代码：\n\n    B b = new B();\n    A a = b;\n\n这个例子显示常规类的继承遵循这样的子类型规则：类 B 是类 A 的一个子类型，如果 B 扩展 A。这个规则不适用于范型类型：\n\n    List<B> lb = new ArrayList<>();\n    List<A> la = lb;   // compile-time error\n\n给定 Integer 是 Number 的一个子类型，List<Integer> 和 List<Number> 之间是什么关系？\n\n公共的父类是 List<?>。![generics-listParent](/uploads/20161203/generics-listParent.gif)\n\n虽然 Integer 是 Number 的一个子类型，List<Integer> 不是 List<Number> 的一个子类型，并且事实上，这两个类型没有关系。List<Number> 和 List<Integer> 的公共父类是 List<?>。\n\n为了在这些类间创建关系，因此代码可以通过 List<Integer> 的元素访问 Number 的方法，那么使用上限通配符：\n\n    List<? extends Integer> intList = new ArrayList<>();\n    List<? extends Number>  numList = intList;  // OK. List<? extends Integer> is a subtype of List<? extends Number>\n\n因为 Integer 是 Number 的一个子类型，且 numList 是 Number 对象的一个列表，现在 intList（Integer 对象的列表）和 numList 之间存在关系了。下面的图片展示了几种用上限通配符和下限通配符声明的 List 类之间的关系。\n\n几种范型 List 类声明的层次。![generics-wildcardSubtyping](/uploads/20161203/generics-wildcardSubtyping.gif)\n\n[通配符使用指南](http://docs.oracle.com/javase/tutorial/java/generics/wildcardGuidelines.html) 一节有更多关于使用上限和下限通配符结果的信息。\n\n### 通配符匹配和辅助方法\n\n在一些场景中，编译器推断通配符的类型。例如，一个列表可能被定义为 List<?>，但是当评估一个表达式时，编译器从代码中推断一个特定的类型。这种情况被称为通配符匹配。\n\n在大多数情况下，你不需要担心通配符匹配，除了当你看到一个错误消息包含短语“捕捉“。\n\n[WildcardError](http://docs.oracle.com/javase/tutorial/displayCode.html?code=http://docs.oracle.com/javase/tutorial/java/generics/examples/WildcardError.java) 例子当编译时产生一个匹配错误：\n\n    import java.util.List;\n\n    public class WildcardError {\n\n      void foo(List<?> i) {\n        i.set(0, i.get(0));\n      }\n    }\n\n在这个例子中，编译器处理 i 输入参数为类型 Object。当 foo 方法调用 [List.set(int, E)](https://docs.oracle.com/javase/8/docs/api/java/util/List.html#set-int-E-)，编译器不能确定被插入列表中的对象的类型，并且会产生一个错误。当这种类型错误发生时，它通常意味着编译器确信你赋值错误类型给一个变量。范型被添加到 Java 语言中就是因为这个原因 - 在编译期增强类型安全。\n\n通过 Oracle 的 JDK 7 javac 实现编译 WildcardError 例子时产生下面的错误：\n\n    WildcardError.java:6: error: method set in interface List<E> cannot be applied to given types;\n        i.set(0, i.get(0));\n         ^\n      required: int,CAP#1\n      found: int,Object\n      reason: actual argument Object cannot be converted to CAP#1 by method invocation conversion\n      where E is a type-variable:\n        E extends Object declared in interface List\n      where CAP#1 is a fresh type-variable:\n        CAP#1 extends Object from capture of ?\n    1 error\n\n在这个例子中，代码尝试执行一个安全操作，那么如何解决编译器错误呢？可以通过写一个匹配通配符的私有辅助方法来解决这个问题。在这个案例中，你可以通过创建私有辅助方法 fooHelper 解决这个问题，像 [WildcardFixed](http://docs.oracle.com/javase/tutorial/displayCode.html?code=http://docs.oracle.com/javase/tutorial/java/generics/examples/WildcardFixed.java) 中显示的：\n\n    public class WildcardFixed {\n\n      void foo(List<?> i) {\n        fooHelper(i);\n      }\n\n\n      // Helper method created so that the wildcard can be captured\n      // through type inference.\n      private <T> void fooHelper(List<T> l) {\n        l.set(0, l.get(0));\n      }\n\n    }\n\n感谢辅助方法，编译器使用推断确定 T 是 CAP#1，在调用中的匹配变量。现在这个例子编译成功。\n\n按照惯例，辅助方法一般命名为*原方法名Helper*。\n\n现在考虑一个更复杂的例子，[WildcardErrorBad](http://docs.oracle.com/javase/tutorial/displayCode.html?code=http://docs.oracle.com/javase/tutorial/java/generics/examples/WildcardErrorBad.java)：\n\n    import java.util.List;\n\n    public class WildcardErrorBad {\n\n      void swapFirst(List<? extends Number> l1, List<? extends Number> l2) {\n        Number temp = l1.get(0);\n        l1.set(0, l2.get(0)); // expected a CAP#1 extends Number,\n                              // got a CAP#2 extends Number;\n                              // same bound, but different types\n        l2.set(0, temp);      // expected a CAP#1 extends Number,\n                              // got a Number\n      }\n    }\n\n在这个例子中，代码尝试一个不安全的操作。例如，考虑下面 swapFirst 方法的调用：\n\n    List<Integer> li = Arrays.asList(1, 2, 3);\n    List<Double>  ld = Arrays.asList(10.10, 20.20, 30.30);\n    swapFirst(li, ld);\n\n因为 List<Integer> 和 List<Double> 都符合 List<? extends Number> 的标准，明显不正确的是取 Integer 列表中的一个值并尝试放入 Double 值的列表中。\n\n用 Oracle JDK javac 编译器编译会生成下面的错误：\n\n    WildcardErrorBad.java:7: error: method set in interface List<E> cannot be applied to given types;\n          l1.set(0, l2.get(0)); // expected a CAP#1 extends Number,\n            ^\n      required: int,CAP#1\n      found: int,Number\n      reason: actual argument Number cannot be converted to CAP#1 by method invocation conversion\n      where E is a type-variable:\n        E extends Object declared in interface List\n      where CAP#1 is a fresh type-variable:\n        CAP#1 extends Number from capture of ? extends Number\n    WildcardErrorBad.java:10: error: method set in interface List<E> cannot be applied to given types;\n          l2.set(0, temp);      // expected a CAP#1 extends Number,\n            ^\n      required: int,CAP#1\n      found: int,Number\n      reason: actual argument Number cannot be converted to CAP#1 by method invocation conversion\n      where E is a type-variable:\n        E extends Object declared in interface List\n      where CAP#1 is a fresh type-variable:\n        CAP#1 extends Number from capture of ? extends Number\n    WildcardErrorBad.java:15: error: method set in interface List<E> cannot be applied to given types;\n            i.set(0, i.get(0));\n             ^\n      required: int,CAP#1\n      found: int,Object\n      reason: actual argument Object cannot be converted to CAP#1 by method invocation conversion\n      where E is a type-variable:\n        E extends Object declared in interface List\n      where CAP#1 is a fresh type-variable:\n        CAP#1 extends Object from capture of ?\n    3 errors\n\n针对这个问题没有辅助方法可以使用，因为代码是基础错误。\n\n### 通配符使用指\n\n当学习使用范型编程时一个混乱的方面是确定何时使用上限通配符和何时使用下限通配符。本页提供了在设计代码时遵循的一些指导方针。\n\n为了这个讨论的目的，考虑变量提供两个功能之一是有帮助的：\n\n**“In”变量**\n\n一个“in”变量提供数据给代码。想象一个有两个参数的 copy 方法：copy(src, dest)。src 参数提供被拷贝的数据，因此它是“in”参数。\n\n**“Out”变量**\n\n一个“Out”变量保持数据为在其他地方使用。在 copy 的例子中，copy(src, dest)，dest 参数接收数据，因此它是“out”参数。\n\n当然，一些变量被同时用作“in”和“out”目的 - 这种情况也在指导方针中。\n\n可以使用“in”和“out”准则，当决定是否使用通配符及什么类型通配符是合适的。下面的列表提供了遵循的指南：\n\n**通配符指南**\n\n- 一个“in”变量用上限通配符被定义，用 extends 关键字。\n- 一个“out”变量用下限通配符被定义，用 super 关键字。\n- 在“in”变量可以用定义在 Object 类中的方法被访问的场景，使用无界通配符。\n- 在代码需要访问同时作为“in”和“out”变量的场景，不要使用通配符。\n\n这些指南不适用于方法的返回类型。使用通配符作为返回类型应该避免，因为它强制开发人员使用代码处理通配符。\n\n通过 List<? extends ...> 定义的列表可以非正式的被认为是只读的，但这不是一个严格的保证。假设有下面两个类：\n\n    class NaturalNumber {\n\n      private int i;\n\n      public NaturalNumber(int i) { this.i = i; }\n      // ...\n    }\n\n    class EvenNumber extends NaturalNumber {\n\n      public EvenNumber(int i) { super(i); }\n      // ...\n    }\n\n考虑下面的代码：\n\n    List<EvenNumber> le = new ArrayList<>();\n    List<? extends NaturalNumber> ln = le;\n    ln.add(new NaturalNumber(35));  // compile-time error\n\n因为 List<EvenNumber> 是 List<? extends NaturalNumber> 的一个子类型，可以赋值 le 给 ln。但是不能用 ln 添加一个自然数到偶数列表中。下面对这个列表的操作是可能的：\n\n- 可以添加 null。\n- 可以调用 clear。\n- 可以获取迭代器且调用 remove。\n- 可以匹配通配符和写从列表中读取的元素。\n\n可以看到通过 List<? extends NaturalNumber> 定义的列表严格意义上不是只读的，但可以这样认为，因为不能存储一个新的元素或改变列表中已经存在的元素。\n","source":"_posts/范型（十）：通配符.md","raw":"title: 范型（十）：通配符\ntags:\n  - Java\ncategories:\n  - 语言\n  - Java\ndate: 2016-12-04 18:03:45\n---\n\n在范型代码中，问好（?），被叫做通配符，表示一个未知的类型。通配符可以被用在多种情况：作为一个参数的类型、域或本地变量；有时作为返回类型（虽然编程的最佳实践是更明确）。通配符永远不会被用作范型方法调用的类型参数、范型类实例化的创建或者超类型。\n\n<!-- more -->\n\n下面的章节更细致的讨论通配符，包括上限通配符、下限通配符和通配符匹配。\n\n### 上限通配符\n\n可以用上限通配符可以放宽对变量的限制。例如，你想写一个工作于 List<Integer>、List<Double> 和 List<Number> 的方法；你可以通过使用上限通配符实现。\n\n声明一个上限通配符，使用通配符字符（‘?‘），后面跟着 extends 关键字，随后是它的上限。注意，关于这点，extends 被用作一般意义上的“extends”（对类而言）或者“implements“（对接口而言）。\n\n为了写工作与 Number 和 Number 子类型（例如：Integer、Double 和 Float）列表的方法，你需要指定 List<? extends Number>。List<Number> 比 List<? extends Number> 更加严格，因为前者只匹配 Number 类型的列表，然而后者匹配 Number 或者它的子类的列表。\n\n考虑下面的 process 方法：\n\n    public static void process(List<? extends Foo> list) { /* ... */ }\n\n上限通配符，<? extends Foo>，这里 Foo 是任意匹配 Foo 和任意 Foo 的子类型。process 方法可以作为类型 Foo 访问列表元素：\n\n    public static void process(List<? extends Foo> list) {\n      for (Foo elem : list) {\n        // ...\n      }\n    }\n\n在 foreach 语句中，elem 变量遍历列表中的每个元素。在 Foo 类中定义的所有方法可用于 elem。\n\nsumOfList 方法返回在列表中数字的和：\n\n    public static double sumOfList(List<? extends Number> list) {\n      double s = 0.0;\n      for (Number n : list)\n        s += n.doubleValue();\n      return s;\n    }\n\n下面的代码，使用 Integer 对象的列表，打印 sum = 6.0：\n\n    List<Integer> li = Arrays.asList(1, 2, 3);\n    System.out.println(\"sum = \" + sumOfList(li));\n\nDouble 数值的列表可以使用同样的 sumOfList 方法。下面的代码打印 sum = 7.0：\n\nList<Double> ld = Arrays.asList(1.2, 2.3, 3.5);\nSystem.out.println(\"sum = \" + sumOfList(ld));\n\n### 无界通配符\n\n无界通配符类型用通配符字符（?）被指定，例如，List<?>。这被叫做一个未知类型的列表。有两种情况无界通配符是有用的方法：\n\n- 如果您正在编写一个可以使用 Object 类中提供的功能来实现的方法。\n- 当代码使用不依赖类型参数的泛型类中的方法时。例如，List.size 或 List.clear。事实上，Class<?> 如此频繁的被使用，因为 Class<T> 中的大部分方法不依赖 T。\n\n考虑下面的方法，printList：\n\n    public static void printList(List<Object> list) {\n      for (Object elem : list)\n        System.out.println(elem + \" \");\n      System.out.println();\n    }\n\nprintList 的目标是打印任何类型的列表，但是这个目标却不能实现 - 它只能打印 Object 的列表实例；它不能打印 List<Integer>、List<String>、List<Double> 等，因为它们不是 List<Object> 的子类型。为了写一个范型 printList 方法，使用 List<?>：\n\n    public static void printList(List<?> list) {\n      for (Object elem: list)\n        System.out.print(elem + \" \");\n      System.out.println();\n    }\n\n因为对于任何具体类型 A，List<A> 是 List<?> 的一个子类型，你可以使用 printList 打印任何类型的列表：\n\n    List<Integer> li = Arrays.asList(1, 2, 3);\n    List<String>  ls = Arrays.asList(\"one\", \"two\", \"three\");\n    printList(li);\n    printList(ls);\n\n> 备注：[Arrays.asList](https://docs.oracle.com/javase/8/docs/api/java/util/Arrays.html#asList-T...-) 在这节课中的例子中被用到。这个静态工厂方法转变指定的数组并返回一个固定大小的列表。\n\n注意到 List<Object> 和 List<?> 不是一样的很重要。你可以插入一个 Object，或 Object 的任意子类型，到 List<Object>。但是你只能插入 null 到 List<?>。[通配符使用指南](http://docs.oracle.com/javase/tutorial/java/generics/wildcardGuidelines.html) 一节有更多信息关于如何确定在一个给定的情况下应该用什么样的通配符，如果有的话。\n\n### 下限通配符\n\n[上限通配符](http://docs.oracle.com/javase/tutorial/java/generics/upperBounded.html)一节展示了上限通配符限制一个未知类型到一个指定类型或这个类型的子类型，并且使用 extends 关键字来表示。同样的方法，下限通配符限制未知类型为一个指定的类型或这个类型的超类型。\n\n下限通配符用通配符字符（?）表示，跟着 super 关键字，后面跟着它的下限：<? super A>。\n\n> 注意：你可以为一个通配符指定上限，或者你可以下限，但是不能同时指定。\n\n你想写一个将 Integer 对象放入列表的方法。为了最大化灵活性，你会希望对 List<Integer>、List<Number> 和 List<Object> 都可用 - 任何可保存 Integer 值的。\n\n为了写可用于 Integer 和 Integer 超类型（如：Integer、Number 和 Object）的列表的方法，你应该指定 List<? super Integer>。List<Integer> 比 List<? super Integer> 限制更严格，因为前者只匹配类型 Integer 的列表，然而后者匹配任何 Integer 超类的列表。\n\n下面的代码添加数字 1 到 10 到一个列表的末尾：\n\n    public static void addNumbers(List<? super Integer> list) {\n      for (int i = 1; i <= 10; i++) {\n        list.add(i);\n      }\n    }\n\n[通配符使用指南](http://docs.oracle.com/javase/tutorial/java/generics/wildcardGuidelines.html) 一节提供关于什么时候使用上限通配符和什么时候使用下限通配符的指南。\n\n### 通配符和子类型\n\n像[范型、继承和子类型](http://docs.oracle.com/javase/tutorial/java/generics/inheritance.html)中描述的，范型类或接口不相关，仅仅因为它们的类型间有关系。然而，可以使用通配符来创建范型类或接口间的关系。\n\n给出下面两个常规（非范型）类：\n\n    class A { /* ... */ }\n    class B extends A { /* ... */ }\n\n有理由写下面的代码：\n\n    B b = new B();\n    A a = b;\n\n这个例子显示常规类的继承遵循这样的子类型规则：类 B 是类 A 的一个子类型，如果 B 扩展 A。这个规则不适用于范型类型：\n\n    List<B> lb = new ArrayList<>();\n    List<A> la = lb;   // compile-time error\n\n给定 Integer 是 Number 的一个子类型，List<Integer> 和 List<Number> 之间是什么关系？\n\n公共的父类是 List<?>。![generics-listParent](/uploads/20161203/generics-listParent.gif)\n\n虽然 Integer 是 Number 的一个子类型，List<Integer> 不是 List<Number> 的一个子类型，并且事实上，这两个类型没有关系。List<Number> 和 List<Integer> 的公共父类是 List<?>。\n\n为了在这些类间创建关系，因此代码可以通过 List<Integer> 的元素访问 Number 的方法，那么使用上限通配符：\n\n    List<? extends Integer> intList = new ArrayList<>();\n    List<? extends Number>  numList = intList;  // OK. List<? extends Integer> is a subtype of List<? extends Number>\n\n因为 Integer 是 Number 的一个子类型，且 numList 是 Number 对象的一个列表，现在 intList（Integer 对象的列表）和 numList 之间存在关系了。下面的图片展示了几种用上限通配符和下限通配符声明的 List 类之间的关系。\n\n几种范型 List 类声明的层次。![generics-wildcardSubtyping](/uploads/20161203/generics-wildcardSubtyping.gif)\n\n[通配符使用指南](http://docs.oracle.com/javase/tutorial/java/generics/wildcardGuidelines.html) 一节有更多关于使用上限和下限通配符结果的信息。\n\n### 通配符匹配和辅助方法\n\n在一些场景中，编译器推断通配符的类型。例如，一个列表可能被定义为 List<?>，但是当评估一个表达式时，编译器从代码中推断一个特定的类型。这种情况被称为通配符匹配。\n\n在大多数情况下，你不需要担心通配符匹配，除了当你看到一个错误消息包含短语“捕捉“。\n\n[WildcardError](http://docs.oracle.com/javase/tutorial/displayCode.html?code=http://docs.oracle.com/javase/tutorial/java/generics/examples/WildcardError.java) 例子当编译时产生一个匹配错误：\n\n    import java.util.List;\n\n    public class WildcardError {\n\n      void foo(List<?> i) {\n        i.set(0, i.get(0));\n      }\n    }\n\n在这个例子中，编译器处理 i 输入参数为类型 Object。当 foo 方法调用 [List.set(int, E)](https://docs.oracle.com/javase/8/docs/api/java/util/List.html#set-int-E-)，编译器不能确定被插入列表中的对象的类型，并且会产生一个错误。当这种类型错误发生时，它通常意味着编译器确信你赋值错误类型给一个变量。范型被添加到 Java 语言中就是因为这个原因 - 在编译期增强类型安全。\n\n通过 Oracle 的 JDK 7 javac 实现编译 WildcardError 例子时产生下面的错误：\n\n    WildcardError.java:6: error: method set in interface List<E> cannot be applied to given types;\n        i.set(0, i.get(0));\n         ^\n      required: int,CAP#1\n      found: int,Object\n      reason: actual argument Object cannot be converted to CAP#1 by method invocation conversion\n      where E is a type-variable:\n        E extends Object declared in interface List\n      where CAP#1 is a fresh type-variable:\n        CAP#1 extends Object from capture of ?\n    1 error\n\n在这个例子中，代码尝试执行一个安全操作，那么如何解决编译器错误呢？可以通过写一个匹配通配符的私有辅助方法来解决这个问题。在这个案例中，你可以通过创建私有辅助方法 fooHelper 解决这个问题，像 [WildcardFixed](http://docs.oracle.com/javase/tutorial/displayCode.html?code=http://docs.oracle.com/javase/tutorial/java/generics/examples/WildcardFixed.java) 中显示的：\n\n    public class WildcardFixed {\n\n      void foo(List<?> i) {\n        fooHelper(i);\n      }\n\n\n      // Helper method created so that the wildcard can be captured\n      // through type inference.\n      private <T> void fooHelper(List<T> l) {\n        l.set(0, l.get(0));\n      }\n\n    }\n\n感谢辅助方法，编译器使用推断确定 T 是 CAP#1，在调用中的匹配变量。现在这个例子编译成功。\n\n按照惯例，辅助方法一般命名为*原方法名Helper*。\n\n现在考虑一个更复杂的例子，[WildcardErrorBad](http://docs.oracle.com/javase/tutorial/displayCode.html?code=http://docs.oracle.com/javase/tutorial/java/generics/examples/WildcardErrorBad.java)：\n\n    import java.util.List;\n\n    public class WildcardErrorBad {\n\n      void swapFirst(List<? extends Number> l1, List<? extends Number> l2) {\n        Number temp = l1.get(0);\n        l1.set(0, l2.get(0)); // expected a CAP#1 extends Number,\n                              // got a CAP#2 extends Number;\n                              // same bound, but different types\n        l2.set(0, temp);      // expected a CAP#1 extends Number,\n                              // got a Number\n      }\n    }\n\n在这个例子中，代码尝试一个不安全的操作。例如，考虑下面 swapFirst 方法的调用：\n\n    List<Integer> li = Arrays.asList(1, 2, 3);\n    List<Double>  ld = Arrays.asList(10.10, 20.20, 30.30);\n    swapFirst(li, ld);\n\n因为 List<Integer> 和 List<Double> 都符合 List<? extends Number> 的标准，明显不正确的是取 Integer 列表中的一个值并尝试放入 Double 值的列表中。\n\n用 Oracle JDK javac 编译器编译会生成下面的错误：\n\n    WildcardErrorBad.java:7: error: method set in interface List<E> cannot be applied to given types;\n          l1.set(0, l2.get(0)); // expected a CAP#1 extends Number,\n            ^\n      required: int,CAP#1\n      found: int,Number\n      reason: actual argument Number cannot be converted to CAP#1 by method invocation conversion\n      where E is a type-variable:\n        E extends Object declared in interface List\n      where CAP#1 is a fresh type-variable:\n        CAP#1 extends Number from capture of ? extends Number\n    WildcardErrorBad.java:10: error: method set in interface List<E> cannot be applied to given types;\n          l2.set(0, temp);      // expected a CAP#1 extends Number,\n            ^\n      required: int,CAP#1\n      found: int,Number\n      reason: actual argument Number cannot be converted to CAP#1 by method invocation conversion\n      where E is a type-variable:\n        E extends Object declared in interface List\n      where CAP#1 is a fresh type-variable:\n        CAP#1 extends Number from capture of ? extends Number\n    WildcardErrorBad.java:15: error: method set in interface List<E> cannot be applied to given types;\n            i.set(0, i.get(0));\n             ^\n      required: int,CAP#1\n      found: int,Object\n      reason: actual argument Object cannot be converted to CAP#1 by method invocation conversion\n      where E is a type-variable:\n        E extends Object declared in interface List\n      where CAP#1 is a fresh type-variable:\n        CAP#1 extends Object from capture of ?\n    3 errors\n\n针对这个问题没有辅助方法可以使用，因为代码是基础错误。\n\n### 通配符使用指\n\n当学习使用范型编程时一个混乱的方面是确定何时使用上限通配符和何时使用下限通配符。本页提供了在设计代码时遵循的一些指导方针。\n\n为了这个讨论的目的，考虑变量提供两个功能之一是有帮助的：\n\n**“In”变量**\n\n一个“in”变量提供数据给代码。想象一个有两个参数的 copy 方法：copy(src, dest)。src 参数提供被拷贝的数据，因此它是“in”参数。\n\n**“Out”变量**\n\n一个“Out”变量保持数据为在其他地方使用。在 copy 的例子中，copy(src, dest)，dest 参数接收数据，因此它是“out”参数。\n\n当然，一些变量被同时用作“in”和“out”目的 - 这种情况也在指导方针中。\n\n可以使用“in”和“out”准则，当决定是否使用通配符及什么类型通配符是合适的。下面的列表提供了遵循的指南：\n\n**通配符指南**\n\n- 一个“in”变量用上限通配符被定义，用 extends 关键字。\n- 一个“out”变量用下限通配符被定义，用 super 关键字。\n- 在“in”变量可以用定义在 Object 类中的方法被访问的场景，使用无界通配符。\n- 在代码需要访问同时作为“in”和“out”变量的场景，不要使用通配符。\n\n这些指南不适用于方法的返回类型。使用通配符作为返回类型应该避免，因为它强制开发人员使用代码处理通配符。\n\n通过 List<? extends ...> 定义的列表可以非正式的被认为是只读的，但这不是一个严格的保证。假设有下面两个类：\n\n    class NaturalNumber {\n\n      private int i;\n\n      public NaturalNumber(int i) { this.i = i; }\n      // ...\n    }\n\n    class EvenNumber extends NaturalNumber {\n\n      public EvenNumber(int i) { super(i); }\n      // ...\n    }\n\n考虑下面的代码：\n\n    List<EvenNumber> le = new ArrayList<>();\n    List<? extends NaturalNumber> ln = le;\n    ln.add(new NaturalNumber(35));  // compile-time error\n\n因为 List<EvenNumber> 是 List<? extends NaturalNumber> 的一个子类型，可以赋值 le 给 ln。但是不能用 ln 添加一个自然数到偶数列表中。下面对这个列表的操作是可能的：\n\n- 可以添加 null。\n- 可以调用 clear。\n- 可以获取迭代器且调用 remove。\n- 可以匹配通配符和写从列表中读取的元素。\n\n可以看到通过 List<? extends NaturalNumber> 定义的列表严格意义上不是只读的，但可以这样认为，因为不能存储一个新的元素或改变列表中已经存在的元素。\n","slug":"范型（十）：通配符","published":1,"updated":"2021-07-19T16:28:00.316Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmpht600iwitd3hqdl8pap","content":"<p>在范型代码中，问好（?），被叫做通配符，表示一个未知的类型。通配符可以被用在多种情况：作为一个参数的类型、域或本地变量；有时作为返回类型（虽然编程的最佳实践是更明确）。通配符永远不会被用作范型方法调用的类型参数、范型类实例化的创建或者超类型。</p>\n<span id=\"more\"></span>\n\n<p>下面的章节更细致的讨论通配符，包括上限通配符、下限通配符和通配符匹配。</p>\n<h3 id=\"上限通配符\"><a href=\"#上限通配符\" class=\"headerlink\" title=\"上限通配符\"></a>上限通配符</h3><p>可以用上限通配符可以放宽对变量的限制。例如，你想写一个工作于 List<Integer>、List<Double> 和 List<Number> 的方法；你可以通过使用上限通配符实现。</p>\n<p>声明一个上限通配符，使用通配符字符（‘?‘），后面跟着 extends 关键字，随后是它的上限。注意，关于这点，extends 被用作一般意义上的“extends”（对类而言）或者“implements“（对接口而言）。</p>\n<p>为了写工作与 Number 和 Number 子类型（例如：Integer、Double 和 Float）列表的方法，你需要指定 List&lt;? extends Number&gt;。List<Number> 比 List&lt;? extends Number&gt; 更加严格，因为前者只匹配 Number 类型的列表，然而后者匹配 Number 或者它的子类的列表。</p>\n<p>考虑下面的 process 方法：</p>\n<pre><code>public static void process(List&lt;? extends Foo&gt; list) &#123; /* ... */ &#125;\n</code></pre>\n<p>上限通配符，&lt;? extends Foo&gt;，这里 Foo 是任意匹配 Foo 和任意 Foo 的子类型。process 方法可以作为类型 Foo 访问列表元素：</p>\n<pre><code>public static void process(List&lt;? extends Foo&gt; list) &#123;\n  for (Foo elem : list) &#123;\n    // ...\n  &#125;\n&#125;\n</code></pre>\n<p>在 foreach 语句中，elem 变量遍历列表中的每个元素。在 Foo 类中定义的所有方法可用于 elem。</p>\n<p>sumOfList 方法返回在列表中数字的和：</p>\n<pre><code>public static double sumOfList(List&lt;? extends Number&gt; list) &#123;\n  double s = 0.0;\n  for (Number n : list)\n    s += n.doubleValue();\n  return s;\n&#125;\n</code></pre>\n<p>下面的代码，使用 Integer 对象的列表，打印 sum = 6.0：</p>\n<pre><code>List&lt;Integer&gt; li = Arrays.asList(1, 2, 3);\nSystem.out.println(&quot;sum = &quot; + sumOfList(li));\n</code></pre>\n<p>Double 数值的列表可以使用同样的 sumOfList 方法。下面的代码打印 sum = 7.0：</p>\n<p>List<Double> ld = Arrays.asList(1.2, 2.3, 3.5);<br>System.out.println(“sum = “ + sumOfList(ld));</p>\n<h3 id=\"无界通配符\"><a href=\"#无界通配符\" class=\"headerlink\" title=\"无界通配符\"></a>无界通配符</h3><p>无界通配符类型用通配符字符（?）被指定，例如，List&lt;?&gt;。这被叫做一个未知类型的列表。有两种情况无界通配符是有用的方法：</p>\n<ul>\n<li>如果您正在编写一个可以使用 Object 类中提供的功能来实现的方法。</li>\n<li>当代码使用不依赖类型参数的泛型类中的方法时。例如，List.size 或 List.clear。事实上，Class&lt;?&gt; 如此频繁的被使用，因为 Class<T> 中的大部分方法不依赖 T。</li>\n</ul>\n<p>考虑下面的方法，printList：</p>\n<pre><code>public static void printList(List&lt;Object&gt; list) &#123;\n  for (Object elem : list)\n    System.out.println(elem + &quot; &quot;);\n  System.out.println();\n&#125;\n</code></pre>\n<p>printList 的目标是打印任何类型的列表，但是这个目标却不能实现 - 它只能打印 Object 的列表实例；它不能打印 List<Integer>、List<String>、List<Double> 等，因为它们不是 List<Object> 的子类型。为了写一个范型 printList 方法，使用 List&lt;?&gt;：</p>\n<pre><code>public static void printList(List&lt;?&gt; list) &#123;\n  for (Object elem: list)\n    System.out.print(elem + &quot; &quot;);\n  System.out.println();\n&#125;\n</code></pre>\n<p>因为对于任何具体类型 A，List<A> 是 List&lt;?&gt; 的一个子类型，你可以使用 printList 打印任何类型的列表：</p>\n<pre><code>List&lt;Integer&gt; li = Arrays.asList(1, 2, 3);\nList&lt;String&gt;  ls = Arrays.asList(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;);\nprintList(li);\nprintList(ls);\n</code></pre>\n<blockquote>\n<p>备注：<a href=\"https://docs.oracle.com/javase/8/docs/api/java/util/Arrays.html#asList-T...-\">Arrays.asList</a> 在这节课中的例子中被用到。这个静态工厂方法转变指定的数组并返回一个固定大小的列表。</p>\n</blockquote>\n<p>注意到 List<Object> 和 List<?> 不是一样的很重要。你可以插入一个 Object，或 Object 的任意子类型，到 List<Object>。但是你只能插入 null 到 List<?>。<a href=\"http://docs.oracle.com/javase/tutorial/java/generics/wildcardGuidelines.html\">通配符使用指南</a> 一节有更多信息关于如何确定在一个给定的情况下应该用什么样的通配符，如果有的话。</p>\n<h3 id=\"下限通配符\"><a href=\"#下限通配符\" class=\"headerlink\" title=\"下限通配符\"></a>下限通配符</h3><p><a href=\"http://docs.oracle.com/javase/tutorial/java/generics/upperBounded.html\">上限通配符</a>一节展示了上限通配符限制一个未知类型到一个指定类型或这个类型的子类型，并且使用 extends 关键字来表示。同样的方法，下限通配符限制未知类型为一个指定的类型或这个类型的超类型。</p>\n<p>下限通配符用通配符字符（?）表示，跟着 super 关键字，后面跟着它的下限：&lt;? super A&gt;。</p>\n<blockquote>\n<p>注意：你可以为一个通配符指定上限，或者你可以下限，但是不能同时指定。</p>\n</blockquote>\n<p>你想写一个将 Integer 对象放入列表的方法。为了最大化灵活性，你会希望对 List<Integer>、List<Number> 和 List<Object> 都可用 - 任何可保存 Integer 值的。</p>\n<p>为了写可用于 Integer 和 Integer 超类型（如：Integer、Number 和 Object）的列表的方法，你应该指定 List&lt;? super Integer&gt;。List<Integer> 比 List&lt;? super Integer&gt; 限制更严格，因为前者只匹配类型 Integer 的列表，然而后者匹配任何 Integer 超类的列表。</p>\n<p>下面的代码添加数字 1 到 10 到一个列表的末尾：</p>\n<pre><code>public static void addNumbers(List&lt;? super Integer&gt; list) &#123;\n  for (int i = 1; i &lt;= 10; i++) &#123;\n    list.add(i);\n  &#125;\n&#125;\n</code></pre>\n<p><a href=\"http://docs.oracle.com/javase/tutorial/java/generics/wildcardGuidelines.html\">通配符使用指南</a> 一节提供关于什么时候使用上限通配符和什么时候使用下限通配符的指南。</p>\n<h3 id=\"通配符和子类型\"><a href=\"#通配符和子类型\" class=\"headerlink\" title=\"通配符和子类型\"></a>通配符和子类型</h3><p>像<a href=\"http://docs.oracle.com/javase/tutorial/java/generics/inheritance.html\">范型、继承和子类型</a>中描述的，范型类或接口不相关，仅仅因为它们的类型间有关系。然而，可以使用通配符来创建范型类或接口间的关系。</p>\n<p>给出下面两个常规（非范型）类：</p>\n<pre><code>class A &#123; /* ... */ &#125;\nclass B extends A &#123; /* ... */ &#125;\n</code></pre>\n<p>有理由写下面的代码：</p>\n<pre><code>B b = new B();\nA a = b;\n</code></pre>\n<p>这个例子显示常规类的继承遵循这样的子类型规则：类 B 是类 A 的一个子类型，如果 B 扩展 A。这个规则不适用于范型类型：</p>\n<pre><code>List&lt;B&gt; lb = new ArrayList&lt;&gt;();\nList&lt;A&gt; la = lb;   // compile-time error\n</code></pre>\n<p>给定 Integer 是 Number 的一个子类型，List<Integer> 和 List<Number> 之间是什么关系？</p>\n<p>公共的父类是 List&lt;?&gt;。<img src=\"/uploads/20161203/generics-listParent.gif\" alt=\"generics-listParent\"></p>\n<p>虽然 Integer 是 Number 的一个子类型，List<Integer> 不是 List<Number> 的一个子类型，并且事实上，这两个类型没有关系。List<Number> 和 List<Integer> 的公共父类是 List&lt;?&gt;。</p>\n<p>为了在这些类间创建关系，因此代码可以通过 List<Integer> 的元素访问 Number 的方法，那么使用上限通配符：</p>\n<pre><code>List&lt;? extends Integer&gt; intList = new ArrayList&lt;&gt;();\nList&lt;? extends Number&gt;  numList = intList;  // OK. List&lt;? extends Integer&gt; is a subtype of List&lt;? extends Number&gt;\n</code></pre>\n<p>因为 Integer 是 Number 的一个子类型，且 numList 是 Number 对象的一个列表，现在 intList（Integer 对象的列表）和 numList 之间存在关系了。下面的图片展示了几种用上限通配符和下限通配符声明的 List 类之间的关系。</p>\n<p>几种范型 List 类声明的层次。<img src=\"/uploads/20161203/generics-wildcardSubtyping.gif\" alt=\"generics-wildcardSubtyping\"></p>\n<p><a href=\"http://docs.oracle.com/javase/tutorial/java/generics/wildcardGuidelines.html\">通配符使用指南</a> 一节有更多关于使用上限和下限通配符结果的信息。</p>\n<h3 id=\"通配符匹配和辅助方法\"><a href=\"#通配符匹配和辅助方法\" class=\"headerlink\" title=\"通配符匹配和辅助方法\"></a>通配符匹配和辅助方法</h3><p>在一些场景中，编译器推断通配符的类型。例如，一个列表可能被定义为 List&lt;?&gt;，但是当评估一个表达式时，编译器从代码中推断一个特定的类型。这种情况被称为通配符匹配。</p>\n<p>在大多数情况下，你不需要担心通配符匹配，除了当你看到一个错误消息包含短语“捕捉“。</p>\n<p><a href=\"http://docs.oracle.com/javase/tutorial/displayCode.html?code=http://docs.oracle.com/javase/tutorial/java/generics/examples/WildcardError.java\">WildcardError</a> 例子当编译时产生一个匹配错误：</p>\n<pre><code>import java.util.List;\n\npublic class WildcardError &#123;\n\n  void foo(List&lt;?&gt; i) &#123;\n    i.set(0, i.get(0));\n  &#125;\n&#125;\n</code></pre>\n<p>在这个例子中，编译器处理 i 输入参数为类型 Object。当 foo 方法调用 <a href=\"https://docs.oracle.com/javase/8/docs/api/java/util/List.html#set-int-E-\">List.set(int, E)</a>，编译器不能确定被插入列表中的对象的类型，并且会产生一个错误。当这种类型错误发生时，它通常意味着编译器确信你赋值错误类型给一个变量。范型被添加到 Java 语言中就是因为这个原因 - 在编译期增强类型安全。</p>\n<p>通过 Oracle 的 JDK 7 javac 实现编译 WildcardError 例子时产生下面的错误：</p>\n<pre><code>WildcardError.java:6: error: method set in interface List&lt;E&gt; cannot be applied to given types;\n    i.set(0, i.get(0));\n     ^\n  required: int,CAP#1\n  found: int,Object\n  reason: actual argument Object cannot be converted to CAP#1 by method invocation conversion\n  where E is a type-variable:\n    E extends Object declared in interface List\n  where CAP#1 is a fresh type-variable:\n    CAP#1 extends Object from capture of ?\n1 error\n</code></pre>\n<p>在这个例子中，代码尝试执行一个安全操作，那么如何解决编译器错误呢？可以通过写一个匹配通配符的私有辅助方法来解决这个问题。在这个案例中，你可以通过创建私有辅助方法 fooHelper 解决这个问题，像 <a href=\"http://docs.oracle.com/javase/tutorial/displayCode.html?code=http://docs.oracle.com/javase/tutorial/java/generics/examples/WildcardFixed.java\">WildcardFixed</a> 中显示的：</p>\n<pre><code>public class WildcardFixed &#123;\n\n  void foo(List&lt;?&gt; i) &#123;\n    fooHelper(i);\n  &#125;\n\n\n  // Helper method created so that the wildcard can be captured\n  // through type inference.\n  private &lt;T&gt; void fooHelper(List&lt;T&gt; l) &#123;\n    l.set(0, l.get(0));\n  &#125;\n\n&#125;\n</code></pre>\n<p>感谢辅助方法，编译器使用推断确定 T 是 CAP#1，在调用中的匹配变量。现在这个例子编译成功。</p>\n<p>按照惯例，辅助方法一般命名为<em>原方法名Helper</em>。</p>\n<p>现在考虑一个更复杂的例子，<a href=\"http://docs.oracle.com/javase/tutorial/displayCode.html?code=http://docs.oracle.com/javase/tutorial/java/generics/examples/WildcardErrorBad.java\">WildcardErrorBad</a>：</p>\n<pre><code>import java.util.List;\n\npublic class WildcardErrorBad &#123;\n\n  void swapFirst(List&lt;? extends Number&gt; l1, List&lt;? extends Number&gt; l2) &#123;\n    Number temp = l1.get(0);\n    l1.set(0, l2.get(0)); // expected a CAP#1 extends Number,\n                          // got a CAP#2 extends Number;\n                          // same bound, but different types\n    l2.set(0, temp);      // expected a CAP#1 extends Number,\n                          // got a Number\n  &#125;\n&#125;\n</code></pre>\n<p>在这个例子中，代码尝试一个不安全的操作。例如，考虑下面 swapFirst 方法的调用：</p>\n<pre><code>List&lt;Integer&gt; li = Arrays.asList(1, 2, 3);\nList&lt;Double&gt;  ld = Arrays.asList(10.10, 20.20, 30.30);\nswapFirst(li, ld);\n</code></pre>\n<p>因为 List<Integer> 和 List<Double> 都符合 List&lt;? extends Number&gt; 的标准，明显不正确的是取 Integer 列表中的一个值并尝试放入 Double 值的列表中。</p>\n<p>用 Oracle JDK javac 编译器编译会生成下面的错误：</p>\n<pre><code>WildcardErrorBad.java:7: error: method set in interface List&lt;E&gt; cannot be applied to given types;\n      l1.set(0, l2.get(0)); // expected a CAP#1 extends Number,\n        ^\n  required: int,CAP#1\n  found: int,Number\n  reason: actual argument Number cannot be converted to CAP#1 by method invocation conversion\n  where E is a type-variable:\n    E extends Object declared in interface List\n  where CAP#1 is a fresh type-variable:\n    CAP#1 extends Number from capture of ? extends Number\nWildcardErrorBad.java:10: error: method set in interface List&lt;E&gt; cannot be applied to given types;\n      l2.set(0, temp);      // expected a CAP#1 extends Number,\n        ^\n  required: int,CAP#1\n  found: int,Number\n  reason: actual argument Number cannot be converted to CAP#1 by method invocation conversion\n  where E is a type-variable:\n    E extends Object declared in interface List\n  where CAP#1 is a fresh type-variable:\n    CAP#1 extends Number from capture of ? extends Number\nWildcardErrorBad.java:15: error: method set in interface List&lt;E&gt; cannot be applied to given types;\n        i.set(0, i.get(0));\n         ^\n  required: int,CAP#1\n  found: int,Object\n  reason: actual argument Object cannot be converted to CAP#1 by method invocation conversion\n  where E is a type-variable:\n    E extends Object declared in interface List\n  where CAP#1 is a fresh type-variable:\n    CAP#1 extends Object from capture of ?\n3 errors\n</code></pre>\n<p>针对这个问题没有辅助方法可以使用，因为代码是基础错误。</p>\n<h3 id=\"通配符使用指\"><a href=\"#通配符使用指\" class=\"headerlink\" title=\"通配符使用指\"></a>通配符使用指</h3><p>当学习使用范型编程时一个混乱的方面是确定何时使用上限通配符和何时使用下限通配符。本页提供了在设计代码时遵循的一些指导方针。</p>\n<p>为了这个讨论的目的，考虑变量提供两个功能之一是有帮助的：</p>\n<p><strong>“In”变量</strong></p>\n<p>一个“in”变量提供数据给代码。想象一个有两个参数的 copy 方法：copy(src, dest)。src 参数提供被拷贝的数据，因此它是“in”参数。</p>\n<p><strong>“Out”变量</strong></p>\n<p>一个“Out”变量保持数据为在其他地方使用。在 copy 的例子中，copy(src, dest)，dest 参数接收数据，因此它是“out”参数。</p>\n<p>当然，一些变量被同时用作“in”和“out”目的 - 这种情况也在指导方针中。</p>\n<p>可以使用“in”和“out”准则，当决定是否使用通配符及什么类型通配符是合适的。下面的列表提供了遵循的指南：</p>\n<p><strong>通配符指南</strong></p>\n<ul>\n<li>一个“in”变量用上限通配符被定义，用 extends 关键字。</li>\n<li>一个“out”变量用下限通配符被定义，用 super 关键字。</li>\n<li>在“in”变量可以用定义在 Object 类中的方法被访问的场景，使用无界通配符。</li>\n<li>在代码需要访问同时作为“in”和“out”变量的场景，不要使用通配符。</li>\n</ul>\n<p>这些指南不适用于方法的返回类型。使用通配符作为返回类型应该避免，因为它强制开发人员使用代码处理通配符。</p>\n<p>通过 List&lt;? extends …&gt; 定义的列表可以非正式的被认为是只读的，但这不是一个严格的保证。假设有下面两个类：</p>\n<pre><code>class NaturalNumber &#123;\n\n  private int i;\n\n  public NaturalNumber(int i) &#123; this.i = i; &#125;\n  // ...\n&#125;\n\nclass EvenNumber extends NaturalNumber &#123;\n\n  public EvenNumber(int i) &#123; super(i); &#125;\n  // ...\n&#125;\n</code></pre>\n<p>考虑下面的代码：</p>\n<pre><code>List&lt;EvenNumber&gt; le = new ArrayList&lt;&gt;();\nList&lt;? extends NaturalNumber&gt; ln = le;\nln.add(new NaturalNumber(35));  // compile-time error\n</code></pre>\n<p>因为 List<EvenNumber> 是 List&lt;? extends NaturalNumber&gt; 的一个子类型，可以赋值 le 给 ln。但是不能用 ln 添加一个自然数到偶数列表中。下面对这个列表的操作是可能的：</p>\n<ul>\n<li>可以添加 null。</li>\n<li>可以调用 clear。</li>\n<li>可以获取迭代器且调用 remove。</li>\n<li>可以匹配通配符和写从列表中读取的元素。</li>\n</ul>\n<p>可以看到通过 List&lt;? extends NaturalNumber&gt; 定义的列表严格意义上不是只读的，但可以这样认为，因为不能存储一个新的元素或改变列表中已经存在的元素。</p>\n","site":{"data":{}},"excerpt":"<p>在范型代码中，问好（?），被叫做通配符，表示一个未知的类型。通配符可以被用在多种情况：作为一个参数的类型、域或本地变量；有时作为返回类型（虽然编程的最佳实践是更明确）。通配符永远不会被用作范型方法调用的类型参数、范型类实例化的创建或者超类型。</p>","more":"<p>下面的章节更细致的讨论通配符，包括上限通配符、下限通配符和通配符匹配。</p>\n<h3 id=\"上限通配符\"><a href=\"#上限通配符\" class=\"headerlink\" title=\"上限通配符\"></a>上限通配符</h3><p>可以用上限通配符可以放宽对变量的限制。例如，你想写一个工作于 List<Integer>、List<Double> 和 List<Number> 的方法；你可以通过使用上限通配符实现。</p>\n<p>声明一个上限通配符，使用通配符字符（‘?‘），后面跟着 extends 关键字，随后是它的上限。注意，关于这点，extends 被用作一般意义上的“extends”（对类而言）或者“implements“（对接口而言）。</p>\n<p>为了写工作与 Number 和 Number 子类型（例如：Integer、Double 和 Float）列表的方法，你需要指定 List&lt;? extends Number&gt;。List<Number> 比 List&lt;? extends Number&gt; 更加严格，因为前者只匹配 Number 类型的列表，然而后者匹配 Number 或者它的子类的列表。</p>\n<p>考虑下面的 process 方法：</p>\n<pre><code>public static void process(List&lt;? extends Foo&gt; list) &#123; /* ... */ &#125;\n</code></pre>\n<p>上限通配符，&lt;? extends Foo&gt;，这里 Foo 是任意匹配 Foo 和任意 Foo 的子类型。process 方法可以作为类型 Foo 访问列表元素：</p>\n<pre><code>public static void process(List&lt;? extends Foo&gt; list) &#123;\n  for (Foo elem : list) &#123;\n    // ...\n  &#125;\n&#125;\n</code></pre>\n<p>在 foreach 语句中，elem 变量遍历列表中的每个元素。在 Foo 类中定义的所有方法可用于 elem。</p>\n<p>sumOfList 方法返回在列表中数字的和：</p>\n<pre><code>public static double sumOfList(List&lt;? extends Number&gt; list) &#123;\n  double s = 0.0;\n  for (Number n : list)\n    s += n.doubleValue();\n  return s;\n&#125;\n</code></pre>\n<p>下面的代码，使用 Integer 对象的列表，打印 sum = 6.0：</p>\n<pre><code>List&lt;Integer&gt; li = Arrays.asList(1, 2, 3);\nSystem.out.println(&quot;sum = &quot; + sumOfList(li));\n</code></pre>\n<p>Double 数值的列表可以使用同样的 sumOfList 方法。下面的代码打印 sum = 7.0：</p>\n<p>List<Double> ld = Arrays.asList(1.2, 2.3, 3.5);<br>System.out.println(“sum = “ + sumOfList(ld));</p>\n<h3 id=\"无界通配符\"><a href=\"#无界通配符\" class=\"headerlink\" title=\"无界通配符\"></a>无界通配符</h3><p>无界通配符类型用通配符字符（?）被指定，例如，List&lt;?&gt;。这被叫做一个未知类型的列表。有两种情况无界通配符是有用的方法：</p>\n<ul>\n<li>如果您正在编写一个可以使用 Object 类中提供的功能来实现的方法。</li>\n<li>当代码使用不依赖类型参数的泛型类中的方法时。例如，List.size 或 List.clear。事实上，Class&lt;?&gt; 如此频繁的被使用，因为 Class<T> 中的大部分方法不依赖 T。</li>\n</ul>\n<p>考虑下面的方法，printList：</p>\n<pre><code>public static void printList(List&lt;Object&gt; list) &#123;\n  for (Object elem : list)\n    System.out.println(elem + &quot; &quot;);\n  System.out.println();\n&#125;\n</code></pre>\n<p>printList 的目标是打印任何类型的列表，但是这个目标却不能实现 - 它只能打印 Object 的列表实例；它不能打印 List<Integer>、List<String>、List<Double> 等，因为它们不是 List<Object> 的子类型。为了写一个范型 printList 方法，使用 List&lt;?&gt;：</p>\n<pre><code>public static void printList(List&lt;?&gt; list) &#123;\n  for (Object elem: list)\n    System.out.print(elem + &quot; &quot;);\n  System.out.println();\n&#125;\n</code></pre>\n<p>因为对于任何具体类型 A，List<A> 是 List&lt;?&gt; 的一个子类型，你可以使用 printList 打印任何类型的列表：</p>\n<pre><code>List&lt;Integer&gt; li = Arrays.asList(1, 2, 3);\nList&lt;String&gt;  ls = Arrays.asList(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;);\nprintList(li);\nprintList(ls);\n</code></pre>\n<blockquote>\n<p>备注：<a href=\"https://docs.oracle.com/javase/8/docs/api/java/util/Arrays.html#asList-T...-\">Arrays.asList</a> 在这节课中的例子中被用到。这个静态工厂方法转变指定的数组并返回一个固定大小的列表。</p>\n</blockquote>\n<p>注意到 List<Object> 和 List<?> 不是一样的很重要。你可以插入一个 Object，或 Object 的任意子类型，到 List<Object>。但是你只能插入 null 到 List<?>。<a href=\"http://docs.oracle.com/javase/tutorial/java/generics/wildcardGuidelines.html\">通配符使用指南</a> 一节有更多信息关于如何确定在一个给定的情况下应该用什么样的通配符，如果有的话。</p>\n<h3 id=\"下限通配符\"><a href=\"#下限通配符\" class=\"headerlink\" title=\"下限通配符\"></a>下限通配符</h3><p><a href=\"http://docs.oracle.com/javase/tutorial/java/generics/upperBounded.html\">上限通配符</a>一节展示了上限通配符限制一个未知类型到一个指定类型或这个类型的子类型，并且使用 extends 关键字来表示。同样的方法，下限通配符限制未知类型为一个指定的类型或这个类型的超类型。</p>\n<p>下限通配符用通配符字符（?）表示，跟着 super 关键字，后面跟着它的下限：&lt;? super A&gt;。</p>\n<blockquote>\n<p>注意：你可以为一个通配符指定上限，或者你可以下限，但是不能同时指定。</p>\n</blockquote>\n<p>你想写一个将 Integer 对象放入列表的方法。为了最大化灵活性，你会希望对 List<Integer>、List<Number> 和 List<Object> 都可用 - 任何可保存 Integer 值的。</p>\n<p>为了写可用于 Integer 和 Integer 超类型（如：Integer、Number 和 Object）的列表的方法，你应该指定 List&lt;? super Integer&gt;。List<Integer> 比 List&lt;? super Integer&gt; 限制更严格，因为前者只匹配类型 Integer 的列表，然而后者匹配任何 Integer 超类的列表。</p>\n<p>下面的代码添加数字 1 到 10 到一个列表的末尾：</p>\n<pre><code>public static void addNumbers(List&lt;? super Integer&gt; list) &#123;\n  for (int i = 1; i &lt;= 10; i++) &#123;\n    list.add(i);\n  &#125;\n&#125;\n</code></pre>\n<p><a href=\"http://docs.oracle.com/javase/tutorial/java/generics/wildcardGuidelines.html\">通配符使用指南</a> 一节提供关于什么时候使用上限通配符和什么时候使用下限通配符的指南。</p>\n<h3 id=\"通配符和子类型\"><a href=\"#通配符和子类型\" class=\"headerlink\" title=\"通配符和子类型\"></a>通配符和子类型</h3><p>像<a href=\"http://docs.oracle.com/javase/tutorial/java/generics/inheritance.html\">范型、继承和子类型</a>中描述的，范型类或接口不相关，仅仅因为它们的类型间有关系。然而，可以使用通配符来创建范型类或接口间的关系。</p>\n<p>给出下面两个常规（非范型）类：</p>\n<pre><code>class A &#123; /* ... */ &#125;\nclass B extends A &#123; /* ... */ &#125;\n</code></pre>\n<p>有理由写下面的代码：</p>\n<pre><code>B b = new B();\nA a = b;\n</code></pre>\n<p>这个例子显示常规类的继承遵循这样的子类型规则：类 B 是类 A 的一个子类型，如果 B 扩展 A。这个规则不适用于范型类型：</p>\n<pre><code>List&lt;B&gt; lb = new ArrayList&lt;&gt;();\nList&lt;A&gt; la = lb;   // compile-time error\n</code></pre>\n<p>给定 Integer 是 Number 的一个子类型，List<Integer> 和 List<Number> 之间是什么关系？</p>\n<p>公共的父类是 List&lt;?&gt;。<img src=\"/uploads/20161203/generics-listParent.gif\" alt=\"generics-listParent\"></p>\n<p>虽然 Integer 是 Number 的一个子类型，List<Integer> 不是 List<Number> 的一个子类型，并且事实上，这两个类型没有关系。List<Number> 和 List<Integer> 的公共父类是 List&lt;?&gt;。</p>\n<p>为了在这些类间创建关系，因此代码可以通过 List<Integer> 的元素访问 Number 的方法，那么使用上限通配符：</p>\n<pre><code>List&lt;? extends Integer&gt; intList = new ArrayList&lt;&gt;();\nList&lt;? extends Number&gt;  numList = intList;  // OK. List&lt;? extends Integer&gt; is a subtype of List&lt;? extends Number&gt;\n</code></pre>\n<p>因为 Integer 是 Number 的一个子类型，且 numList 是 Number 对象的一个列表，现在 intList（Integer 对象的列表）和 numList 之间存在关系了。下面的图片展示了几种用上限通配符和下限通配符声明的 List 类之间的关系。</p>\n<p>几种范型 List 类声明的层次。<img src=\"/uploads/20161203/generics-wildcardSubtyping.gif\" alt=\"generics-wildcardSubtyping\"></p>\n<p><a href=\"http://docs.oracle.com/javase/tutorial/java/generics/wildcardGuidelines.html\">通配符使用指南</a> 一节有更多关于使用上限和下限通配符结果的信息。</p>\n<h3 id=\"通配符匹配和辅助方法\"><a href=\"#通配符匹配和辅助方法\" class=\"headerlink\" title=\"通配符匹配和辅助方法\"></a>通配符匹配和辅助方法</h3><p>在一些场景中，编译器推断通配符的类型。例如，一个列表可能被定义为 List&lt;?&gt;，但是当评估一个表达式时，编译器从代码中推断一个特定的类型。这种情况被称为通配符匹配。</p>\n<p>在大多数情况下，你不需要担心通配符匹配，除了当你看到一个错误消息包含短语“捕捉“。</p>\n<p><a href=\"http://docs.oracle.com/javase/tutorial/displayCode.html?code=http://docs.oracle.com/javase/tutorial/java/generics/examples/WildcardError.java\">WildcardError</a> 例子当编译时产生一个匹配错误：</p>\n<pre><code>import java.util.List;\n\npublic class WildcardError &#123;\n\n  void foo(List&lt;?&gt; i) &#123;\n    i.set(0, i.get(0));\n  &#125;\n&#125;\n</code></pre>\n<p>在这个例子中，编译器处理 i 输入参数为类型 Object。当 foo 方法调用 <a href=\"https://docs.oracle.com/javase/8/docs/api/java/util/List.html#set-int-E-\">List.set(int, E)</a>，编译器不能确定被插入列表中的对象的类型，并且会产生一个错误。当这种类型错误发生时，它通常意味着编译器确信你赋值错误类型给一个变量。范型被添加到 Java 语言中就是因为这个原因 - 在编译期增强类型安全。</p>\n<p>通过 Oracle 的 JDK 7 javac 实现编译 WildcardError 例子时产生下面的错误：</p>\n<pre><code>WildcardError.java:6: error: method set in interface List&lt;E&gt; cannot be applied to given types;\n    i.set(0, i.get(0));\n     ^\n  required: int,CAP#1\n  found: int,Object\n  reason: actual argument Object cannot be converted to CAP#1 by method invocation conversion\n  where E is a type-variable:\n    E extends Object declared in interface List\n  where CAP#1 is a fresh type-variable:\n    CAP#1 extends Object from capture of ?\n1 error\n</code></pre>\n<p>在这个例子中，代码尝试执行一个安全操作，那么如何解决编译器错误呢？可以通过写一个匹配通配符的私有辅助方法来解决这个问题。在这个案例中，你可以通过创建私有辅助方法 fooHelper 解决这个问题，像 <a href=\"http://docs.oracle.com/javase/tutorial/displayCode.html?code=http://docs.oracle.com/javase/tutorial/java/generics/examples/WildcardFixed.java\">WildcardFixed</a> 中显示的：</p>\n<pre><code>public class WildcardFixed &#123;\n\n  void foo(List&lt;?&gt; i) &#123;\n    fooHelper(i);\n  &#125;\n\n\n  // Helper method created so that the wildcard can be captured\n  // through type inference.\n  private &lt;T&gt; void fooHelper(List&lt;T&gt; l) &#123;\n    l.set(0, l.get(0));\n  &#125;\n\n&#125;\n</code></pre>\n<p>感谢辅助方法，编译器使用推断确定 T 是 CAP#1，在调用中的匹配变量。现在这个例子编译成功。</p>\n<p>按照惯例，辅助方法一般命名为<em>原方法名Helper</em>。</p>\n<p>现在考虑一个更复杂的例子，<a href=\"http://docs.oracle.com/javase/tutorial/displayCode.html?code=http://docs.oracle.com/javase/tutorial/java/generics/examples/WildcardErrorBad.java\">WildcardErrorBad</a>：</p>\n<pre><code>import java.util.List;\n\npublic class WildcardErrorBad &#123;\n\n  void swapFirst(List&lt;? extends Number&gt; l1, List&lt;? extends Number&gt; l2) &#123;\n    Number temp = l1.get(0);\n    l1.set(0, l2.get(0)); // expected a CAP#1 extends Number,\n                          // got a CAP#2 extends Number;\n                          // same bound, but different types\n    l2.set(0, temp);      // expected a CAP#1 extends Number,\n                          // got a Number\n  &#125;\n&#125;\n</code></pre>\n<p>在这个例子中，代码尝试一个不安全的操作。例如，考虑下面 swapFirst 方法的调用：</p>\n<pre><code>List&lt;Integer&gt; li = Arrays.asList(1, 2, 3);\nList&lt;Double&gt;  ld = Arrays.asList(10.10, 20.20, 30.30);\nswapFirst(li, ld);\n</code></pre>\n<p>因为 List<Integer> 和 List<Double> 都符合 List&lt;? extends Number&gt; 的标准，明显不正确的是取 Integer 列表中的一个值并尝试放入 Double 值的列表中。</p>\n<p>用 Oracle JDK javac 编译器编译会生成下面的错误：</p>\n<pre><code>WildcardErrorBad.java:7: error: method set in interface List&lt;E&gt; cannot be applied to given types;\n      l1.set(0, l2.get(0)); // expected a CAP#1 extends Number,\n        ^\n  required: int,CAP#1\n  found: int,Number\n  reason: actual argument Number cannot be converted to CAP#1 by method invocation conversion\n  where E is a type-variable:\n    E extends Object declared in interface List\n  where CAP#1 is a fresh type-variable:\n    CAP#1 extends Number from capture of ? extends Number\nWildcardErrorBad.java:10: error: method set in interface List&lt;E&gt; cannot be applied to given types;\n      l2.set(0, temp);      // expected a CAP#1 extends Number,\n        ^\n  required: int,CAP#1\n  found: int,Number\n  reason: actual argument Number cannot be converted to CAP#1 by method invocation conversion\n  where E is a type-variable:\n    E extends Object declared in interface List\n  where CAP#1 is a fresh type-variable:\n    CAP#1 extends Number from capture of ? extends Number\nWildcardErrorBad.java:15: error: method set in interface List&lt;E&gt; cannot be applied to given types;\n        i.set(0, i.get(0));\n         ^\n  required: int,CAP#1\n  found: int,Object\n  reason: actual argument Object cannot be converted to CAP#1 by method invocation conversion\n  where E is a type-variable:\n    E extends Object declared in interface List\n  where CAP#1 is a fresh type-variable:\n    CAP#1 extends Object from capture of ?\n3 errors\n</code></pre>\n<p>针对这个问题没有辅助方法可以使用，因为代码是基础错误。</p>\n<h3 id=\"通配符使用指\"><a href=\"#通配符使用指\" class=\"headerlink\" title=\"通配符使用指\"></a>通配符使用指</h3><p>当学习使用范型编程时一个混乱的方面是确定何时使用上限通配符和何时使用下限通配符。本页提供了在设计代码时遵循的一些指导方针。</p>\n<p>为了这个讨论的目的，考虑变量提供两个功能之一是有帮助的：</p>\n<p><strong>“In”变量</strong></p>\n<p>一个“in”变量提供数据给代码。想象一个有两个参数的 copy 方法：copy(src, dest)。src 参数提供被拷贝的数据，因此它是“in”参数。</p>\n<p><strong>“Out”变量</strong></p>\n<p>一个“Out”变量保持数据为在其他地方使用。在 copy 的例子中，copy(src, dest)，dest 参数接收数据，因此它是“out”参数。</p>\n<p>当然，一些变量被同时用作“in”和“out”目的 - 这种情况也在指导方针中。</p>\n<p>可以使用“in”和“out”准则，当决定是否使用通配符及什么类型通配符是合适的。下面的列表提供了遵循的指南：</p>\n<p><strong>通配符指南</strong></p>\n<ul>\n<li>一个“in”变量用上限通配符被定义，用 extends 关键字。</li>\n<li>一个“out”变量用下限通配符被定义，用 super 关键字。</li>\n<li>在“in”变量可以用定义在 Object 类中的方法被访问的场景，使用无界通配符。</li>\n<li>在代码需要访问同时作为“in”和“out”变量的场景，不要使用通配符。</li>\n</ul>\n<p>这些指南不适用于方法的返回类型。使用通配符作为返回类型应该避免，因为它强制开发人员使用代码处理通配符。</p>\n<p>通过 List&lt;? extends …&gt; 定义的列表可以非正式的被认为是只读的，但这不是一个严格的保证。假设有下面两个类：</p>\n<pre><code>class NaturalNumber &#123;\n\n  private int i;\n\n  public NaturalNumber(int i) &#123; this.i = i; &#125;\n  // ...\n&#125;\n\nclass EvenNumber extends NaturalNumber &#123;\n\n  public EvenNumber(int i) &#123; super(i); &#125;\n  // ...\n&#125;\n</code></pre>\n<p>考虑下面的代码：</p>\n<pre><code>List&lt;EvenNumber&gt; le = new ArrayList&lt;&gt;();\nList&lt;? extends NaturalNumber&gt; ln = le;\nln.add(new NaturalNumber(35));  // compile-time error\n</code></pre>\n<p>因为 List<EvenNumber> 是 List&lt;? extends NaturalNumber&gt; 的一个子类型，可以赋值 le 给 ln。但是不能用 ln 添加一个自然数到偶数列表中。下面对这个列表的操作是可能的：</p>\n<ul>\n<li>可以添加 null。</li>\n<li>可以调用 clear。</li>\n<li>可以获取迭代器且调用 remove。</li>\n<li>可以匹配通配符和写从列表中读取的元素。</li>\n</ul>\n<p>可以看到通过 List&lt;? extends NaturalNumber&gt; 定义的列表严格意义上不是只读的，但可以这样认为，因为不能存储一个新的元素或改变列表中已经存在的元素。</p>"},{"title":"范型（四）：范型类型之原始类型","date":"2016-11-26T14:34:59.000Z","_content":"\n原始类型是不带有任何类型参数的范型类或接口的名称。例如，Box 范型类：\n\n    public class Box<T> {\n      public void set(T t) { /* ... */ }\n      // ...\n    }\n\n为了创建 Box<T> 的参数化类型，为正式的类型参数 T 提供一个实际的类型参数：\n\n    Box<Integer> intBox = new Box<>();\n\n如果实际的类型参数被省略，就创建了 Box<T> 的一个原始类型：\n\n    Box rawBox = new Box();\n\n因此，Box 是范型类型 Box<T> 的原始类型。然而，一个非范型类或接口类型不是一个原始类型。\n\n<!-- more -->\n\n原始类型在遗留的代码中是显而易见的，因为很多 API 类（像 Collections 类）在 JDK 5.0 以前不是范型类。当使用原始类型时，实际上获得范型前的行为 - Box 提供给你一些 Object。为了向后兼容，将一个参数化的类型赋值给它的原始类型是允许的：\n\n    Box<String> stringBox = new Box<>();\n    Box rawBox = stringBox;               // OK\n\n但是如果你赋值一个原始类型给一个参数化的类型，将得到一个警告：\n\n    Box rawBox = new Box();           // rawBox is a raw type of Box<T>\n    Box<Integer> intBox = rawBox;     // warning: unchecked conversion\n\n如果使用原始类型来调用在对应的范型类型中定义的范型方法也会得到一个警告：\n\n    Box<String> stringBox = new Box<>();\n    Box rawBox = stringBox;\n    rawBox.set(8);  // warning: unchecked invocation to set(T)\n\n这个警告显示原始类型忽略了范型类型检查，推迟不安全代码的隐患到运行时。因此，应该避免使用原始类型。\n\n[类型擦除](http://docs.oracle.com/javase/tutorial/java/generics/erasure.html) 一节有关于 Java 编译器如何使用原始类型的更多信息。\n\n#### 未检查的错误信息\n\n像前面提到的，当遗留代码与范型代码混合使用时，可能遇到像下面的警告信息：\n\n    Note: Example.java uses unchecked or unsafe operations.\n    Note: Recompile with -Xlint:unchecked for details.\n\n当使用一个旧的 API 操作原始类型时这可能发生，像下面示例展示的：\n\n    public class WarningDemo {\n      public static void main(String[] args){\n        Box<Integer> bi;\n        bi = createBox();\n      }\n\n      static Box createBox(){\n        return new Box();\n      }\n    }\n\n术语“unchecked”意思是编译器没有足够类型信息来执行所有必需的类型检查来确保类型安全。“unchecked”警告默认被禁用，虽然编译器给出了一个提示。要看所有“unchecked”警告信息，用 -Xlint:unchecked 重新编译。\n\n用 -Xlint:unchecked 重新编译前面的例子展示下面额外的信息：\n\n    WarningDemo.java:4: warning: [unchecked] unchecked conversion\n    found   : Box\n    required: Box<java.lang.Integer>\n            bi = createBox();\n                      ^\n    1 warning\n\n要完全禁用未检查警告，使用 -Xlint:-unchecked 标识。@SuppressWarnings(\"unchecked\") 注解禁止未检查告警。如果你不熟悉 @SuppressWarnings 语法，参见 [注解](http://docs.oracle.com/javase/tutorial/java/annotations/index.html)。\n","source":"_posts/范型（四）：范型类型之原始类型.md","raw":"title: 范型（四）：范型类型之原始类型\ntags:\n  - Java\ncategories:\n  - 语言\n  - Java\ndate: 2016-11-26 22:34:59\n---\n\n原始类型是不带有任何类型参数的范型类或接口的名称。例如，Box 范型类：\n\n    public class Box<T> {\n      public void set(T t) { /* ... */ }\n      // ...\n    }\n\n为了创建 Box<T> 的参数化类型，为正式的类型参数 T 提供一个实际的类型参数：\n\n    Box<Integer> intBox = new Box<>();\n\n如果实际的类型参数被省略，就创建了 Box<T> 的一个原始类型：\n\n    Box rawBox = new Box();\n\n因此，Box 是范型类型 Box<T> 的原始类型。然而，一个非范型类或接口类型不是一个原始类型。\n\n<!-- more -->\n\n原始类型在遗留的代码中是显而易见的，因为很多 API 类（像 Collections 类）在 JDK 5.0 以前不是范型类。当使用原始类型时，实际上获得范型前的行为 - Box 提供给你一些 Object。为了向后兼容，将一个参数化的类型赋值给它的原始类型是允许的：\n\n    Box<String> stringBox = new Box<>();\n    Box rawBox = stringBox;               // OK\n\n但是如果你赋值一个原始类型给一个参数化的类型，将得到一个警告：\n\n    Box rawBox = new Box();           // rawBox is a raw type of Box<T>\n    Box<Integer> intBox = rawBox;     // warning: unchecked conversion\n\n如果使用原始类型来调用在对应的范型类型中定义的范型方法也会得到一个警告：\n\n    Box<String> stringBox = new Box<>();\n    Box rawBox = stringBox;\n    rawBox.set(8);  // warning: unchecked invocation to set(T)\n\n这个警告显示原始类型忽略了范型类型检查，推迟不安全代码的隐患到运行时。因此，应该避免使用原始类型。\n\n[类型擦除](http://docs.oracle.com/javase/tutorial/java/generics/erasure.html) 一节有关于 Java 编译器如何使用原始类型的更多信息。\n\n#### 未检查的错误信息\n\n像前面提到的，当遗留代码与范型代码混合使用时，可能遇到像下面的警告信息：\n\n    Note: Example.java uses unchecked or unsafe operations.\n    Note: Recompile with -Xlint:unchecked for details.\n\n当使用一个旧的 API 操作原始类型时这可能发生，像下面示例展示的：\n\n    public class WarningDemo {\n      public static void main(String[] args){\n        Box<Integer> bi;\n        bi = createBox();\n      }\n\n      static Box createBox(){\n        return new Box();\n      }\n    }\n\n术语“unchecked”意思是编译器没有足够类型信息来执行所有必需的类型检查来确保类型安全。“unchecked”警告默认被禁用，虽然编译器给出了一个提示。要看所有“unchecked”警告信息，用 -Xlint:unchecked 重新编译。\n\n用 -Xlint:unchecked 重新编译前面的例子展示下面额外的信息：\n\n    WarningDemo.java:4: warning: [unchecked] unchecked conversion\n    found   : Box\n    required: Box<java.lang.Integer>\n            bi = createBox();\n                      ^\n    1 warning\n\n要完全禁用未检查警告，使用 -Xlint:-unchecked 标识。@SuppressWarnings(\"unchecked\") 注解禁止未检查告警。如果你不熟悉 @SuppressWarnings 语法，参见 [注解](http://docs.oracle.com/javase/tutorial/java/annotations/index.html)。\n","slug":"范型（四）：范型类型之原始类型","published":1,"updated":"2021-07-19T16:28:00.316Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmpht700izitd3fkfc3z95","content":"<p>原始类型是不带有任何类型参数的范型类或接口的名称。例如，Box 范型类：</p>\n<pre><code>public class Box&lt;T&gt; &#123;\n  public void set(T t) &#123; /* ... */ &#125;\n  // ...\n&#125;\n</code></pre>\n<p>为了创建 Box<T> 的参数化类型，为正式的类型参数 T 提供一个实际的类型参数：</p>\n<pre><code>Box&lt;Integer&gt; intBox = new Box&lt;&gt;();\n</code></pre>\n<p>如果实际的类型参数被省略，就创建了 Box<T> 的一个原始类型：</p>\n<pre><code>Box rawBox = new Box();\n</code></pre>\n<p>因此，Box 是范型类型 Box<T> 的原始类型。然而，一个非范型类或接口类型不是一个原始类型。</p>\n<span id=\"more\"></span>\n\n<p>原始类型在遗留的代码中是显而易见的，因为很多 API 类（像 Collections 类）在 JDK 5.0 以前不是范型类。当使用原始类型时，实际上获得范型前的行为 - Box 提供给你一些 Object。为了向后兼容，将一个参数化的类型赋值给它的原始类型是允许的：</p>\n<pre><code>Box&lt;String&gt; stringBox = new Box&lt;&gt;();\nBox rawBox = stringBox;               // OK\n</code></pre>\n<p>但是如果你赋值一个原始类型给一个参数化的类型，将得到一个警告：</p>\n<pre><code>Box rawBox = new Box();           // rawBox is a raw type of Box&lt;T&gt;\nBox&lt;Integer&gt; intBox = rawBox;     // warning: unchecked conversion\n</code></pre>\n<p>如果使用原始类型来调用在对应的范型类型中定义的范型方法也会得到一个警告：</p>\n<pre><code>Box&lt;String&gt; stringBox = new Box&lt;&gt;();\nBox rawBox = stringBox;\nrawBox.set(8);  // warning: unchecked invocation to set(T)\n</code></pre>\n<p>这个警告显示原始类型忽略了范型类型检查，推迟不安全代码的隐患到运行时。因此，应该避免使用原始类型。</p>\n<p><a href=\"http://docs.oracle.com/javase/tutorial/java/generics/erasure.html\">类型擦除</a> 一节有关于 Java 编译器如何使用原始类型的更多信息。</p>\n<h4 id=\"未检查的错误信息\"><a href=\"#未检查的错误信息\" class=\"headerlink\" title=\"未检查的错误信息\"></a>未检查的错误信息</h4><p>像前面提到的，当遗留代码与范型代码混合使用时，可能遇到像下面的警告信息：</p>\n<pre><code>Note: Example.java uses unchecked or unsafe operations.\nNote: Recompile with -Xlint:unchecked for details.\n</code></pre>\n<p>当使用一个旧的 API 操作原始类型时这可能发生，像下面示例展示的：</p>\n<pre><code>public class WarningDemo &#123;\n  public static void main(String[] args)&#123;\n    Box&lt;Integer&gt; bi;\n    bi = createBox();\n  &#125;\n\n  static Box createBox()&#123;\n    return new Box();\n  &#125;\n&#125;\n</code></pre>\n<p>术语“unchecked”意思是编译器没有足够类型信息来执行所有必需的类型检查来确保类型安全。“unchecked”警告默认被禁用，虽然编译器给出了一个提示。要看所有“unchecked”警告信息，用 -Xlint:unchecked 重新编译。</p>\n<p>用 -Xlint:unchecked 重新编译前面的例子展示下面额外的信息：</p>\n<pre><code>WarningDemo.java:4: warning: [unchecked] unchecked conversion\nfound   : Box\nrequired: Box&lt;java.lang.Integer&gt;\n        bi = createBox();\n                  ^\n1 warning\n</code></pre>\n<p>要完全禁用未检查警告，使用 -Xlint:-unchecked 标识。@SuppressWarnings(“unchecked”) 注解禁止未检查告警。如果你不熟悉 @SuppressWarnings 语法，参见 <a href=\"http://docs.oracle.com/javase/tutorial/java/annotations/index.html\">注解</a>。</p>\n","site":{"data":{}},"excerpt":"<p>原始类型是不带有任何类型参数的范型类或接口的名称。例如，Box 范型类：</p>\n<pre><code>public class Box&lt;T&gt; &#123;\n  public void set(T t) &#123; /* ... */ &#125;\n  // ...\n&#125;\n</code></pre>\n<p>为了创建 Box<T> 的参数化类型，为正式的类型参数 T 提供一个实际的类型参数：</p>\n<pre><code>Box&lt;Integer&gt; intBox = new Box&lt;&gt;();\n</code></pre>\n<p>如果实际的类型参数被省略，就创建了 Box<T> 的一个原始类型：</p>\n<pre><code>Box rawBox = new Box();\n</code></pre>\n<p>因此，Box 是范型类型 Box<T> 的原始类型。然而，一个非范型类或接口类型不是一个原始类型。</p>","more":"<p>原始类型在遗留的代码中是显而易见的，因为很多 API 类（像 Collections 类）在 JDK 5.0 以前不是范型类。当使用原始类型时，实际上获得范型前的行为 - Box 提供给你一些 Object。为了向后兼容，将一个参数化的类型赋值给它的原始类型是允许的：</p>\n<pre><code>Box&lt;String&gt; stringBox = new Box&lt;&gt;();\nBox rawBox = stringBox;               // OK\n</code></pre>\n<p>但是如果你赋值一个原始类型给一个参数化的类型，将得到一个警告：</p>\n<pre><code>Box rawBox = new Box();           // rawBox is a raw type of Box&lt;T&gt;\nBox&lt;Integer&gt; intBox = rawBox;     // warning: unchecked conversion\n</code></pre>\n<p>如果使用原始类型来调用在对应的范型类型中定义的范型方法也会得到一个警告：</p>\n<pre><code>Box&lt;String&gt; stringBox = new Box&lt;&gt;();\nBox rawBox = stringBox;\nrawBox.set(8);  // warning: unchecked invocation to set(T)\n</code></pre>\n<p>这个警告显示原始类型忽略了范型类型检查，推迟不安全代码的隐患到运行时。因此，应该避免使用原始类型。</p>\n<p><a href=\"http://docs.oracle.com/javase/tutorial/java/generics/erasure.html\">类型擦除</a> 一节有关于 Java 编译器如何使用原始类型的更多信息。</p>\n<h4 id=\"未检查的错误信息\"><a href=\"#未检查的错误信息\" class=\"headerlink\" title=\"未检查的错误信息\"></a>未检查的错误信息</h4><p>像前面提到的，当遗留代码与范型代码混合使用时，可能遇到像下面的警告信息：</p>\n<pre><code>Note: Example.java uses unchecked or unsafe operations.\nNote: Recompile with -Xlint:unchecked for details.\n</code></pre>\n<p>当使用一个旧的 API 操作原始类型时这可能发生，像下面示例展示的：</p>\n<pre><code>public class WarningDemo &#123;\n  public static void main(String[] args)&#123;\n    Box&lt;Integer&gt; bi;\n    bi = createBox();\n  &#125;\n\n  static Box createBox()&#123;\n    return new Box();\n  &#125;\n&#125;\n</code></pre>\n<p>术语“unchecked”意思是编译器没有足够类型信息来执行所有必需的类型检查来确保类型安全。“unchecked”警告默认被禁用，虽然编译器给出了一个提示。要看所有“unchecked”警告信息，用 -Xlint:unchecked 重新编译。</p>\n<p>用 -Xlint:unchecked 重新编译前面的例子展示下面额外的信息：</p>\n<pre><code>WarningDemo.java:4: warning: [unchecked] unchecked conversion\nfound   : Box\nrequired: Box&lt;java.lang.Integer&gt;\n        bi = createBox();\n                  ^\n1 warning\n</code></pre>\n<p>要完全禁用未检查警告，使用 -Xlint:-unchecked 标识。@SuppressWarnings(“unchecked”) 注解禁止未检查告警。如果你不熟悉 @SuppressWarnings 语法，参见 <a href=\"http://docs.oracle.com/javase/tutorial/java/annotations/index.html\">注解</a>。</p>"},{"title":"解决 HTTP POST 请求 Nginx 静态内容 405 错误","date":"2019-04-04T08:09:57.000Z","_content":"\nNginx 是不支持 POST 请求静态内容的，通过 POST 请求时出现以下错误：\n\n<!--more-->\n\n    # curl -d \"a=b\" \"http://192.16.36.15:11013/upgrade\"\n    <html>\n    <head><title>405 Not Allowed</title></head>\n    <body bgcolor=\"white\">\n    <center><h1>405 Not Allowed</h1></center>\n    <hr><center>nginx/1.14.2</center>\n    </body>\n    </html>\n\n解决方法是在 nginx 配置中添加以下配置：\n\n    error_page 405 =200 $uri;\n\n完整配置示例如下：\n\n    server {\n        listen       80;\n        server_name  localhost;\n    \n        location /upgrade {\n            empty_gif;\n            access_log /data/nginx/upgrade.log;\n        }\n    \n        location / {\n            root   /usr/share/nginx/html;\n            index  index.html index.htm;\n        }\n    \n        error_page 405 =200 $uri;\n        error_page   500 502 503 504  /50x.html;\n        location = /50x.html {\n            root   /usr/share/nginx/html;\n        }\n    \n    }","source":"_posts/解决-HTTP-POST-请求-Nginx-静态内容-405-错误.md","raw":"title: 解决 HTTP POST 请求 Nginx 静态内容 405 错误\ndate: 2019-04-04 16:09:57\ntags:\n- Nginx\ncategories:\n- 开发工具\n- Nginx\n---\n\nNginx 是不支持 POST 请求静态内容的，通过 POST 请求时出现以下错误：\n\n<!--more-->\n\n    # curl -d \"a=b\" \"http://192.16.36.15:11013/upgrade\"\n    <html>\n    <head><title>405 Not Allowed</title></head>\n    <body bgcolor=\"white\">\n    <center><h1>405 Not Allowed</h1></center>\n    <hr><center>nginx/1.14.2</center>\n    </body>\n    </html>\n\n解决方法是在 nginx 配置中添加以下配置：\n\n    error_page 405 =200 $uri;\n\n完整配置示例如下：\n\n    server {\n        listen       80;\n        server_name  localhost;\n    \n        location /upgrade {\n            empty_gif;\n            access_log /data/nginx/upgrade.log;\n        }\n    \n        location / {\n            root   /usr/share/nginx/html;\n            index  index.html index.htm;\n        }\n    \n        error_page 405 =200 $uri;\n        error_page   500 502 503 504  /50x.html;\n        location = /50x.html {\n            root   /usr/share/nginx/html;\n        }\n    \n    }","slug":"解决-HTTP-POST-请求-Nginx-静态内容-405-错误","published":1,"updated":"2021-07-19T16:28:00.316Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphtb00j4itd39zw0drlj","content":"<p>Nginx 是不支持 POST 请求静态内容的，通过 POST 请求时出现以下错误：</p>\n<span id=\"more\"></span>\n\n<pre><code># curl -d &quot;a=b&quot; &quot;http://192.16.36.15:11013/upgrade&quot;\n&lt;html&gt;\n&lt;head&gt;&lt;title&gt;405 Not Allowed&lt;/title&gt;&lt;/head&gt;\n&lt;body bgcolor=&quot;white&quot;&gt;\n&lt;center&gt;&lt;h1&gt;405 Not Allowed&lt;/h1&gt;&lt;/center&gt;\n&lt;hr&gt;&lt;center&gt;nginx/1.14.2&lt;/center&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>\n<p>解决方法是在 nginx 配置中添加以下配置：</p>\n<pre><code>error_page 405 =200 $uri;\n</code></pre>\n<p>完整配置示例如下：</p>\n<pre><code>server &#123;\n    listen       80;\n    server_name  localhost;\n\n    location /upgrade &#123;\n        empty_gif;\n        access_log /data/nginx/upgrade.log;\n    &#125;\n\n    location / &#123;\n        root   /usr/share/nginx/html;\n        index  index.html index.htm;\n    &#125;\n\n    error_page 405 =200 $uri;\n    error_page   500 502 503 504  /50x.html;\n    location = /50x.html &#123;\n        root   /usr/share/nginx/html;\n    &#125;\n\n&#125;\n</code></pre>\n","site":{"data":{}},"excerpt":"<p>Nginx 是不支持 POST 请求静态内容的，通过 POST 请求时出现以下错误：</p>","more":"<pre><code># curl -d &quot;a=b&quot; &quot;http://192.16.36.15:11013/upgrade&quot;\n&lt;html&gt;\n&lt;head&gt;&lt;title&gt;405 Not Allowed&lt;/title&gt;&lt;/head&gt;\n&lt;body bgcolor=&quot;white&quot;&gt;\n&lt;center&gt;&lt;h1&gt;405 Not Allowed&lt;/h1&gt;&lt;/center&gt;\n&lt;hr&gt;&lt;center&gt;nginx/1.14.2&lt;/center&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>\n<p>解决方法是在 nginx 配置中添加以下配置：</p>\n<pre><code>error_page 405 =200 $uri;\n</code></pre>\n<p>完整配置示例如下：</p>\n<pre><code>server &#123;\n    listen       80;\n    server_name  localhost;\n\n    location /upgrade &#123;\n        empty_gif;\n        access_log /data/nginx/upgrade.log;\n    &#125;\n\n    location / &#123;\n        root   /usr/share/nginx/html;\n        index  index.html index.htm;\n    &#125;\n\n    error_page 405 =200 $uri;\n    error_page   500 502 503 504  /50x.html;\n    location = /50x.html &#123;\n        root   /usr/share/nginx/html;\n    &#125;\n\n&#125;\n</code></pre>"},{"title":"记录一次 Sqoop 从 MySQL 导入数据到 Hive 问题的排查经过","date":"2018-12-11T11:07:03.000Z","_content":"\n#### 问题描述\n\nMySQL 中原始数据有 790W+ 的记录数，在 Sqoop 抽取作业成功的情况下在 Hive 中只有 500W 左右的记录数。\n\n<!-- more -->\n\n#### 排查过程\n\n##### 数据导入脚本 Log\n\n通过 Log 可以发现以下信息：\n\n1. 该 Sqoop 任务被分解为 4 个 MapTask。\n2. MapTask 执行期间有异常，是网络异常导致 MySQL 连接不成功。\n3. Sqoop 任务对应的 MR 执行过程中总的被调起 9 个 MapTask，其中 3 个失败、2 个被 kill，理论上剩余的 4 个 MapTask 是成功执行的。\n4. Sqoop 导入对应的 MR 只有 MapTask，且 MapTask 的数据记录数为 790W+。所以，单纯看 MR 的输出是正常的。\n5. Sqoop 导入完成后，紧跟着有一个读取 Sqoop 目标表数据的 insert overwrite 的操作。该操作只被分解为 2 个 MapTask，说明原数据文件只有两个块。\n6. 根据以上信息说明 Sqoop 之后确实只生成了 2 个数据文件，有两个文件丢失了。\n\n详细原始 Log 信息见附件：[Sqoop 执行日志](/uploads/20181214/sqoop.txt)\n\n##### 查看 Sqoop 任务对应 MR 的执行日志\n\n根据上面的 Log 中的信息，从 HDFS 上查找对应的日志。Yarn 所有的应用执行日志在 HDFS 的 /data/hadoop/yarn-logs/hadoop/logs/ 目录下。从该目录下查找应用程序 application_1533196506314_4460157 的日志。日志会包含 MR 在各个节点上执行的信息。\n\n从 Log 中发现以下异常信息：\n\n    2018-12-10 00:42:30,595 FATAL [IPC Server handler 17 on 8046] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1533196506314_4460157_m_000001_0 - exited : org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/hive/warehouse/ods.db/bss_customer_fj/_SCRATCH0.3130352759450352/dt=20181209/_temporary/1/_temporary/attempt_1533196506314_4460157_m_000001_0/part-m-00001 (inode 761544109): File does not exist. Holder DFSClient_attempt_1533196506314_4460157_m_000001_0_-1729942809_1 does not have any open files.\n\n所以，怀疑是 MR 在执行结束时，将临时文件移动到正式目录时发生错误。\n\n原始 Log 文件见目录：\n\n[192-168-72-12_27463](/uploads/20181214/application_1533196506314_4460157/192-168-72-12_27463.txt)  \n[192-168-72-24_16310](/uploads/20181214/application_1533196506314_4460157/192-168-72-24_16310.txt)  \n[192-168-72-84_13498](/uploads/20181214/application_1533196506314_4460157/192-168-72-84_13498.txt)  \n[192-168-72-93_53778](/uploads/20181214/application_1533196506314_4460157/192-168-72-93_53778.txt)\n[192-168-72-23_18284](/uploads/20181214/application_1533196506314_4460157/192-168-72-23_18284.txt)  \n[192-168-72-73_2363](/uploads/20181214/application_1533196506314_4460157/192-168-72-73_2363.txt)  \n[192-168-72-88_24481](/uploads/20181214/application_1533196506314_4460157/192-168-72-88_24481.txt)  \n[192-168-72-94_54353](/uploads/20181214/application_1533196506314_4460157/192-168-72-94_54353.txt)\n\n##### 查看 DN Log 信息\n\n根据上面的 Log 信息，发现 DN 节点 192-168-72-24 上的 MapTask 各有以下异常信息。\n\n192-168-72-24 异常信息：\n\n    2018-12-10 00:42:34,410 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/hive/warehouse/ods.db/bss_customer_fj/_SCRATCH0.3130352759450352/dt=20181209/_temporary/1/_temporary/attempt_1533196506314_4460157_m_000000_0/part-m-00000 (inode 761544157): File does not exist. Holder DFSClient_attempt_1533196506314_4460157_m_000000_0_798513081_1 does not have any open files.\n\n所以怀疑是 MapTask 的最后阶段写文件的时候未成功。检查该 DN 节点的 Log 发现以下异常信息：\n\n    2018-12-10 00:42:32,643 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: BlockSender.sendChunks() exception:\n    java.io.IOException: 断开的管道\n            at sun.nio.ch.FileChannelImpl.transferTo0(Native Method)\n            at sun.nio.ch.FileChannelImpl.transferToDirectlyInternal(FileChannelImpl.java:428)\n            at sun.nio.ch.FileChannelImpl.transferToDirectly(FileChannelImpl.java:493)\n            at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:608)\n            at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:223)\n            at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:583)\n            at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:763)\n            at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:710)\n            at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:552)\n            at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:116)\n            at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:71)\n            at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:253)\n            at java.lang.Thread.run(Thread.java:748)\n\n百度之后发现该异常信息跟 DN 的一个配置有关系，具体配置项是 DN 可以同时处理的文件上限。对于老版本配置项名称为“dfs.datanode.max.xcievers”，对于新版本配置项名称改为“dfs.datanode.max.transfer.threads”。该参数的默认值为 4096，所以需要修改为 8192。\n\n##### 查看 NN Log 信息\n\n默认的，Log4j 输出的 NameNode 日志文件只保留最近的 20 个文件。因为 NN 的 Log 信息比较多，20 个文件保存的日志不足 1 天，异常时间的日志已经被冲掉了。\n\n![NameNode Log](/uploads/20181214/namenodeLogs.png)\n\n##### 查看 NN 信息\n\n通过 NN WebUI 查看发现 NN 内存的使用已经 70% 左右，负载已经比较高。\n\n![NameNode Info](/uploads/20181214/namenodeInfo.png)\n\n##### 处理措施\n\n综合以上信息，其实问题发生的根本原因未能查明。基于对 Hadoop 了解的深度、精力及对故障恢复的容忍程度的考虑，待定位根本原因再解决问题的方案不可控。所以，采取以下改进措施：\n\n（1）修改 DN 最大处理文件数量上限至 8192。\n（2）将 NN 内存扩展到 100G。\n（3）修改 Sqoop 源代码，当落地到目标 HDFS 目录下的文件数量与 MapTask 数量不一致时返回错误状态，并由调度系统进行重新抽取。\n","source":"_posts/记录一次-Sqoop-从-MySQL-导入数据到-Hive-问题的排查经过.md","raw":"title: 记录一次 Sqoop 从 MySQL 导入数据到 Hive 问题的排查经过\ndate: 2018-12-11 19:07:03\ntags:\n- Sqoop\n- MySQL\n- Hive\n- 大数据\ncategories:\n- 大数据\n- Sqoop\n---\n\n#### 问题描述\n\nMySQL 中原始数据有 790W+ 的记录数，在 Sqoop 抽取作业成功的情况下在 Hive 中只有 500W 左右的记录数。\n\n<!-- more -->\n\n#### 排查过程\n\n##### 数据导入脚本 Log\n\n通过 Log 可以发现以下信息：\n\n1. 该 Sqoop 任务被分解为 4 个 MapTask。\n2. MapTask 执行期间有异常，是网络异常导致 MySQL 连接不成功。\n3. Sqoop 任务对应的 MR 执行过程中总的被调起 9 个 MapTask，其中 3 个失败、2 个被 kill，理论上剩余的 4 个 MapTask 是成功执行的。\n4. Sqoop 导入对应的 MR 只有 MapTask，且 MapTask 的数据记录数为 790W+。所以，单纯看 MR 的输出是正常的。\n5. Sqoop 导入完成后，紧跟着有一个读取 Sqoop 目标表数据的 insert overwrite 的操作。该操作只被分解为 2 个 MapTask，说明原数据文件只有两个块。\n6. 根据以上信息说明 Sqoop 之后确实只生成了 2 个数据文件，有两个文件丢失了。\n\n详细原始 Log 信息见附件：[Sqoop 执行日志](/uploads/20181214/sqoop.txt)\n\n##### 查看 Sqoop 任务对应 MR 的执行日志\n\n根据上面的 Log 中的信息，从 HDFS 上查找对应的日志。Yarn 所有的应用执行日志在 HDFS 的 /data/hadoop/yarn-logs/hadoop/logs/ 目录下。从该目录下查找应用程序 application_1533196506314_4460157 的日志。日志会包含 MR 在各个节点上执行的信息。\n\n从 Log 中发现以下异常信息：\n\n    2018-12-10 00:42:30,595 FATAL [IPC Server handler 17 on 8046] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1533196506314_4460157_m_000001_0 - exited : org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/hive/warehouse/ods.db/bss_customer_fj/_SCRATCH0.3130352759450352/dt=20181209/_temporary/1/_temporary/attempt_1533196506314_4460157_m_000001_0/part-m-00001 (inode 761544109): File does not exist. Holder DFSClient_attempt_1533196506314_4460157_m_000001_0_-1729942809_1 does not have any open files.\n\n所以，怀疑是 MR 在执行结束时，将临时文件移动到正式目录时发生错误。\n\n原始 Log 文件见目录：\n\n[192-168-72-12_27463](/uploads/20181214/application_1533196506314_4460157/192-168-72-12_27463.txt)  \n[192-168-72-24_16310](/uploads/20181214/application_1533196506314_4460157/192-168-72-24_16310.txt)  \n[192-168-72-84_13498](/uploads/20181214/application_1533196506314_4460157/192-168-72-84_13498.txt)  \n[192-168-72-93_53778](/uploads/20181214/application_1533196506314_4460157/192-168-72-93_53778.txt)\n[192-168-72-23_18284](/uploads/20181214/application_1533196506314_4460157/192-168-72-23_18284.txt)  \n[192-168-72-73_2363](/uploads/20181214/application_1533196506314_4460157/192-168-72-73_2363.txt)  \n[192-168-72-88_24481](/uploads/20181214/application_1533196506314_4460157/192-168-72-88_24481.txt)  \n[192-168-72-94_54353](/uploads/20181214/application_1533196506314_4460157/192-168-72-94_54353.txt)\n\n##### 查看 DN Log 信息\n\n根据上面的 Log 信息，发现 DN 节点 192-168-72-24 上的 MapTask 各有以下异常信息。\n\n192-168-72-24 异常信息：\n\n    2018-12-10 00:42:34,410 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/hive/warehouse/ods.db/bss_customer_fj/_SCRATCH0.3130352759450352/dt=20181209/_temporary/1/_temporary/attempt_1533196506314_4460157_m_000000_0/part-m-00000 (inode 761544157): File does not exist. Holder DFSClient_attempt_1533196506314_4460157_m_000000_0_798513081_1 does not have any open files.\n\n所以怀疑是 MapTask 的最后阶段写文件的时候未成功。检查该 DN 节点的 Log 发现以下异常信息：\n\n    2018-12-10 00:42:32,643 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: BlockSender.sendChunks() exception:\n    java.io.IOException: 断开的管道\n            at sun.nio.ch.FileChannelImpl.transferTo0(Native Method)\n            at sun.nio.ch.FileChannelImpl.transferToDirectlyInternal(FileChannelImpl.java:428)\n            at sun.nio.ch.FileChannelImpl.transferToDirectly(FileChannelImpl.java:493)\n            at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:608)\n            at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:223)\n            at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:583)\n            at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:763)\n            at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:710)\n            at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:552)\n            at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:116)\n            at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:71)\n            at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:253)\n            at java.lang.Thread.run(Thread.java:748)\n\n百度之后发现该异常信息跟 DN 的一个配置有关系，具体配置项是 DN 可以同时处理的文件上限。对于老版本配置项名称为“dfs.datanode.max.xcievers”，对于新版本配置项名称改为“dfs.datanode.max.transfer.threads”。该参数的默认值为 4096，所以需要修改为 8192。\n\n##### 查看 NN Log 信息\n\n默认的，Log4j 输出的 NameNode 日志文件只保留最近的 20 个文件。因为 NN 的 Log 信息比较多，20 个文件保存的日志不足 1 天，异常时间的日志已经被冲掉了。\n\n![NameNode Log](/uploads/20181214/namenodeLogs.png)\n\n##### 查看 NN 信息\n\n通过 NN WebUI 查看发现 NN 内存的使用已经 70% 左右，负载已经比较高。\n\n![NameNode Info](/uploads/20181214/namenodeInfo.png)\n\n##### 处理措施\n\n综合以上信息，其实问题发生的根本原因未能查明。基于对 Hadoop 了解的深度、精力及对故障恢复的容忍程度的考虑，待定位根本原因再解决问题的方案不可控。所以，采取以下改进措施：\n\n（1）修改 DN 最大处理文件数量上限至 8192。\n（2）将 NN 内存扩展到 100G。\n（3）修改 Sqoop 源代码，当落地到目标 HDFS 目录下的文件数量与 MapTask 数量不一致时返回错误状态，并由调度系统进行重新抽取。\n","slug":"记录一次-Sqoop-从-MySQL-导入数据到-Hive-问题的排查经过","published":1,"updated":"2021-07-19T16:28:00.316Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphtc00j7itd3hd50e3o7","content":"<h4 id=\"问题描述\"><a href=\"#问题描述\" class=\"headerlink\" title=\"问题描述\"></a>问题描述</h4><p>MySQL 中原始数据有 790W+ 的记录数，在 Sqoop 抽取作业成功的情况下在 Hive 中只有 500W 左右的记录数。</p>\n<span id=\"more\"></span>\n\n<h4 id=\"排查过程\"><a href=\"#排查过程\" class=\"headerlink\" title=\"排查过程\"></a>排查过程</h4><h5 id=\"数据导入脚本-Log\"><a href=\"#数据导入脚本-Log\" class=\"headerlink\" title=\"数据导入脚本 Log\"></a>数据导入脚本 Log</h5><p>通过 Log 可以发现以下信息：</p>\n<ol>\n<li>该 Sqoop 任务被分解为 4 个 MapTask。</li>\n<li>MapTask 执行期间有异常，是网络异常导致 MySQL 连接不成功。</li>\n<li>Sqoop 任务对应的 MR 执行过程中总的被调起 9 个 MapTask，其中 3 个失败、2 个被 kill，理论上剩余的 4 个 MapTask 是成功执行的。</li>\n<li>Sqoop 导入对应的 MR 只有 MapTask，且 MapTask 的数据记录数为 790W+。所以，单纯看 MR 的输出是正常的。</li>\n<li>Sqoop 导入完成后，紧跟着有一个读取 Sqoop 目标表数据的 insert overwrite 的操作。该操作只被分解为 2 个 MapTask，说明原数据文件只有两个块。</li>\n<li>根据以上信息说明 Sqoop 之后确实只生成了 2 个数据文件，有两个文件丢失了。</li>\n</ol>\n<p>详细原始 Log 信息见附件：<a href=\"/uploads/20181214/sqoop.txt\">Sqoop 执行日志</a></p>\n<h5 id=\"查看-Sqoop-任务对应-MR-的执行日志\"><a href=\"#查看-Sqoop-任务对应-MR-的执行日志\" class=\"headerlink\" title=\"查看 Sqoop 任务对应 MR 的执行日志\"></a>查看 Sqoop 任务对应 MR 的执行日志</h5><p>根据上面的 Log 中的信息，从 HDFS 上查找对应的日志。Yarn 所有的应用执行日志在 HDFS 的 /data/hadoop/yarn-logs/hadoop/logs/ 目录下。从该目录下查找应用程序 application_1533196506314_4460157 的日志。日志会包含 MR 在各个节点上执行的信息。</p>\n<p>从 Log 中发现以下异常信息：</p>\n<pre><code>2018-12-10 00:42:30,595 FATAL [IPC Server handler 17 on 8046] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1533196506314_4460157_m_000001_0 - exited : org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/hive/warehouse/ods.db/bss_customer_fj/_SCRATCH0.3130352759450352/dt=20181209/_temporary/1/_temporary/attempt_1533196506314_4460157_m_000001_0/part-m-00001 (inode 761544109): File does not exist. Holder DFSClient_attempt_1533196506314_4460157_m_000001_0_-1729942809_1 does not have any open files.\n</code></pre>\n<p>所以，怀疑是 MR 在执行结束时，将临时文件移动到正式目录时发生错误。</p>\n<p>原始 Log 文件见目录：</p>\n<p><a href=\"/uploads/20181214/application_1533196506314_4460157/192-168-72-12_27463.txt\">192-168-72-12_27463</a><br><a href=\"/uploads/20181214/application_1533196506314_4460157/192-168-72-24_16310.txt\">192-168-72-24_16310</a><br><a href=\"/uploads/20181214/application_1533196506314_4460157/192-168-72-84_13498.txt\">192-168-72-84_13498</a><br><a href=\"/uploads/20181214/application_1533196506314_4460157/192-168-72-93_53778.txt\">192-168-72-93_53778</a><br><a href=\"/uploads/20181214/application_1533196506314_4460157/192-168-72-23_18284.txt\">192-168-72-23_18284</a><br><a href=\"/uploads/20181214/application_1533196506314_4460157/192-168-72-73_2363.txt\">192-168-72-73_2363</a><br><a href=\"/uploads/20181214/application_1533196506314_4460157/192-168-72-88_24481.txt\">192-168-72-88_24481</a><br><a href=\"/uploads/20181214/application_1533196506314_4460157/192-168-72-94_54353.txt\">192-168-72-94_54353</a></p>\n<h5 id=\"查看-DN-Log-信息\"><a href=\"#查看-DN-Log-信息\" class=\"headerlink\" title=\"查看 DN Log 信息\"></a>查看 DN Log 信息</h5><p>根据上面的 Log 信息，发现 DN 节点 192-168-72-24 上的 MapTask 各有以下异常信息。</p>\n<p>192-168-72-24 异常信息：</p>\n<pre><code>2018-12-10 00:42:34,410 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/hive/warehouse/ods.db/bss_customer_fj/_SCRATCH0.3130352759450352/dt=20181209/_temporary/1/_temporary/attempt_1533196506314_4460157_m_000000_0/part-m-00000 (inode 761544157): File does not exist. Holder DFSClient_attempt_1533196506314_4460157_m_000000_0_798513081_1 does not have any open files.\n</code></pre>\n<p>所以怀疑是 MapTask 的最后阶段写文件的时候未成功。检查该 DN 节点的 Log 发现以下异常信息：</p>\n<pre><code>2018-12-10 00:42:32,643 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: BlockSender.sendChunks() exception:\njava.io.IOException: 断开的管道\n        at sun.nio.ch.FileChannelImpl.transferTo0(Native Method)\n        at sun.nio.ch.FileChannelImpl.transferToDirectlyInternal(FileChannelImpl.java:428)\n        at sun.nio.ch.FileChannelImpl.transferToDirectly(FileChannelImpl.java:493)\n        at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:608)\n        at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:223)\n        at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:583)\n        at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:763)\n        at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:710)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:552)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:116)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:71)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:253)\n        at java.lang.Thread.run(Thread.java:748)\n</code></pre>\n<p>百度之后发现该异常信息跟 DN 的一个配置有关系，具体配置项是 DN 可以同时处理的文件上限。对于老版本配置项名称为“dfs.datanode.max.xcievers”，对于新版本配置项名称改为“dfs.datanode.max.transfer.threads”。该参数的默认值为 4096，所以需要修改为 8192。</p>\n<h5 id=\"查看-NN-Log-信息\"><a href=\"#查看-NN-Log-信息\" class=\"headerlink\" title=\"查看 NN Log 信息\"></a>查看 NN Log 信息</h5><p>默认的，Log4j 输出的 NameNode 日志文件只保留最近的 20 个文件。因为 NN 的 Log 信息比较多，20 个文件保存的日志不足 1 天，异常时间的日志已经被冲掉了。</p>\n<p><img src=\"/uploads/20181214/namenodeLogs.png\" alt=\"NameNode Log\"></p>\n<h5 id=\"查看-NN-信息\"><a href=\"#查看-NN-信息\" class=\"headerlink\" title=\"查看 NN 信息\"></a>查看 NN 信息</h5><p>通过 NN WebUI 查看发现 NN 内存的使用已经 70% 左右，负载已经比较高。</p>\n<p><img src=\"/uploads/20181214/namenodeInfo.png\" alt=\"NameNode Info\"></p>\n<h5 id=\"处理措施\"><a href=\"#处理措施\" class=\"headerlink\" title=\"处理措施\"></a>处理措施</h5><p>综合以上信息，其实问题发生的根本原因未能查明。基于对 Hadoop 了解的深度、精力及对故障恢复的容忍程度的考虑，待定位根本原因再解决问题的方案不可控。所以，采取以下改进措施：</p>\n<p>（1）修改 DN 最大处理文件数量上限至 8192。<br>（2）将 NN 内存扩展到 100G。<br>（3）修改 Sqoop 源代码，当落地到目标 HDFS 目录下的文件数量与 MapTask 数量不一致时返回错误状态，并由调度系统进行重新抽取。</p>\n","site":{"data":{}},"excerpt":"<h4 id=\"问题描述\"><a href=\"#问题描述\" class=\"headerlink\" title=\"问题描述\"></a>问题描述</h4><p>MySQL 中原始数据有 790W+ 的记录数，在 Sqoop 抽取作业成功的情况下在 Hive 中只有 500W 左右的记录数。</p>","more":"<h4 id=\"排查过程\"><a href=\"#排查过程\" class=\"headerlink\" title=\"排查过程\"></a>排查过程</h4><h5 id=\"数据导入脚本-Log\"><a href=\"#数据导入脚本-Log\" class=\"headerlink\" title=\"数据导入脚本 Log\"></a>数据导入脚本 Log</h5><p>通过 Log 可以发现以下信息：</p>\n<ol>\n<li>该 Sqoop 任务被分解为 4 个 MapTask。</li>\n<li>MapTask 执行期间有异常，是网络异常导致 MySQL 连接不成功。</li>\n<li>Sqoop 任务对应的 MR 执行过程中总的被调起 9 个 MapTask，其中 3 个失败、2 个被 kill，理论上剩余的 4 个 MapTask 是成功执行的。</li>\n<li>Sqoop 导入对应的 MR 只有 MapTask，且 MapTask 的数据记录数为 790W+。所以，单纯看 MR 的输出是正常的。</li>\n<li>Sqoop 导入完成后，紧跟着有一个读取 Sqoop 目标表数据的 insert overwrite 的操作。该操作只被分解为 2 个 MapTask，说明原数据文件只有两个块。</li>\n<li>根据以上信息说明 Sqoop 之后确实只生成了 2 个数据文件，有两个文件丢失了。</li>\n</ol>\n<p>详细原始 Log 信息见附件：<a href=\"/uploads/20181214/sqoop.txt\">Sqoop 执行日志</a></p>\n<h5 id=\"查看-Sqoop-任务对应-MR-的执行日志\"><a href=\"#查看-Sqoop-任务对应-MR-的执行日志\" class=\"headerlink\" title=\"查看 Sqoop 任务对应 MR 的执行日志\"></a>查看 Sqoop 任务对应 MR 的执行日志</h5><p>根据上面的 Log 中的信息，从 HDFS 上查找对应的日志。Yarn 所有的应用执行日志在 HDFS 的 /data/hadoop/yarn-logs/hadoop/logs/ 目录下。从该目录下查找应用程序 application_1533196506314_4460157 的日志。日志会包含 MR 在各个节点上执行的信息。</p>\n<p>从 Log 中发现以下异常信息：</p>\n<pre><code>2018-12-10 00:42:30,595 FATAL [IPC Server handler 17 on 8046] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1533196506314_4460157_m_000001_0 - exited : org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/hive/warehouse/ods.db/bss_customer_fj/_SCRATCH0.3130352759450352/dt=20181209/_temporary/1/_temporary/attempt_1533196506314_4460157_m_000001_0/part-m-00001 (inode 761544109): File does not exist. Holder DFSClient_attempt_1533196506314_4460157_m_000001_0_-1729942809_1 does not have any open files.\n</code></pre>\n<p>所以，怀疑是 MR 在执行结束时，将临时文件移动到正式目录时发生错误。</p>\n<p>原始 Log 文件见目录：</p>\n<p><a href=\"/uploads/20181214/application_1533196506314_4460157/192-168-72-12_27463.txt\">192-168-72-12_27463</a><br><a href=\"/uploads/20181214/application_1533196506314_4460157/192-168-72-24_16310.txt\">192-168-72-24_16310</a><br><a href=\"/uploads/20181214/application_1533196506314_4460157/192-168-72-84_13498.txt\">192-168-72-84_13498</a><br><a href=\"/uploads/20181214/application_1533196506314_4460157/192-168-72-93_53778.txt\">192-168-72-93_53778</a><br><a href=\"/uploads/20181214/application_1533196506314_4460157/192-168-72-23_18284.txt\">192-168-72-23_18284</a><br><a href=\"/uploads/20181214/application_1533196506314_4460157/192-168-72-73_2363.txt\">192-168-72-73_2363</a><br><a href=\"/uploads/20181214/application_1533196506314_4460157/192-168-72-88_24481.txt\">192-168-72-88_24481</a><br><a href=\"/uploads/20181214/application_1533196506314_4460157/192-168-72-94_54353.txt\">192-168-72-94_54353</a></p>\n<h5 id=\"查看-DN-Log-信息\"><a href=\"#查看-DN-Log-信息\" class=\"headerlink\" title=\"查看 DN Log 信息\"></a>查看 DN Log 信息</h5><p>根据上面的 Log 信息，发现 DN 节点 192-168-72-24 上的 MapTask 各有以下异常信息。</p>\n<p>192-168-72-24 异常信息：</p>\n<pre><code>2018-12-10 00:42:34,410 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/hive/warehouse/ods.db/bss_customer_fj/_SCRATCH0.3130352759450352/dt=20181209/_temporary/1/_temporary/attempt_1533196506314_4460157_m_000000_0/part-m-00000 (inode 761544157): File does not exist. Holder DFSClient_attempt_1533196506314_4460157_m_000000_0_798513081_1 does not have any open files.\n</code></pre>\n<p>所以怀疑是 MapTask 的最后阶段写文件的时候未成功。检查该 DN 节点的 Log 发现以下异常信息：</p>\n<pre><code>2018-12-10 00:42:32,643 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: BlockSender.sendChunks() exception:\njava.io.IOException: 断开的管道\n        at sun.nio.ch.FileChannelImpl.transferTo0(Native Method)\n        at sun.nio.ch.FileChannelImpl.transferToDirectlyInternal(FileChannelImpl.java:428)\n        at sun.nio.ch.FileChannelImpl.transferToDirectly(FileChannelImpl.java:493)\n        at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:608)\n        at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:223)\n        at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket(BlockSender.java:583)\n        at org.apache.hadoop.hdfs.server.datanode.BlockSender.doSendBlock(BlockSender.java:763)\n        at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:710)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:552)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:116)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:71)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:253)\n        at java.lang.Thread.run(Thread.java:748)\n</code></pre>\n<p>百度之后发现该异常信息跟 DN 的一个配置有关系，具体配置项是 DN 可以同时处理的文件上限。对于老版本配置项名称为“dfs.datanode.max.xcievers”，对于新版本配置项名称改为“dfs.datanode.max.transfer.threads”。该参数的默认值为 4096，所以需要修改为 8192。</p>\n<h5 id=\"查看-NN-Log-信息\"><a href=\"#查看-NN-Log-信息\" class=\"headerlink\" title=\"查看 NN Log 信息\"></a>查看 NN Log 信息</h5><p>默认的，Log4j 输出的 NameNode 日志文件只保留最近的 20 个文件。因为 NN 的 Log 信息比较多，20 个文件保存的日志不足 1 天，异常时间的日志已经被冲掉了。</p>\n<p><img src=\"/uploads/20181214/namenodeLogs.png\" alt=\"NameNode Log\"></p>\n<h5 id=\"查看-NN-信息\"><a href=\"#查看-NN-信息\" class=\"headerlink\" title=\"查看 NN 信息\"></a>查看 NN 信息</h5><p>通过 NN WebUI 查看发现 NN 内存的使用已经 70% 左右，负载已经比较高。</p>\n<p><img src=\"/uploads/20181214/namenodeInfo.png\" alt=\"NameNode Info\"></p>\n<h5 id=\"处理措施\"><a href=\"#处理措施\" class=\"headerlink\" title=\"处理措施\"></a>处理措施</h5><p>综合以上信息，其实问题发生的根本原因未能查明。基于对 Hadoop 了解的深度、精力及对故障恢复的容忍程度的考虑，待定位根本原因再解决问题的方案不可控。所以，采取以下改进措施：</p>\n<p>（1）修改 DN 最大处理文件数量上限至 8192。<br>（2）将 NN 内存扩展到 100G。<br>（3）修改 Sqoop 源代码，当落地到目标 HDFS 目录下的文件数量与 MapTask 数量不一致时返回错误状态，并由调度系统进行重新抽取。</p>"},{"title":"调试 Hadoop 源代码","date":"2016-09-11T04:26:51.000Z","_content":"\n### Hadoop 版本\n\nHadoop 2.7.3\n\n<!-- more -->\n\n### 调试模式下启动 Hadoop NameNode\n\n在 ${HADOOP_HOME}/etc/hadoop/hadoop-env.sh 中设置 NameNode 启动的 JVM 参数，如下：\n\n    export HADOOP_NAMENODE_OPTS=\"-Xdebug -Xrunjdwp:transport=dt_socket,address=8788,server=y,suspend=y\"\n    export HADOOP_NAMENODE_OPTS=\"-Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,RFAS} -Dhdfs.audit.logger=${HDFS_AUDIT_LOGGER:-INFO,NullAppender} $HADOOP_NAMENODE_OPTS\"\n\n使用脚本 ${HADOOP_HOME}/sbin/start-dfs.sh 启动 HDFS，如果有以下提示信息则说明调试模式下启动 NameNode 成功：\n\n    Listening for transport dt_socket at address: 8788\n\n此时，如果执行 jps 查看 java 进程信息会有以下信息，是因为 NameNode 进程被挂起并处于监听状态，直到收到 debug 确认信息。\n\n    $ jps\n    10638 Jps\n    10438 -- main class information unavailable\n    2171 \n    10508 DataNode\n\n### 设置断点\n\n找到 hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java 并在 main 函数中设置断点，如下图：![NameNode Breakpoints](/uploads/20160911/namenode-breakpoints.png)\n\n### 在 Eclipse 中调试\n\n在 NameNode.java 代码中点击右键，在弹出的菜单中选择 Debug As -> Debug Configurations...，在弹出的对话框中双击 Remote Java Application，配置内容如图：![Remote Java Application Debug Configuration](/uploads/20160911/remote-java-application-debug-conf.png)\n\n配置完成后点击 Debug 按钮进入调试界面。","source":"_posts/调试-Hadoop-源代码.md","raw":"title: 调试 Hadoop 源代码\ntags:\n  - Hadoop\n  - Eclipse\ncategories:\n  - 大数据\n  - Hadoop\ndate: 2016-09-11 12:26:51\n---\n\n### Hadoop 版本\n\nHadoop 2.7.3\n\n<!-- more -->\n\n### 调试模式下启动 Hadoop NameNode\n\n在 ${HADOOP_HOME}/etc/hadoop/hadoop-env.sh 中设置 NameNode 启动的 JVM 参数，如下：\n\n    export HADOOP_NAMENODE_OPTS=\"-Xdebug -Xrunjdwp:transport=dt_socket,address=8788,server=y,suspend=y\"\n    export HADOOP_NAMENODE_OPTS=\"-Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,RFAS} -Dhdfs.audit.logger=${HDFS_AUDIT_LOGGER:-INFO,NullAppender} $HADOOP_NAMENODE_OPTS\"\n\n使用脚本 ${HADOOP_HOME}/sbin/start-dfs.sh 启动 HDFS，如果有以下提示信息则说明调试模式下启动 NameNode 成功：\n\n    Listening for transport dt_socket at address: 8788\n\n此时，如果执行 jps 查看 java 进程信息会有以下信息，是因为 NameNode 进程被挂起并处于监听状态，直到收到 debug 确认信息。\n\n    $ jps\n    10638 Jps\n    10438 -- main class information unavailable\n    2171 \n    10508 DataNode\n\n### 设置断点\n\n找到 hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java 并在 main 函数中设置断点，如下图：![NameNode Breakpoints](/uploads/20160911/namenode-breakpoints.png)\n\n### 在 Eclipse 中调试\n\n在 NameNode.java 代码中点击右键，在弹出的菜单中选择 Debug As -> Debug Configurations...，在弹出的对话框中双击 Remote Java Application，配置内容如图：![Remote Java Application Debug Configuration](/uploads/20160911/remote-java-application-debug-conf.png)\n\n配置完成后点击 Debug 按钮进入调试界面。","slug":"调试-Hadoop-源代码","published":1,"updated":"2021-07-19T16:28:00.316Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphte00jcitd31rao8kzf","content":"<h3 id=\"Hadoop-版本\"><a href=\"#Hadoop-版本\" class=\"headerlink\" title=\"Hadoop 版本\"></a>Hadoop 版本</h3><p>Hadoop 2.7.3</p>\n<span id=\"more\"></span>\n\n<h3 id=\"调试模式下启动-Hadoop-NameNode\"><a href=\"#调试模式下启动-Hadoop-NameNode\" class=\"headerlink\" title=\"调试模式下启动 Hadoop NameNode\"></a>调试模式下启动 Hadoop NameNode</h3><p>在 ${HADOOP_HOME}/etc/hadoop/hadoop-env.sh 中设置 NameNode 启动的 JVM 参数，如下：</p>\n<pre><code>export HADOOP_NAMENODE_OPTS=&quot;-Xdebug -Xrunjdwp:transport=dt_socket,address=8788,server=y,suspend=y&quot;\nexport HADOOP_NAMENODE_OPTS=&quot;-Dhadoop.security.logger=$&#123;HADOOP_SECURITY_LOGGER:-INFO,RFAS&#125; -Dhdfs.audit.logger=$&#123;HDFS_AUDIT_LOGGER:-INFO,NullAppender&#125; $HADOOP_NAMENODE_OPTS&quot;\n</code></pre>\n<p>使用脚本 ${HADOOP_HOME}/sbin/start-dfs.sh 启动 HDFS，如果有以下提示信息则说明调试模式下启动 NameNode 成功：</p>\n<pre><code>Listening for transport dt_socket at address: 8788\n</code></pre>\n<p>此时，如果执行 jps 查看 java 进程信息会有以下信息，是因为 NameNode 进程被挂起并处于监听状态，直到收到 debug 确认信息。</p>\n<pre><code>$ jps\n10638 Jps\n10438 -- main class information unavailable\n2171 \n10508 DataNode\n</code></pre>\n<h3 id=\"设置断点\"><a href=\"#设置断点\" class=\"headerlink\" title=\"设置断点\"></a>设置断点</h3><p>找到 hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java 并在 main 函数中设置断点，如下图：<img src=\"/uploads/20160911/namenode-breakpoints.png\" alt=\"NameNode Breakpoints\"></p>\n<h3 id=\"在-Eclipse-中调试\"><a href=\"#在-Eclipse-中调试\" class=\"headerlink\" title=\"在 Eclipse 中调试\"></a>在 Eclipse 中调试</h3><p>在 NameNode.java 代码中点击右键，在弹出的菜单中选择 Debug As -&gt; Debug Configurations…，在弹出的对话框中双击 Remote Java Application，配置内容如图：<img src=\"/uploads/20160911/remote-java-application-debug-conf.png\" alt=\"Remote Java Application Debug Configuration\"></p>\n<p>配置完成后点击 Debug 按钮进入调试界面。</p>\n","site":{"data":{}},"excerpt":"<h3 id=\"Hadoop-版本\"><a href=\"#Hadoop-版本\" class=\"headerlink\" title=\"Hadoop 版本\"></a>Hadoop 版本</h3><p>Hadoop 2.7.3</p>","more":"<h3 id=\"调试模式下启动-Hadoop-NameNode\"><a href=\"#调试模式下启动-Hadoop-NameNode\" class=\"headerlink\" title=\"调试模式下启动 Hadoop NameNode\"></a>调试模式下启动 Hadoop NameNode</h3><p>在 ${HADOOP_HOME}/etc/hadoop/hadoop-env.sh 中设置 NameNode 启动的 JVM 参数，如下：</p>\n<pre><code>export HADOOP_NAMENODE_OPTS=&quot;-Xdebug -Xrunjdwp:transport=dt_socket,address=8788,server=y,suspend=y&quot;\nexport HADOOP_NAMENODE_OPTS=&quot;-Dhadoop.security.logger=$&#123;HADOOP_SECURITY_LOGGER:-INFO,RFAS&#125; -Dhdfs.audit.logger=$&#123;HDFS_AUDIT_LOGGER:-INFO,NullAppender&#125; $HADOOP_NAMENODE_OPTS&quot;\n</code></pre>\n<p>使用脚本 ${HADOOP_HOME}/sbin/start-dfs.sh 启动 HDFS，如果有以下提示信息则说明调试模式下启动 NameNode 成功：</p>\n<pre><code>Listening for transport dt_socket at address: 8788\n</code></pre>\n<p>此时，如果执行 jps 查看 java 进程信息会有以下信息，是因为 NameNode 进程被挂起并处于监听状态，直到收到 debug 确认信息。</p>\n<pre><code>$ jps\n10638 Jps\n10438 -- main class information unavailable\n2171 \n10508 DataNode\n</code></pre>\n<h3 id=\"设置断点\"><a href=\"#设置断点\" class=\"headerlink\" title=\"设置断点\"></a>设置断点</h3><p>找到 hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java 并在 main 函数中设置断点，如下图：<img src=\"/uploads/20160911/namenode-breakpoints.png\" alt=\"NameNode Breakpoints\"></p>\n<h3 id=\"在-Eclipse-中调试\"><a href=\"#在-Eclipse-中调试\" class=\"headerlink\" title=\"在 Eclipse 中调试\"></a>在 Eclipse 中调试</h3><p>在 NameNode.java 代码中点击右键，在弹出的菜单中选择 Debug As -&gt; Debug Configurations…，在弹出的对话框中双击 Remote Java Application，配置内容如图：<img src=\"/uploads/20160911/remote-java-application-debug-conf.png\" alt=\"Remote Java Application Debug Configuration\"></p>\n<p>配置完成后点击 Debug 按钮进入调试界面。</p>"},{"title":"通过 MongoDB 日志信息定位操作来源","date":"2016-05-30T15:18:10.000Z","_content":"\n### 起因\n\n一天中午突然发现生产系统使用的 MongoDB 库被删掉了。幸亏有备份，很快恢复，生产系统并未受很大影响。问了团队成员，都未进行过删除操作，说明可能是两种情况：一、误删，连操作者自己也没意识到做了一个删除操作；二、bug，在某个程序中存在一个 bug。\n\n<!-- more -->\n\n### local.oplog.rs\n\nMongoDB 的复制集是通过 oplog 来实现的，主库的更改操作会被记录到主库的 oplog 日志中，然后从库通过异步方式复制主库的 oplog 文件并且将 oplog 日志应用到从库，从而实现了与主库的同步。\n\n先查看 oplog 看是否有删除操作。查过之后，并未发现有 remove 的操作。\n\noplog 说明：\n\n    > db.oplog.rs.findOne()\n    {\n      \"ts\" : Timestamp(1419392438, 1),\n      \"h\" : NumberLong(\"3445526183368758260\"),\n      \"v\" : 2,\n      \"op\" : \"n\",\n      \"ns\" : \"\",\n      \"o\" : {\n        \"msg\" : \"Reconfig set\",\n        \"version\" : 3\n      }\n    }\n\n- ts：8字节的时间戳，由4字节unix timestamp + 4字节自增计数表示。这个值很重要，在选举(如master宕机时)新primary时，会选择ts最大的那个secondary作为新primary。\n- op：1字节的操作类型，例如：\n  - \"i\"： insert\n  - \"u\"： update\n  - \"d\"： delete\n  - \"c\"： db cmd\n  - \"db\"：声明当前数据库 (其中ns 被设置成为=>数据库名称+ '.')\n  - \"n\":  no op,即空操作，其会定期执行以确保时效性\n- ns：操作所在的namespace。\n- o：操作所对应的document，即当前操作的内容（比如更新操作时要更新的的字段和值）\n- o2: 在执行更新操作时的where条件，仅限于update时才有该属性。\n\n### MongoDB 系统日志\n\nMongoDB 系统日志路径从配置文件中可以查看。MongoDB 进程信息中可以看到配置文件的位置：\n\n    [root@10-180-86-57 ~]# ps -ef|grep mongo\n    root      1564     1  1  2015 ?        2-21:32:34 /usr/local/mongodb/bin/mongod -f /usr/local/mongodb/mongodb.conf\n\n日志文件路径配置如下：\n\n    [root@10-180-86-57 ~]# more /usr/local/mongodb/mongodb.conf\n    systemLog:\n      destination: file\n      path: \"/data/mongodb/log/mongodb.log\"\n      logAppend: true\n\n查看日志信息发现有 dropDatabase 的操作：\n\n    [root@10-180-86-57 log]# cat mongodb.log |grep dropDatabase\n    2016-05-24T11:24:37.994+0800 [repl writer worker 1] dropDatabase md starting\n    2016-05-24T11:24:38.357+0800 [repl writer worker 1] dropDatabase md finished\n\nmd 正是被删除的库。时间范围已经明确，剩下的就是找到删除操作的来源，通过时间查看该时间前有哪些 IP 连接了数据库：\n\n    [root@10-180-86-57 log]# cat mongodb.log |grep \"2016-05-24T11:24\"\n    2016-05-24T11:24:34.960+0800 [initandlisten] connection accepted from 10.57.172.146:58082 #5171827 (55 connections now open)\n    2016-05-24T11:24:35.011+0800 [conn5171827]  authenticate db: admin { authenticate: 1, user: \"bigdata\", nonce: \"xxx\", key: \"xxx\" }\n    2016-05-24T11:24:36.483+0800 [conn5171600] end connection 10.149.13.17:4202 (54 connections now open)\n    2016-05-24T11:24:37.994+0800 [repl writer worker 1] dropDatabase md starting\n    2016-05-24T11:24:38.137+0800 [conn5171338] end connection 10.149.13.6:32360 (53 connections now open)\n    2016-05-24T11:24:38.252+0800 [repl writer worker 1] removeJournalFiles\n    2016-05-24T11:24:38.357+0800 [repl writer worker 1] dropDatabase md finished\n\n发现在这个时间有一个内容 IP（10.57.172.146） 有连接数据库的操作。虽然问题没有重新，但基本可能定位为误删操作。review 同事代码确定没有问题后，问题没有再现。\n\n### 解决方案\n#### 读写权限分离\n\n这个库下存放的都是配置信息，变动很少，但读取很频繁，而且多个地方都会读取。为了控制权限，创建这个库的只读账号，对外只开放只读账号，只有管理功能使用可写账号。\n\n#### 备份\n\n这次事故就是因为有备份才可以快速恢复的。\n","source":"_posts/通过-MongoDB-日志信息定位操作来源.md","raw":"title: 通过 MongoDB 日志信息定位操作来源\ntags:\n  - MongoDB\ncategories:\n  - 数据库\n  - MongoDB\ndate: 2016-05-30 23:18:10\n---\n\n### 起因\n\n一天中午突然发现生产系统使用的 MongoDB 库被删掉了。幸亏有备份，很快恢复，生产系统并未受很大影响。问了团队成员，都未进行过删除操作，说明可能是两种情况：一、误删，连操作者自己也没意识到做了一个删除操作；二、bug，在某个程序中存在一个 bug。\n\n<!-- more -->\n\n### local.oplog.rs\n\nMongoDB 的复制集是通过 oplog 来实现的，主库的更改操作会被记录到主库的 oplog 日志中，然后从库通过异步方式复制主库的 oplog 文件并且将 oplog 日志应用到从库，从而实现了与主库的同步。\n\n先查看 oplog 看是否有删除操作。查过之后，并未发现有 remove 的操作。\n\noplog 说明：\n\n    > db.oplog.rs.findOne()\n    {\n      \"ts\" : Timestamp(1419392438, 1),\n      \"h\" : NumberLong(\"3445526183368758260\"),\n      \"v\" : 2,\n      \"op\" : \"n\",\n      \"ns\" : \"\",\n      \"o\" : {\n        \"msg\" : \"Reconfig set\",\n        \"version\" : 3\n      }\n    }\n\n- ts：8字节的时间戳，由4字节unix timestamp + 4字节自增计数表示。这个值很重要，在选举(如master宕机时)新primary时，会选择ts最大的那个secondary作为新primary。\n- op：1字节的操作类型，例如：\n  - \"i\"： insert\n  - \"u\"： update\n  - \"d\"： delete\n  - \"c\"： db cmd\n  - \"db\"：声明当前数据库 (其中ns 被设置成为=>数据库名称+ '.')\n  - \"n\":  no op,即空操作，其会定期执行以确保时效性\n- ns：操作所在的namespace。\n- o：操作所对应的document，即当前操作的内容（比如更新操作时要更新的的字段和值）\n- o2: 在执行更新操作时的where条件，仅限于update时才有该属性。\n\n### MongoDB 系统日志\n\nMongoDB 系统日志路径从配置文件中可以查看。MongoDB 进程信息中可以看到配置文件的位置：\n\n    [root@10-180-86-57 ~]# ps -ef|grep mongo\n    root      1564     1  1  2015 ?        2-21:32:34 /usr/local/mongodb/bin/mongod -f /usr/local/mongodb/mongodb.conf\n\n日志文件路径配置如下：\n\n    [root@10-180-86-57 ~]# more /usr/local/mongodb/mongodb.conf\n    systemLog:\n      destination: file\n      path: \"/data/mongodb/log/mongodb.log\"\n      logAppend: true\n\n查看日志信息发现有 dropDatabase 的操作：\n\n    [root@10-180-86-57 log]# cat mongodb.log |grep dropDatabase\n    2016-05-24T11:24:37.994+0800 [repl writer worker 1] dropDatabase md starting\n    2016-05-24T11:24:38.357+0800 [repl writer worker 1] dropDatabase md finished\n\nmd 正是被删除的库。时间范围已经明确，剩下的就是找到删除操作的来源，通过时间查看该时间前有哪些 IP 连接了数据库：\n\n    [root@10-180-86-57 log]# cat mongodb.log |grep \"2016-05-24T11:24\"\n    2016-05-24T11:24:34.960+0800 [initandlisten] connection accepted from 10.57.172.146:58082 #5171827 (55 connections now open)\n    2016-05-24T11:24:35.011+0800 [conn5171827]  authenticate db: admin { authenticate: 1, user: \"bigdata\", nonce: \"xxx\", key: \"xxx\" }\n    2016-05-24T11:24:36.483+0800 [conn5171600] end connection 10.149.13.17:4202 (54 connections now open)\n    2016-05-24T11:24:37.994+0800 [repl writer worker 1] dropDatabase md starting\n    2016-05-24T11:24:38.137+0800 [conn5171338] end connection 10.149.13.6:32360 (53 connections now open)\n    2016-05-24T11:24:38.252+0800 [repl writer worker 1] removeJournalFiles\n    2016-05-24T11:24:38.357+0800 [repl writer worker 1] dropDatabase md finished\n\n发现在这个时间有一个内容 IP（10.57.172.146） 有连接数据库的操作。虽然问题没有重新，但基本可能定位为误删操作。review 同事代码确定没有问题后，问题没有再现。\n\n### 解决方案\n#### 读写权限分离\n\n这个库下存放的都是配置信息，变动很少，但读取很频繁，而且多个地方都会读取。为了控制权限，创建这个库的只读账号，对外只开放只读账号，只有管理功能使用可写账号。\n\n#### 备份\n\n这次事故就是因为有备份才可以快速恢复的。\n","slug":"通过-MongoDB-日志信息定位操作来源","published":1,"updated":"2021-07-19T16:28:00.316Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphtf00jfitd3f32j9xds","content":"<h3 id=\"起因\"><a href=\"#起因\" class=\"headerlink\" title=\"起因\"></a>起因</h3><p>一天中午突然发现生产系统使用的 MongoDB 库被删掉了。幸亏有备份，很快恢复，生产系统并未受很大影响。问了团队成员，都未进行过删除操作，说明可能是两种情况：一、误删，连操作者自己也没意识到做了一个删除操作；二、bug，在某个程序中存在一个 bug。</p>\n<span id=\"more\"></span>\n\n<h3 id=\"local-oplog-rs\"><a href=\"#local-oplog-rs\" class=\"headerlink\" title=\"local.oplog.rs\"></a>local.oplog.rs</h3><p>MongoDB 的复制集是通过 oplog 来实现的，主库的更改操作会被记录到主库的 oplog 日志中，然后从库通过异步方式复制主库的 oplog 文件并且将 oplog 日志应用到从库，从而实现了与主库的同步。</p>\n<p>先查看 oplog 看是否有删除操作。查过之后，并未发现有 remove 的操作。</p>\n<p>oplog 说明：</p>\n<pre><code>&gt; db.oplog.rs.findOne()\n&#123;\n  &quot;ts&quot; : Timestamp(1419392438, 1),\n  &quot;h&quot; : NumberLong(&quot;3445526183368758260&quot;),\n  &quot;v&quot; : 2,\n  &quot;op&quot; : &quot;n&quot;,\n  &quot;ns&quot; : &quot;&quot;,\n  &quot;o&quot; : &#123;\n    &quot;msg&quot; : &quot;Reconfig set&quot;,\n    &quot;version&quot; : 3\n  &#125;\n&#125;\n</code></pre>\n<ul>\n<li>ts：8字节的时间戳，由4字节unix timestamp + 4字节自增计数表示。这个值很重要，在选举(如master宕机时)新primary时，会选择ts最大的那个secondary作为新primary。</li>\n<li>op：1字节的操作类型，例如：<ul>\n<li>“i”： insert</li>\n<li>“u”： update</li>\n<li>“d”： delete</li>\n<li>“c”： db cmd</li>\n<li>“db”：声明当前数据库 (其中ns 被设置成为=&gt;数据库名称+ ‘.’)</li>\n<li>“n”:  no op,即空操作，其会定期执行以确保时效性</li>\n</ul>\n</li>\n<li>ns：操作所在的namespace。</li>\n<li>o：操作所对应的document，即当前操作的内容（比如更新操作时要更新的的字段和值）</li>\n<li>o2: 在执行更新操作时的where条件，仅限于update时才有该属性。</li>\n</ul>\n<h3 id=\"MongoDB-系统日志\"><a href=\"#MongoDB-系统日志\" class=\"headerlink\" title=\"MongoDB 系统日志\"></a>MongoDB 系统日志</h3><p>MongoDB 系统日志路径从配置文件中可以查看。MongoDB 进程信息中可以看到配置文件的位置：</p>\n<pre><code>[root@10-180-86-57 ~]# ps -ef|grep mongo\nroot      1564     1  1  2015 ?        2-21:32:34 /usr/local/mongodb/bin/mongod -f /usr/local/mongodb/mongodb.conf\n</code></pre>\n<p>日志文件路径配置如下：</p>\n<pre><code>[root@10-180-86-57 ~]# more /usr/local/mongodb/mongodb.conf\nsystemLog:\n  destination: file\n  path: &quot;/data/mongodb/log/mongodb.log&quot;\n  logAppend: true\n</code></pre>\n<p>查看日志信息发现有 dropDatabase 的操作：</p>\n<pre><code>[root@10-180-86-57 log]# cat mongodb.log |grep dropDatabase\n2016-05-24T11:24:37.994+0800 [repl writer worker 1] dropDatabase md starting\n2016-05-24T11:24:38.357+0800 [repl writer worker 1] dropDatabase md finished\n</code></pre>\n<p>md 正是被删除的库。时间范围已经明确，剩下的就是找到删除操作的来源，通过时间查看该时间前有哪些 IP 连接了数据库：</p>\n<pre><code>[root@10-180-86-57 log]# cat mongodb.log |grep &quot;2016-05-24T11:24&quot;\n2016-05-24T11:24:34.960+0800 [initandlisten] connection accepted from 10.57.172.146:58082 #5171827 (55 connections now open)\n2016-05-24T11:24:35.011+0800 [conn5171827]  authenticate db: admin &#123; authenticate: 1, user: &quot;bigdata&quot;, nonce: &quot;xxx&quot;, key: &quot;xxx&quot; &#125;\n2016-05-24T11:24:36.483+0800 [conn5171600] end connection 10.149.13.17:4202 (54 connections now open)\n2016-05-24T11:24:37.994+0800 [repl writer worker 1] dropDatabase md starting\n2016-05-24T11:24:38.137+0800 [conn5171338] end connection 10.149.13.6:32360 (53 connections now open)\n2016-05-24T11:24:38.252+0800 [repl writer worker 1] removeJournalFiles\n2016-05-24T11:24:38.357+0800 [repl writer worker 1] dropDatabase md finished\n</code></pre>\n<p>发现在这个时间有一个内容 IP（10.57.172.146） 有连接数据库的操作。虽然问题没有重新，但基本可能定位为误删操作。review 同事代码确定没有问题后，问题没有再现。</p>\n<h3 id=\"解决方案\"><a href=\"#解决方案\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h3><h4 id=\"读写权限分离\"><a href=\"#读写权限分离\" class=\"headerlink\" title=\"读写权限分离\"></a>读写权限分离</h4><p>这个库下存放的都是配置信息，变动很少，但读取很频繁，而且多个地方都会读取。为了控制权限，创建这个库的只读账号，对外只开放只读账号，只有管理功能使用可写账号。</p>\n<h4 id=\"备份\"><a href=\"#备份\" class=\"headerlink\" title=\"备份\"></a>备份</h4><p>这次事故就是因为有备份才可以快速恢复的。</p>\n","site":{"data":{}},"excerpt":"<h3 id=\"起因\"><a href=\"#起因\" class=\"headerlink\" title=\"起因\"></a>起因</h3><p>一天中午突然发现生产系统使用的 MongoDB 库被删掉了。幸亏有备份，很快恢复，生产系统并未受很大影响。问了团队成员，都未进行过删除操作，说明可能是两种情况：一、误删，连操作者自己也没意识到做了一个删除操作；二、bug，在某个程序中存在一个 bug。</p>","more":"<h3 id=\"local-oplog-rs\"><a href=\"#local-oplog-rs\" class=\"headerlink\" title=\"local.oplog.rs\"></a>local.oplog.rs</h3><p>MongoDB 的复制集是通过 oplog 来实现的，主库的更改操作会被记录到主库的 oplog 日志中，然后从库通过异步方式复制主库的 oplog 文件并且将 oplog 日志应用到从库，从而实现了与主库的同步。</p>\n<p>先查看 oplog 看是否有删除操作。查过之后，并未发现有 remove 的操作。</p>\n<p>oplog 说明：</p>\n<pre><code>&gt; db.oplog.rs.findOne()\n&#123;\n  &quot;ts&quot; : Timestamp(1419392438, 1),\n  &quot;h&quot; : NumberLong(&quot;3445526183368758260&quot;),\n  &quot;v&quot; : 2,\n  &quot;op&quot; : &quot;n&quot;,\n  &quot;ns&quot; : &quot;&quot;,\n  &quot;o&quot; : &#123;\n    &quot;msg&quot; : &quot;Reconfig set&quot;,\n    &quot;version&quot; : 3\n  &#125;\n&#125;\n</code></pre>\n<ul>\n<li>ts：8字节的时间戳，由4字节unix timestamp + 4字节自增计数表示。这个值很重要，在选举(如master宕机时)新primary时，会选择ts最大的那个secondary作为新primary。</li>\n<li>op：1字节的操作类型，例如：<ul>\n<li>“i”： insert</li>\n<li>“u”： update</li>\n<li>“d”： delete</li>\n<li>“c”： db cmd</li>\n<li>“db”：声明当前数据库 (其中ns 被设置成为=&gt;数据库名称+ ‘.’)</li>\n<li>“n”:  no op,即空操作，其会定期执行以确保时效性</li>\n</ul>\n</li>\n<li>ns：操作所在的namespace。</li>\n<li>o：操作所对应的document，即当前操作的内容（比如更新操作时要更新的的字段和值）</li>\n<li>o2: 在执行更新操作时的where条件，仅限于update时才有该属性。</li>\n</ul>\n<h3 id=\"MongoDB-系统日志\"><a href=\"#MongoDB-系统日志\" class=\"headerlink\" title=\"MongoDB 系统日志\"></a>MongoDB 系统日志</h3><p>MongoDB 系统日志路径从配置文件中可以查看。MongoDB 进程信息中可以看到配置文件的位置：</p>\n<pre><code>[root@10-180-86-57 ~]# ps -ef|grep mongo\nroot      1564     1  1  2015 ?        2-21:32:34 /usr/local/mongodb/bin/mongod -f /usr/local/mongodb/mongodb.conf\n</code></pre>\n<p>日志文件路径配置如下：</p>\n<pre><code>[root@10-180-86-57 ~]# more /usr/local/mongodb/mongodb.conf\nsystemLog:\n  destination: file\n  path: &quot;/data/mongodb/log/mongodb.log&quot;\n  logAppend: true\n</code></pre>\n<p>查看日志信息发现有 dropDatabase 的操作：</p>\n<pre><code>[root@10-180-86-57 log]# cat mongodb.log |grep dropDatabase\n2016-05-24T11:24:37.994+0800 [repl writer worker 1] dropDatabase md starting\n2016-05-24T11:24:38.357+0800 [repl writer worker 1] dropDatabase md finished\n</code></pre>\n<p>md 正是被删除的库。时间范围已经明确，剩下的就是找到删除操作的来源，通过时间查看该时间前有哪些 IP 连接了数据库：</p>\n<pre><code>[root@10-180-86-57 log]# cat mongodb.log |grep &quot;2016-05-24T11:24&quot;\n2016-05-24T11:24:34.960+0800 [initandlisten] connection accepted from 10.57.172.146:58082 #5171827 (55 connections now open)\n2016-05-24T11:24:35.011+0800 [conn5171827]  authenticate db: admin &#123; authenticate: 1, user: &quot;bigdata&quot;, nonce: &quot;xxx&quot;, key: &quot;xxx&quot; &#125;\n2016-05-24T11:24:36.483+0800 [conn5171600] end connection 10.149.13.17:4202 (54 connections now open)\n2016-05-24T11:24:37.994+0800 [repl writer worker 1] dropDatabase md starting\n2016-05-24T11:24:38.137+0800 [conn5171338] end connection 10.149.13.6:32360 (53 connections now open)\n2016-05-24T11:24:38.252+0800 [repl writer worker 1] removeJournalFiles\n2016-05-24T11:24:38.357+0800 [repl writer worker 1] dropDatabase md finished\n</code></pre>\n<p>发现在这个时间有一个内容 IP（10.57.172.146） 有连接数据库的操作。虽然问题没有重新，但基本可能定位为误删操作。review 同事代码确定没有问题后，问题没有再现。</p>\n<h3 id=\"解决方案\"><a href=\"#解决方案\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h3><h4 id=\"读写权限分离\"><a href=\"#读写权限分离\" class=\"headerlink\" title=\"读写权限分离\"></a>读写权限分离</h4><p>这个库下存放的都是配置信息，变动很少，但读取很频繁，而且多个地方都会读取。为了控制权限，创建这个库的只读账号，对外只开放只读账号，只有管理功能使用可写账号。</p>\n<h4 id=\"备份\"><a href=\"#备份\" class=\"headerlink\" title=\"备份\"></a>备份</h4><p>这次事故就是因为有备份才可以快速恢复的。</p>"},{"title":"Hadoop 集群安装","date":"2016-07-02T14:33:29.000Z","_content":"\n### 目旳\n\n这篇文档描述如何安装并配置规模从几台节点到上千台节点的集群。为了练习 Hadoop，可以先在单台机器上安装（参见[Hadoop：安装单个节点的集群](http://zhang-jc.github.io/2016/06/24/Hadoop%EF%BC%9A%E5%AE%89%E8%A3%85%E5%8D%95%E4%B8%AA%E8%8A%82%E7%82%B9%E7%9A%84%E9%9B%86%E7%BE%A4/))。\n\n这篇文档不含盖像[安全](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SecureMode.html)或高可用等高级主题。\n\n<!-- more -->\n\n### 前提\n\n- 安装 Java。查阅 [Hadoop Wiki](http://wiki.apache.org/hadoop/HadoopJavaVersions) 获取已知的适合版本。\n- 从 Apache 镜像站点下载 Hadoop 稳定版。\n\n### 安装\n\n安装 Hadoop 集群通常在集群所有机器上解包软件或者通过对应你的操作系统的包管理系统安装。重要的是分割硬件给不同的功能模块。\n\n通常集群中的一台机器专门设计为 NameNode，另外一台机器专门作为 ResourceManager。这些是主节点。其他服务（例如 Web 应用代理服务器和 MapReduce 任务历史服务器）通常既可以运行在专门的硬件上也可以运行在共享的基础设施上，视负载而定。\n\n集群中其他机器同时作为 DataNode 和 NodeManager。这些是从节点。\n\n### 配置 Hadoop 非安全模式\n\nHadoop 的 Java 配置设计为两种重要的配置文件：\n\n- 只读的默认配置文件：core-default.xml、hdfs-default.xml、yarn-default.xml 和 mapred-default.xml。\n- 站点特定配置文件：etc/hadoop/core-site.xml、etc/hadoop/hdfs-site.xml、etc/hadoop/yarn-site.xml 和 etc/hadoop/mapred-site.xml。\n\n另外，通过 etc/hadoop/hadoop-env.sh 和 etc/hadoop/yarn-env.sh 设置站点特定值可以控制 bin 目录下的 Hadoop 脚本。\n\n为了配置 Hadoop 集群，需要配置 Hadoop 守护进程执行的环境以及参数。\n\nHDFS 守护进程是 NameNode、SecondaryNameNode 和 DataNode。YARN 守护进程是 ResourceManager、NodeManager 和 WebAppProxy。如果要使用 MapReduce，则 MapReduce 作业历史记录服务器也将运行。对于大型集群安装，这些通常都运行在独立主机上。\n\n#### 配置 Hadoop 守护进程的环境\n\n管理员需要使用 etc/hadoop/hadoop-env.sh 以及可选的 etc/hadoop/mapred-env.sh 和 etc/hadoop/yarn-env.sh 脚本来设置 Hadoop 守护进程环境的个性化站点特定参数。\n\n至少必需指定 JAVA_HOME，这样才能在每个远程节点上正确的定义。\n\n管理员可以使用下面表格中展示的配置项配置每个独立的守护进程：\n\n| 守护进程                       | 环境变量                        |\n| :---------------------------- | :---------------------------- |\n| NameNode                      | HADOOP_NAMENODE_OPTS          |\n| DataNode                      | HADOOP_DATANODE_OPTS          |\n| Secondary NameNode            | HADOOP_SECONDARYNAMENODE_OPTS |\n| ResourceManager               | YARN_RESOURCEMANAGER_OPTS     |\n| NodeManager                   | YARN_NODEMANAGER_OPTS         |\n| WebAppProxy                   | YARN_PROXYSERVER_OPTS         |\n| Map Reduce Job History Server | HADOOP_JOB_HISTORYSERVER_OPTS |\n\n例如，配置 Namenode 使用并行垃圾回收，需要在 hadoop-env.sh 中添加下面的语句：\n\n    export HADOOP_NAMENODE_OPTS=\"-XX:+UseParallelGC\"\n\n查看 etc/hadoop/hadoop-env.sh 获取另外一个示例。\n\n其他可以定制的有用配置参数包括：\n\n- HADOOP_PID_DIR － 守护进程 ID 文件存放的目录。\n- HADOOP_LOG_DIR － 守护进程日志文件存放的目录。如果日志文件不存在会被自动创建。\n- HADOOP_HEAPSIZE / YARN_HEAPSIZE － 可使用的最大堆内存大小（单位：MB），例如参数设置为 1000 则堆被设置为 1000MB。这个是用来设置守护进程堆大小的。默认值为 1000。可以用来为每个守护进程单独设置。\n\n在大多数情况下，需要指定运行 Hadoop 守护进程的用户可以写入的 HADOOP_PID_DIR 和 HADOOP_LOG_DIR 目录。否则存在符号链接攻击的可能性。\n\n在全系统 Shell 环境配置中通常也配置 HADOOP_PREFIX。例如，在 /etc/profile.d 内一个简单的脚本：\n\n    HADOOP_PREFIX=/path/to/hadoop\n    export HADOOP_PREFIX\n\n| 守护进程                       | 环境变量                           |\n| :---------------------------- | :-------------------------------- |\n| ResourceManager               | YARN_RESOURCEMANAGER_HEAPSIZE     |\n| NodeManager                   | YARN_NODEMANAGER_HEAPSIZE         |\n| WebAppProxy                   | YARN_PROXYSERVER_HEAPSIZE         |\n| Map Reduce Job History Server | HADOOP_JOB_HISTORYSERVER_HEAPSIZE |\n\n#### 配置 Hadoop 守护进程\n\n这一节处理指定配置文件中的重要参数：\n\n##### etc/hadoop/core-site.xml\n\n| 参数                | 值            | 备注                             |\n| :------------------ | :----------- | :------------------------------ |\n| fs.defaultFS        | NameNode URI | hdfs://host:port/               |\n| io.file.buffer.size | 131072       | 读/写 SequenceFile 使用的缓冲大小。 |\n\n##### etc/hadoop/hdfs-site.xml\n\n- 配置 NameNode：\n\n| 参数 | 值 | 备注 |\n| :---------------------------- | :--------------------------------------- | :-------------------------------------- |\n| dfs.namenode.name.dir | 永久保存 NameNode 命名空间和事务日志的本地文件系统路径。 | 如果是一个逗号分隔的目录列表，那么名称表格会冗余的在所有目录中复制一份。 |\n| dfs.hosts / dfs.hosts.exclude | 被允许的 / 排除的 DataNode 列表             | 如果必要，使用这些文件控制允许的 DataNode 列表。 |\n| dfs.blocksize | 268435456     | 大文件系统 HDFS 块大小为 256MB。            |\n| dfs.namenode.handler.count    | 100                                      | 更多 NameNode 服务器线程来处理从大量 DataNode 来的 RPC 请求。 |\n\n- 配置 DataNode：\n\n| 参数 | 值    | 备注                             |\n| :------------------ | :----------- | :------------------------------ |\n| dfs.datanode.data.dir | DataNode 存储数据块的逗号分隔的本地文件系统的路径列表。 | 如果是一个逗号分隔的目录列表，数据会存储在所有命名的目录下，通常在不同的设备上。 |\n\n##### etc/hadoop/yarn-site.xml\n\n- 设置 ResourceManager 和 NodeManager：\n\n| 参数 | 值    | 备注                             |\n| :------------------ | :----------- | :------------------------------ |\n| yarn.acl.enable | true / false | 启用 ACL？默认为 false。 |\n| yarn.admin.acl | Admin ACL | 设置集群管理的 ACL。ACL 格式是：user1,user2 group1,group2。默认为特殊字符 *，意思是任何人。只有一个空格的特殊值意味着没有人可以访问。 |\n| yarn.log-aggregation-enable | false | 配置启用或禁用日志聚合 |\n\n- 设置 ResourceManager：\n\n| 参数 | 值    | 备注                             |\n| :------------------ | :----------- | :------------------------------ |\n| yarn.resourcemanager.address | 客户端提交作业的 ResourceManager 的 host:port | 如果设置了 host:port，会重写 yarn.resourcemanager.hostname 设置的主机名 |\n| yarn.resourcemanager.scheduler.address | ApplicationMasters 告诉 Scheduler 获取资源的 ResourceManager 的 host:port | 如果设置了 host:port，会重写 yarn.resourcemanager.hostname 设置的主机名 |\n| yarn.resourcemanager.resource-tracker.address | NodeManager 的 ResourceManager 的 host:port | 如果设置了 host:port，会重写 yarn.resourcemanager.hostname 设置的主机名 |\n| yarn.resourcemanager.admin.address | 管理命令使用的 ResourceManager 的 host:port | 如果设置了 host:port，会重写 yarn.resourcemanager.hostname 设置的主机名 |\n| yarn.resourcemanager.webapp.address | ResourceManager Web 用户界面的 host:port | 如果设置了 host:port，会重写 yarn.resourcemanager.hostname 设置的主机名 |\n| yarn.resourcemanager.hostname | ResourceManager 主机 | 用来替换所有资源设置项 yarn.resourcemanager*address 的单独主机名。启用这个设置 ResourceManager 组件使用默认端口。 |\n| yarn.resourcemanager.scheduler.class | ResourceManager Scheduler 类 | CapacityScheduler（推荐），FairScheduler（也推荐）或者 FifoScheduler |\n| yarn.scheduler.minimum-allocation-mb | 在 ResourceManager 中分配给每个容器请求的内存的最小限制。 | 单位：MB |\n| yarn.scheduler.maximum-allocation-mb | 在 ResourceManager 中分配给每个容器请求的内存的最大限制。 | 单位：MB |\n| yarn.resourcemanager.nodes.include-path / yarn.resourcemanager.nodes.exclude-path | 允许的 / 排除的 NodeManager 列表 | 如果需要，使用这些文件控制允许的 NodeManager 列表。 |\n\n- 设置 NodeManager：\n\n| 参数 | 值    | 备注 |\n| :------------------ | :----------- | :------------------------------ |\n| yarn.nodemanager.resource.memory-mb | 给 NodeManager 的资源，如可获取的物理内存，单位 MB。 | 定义了 NodeManager 上可以获取的全部资源来运行容器。 |\n| yarn.nodemanager.vmem-pmem-ratio | 任务的虚拟内存使用可能超过物理内存的最大比例。 | 通过配置该参数每个任务使用的虚拟内存可能超过它可以使用的物理内存限制。通过这个参数 NodeManager 上的任务使用的虚拟内存总量可能超过它的物理内存。 |\n| yarn.nodemanager.local-dirs | 逗号分隔的本地文件系统的路径列表，用来写入中间结果。 | 多个路径帮助提高磁盘 I／O。 |\n| yarn.nodemanager.log-dirs | 逗号分隔的本地文件系统的路径列表，用来写入日志。 | 多个路径帮助提高磁盘 I／O。 |\n| yarn.nodemanager.log.retain-seconds | 10800 | 如果禁用日志收集，在 NodeManager 上适当的保存日志文件的默认时间（单位：秒）。 |\n| yarn.nodemanager.remote-app-log-dir | /logs | HDFS 的目录，应用程序完成时会将日志移动到这个目录。需要设置合适的权限。只有启用 log-aggregation 该设置才可用。 |\n| yarn.nodemanager.remote-app-log-dir-suffix | logs | 追加到远程日志目录的后缀。日志将被收集到 ${yarn.nodemanager.remote-app-log-dir}/${user}/${thisParam} 下。只有启用 log-aggregation 该设置才可用。 |\n| yarn.nodemanager.aux-services | mapreduce_shuffle | Map Reduce 应用程序需要设置的 Shuffle 服务。 |\n\n- 设置 History Server（如果需要移动到其他地方）：\n\n| 参数 | 值 | 备注 |\n| :------------------ | :----------- | :------------------------------ |\n| yarn.log-aggregation.retain-seconds | -1 | 收集的日志在删除前保留多长时间。-1 禁用。注意，这个参数设置太小会对 NameNode 发送垃圾信息。 |\n| yarn.log-aggregation.retain-check-interval-seconds | -1 | 检查收集日志保留的间隔时间。如果设置为 0 或一个负数，那么值被计算为十分之一的聚合日志保留时间。注意，这个参数设置太小会对 NameNode 发送垃圾信息。 |\n\n##### etc/hadoop/mapred-site.xml\n\n- 设置 MapReduce Applications：\n\n| 参数 | 值 | 备注 |\n| :----------------------- | :--- | :------------------------ |\n| mapreduce.framework.name | yarn | 执行框架设置为 Hadoop YARN。 |\n| mapreduce.map.memory.mb  | 1536 | Map 任务较大的资源限制。      |\n| mapreduce.map.java.opts  | -Xmx1024M | Map 任务子虚拟机较大的堆大小。 |\n| mapreduce.reduce.memory.mb | 3072 | Reduce 任务较大的资源限制。 |\n| mapreduce.reduce.java.opts | -Xmx2560M | Reduce 任务子虚拟机较大的堆大小。 |\n| mapreduce.task.io.sort.mb | 512 | 当为了提高数据排序性能设置的较高内存限制。 |\n| mapreduce.task.io.sort.factor | 100 | 在排序文件时，多个流合并一次。 |\n| mapreduce.reduce.shuffle.parallelcopies | 50 | 当 Reduce 从大量 Map 任务获取输出时更高数量的并行拷贝。 |\n\n- 设置 MapReduce JobHistory Server：\n\n| 参数 | 值 | 备注 |\n| :--------------------------- | :-------------------------------------- | :-------------- |\n| mapreduce.jobhistory.address | MapReduce JobHistory Server 的 host:port | 默认端口是 10020 |\n| mapreduce.jobhistory.webapp.address | MapReduce JobHistory Server Web UI 的 host:port | 默认端口是 19888 |\n| mapreduce.jobhistory.intermediate-done-dir | /mr-history/tmp | MapReduce 作业写入历史文件的目录 |\n| mapreduce.jobhistory.done-dir | /mr-history/done | MR JobHistory Server 管理历史文件的目录 |\n\n### 监测 NodeManager 的健康状态：\n\nHadoop 提供了一种机制，管理员可以配置 NodeManager 周期性运行管理脚本来检测一个节点是否健康。\n\n管理员通过执行任何他们选择的脚本中的检查来监测节点是否在健康状态。如果脚本监测到节点处于异常状态，它必须向标准输出打印一行以 ERROR 开头的字符串。NodeManager 周期性调用脚本并检查它的输出。如果脚本的输出包含字符串 ERROR，像上面描述的，节点的状态报告为 *不健康* 并且这个节点会被 ResourceManager 列入黑名单。不会再有任务指派到这个节点。然而，NodeManager 会继续执行脚本，因此如果节点再次变为健康的，它会自动被 ResourceManager 从黑名单中移除。节点的健康状态随着脚本的输出，如果节点不健康，管理员可以在 ResourceManager Web 界面获得。当节点变为健康的时候也可以在 Web 界面看到。\n\n下面在 etc/hadoop/yarn-site.xml 中的参数可以用来控制节点健康监测脚本：\n\n| 参数 | 值 | 备注 |\n| :--------------------------- | :-------------------------------------- | :-------------- |\n| yarn.nodemanager.health-checker.script.path | 节点健康监测脚本 | 节点健康状态检查脚本 |\n| yarn.nodemanager.health-checker.script.opts | 节点健康监测脚本选项 | 节点健康状态检查脚本选项 |\n| yarn.nodemanager.health-checker.script.interval-ms | 节点健康监测脚本执行间隔 | 执行健康监测脚本的间隔 |\n| yarn.nodemanager.health-checker.script.timeout-ms | 节点健康监测脚本超时时长 | 节点健康监测脚本执行超时时长 |\n\n健康检查脚本不应该给错误，如果只有一些本地磁盘坏了。NodeManager 有周期性检查本地磁盘健康的能力（特别是检查 nodemanager-local-dirs 和 nodemanager-log-dirs）并且坏掉的目录数量达到 yarn.nodemanager.disk-health-checker.min-healthy-disks 属性设置的值，整个节点被标示为不健康并且这个信息被发送给资源管理器。引导磁盘要么做磁盘阵列，要么通过健康检查脚本识别引导磁盘失败。\n\n### Slaves 文件\n\n在 etc/hadoop/slaves 文件中列出所有从节点的主机名或者 IP，在一行中。帮助脚本（下面描述的）将使用 etc/hadoop/slaves 一次性在很多主机上执行命令。它不会用在任何基于 Java 的 Hadoop 配置。为了用这个功能，必须为运行 Hadoop 的账号建立 ssh 信任（通过免密码 ssh 或者其他方式，如 Kerberos）。\n\n### Hadoop 机架意识\n\n很多 Hadoop 组件是机架感知的并且利用网络拓扑提高性能和安全。Hadoop 守护进程通过调用一个管理员配置模块获取集群中从节点的机架信息。查看[机架意识](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/RackAwareness.html)获取更多详细信息。\n\n强烈推荐配置机架意识优先来启动 HDFS。\n\n### 日志\n\nHadoop 用 [Apache log4j](http://logging.apache.org/log4j/2.x/) 通过 Apache Commons Logging 框架来记录日志。编辑 etc/hadoop/log4j.properties 定制 Hadoop 守护进程的日志配置（日志格式等）。\n\n### 操作 Hadoop 集群\n\n当所有必须的配置完成后，分发所有配置文件到所有机器上的 HADOOP_CONF_DIR 目录。这应该是在所有机器上相同的目录。\n\n通常，推荐 HDFS 和 YARN 作为分别独立的用户运行。在大多数安装中，HDFS 进程执行为 hdfs。YARN 通常使用 yarn 帐号。\n\n#### 启动 Hadoop\n\n启动 Hadoop 集群需要启动 HDFS 和 YARN 集群。\n\n第一次启动 HDFS，必须先格式化。用 hdfs 帐号格式化一个新的分布式文件系统：\n\n    [hdfs]$ $HADOOP_PREFIX/bin/hdfs namenode -format <cluster_name>\n\n用 hdfs 帐号用下面的命令在指定的节点上启动 HDFS NameNode：\n\n    [hdfs]$ $HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs start namenode\n\n用 hdfs 帐号用下面的命令在每个指定的节点上启动 HDFS DataNode：\n\n    [hdfs]$ $HADOOP_PREFIX/sbin/hadoop-daemons.sh --config $HADOOP_CONF_DIR --script hdfs start datanode\n\n如果 etc/hadoop/slaves 和 ssh 信任登录被配置（查看 [Single Node Setup](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html))，所有的 HDFS 进程可以用一个工具脚本启动。使用 hdfs 帐号：\n\n    [hdfs]$ $HADOOP_PREFIX/sbin/start-dfs.sh\n\n用下面的命令启动 YARN，用 yarn 帐号在指定的 ResourceManager 上运行：\n\n    [yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR start resourcemanager\n\n用 yarn 帐号在每个指定的节点上运行一个脚本启动 NodeManager：\n\n    [yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemons.sh --config $HADOOP_CONF_DIR start nodemanager\n\n启动一个单独的 WebAppProxy 服务器。在 WebAppProxy 服务器上用 yarn 帐号运行。如果用多台服务器来做负载均衡，那么应该在它们每个上面运行：\n\n    [yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR start proxyserver\n\n如果 etc/hadoop/slaves 和 ssh 信任登录被配置（查看 [Single Node Setup](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html))，所有的 YARN 进程可以用一个工具脚本启动。使用 yarn 帐号：\n\n    [yarn]$ $HADOOP_PREFIX/sbin/start-yarn.sh\n\n用下面的命令启动 MapReduce 作业历史服务器，在指定的服务器上用 mapred 帐号：\n\n    [mapred]$ $HADOOP_PREFIX/sbin/mr-jobhistory-daemon.sh --config $HADOOP_CONF_DIR start historyserver\n\n#### 关闭 Hadoop\n\n用下面的命令停止 NameNode，在指定的 NameNode 上用帐号 hdfs 执行：\n\n    [hdfs]$ $HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs stop namenode\n\n用 hdfs 帐号执行一个脚本停止 DataNode：\n\n    [hdfs]$ $HADOOP_PREFIX/sbin/hadoop-daemons.sh --config $HADOOP_CONF_DIR --script hdfs stop datanode\n\n如果 etc/hadoop/slaves 和 ssh 信任登录被配置（查看 [Single Node Setup](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html))，所有的 HDFS 进程可以用一个工具脚本停止。使用 hdfs 帐号：\n\n    [hdfs]$ $HADOOP_PREFIX/sbin/stop-dfs.sh\n\n用下面的命令停止 ResourceManager，用 yarn 帐号在指定的 ResourceManager 上运行：\n\n    [yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR stop resourcemanager\n\n在从节点上用 yarn 帐号执行一个脚本来停止 NodeManager：\n\n    [yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemons.sh --config $HADOOP_CONF_DIR stop nodemanager\n\n如果 etc/hadoop/slaves 和 ssh 信任登录被配置（查看 [Single Node Setup](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html))，所有的 YARN 进程可以用一个工具脚本停止。使用 yarn 帐号：\n\n    [yarn]$ $HADOOP_PREFIX/sbin/stop-yarn.sh\n\n停止 WebAppProxy 服务器。用 yarn 帐号在 WebAppProxy 服务器上执行。如果用多台服务器来做负载均衡，那么应该在它们每个上面运行：\n\n    [yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR stop proxyserver\n\n用下面的命令停止 MapReduce 作业历史服务器，在指定的服务器上用 mapred 帐号：\n\n    [mapred]$ $HADOOP_PREFIX/sbin/mr-jobhistory-daemon.sh --config $HADOOP_CONF_DIR stop historyserver\n\n### Web 用户界面\n\n当 Hadoop 集群启动后，像下面描述的运行检查组件的 web-ui：\n\n| 守护进程 | Web 用户界面 | 备注 |\n| :--------------------------- | :-------------------------------------- | :-------------- |\n| NameNode | http://nn_host:port/ | 默认的 HTTP 端口是 50070 |\n| ResourceManager | http://rm_host:port/ | 默认的 HTTP 端口是 8088 |\n| MapReduce JobHistory Server | http://jhs_host:port/ | 默认的 HTTP 端口是 19888 |\n","source":"_posts/Hadoop-集群安装.md","raw":"title: Hadoop 集群安装\ntags:\n  - 大数据\n  - Hadoop\ncategories:\n  - 大数据\n  - Hadoop\ndate: 2016-07-02 22:33:29\n---\n\n### 目旳\n\n这篇文档描述如何安装并配置规模从几台节点到上千台节点的集群。为了练习 Hadoop，可以先在单台机器上安装（参见[Hadoop：安装单个节点的集群](http://zhang-jc.github.io/2016/06/24/Hadoop%EF%BC%9A%E5%AE%89%E8%A3%85%E5%8D%95%E4%B8%AA%E8%8A%82%E7%82%B9%E7%9A%84%E9%9B%86%E7%BE%A4/))。\n\n这篇文档不含盖像[安全](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SecureMode.html)或高可用等高级主题。\n\n<!-- more -->\n\n### 前提\n\n- 安装 Java。查阅 [Hadoop Wiki](http://wiki.apache.org/hadoop/HadoopJavaVersions) 获取已知的适合版本。\n- 从 Apache 镜像站点下载 Hadoop 稳定版。\n\n### 安装\n\n安装 Hadoop 集群通常在集群所有机器上解包软件或者通过对应你的操作系统的包管理系统安装。重要的是分割硬件给不同的功能模块。\n\n通常集群中的一台机器专门设计为 NameNode，另外一台机器专门作为 ResourceManager。这些是主节点。其他服务（例如 Web 应用代理服务器和 MapReduce 任务历史服务器）通常既可以运行在专门的硬件上也可以运行在共享的基础设施上，视负载而定。\n\n集群中其他机器同时作为 DataNode 和 NodeManager。这些是从节点。\n\n### 配置 Hadoop 非安全模式\n\nHadoop 的 Java 配置设计为两种重要的配置文件：\n\n- 只读的默认配置文件：core-default.xml、hdfs-default.xml、yarn-default.xml 和 mapred-default.xml。\n- 站点特定配置文件：etc/hadoop/core-site.xml、etc/hadoop/hdfs-site.xml、etc/hadoop/yarn-site.xml 和 etc/hadoop/mapred-site.xml。\n\n另外，通过 etc/hadoop/hadoop-env.sh 和 etc/hadoop/yarn-env.sh 设置站点特定值可以控制 bin 目录下的 Hadoop 脚本。\n\n为了配置 Hadoop 集群，需要配置 Hadoop 守护进程执行的环境以及参数。\n\nHDFS 守护进程是 NameNode、SecondaryNameNode 和 DataNode。YARN 守护进程是 ResourceManager、NodeManager 和 WebAppProxy。如果要使用 MapReduce，则 MapReduce 作业历史记录服务器也将运行。对于大型集群安装，这些通常都运行在独立主机上。\n\n#### 配置 Hadoop 守护进程的环境\n\n管理员需要使用 etc/hadoop/hadoop-env.sh 以及可选的 etc/hadoop/mapred-env.sh 和 etc/hadoop/yarn-env.sh 脚本来设置 Hadoop 守护进程环境的个性化站点特定参数。\n\n至少必需指定 JAVA_HOME，这样才能在每个远程节点上正确的定义。\n\n管理员可以使用下面表格中展示的配置项配置每个独立的守护进程：\n\n| 守护进程                       | 环境变量                        |\n| :---------------------------- | :---------------------------- |\n| NameNode                      | HADOOP_NAMENODE_OPTS          |\n| DataNode                      | HADOOP_DATANODE_OPTS          |\n| Secondary NameNode            | HADOOP_SECONDARYNAMENODE_OPTS |\n| ResourceManager               | YARN_RESOURCEMANAGER_OPTS     |\n| NodeManager                   | YARN_NODEMANAGER_OPTS         |\n| WebAppProxy                   | YARN_PROXYSERVER_OPTS         |\n| Map Reduce Job History Server | HADOOP_JOB_HISTORYSERVER_OPTS |\n\n例如，配置 Namenode 使用并行垃圾回收，需要在 hadoop-env.sh 中添加下面的语句：\n\n    export HADOOP_NAMENODE_OPTS=\"-XX:+UseParallelGC\"\n\n查看 etc/hadoop/hadoop-env.sh 获取另外一个示例。\n\n其他可以定制的有用配置参数包括：\n\n- HADOOP_PID_DIR － 守护进程 ID 文件存放的目录。\n- HADOOP_LOG_DIR － 守护进程日志文件存放的目录。如果日志文件不存在会被自动创建。\n- HADOOP_HEAPSIZE / YARN_HEAPSIZE － 可使用的最大堆内存大小（单位：MB），例如参数设置为 1000 则堆被设置为 1000MB。这个是用来设置守护进程堆大小的。默认值为 1000。可以用来为每个守护进程单独设置。\n\n在大多数情况下，需要指定运行 Hadoop 守护进程的用户可以写入的 HADOOP_PID_DIR 和 HADOOP_LOG_DIR 目录。否则存在符号链接攻击的可能性。\n\n在全系统 Shell 环境配置中通常也配置 HADOOP_PREFIX。例如，在 /etc/profile.d 内一个简单的脚本：\n\n    HADOOP_PREFIX=/path/to/hadoop\n    export HADOOP_PREFIX\n\n| 守护进程                       | 环境变量                           |\n| :---------------------------- | :-------------------------------- |\n| ResourceManager               | YARN_RESOURCEMANAGER_HEAPSIZE     |\n| NodeManager                   | YARN_NODEMANAGER_HEAPSIZE         |\n| WebAppProxy                   | YARN_PROXYSERVER_HEAPSIZE         |\n| Map Reduce Job History Server | HADOOP_JOB_HISTORYSERVER_HEAPSIZE |\n\n#### 配置 Hadoop 守护进程\n\n这一节处理指定配置文件中的重要参数：\n\n##### etc/hadoop/core-site.xml\n\n| 参数                | 值            | 备注                             |\n| :------------------ | :----------- | :------------------------------ |\n| fs.defaultFS        | NameNode URI | hdfs://host:port/               |\n| io.file.buffer.size | 131072       | 读/写 SequenceFile 使用的缓冲大小。 |\n\n##### etc/hadoop/hdfs-site.xml\n\n- 配置 NameNode：\n\n| 参数 | 值 | 备注 |\n| :---------------------------- | :--------------------------------------- | :-------------------------------------- |\n| dfs.namenode.name.dir | 永久保存 NameNode 命名空间和事务日志的本地文件系统路径。 | 如果是一个逗号分隔的目录列表，那么名称表格会冗余的在所有目录中复制一份。 |\n| dfs.hosts / dfs.hosts.exclude | 被允许的 / 排除的 DataNode 列表             | 如果必要，使用这些文件控制允许的 DataNode 列表。 |\n| dfs.blocksize | 268435456     | 大文件系统 HDFS 块大小为 256MB。            |\n| dfs.namenode.handler.count    | 100                                      | 更多 NameNode 服务器线程来处理从大量 DataNode 来的 RPC 请求。 |\n\n- 配置 DataNode：\n\n| 参数 | 值    | 备注                             |\n| :------------------ | :----------- | :------------------------------ |\n| dfs.datanode.data.dir | DataNode 存储数据块的逗号分隔的本地文件系统的路径列表。 | 如果是一个逗号分隔的目录列表，数据会存储在所有命名的目录下，通常在不同的设备上。 |\n\n##### etc/hadoop/yarn-site.xml\n\n- 设置 ResourceManager 和 NodeManager：\n\n| 参数 | 值    | 备注                             |\n| :------------------ | :----------- | :------------------------------ |\n| yarn.acl.enable | true / false | 启用 ACL？默认为 false。 |\n| yarn.admin.acl | Admin ACL | 设置集群管理的 ACL。ACL 格式是：user1,user2 group1,group2。默认为特殊字符 *，意思是任何人。只有一个空格的特殊值意味着没有人可以访问。 |\n| yarn.log-aggregation-enable | false | 配置启用或禁用日志聚合 |\n\n- 设置 ResourceManager：\n\n| 参数 | 值    | 备注                             |\n| :------------------ | :----------- | :------------------------------ |\n| yarn.resourcemanager.address | 客户端提交作业的 ResourceManager 的 host:port | 如果设置了 host:port，会重写 yarn.resourcemanager.hostname 设置的主机名 |\n| yarn.resourcemanager.scheduler.address | ApplicationMasters 告诉 Scheduler 获取资源的 ResourceManager 的 host:port | 如果设置了 host:port，会重写 yarn.resourcemanager.hostname 设置的主机名 |\n| yarn.resourcemanager.resource-tracker.address | NodeManager 的 ResourceManager 的 host:port | 如果设置了 host:port，会重写 yarn.resourcemanager.hostname 设置的主机名 |\n| yarn.resourcemanager.admin.address | 管理命令使用的 ResourceManager 的 host:port | 如果设置了 host:port，会重写 yarn.resourcemanager.hostname 设置的主机名 |\n| yarn.resourcemanager.webapp.address | ResourceManager Web 用户界面的 host:port | 如果设置了 host:port，会重写 yarn.resourcemanager.hostname 设置的主机名 |\n| yarn.resourcemanager.hostname | ResourceManager 主机 | 用来替换所有资源设置项 yarn.resourcemanager*address 的单独主机名。启用这个设置 ResourceManager 组件使用默认端口。 |\n| yarn.resourcemanager.scheduler.class | ResourceManager Scheduler 类 | CapacityScheduler（推荐），FairScheduler（也推荐）或者 FifoScheduler |\n| yarn.scheduler.minimum-allocation-mb | 在 ResourceManager 中分配给每个容器请求的内存的最小限制。 | 单位：MB |\n| yarn.scheduler.maximum-allocation-mb | 在 ResourceManager 中分配给每个容器请求的内存的最大限制。 | 单位：MB |\n| yarn.resourcemanager.nodes.include-path / yarn.resourcemanager.nodes.exclude-path | 允许的 / 排除的 NodeManager 列表 | 如果需要，使用这些文件控制允许的 NodeManager 列表。 |\n\n- 设置 NodeManager：\n\n| 参数 | 值    | 备注 |\n| :------------------ | :----------- | :------------------------------ |\n| yarn.nodemanager.resource.memory-mb | 给 NodeManager 的资源，如可获取的物理内存，单位 MB。 | 定义了 NodeManager 上可以获取的全部资源来运行容器。 |\n| yarn.nodemanager.vmem-pmem-ratio | 任务的虚拟内存使用可能超过物理内存的最大比例。 | 通过配置该参数每个任务使用的虚拟内存可能超过它可以使用的物理内存限制。通过这个参数 NodeManager 上的任务使用的虚拟内存总量可能超过它的物理内存。 |\n| yarn.nodemanager.local-dirs | 逗号分隔的本地文件系统的路径列表，用来写入中间结果。 | 多个路径帮助提高磁盘 I／O。 |\n| yarn.nodemanager.log-dirs | 逗号分隔的本地文件系统的路径列表，用来写入日志。 | 多个路径帮助提高磁盘 I／O。 |\n| yarn.nodemanager.log.retain-seconds | 10800 | 如果禁用日志收集，在 NodeManager 上适当的保存日志文件的默认时间（单位：秒）。 |\n| yarn.nodemanager.remote-app-log-dir | /logs | HDFS 的目录，应用程序完成时会将日志移动到这个目录。需要设置合适的权限。只有启用 log-aggregation 该设置才可用。 |\n| yarn.nodemanager.remote-app-log-dir-suffix | logs | 追加到远程日志目录的后缀。日志将被收集到 ${yarn.nodemanager.remote-app-log-dir}/${user}/${thisParam} 下。只有启用 log-aggregation 该设置才可用。 |\n| yarn.nodemanager.aux-services | mapreduce_shuffle | Map Reduce 应用程序需要设置的 Shuffle 服务。 |\n\n- 设置 History Server（如果需要移动到其他地方）：\n\n| 参数 | 值 | 备注 |\n| :------------------ | :----------- | :------------------------------ |\n| yarn.log-aggregation.retain-seconds | -1 | 收集的日志在删除前保留多长时间。-1 禁用。注意，这个参数设置太小会对 NameNode 发送垃圾信息。 |\n| yarn.log-aggregation.retain-check-interval-seconds | -1 | 检查收集日志保留的间隔时间。如果设置为 0 或一个负数，那么值被计算为十分之一的聚合日志保留时间。注意，这个参数设置太小会对 NameNode 发送垃圾信息。 |\n\n##### etc/hadoop/mapred-site.xml\n\n- 设置 MapReduce Applications：\n\n| 参数 | 值 | 备注 |\n| :----------------------- | :--- | :------------------------ |\n| mapreduce.framework.name | yarn | 执行框架设置为 Hadoop YARN。 |\n| mapreduce.map.memory.mb  | 1536 | Map 任务较大的资源限制。      |\n| mapreduce.map.java.opts  | -Xmx1024M | Map 任务子虚拟机较大的堆大小。 |\n| mapreduce.reduce.memory.mb | 3072 | Reduce 任务较大的资源限制。 |\n| mapreduce.reduce.java.opts | -Xmx2560M | Reduce 任务子虚拟机较大的堆大小。 |\n| mapreduce.task.io.sort.mb | 512 | 当为了提高数据排序性能设置的较高内存限制。 |\n| mapreduce.task.io.sort.factor | 100 | 在排序文件时，多个流合并一次。 |\n| mapreduce.reduce.shuffle.parallelcopies | 50 | 当 Reduce 从大量 Map 任务获取输出时更高数量的并行拷贝。 |\n\n- 设置 MapReduce JobHistory Server：\n\n| 参数 | 值 | 备注 |\n| :--------------------------- | :-------------------------------------- | :-------------- |\n| mapreduce.jobhistory.address | MapReduce JobHistory Server 的 host:port | 默认端口是 10020 |\n| mapreduce.jobhistory.webapp.address | MapReduce JobHistory Server Web UI 的 host:port | 默认端口是 19888 |\n| mapreduce.jobhistory.intermediate-done-dir | /mr-history/tmp | MapReduce 作业写入历史文件的目录 |\n| mapreduce.jobhistory.done-dir | /mr-history/done | MR JobHistory Server 管理历史文件的目录 |\n\n### 监测 NodeManager 的健康状态：\n\nHadoop 提供了一种机制，管理员可以配置 NodeManager 周期性运行管理脚本来检测一个节点是否健康。\n\n管理员通过执行任何他们选择的脚本中的检查来监测节点是否在健康状态。如果脚本监测到节点处于异常状态，它必须向标准输出打印一行以 ERROR 开头的字符串。NodeManager 周期性调用脚本并检查它的输出。如果脚本的输出包含字符串 ERROR，像上面描述的，节点的状态报告为 *不健康* 并且这个节点会被 ResourceManager 列入黑名单。不会再有任务指派到这个节点。然而，NodeManager 会继续执行脚本，因此如果节点再次变为健康的，它会自动被 ResourceManager 从黑名单中移除。节点的健康状态随着脚本的输出，如果节点不健康，管理员可以在 ResourceManager Web 界面获得。当节点变为健康的时候也可以在 Web 界面看到。\n\n下面在 etc/hadoop/yarn-site.xml 中的参数可以用来控制节点健康监测脚本：\n\n| 参数 | 值 | 备注 |\n| :--------------------------- | :-------------------------------------- | :-------------- |\n| yarn.nodemanager.health-checker.script.path | 节点健康监测脚本 | 节点健康状态检查脚本 |\n| yarn.nodemanager.health-checker.script.opts | 节点健康监测脚本选项 | 节点健康状态检查脚本选项 |\n| yarn.nodemanager.health-checker.script.interval-ms | 节点健康监测脚本执行间隔 | 执行健康监测脚本的间隔 |\n| yarn.nodemanager.health-checker.script.timeout-ms | 节点健康监测脚本超时时长 | 节点健康监测脚本执行超时时长 |\n\n健康检查脚本不应该给错误，如果只有一些本地磁盘坏了。NodeManager 有周期性检查本地磁盘健康的能力（特别是检查 nodemanager-local-dirs 和 nodemanager-log-dirs）并且坏掉的目录数量达到 yarn.nodemanager.disk-health-checker.min-healthy-disks 属性设置的值，整个节点被标示为不健康并且这个信息被发送给资源管理器。引导磁盘要么做磁盘阵列，要么通过健康检查脚本识别引导磁盘失败。\n\n### Slaves 文件\n\n在 etc/hadoop/slaves 文件中列出所有从节点的主机名或者 IP，在一行中。帮助脚本（下面描述的）将使用 etc/hadoop/slaves 一次性在很多主机上执行命令。它不会用在任何基于 Java 的 Hadoop 配置。为了用这个功能，必须为运行 Hadoop 的账号建立 ssh 信任（通过免密码 ssh 或者其他方式，如 Kerberos）。\n\n### Hadoop 机架意识\n\n很多 Hadoop 组件是机架感知的并且利用网络拓扑提高性能和安全。Hadoop 守护进程通过调用一个管理员配置模块获取集群中从节点的机架信息。查看[机架意识](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/RackAwareness.html)获取更多详细信息。\n\n强烈推荐配置机架意识优先来启动 HDFS。\n\n### 日志\n\nHadoop 用 [Apache log4j](http://logging.apache.org/log4j/2.x/) 通过 Apache Commons Logging 框架来记录日志。编辑 etc/hadoop/log4j.properties 定制 Hadoop 守护进程的日志配置（日志格式等）。\n\n### 操作 Hadoop 集群\n\n当所有必须的配置完成后，分发所有配置文件到所有机器上的 HADOOP_CONF_DIR 目录。这应该是在所有机器上相同的目录。\n\n通常，推荐 HDFS 和 YARN 作为分别独立的用户运行。在大多数安装中，HDFS 进程执行为 hdfs。YARN 通常使用 yarn 帐号。\n\n#### 启动 Hadoop\n\n启动 Hadoop 集群需要启动 HDFS 和 YARN 集群。\n\n第一次启动 HDFS，必须先格式化。用 hdfs 帐号格式化一个新的分布式文件系统：\n\n    [hdfs]$ $HADOOP_PREFIX/bin/hdfs namenode -format <cluster_name>\n\n用 hdfs 帐号用下面的命令在指定的节点上启动 HDFS NameNode：\n\n    [hdfs]$ $HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs start namenode\n\n用 hdfs 帐号用下面的命令在每个指定的节点上启动 HDFS DataNode：\n\n    [hdfs]$ $HADOOP_PREFIX/sbin/hadoop-daemons.sh --config $HADOOP_CONF_DIR --script hdfs start datanode\n\n如果 etc/hadoop/slaves 和 ssh 信任登录被配置（查看 [Single Node Setup](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html))，所有的 HDFS 进程可以用一个工具脚本启动。使用 hdfs 帐号：\n\n    [hdfs]$ $HADOOP_PREFIX/sbin/start-dfs.sh\n\n用下面的命令启动 YARN，用 yarn 帐号在指定的 ResourceManager 上运行：\n\n    [yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR start resourcemanager\n\n用 yarn 帐号在每个指定的节点上运行一个脚本启动 NodeManager：\n\n    [yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemons.sh --config $HADOOP_CONF_DIR start nodemanager\n\n启动一个单独的 WebAppProxy 服务器。在 WebAppProxy 服务器上用 yarn 帐号运行。如果用多台服务器来做负载均衡，那么应该在它们每个上面运行：\n\n    [yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR start proxyserver\n\n如果 etc/hadoop/slaves 和 ssh 信任登录被配置（查看 [Single Node Setup](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html))，所有的 YARN 进程可以用一个工具脚本启动。使用 yarn 帐号：\n\n    [yarn]$ $HADOOP_PREFIX/sbin/start-yarn.sh\n\n用下面的命令启动 MapReduce 作业历史服务器，在指定的服务器上用 mapred 帐号：\n\n    [mapred]$ $HADOOP_PREFIX/sbin/mr-jobhistory-daemon.sh --config $HADOOP_CONF_DIR start historyserver\n\n#### 关闭 Hadoop\n\n用下面的命令停止 NameNode，在指定的 NameNode 上用帐号 hdfs 执行：\n\n    [hdfs]$ $HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs stop namenode\n\n用 hdfs 帐号执行一个脚本停止 DataNode：\n\n    [hdfs]$ $HADOOP_PREFIX/sbin/hadoop-daemons.sh --config $HADOOP_CONF_DIR --script hdfs stop datanode\n\n如果 etc/hadoop/slaves 和 ssh 信任登录被配置（查看 [Single Node Setup](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html))，所有的 HDFS 进程可以用一个工具脚本停止。使用 hdfs 帐号：\n\n    [hdfs]$ $HADOOP_PREFIX/sbin/stop-dfs.sh\n\n用下面的命令停止 ResourceManager，用 yarn 帐号在指定的 ResourceManager 上运行：\n\n    [yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR stop resourcemanager\n\n在从节点上用 yarn 帐号执行一个脚本来停止 NodeManager：\n\n    [yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemons.sh --config $HADOOP_CONF_DIR stop nodemanager\n\n如果 etc/hadoop/slaves 和 ssh 信任登录被配置（查看 [Single Node Setup](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html))，所有的 YARN 进程可以用一个工具脚本停止。使用 yarn 帐号：\n\n    [yarn]$ $HADOOP_PREFIX/sbin/stop-yarn.sh\n\n停止 WebAppProxy 服务器。用 yarn 帐号在 WebAppProxy 服务器上执行。如果用多台服务器来做负载均衡，那么应该在它们每个上面运行：\n\n    [yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR stop proxyserver\n\n用下面的命令停止 MapReduce 作业历史服务器，在指定的服务器上用 mapred 帐号：\n\n    [mapred]$ $HADOOP_PREFIX/sbin/mr-jobhistory-daemon.sh --config $HADOOP_CONF_DIR stop historyserver\n\n### Web 用户界面\n\n当 Hadoop 集群启动后，像下面描述的运行检查组件的 web-ui：\n\n| 守护进程 | Web 用户界面 | 备注 |\n| :--------------------------- | :-------------------------------------- | :-------------- |\n| NameNode | http://nn_host:port/ | 默认的 HTTP 端口是 50070 |\n| ResourceManager | http://rm_host:port/ | 默认的 HTTP 端口是 8088 |\n| MapReduce JobHistory Server | http://jhs_host:port/ | 默认的 HTTP 端口是 19888 |\n","slug":"Hadoop-集群安装","published":1,"updated":"2021-07-19T16:28:00.252Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphx800ynitd33svn9gea","content":"<h3 id=\"目旳\"><a href=\"#目旳\" class=\"headerlink\" title=\"目旳\"></a>目旳</h3><p>这篇文档描述如何安装并配置规模从几台节点到上千台节点的集群。为了练习 Hadoop，可以先在单台机器上安装（参见<a href=\"http://zhang-jc.github.io/2016/06/24/Hadoop%EF%BC%9A%E5%AE%89%E8%A3%85%E5%8D%95%E4%B8%AA%E8%8A%82%E7%82%B9%E7%9A%84%E9%9B%86%E7%BE%A4/\">Hadoop：安装单个节点的集群</a>)。</p>\n<p>这篇文档不含盖像<a href=\"http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SecureMode.html\">安全</a>或高可用等高级主题。</p>\n<span id=\"more\"></span>\n\n<h3 id=\"前提\"><a href=\"#前提\" class=\"headerlink\" title=\"前提\"></a>前提</h3><ul>\n<li>安装 Java。查阅 <a href=\"http://wiki.apache.org/hadoop/HadoopJavaVersions\">Hadoop Wiki</a> 获取已知的适合版本。</li>\n<li>从 Apache 镜像站点下载 Hadoop 稳定版。</li>\n</ul>\n<h3 id=\"安装\"><a href=\"#安装\" class=\"headerlink\" title=\"安装\"></a>安装</h3><p>安装 Hadoop 集群通常在集群所有机器上解包软件或者通过对应你的操作系统的包管理系统安装。重要的是分割硬件给不同的功能模块。</p>\n<p>通常集群中的一台机器专门设计为 NameNode，另外一台机器专门作为 ResourceManager。这些是主节点。其他服务（例如 Web 应用代理服务器和 MapReduce 任务历史服务器）通常既可以运行在专门的硬件上也可以运行在共享的基础设施上，视负载而定。</p>\n<p>集群中其他机器同时作为 DataNode 和 NodeManager。这些是从节点。</p>\n<h3 id=\"配置-Hadoop-非安全模式\"><a href=\"#配置-Hadoop-非安全模式\" class=\"headerlink\" title=\"配置 Hadoop 非安全模式\"></a>配置 Hadoop 非安全模式</h3><p>Hadoop 的 Java 配置设计为两种重要的配置文件：</p>\n<ul>\n<li>只读的默认配置文件：core-default.xml、hdfs-default.xml、yarn-default.xml 和 mapred-default.xml。</li>\n<li>站点特定配置文件：etc/hadoop/core-site.xml、etc/hadoop/hdfs-site.xml、etc/hadoop/yarn-site.xml 和 etc/hadoop/mapred-site.xml。</li>\n</ul>\n<p>另外，通过 etc/hadoop/hadoop-env.sh 和 etc/hadoop/yarn-env.sh 设置站点特定值可以控制 bin 目录下的 Hadoop 脚本。</p>\n<p>为了配置 Hadoop 集群，需要配置 Hadoop 守护进程执行的环境以及参数。</p>\n<p>HDFS 守护进程是 NameNode、SecondaryNameNode 和 DataNode。YARN 守护进程是 ResourceManager、NodeManager 和 WebAppProxy。如果要使用 MapReduce，则 MapReduce 作业历史记录服务器也将运行。对于大型集群安装，这些通常都运行在独立主机上。</p>\n<h4 id=\"配置-Hadoop-守护进程的环境\"><a href=\"#配置-Hadoop-守护进程的环境\" class=\"headerlink\" title=\"配置 Hadoop 守护进程的环境\"></a>配置 Hadoop 守护进程的环境</h4><p>管理员需要使用 etc/hadoop/hadoop-env.sh 以及可选的 etc/hadoop/mapred-env.sh 和 etc/hadoop/yarn-env.sh 脚本来设置 Hadoop 守护进程环境的个性化站点特定参数。</p>\n<p>至少必需指定 JAVA_HOME，这样才能在每个远程节点上正确的定义。</p>\n<p>管理员可以使用下面表格中展示的配置项配置每个独立的守护进程：</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">守护进程</th>\n<th align=\"left\">环境变量</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">NameNode</td>\n<td align=\"left\">HADOOP_NAMENODE_OPTS</td>\n</tr>\n<tr>\n<td align=\"left\">DataNode</td>\n<td align=\"left\">HADOOP_DATANODE_OPTS</td>\n</tr>\n<tr>\n<td align=\"left\">Secondary NameNode</td>\n<td align=\"left\">HADOOP_SECONDARYNAMENODE_OPTS</td>\n</tr>\n<tr>\n<td align=\"left\">ResourceManager</td>\n<td align=\"left\">YARN_RESOURCEMANAGER_OPTS</td>\n</tr>\n<tr>\n<td align=\"left\">NodeManager</td>\n<td align=\"left\">YARN_NODEMANAGER_OPTS</td>\n</tr>\n<tr>\n<td align=\"left\">WebAppProxy</td>\n<td align=\"left\">YARN_PROXYSERVER_OPTS</td>\n</tr>\n<tr>\n<td align=\"left\">Map Reduce Job History Server</td>\n<td align=\"left\">HADOOP_JOB_HISTORYSERVER_OPTS</td>\n</tr>\n</tbody></table>\n<p>例如，配置 Namenode 使用并行垃圾回收，需要在 hadoop-env.sh 中添加下面的语句：</p>\n<pre><code>export HADOOP_NAMENODE_OPTS=&quot;-XX:+UseParallelGC&quot;\n</code></pre>\n<p>查看 etc/hadoop/hadoop-env.sh 获取另外一个示例。</p>\n<p>其他可以定制的有用配置参数包括：</p>\n<ul>\n<li>HADOOP_PID_DIR － 守护进程 ID 文件存放的目录。</li>\n<li>HADOOP_LOG_DIR － 守护进程日志文件存放的目录。如果日志文件不存在会被自动创建。</li>\n<li>HADOOP_HEAPSIZE / YARN_HEAPSIZE － 可使用的最大堆内存大小（单位：MB），例如参数设置为 1000 则堆被设置为 1000MB。这个是用来设置守护进程堆大小的。默认值为 1000。可以用来为每个守护进程单独设置。</li>\n</ul>\n<p>在大多数情况下，需要指定运行 Hadoop 守护进程的用户可以写入的 HADOOP_PID_DIR 和 HADOOP_LOG_DIR 目录。否则存在符号链接攻击的可能性。</p>\n<p>在全系统 Shell 环境配置中通常也配置 HADOOP_PREFIX。例如，在 /etc/profile.d 内一个简单的脚本：</p>\n<pre><code>HADOOP_PREFIX=/path/to/hadoop\nexport HADOOP_PREFIX\n</code></pre>\n<table>\n<thead>\n<tr>\n<th align=\"left\">守护进程</th>\n<th align=\"left\">环境变量</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">ResourceManager</td>\n<td align=\"left\">YARN_RESOURCEMANAGER_HEAPSIZE</td>\n</tr>\n<tr>\n<td align=\"left\">NodeManager</td>\n<td align=\"left\">YARN_NODEMANAGER_HEAPSIZE</td>\n</tr>\n<tr>\n<td align=\"left\">WebAppProxy</td>\n<td align=\"left\">YARN_PROXYSERVER_HEAPSIZE</td>\n</tr>\n<tr>\n<td align=\"left\">Map Reduce Job History Server</td>\n<td align=\"left\">HADOOP_JOB_HISTORYSERVER_HEAPSIZE</td>\n</tr>\n</tbody></table>\n<h4 id=\"配置-Hadoop-守护进程\"><a href=\"#配置-Hadoop-守护进程\" class=\"headerlink\" title=\"配置 Hadoop 守护进程\"></a>配置 Hadoop 守护进程</h4><p>这一节处理指定配置文件中的重要参数：</p>\n<h5 id=\"etc-hadoop-core-site-xml\"><a href=\"#etc-hadoop-core-site-xml\" class=\"headerlink\" title=\"etc/hadoop/core-site.xml\"></a>etc/hadoop/core-site.xml</h5><table>\n<thead>\n<tr>\n<th align=\"left\">参数</th>\n<th align=\"left\">值</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">fs.defaultFS</td>\n<td align=\"left\">NameNode URI</td>\n<td align=\"left\">hdfs://host:port/</td>\n</tr>\n<tr>\n<td align=\"left\">io.file.buffer.size</td>\n<td align=\"left\">131072</td>\n<td align=\"left\">读/写 SequenceFile 使用的缓冲大小。</td>\n</tr>\n</tbody></table>\n<h5 id=\"etc-hadoop-hdfs-site-xml\"><a href=\"#etc-hadoop-hdfs-site-xml\" class=\"headerlink\" title=\"etc/hadoop/hdfs-site.xml\"></a>etc/hadoop/hdfs-site.xml</h5><ul>\n<li>配置 NameNode：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th align=\"left\">参数</th>\n<th align=\"left\">值</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">dfs.namenode.name.dir</td>\n<td align=\"left\">永久保存 NameNode 命名空间和事务日志的本地文件系统路径。</td>\n<td align=\"left\">如果是一个逗号分隔的目录列表，那么名称表格会冗余的在所有目录中复制一份。</td>\n</tr>\n<tr>\n<td align=\"left\">dfs.hosts / dfs.hosts.exclude</td>\n<td align=\"left\">被允许的 / 排除的 DataNode 列表</td>\n<td align=\"left\">如果必要，使用这些文件控制允许的 DataNode 列表。</td>\n</tr>\n<tr>\n<td align=\"left\">dfs.blocksize</td>\n<td align=\"left\">268435456</td>\n<td align=\"left\">大文件系统 HDFS 块大小为 256MB。</td>\n</tr>\n<tr>\n<td align=\"left\">dfs.namenode.handler.count</td>\n<td align=\"left\">100</td>\n<td align=\"left\">更多 NameNode 服务器线程来处理从大量 DataNode 来的 RPC 请求。</td>\n</tr>\n</tbody></table>\n<ul>\n<li>配置 DataNode：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th align=\"left\">参数</th>\n<th align=\"left\">值</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">dfs.datanode.data.dir</td>\n<td align=\"left\">DataNode 存储数据块的逗号分隔的本地文件系统的路径列表。</td>\n<td align=\"left\">如果是一个逗号分隔的目录列表，数据会存储在所有命名的目录下，通常在不同的设备上。</td>\n</tr>\n</tbody></table>\n<h5 id=\"etc-hadoop-yarn-site-xml\"><a href=\"#etc-hadoop-yarn-site-xml\" class=\"headerlink\" title=\"etc/hadoop/yarn-site.xml\"></a>etc/hadoop/yarn-site.xml</h5><ul>\n<li>设置 ResourceManager 和 NodeManager：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th align=\"left\">参数</th>\n<th align=\"left\">值</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">yarn.acl.enable</td>\n<td align=\"left\">true / false</td>\n<td align=\"left\">启用 ACL？默认为 false。</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.admin.acl</td>\n<td align=\"left\">Admin ACL</td>\n<td align=\"left\">设置集群管理的 ACL。ACL 格式是：user1,user2 group1,group2。默认为特殊字符 *，意思是任何人。只有一个空格的特殊值意味着没有人可以访问。</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.log-aggregation-enable</td>\n<td align=\"left\">false</td>\n<td align=\"left\">配置启用或禁用日志聚合</td>\n</tr>\n</tbody></table>\n<ul>\n<li>设置 ResourceManager：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th align=\"left\">参数</th>\n<th align=\"left\">值</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">yarn.resourcemanager.address</td>\n<td align=\"left\">客户端提交作业的 ResourceManager 的 host:port</td>\n<td align=\"left\">如果设置了 host:port，会重写 yarn.resourcemanager.hostname 设置的主机名</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.resourcemanager.scheduler.address</td>\n<td align=\"left\">ApplicationMasters 告诉 Scheduler 获取资源的 ResourceManager 的 host:port</td>\n<td align=\"left\">如果设置了 host:port，会重写 yarn.resourcemanager.hostname 设置的主机名</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.resourcemanager.resource-tracker.address</td>\n<td align=\"left\">NodeManager 的 ResourceManager 的 host:port</td>\n<td align=\"left\">如果设置了 host:port，会重写 yarn.resourcemanager.hostname 设置的主机名</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.resourcemanager.admin.address</td>\n<td align=\"left\">管理命令使用的 ResourceManager 的 host:port</td>\n<td align=\"left\">如果设置了 host:port，会重写 yarn.resourcemanager.hostname 设置的主机名</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.resourcemanager.webapp.address</td>\n<td align=\"left\">ResourceManager Web 用户界面的 host:port</td>\n<td align=\"left\">如果设置了 host:port，会重写 yarn.resourcemanager.hostname 设置的主机名</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.resourcemanager.hostname</td>\n<td align=\"left\">ResourceManager 主机</td>\n<td align=\"left\">用来替换所有资源设置项 yarn.resourcemanager*address 的单独主机名。启用这个设置 ResourceManager 组件使用默认端口。</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.resourcemanager.scheduler.class</td>\n<td align=\"left\">ResourceManager Scheduler 类</td>\n<td align=\"left\">CapacityScheduler（推荐），FairScheduler（也推荐）或者 FifoScheduler</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.scheduler.minimum-allocation-mb</td>\n<td align=\"left\">在 ResourceManager 中分配给每个容器请求的内存的最小限制。</td>\n<td align=\"left\">单位：MB</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.scheduler.maximum-allocation-mb</td>\n<td align=\"left\">在 ResourceManager 中分配给每个容器请求的内存的最大限制。</td>\n<td align=\"left\">单位：MB</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.resourcemanager.nodes.include-path / yarn.resourcemanager.nodes.exclude-path</td>\n<td align=\"left\">允许的 / 排除的 NodeManager 列表</td>\n<td align=\"left\">如果需要，使用这些文件控制允许的 NodeManager 列表。</td>\n</tr>\n</tbody></table>\n<ul>\n<li>设置 NodeManager：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th align=\"left\">参数</th>\n<th align=\"left\">值</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">yarn.nodemanager.resource.memory-mb</td>\n<td align=\"left\">给 NodeManager 的资源，如可获取的物理内存，单位 MB。</td>\n<td align=\"left\">定义了 NodeManager 上可以获取的全部资源来运行容器。</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.nodemanager.vmem-pmem-ratio</td>\n<td align=\"left\">任务的虚拟内存使用可能超过物理内存的最大比例。</td>\n<td align=\"left\">通过配置该参数每个任务使用的虚拟内存可能超过它可以使用的物理内存限制。通过这个参数 NodeManager 上的任务使用的虚拟内存总量可能超过它的物理内存。</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.nodemanager.local-dirs</td>\n<td align=\"left\">逗号分隔的本地文件系统的路径列表，用来写入中间结果。</td>\n<td align=\"left\">多个路径帮助提高磁盘 I／O。</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.nodemanager.log-dirs</td>\n<td align=\"left\">逗号分隔的本地文件系统的路径列表，用来写入日志。</td>\n<td align=\"left\">多个路径帮助提高磁盘 I／O。</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.nodemanager.log.retain-seconds</td>\n<td align=\"left\">10800</td>\n<td align=\"left\">如果禁用日志收集，在 NodeManager 上适当的保存日志文件的默认时间（单位：秒）。</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.nodemanager.remote-app-log-dir</td>\n<td align=\"left\">/logs</td>\n<td align=\"left\">HDFS 的目录，应用程序完成时会将日志移动到这个目录。需要设置合适的权限。只有启用 log-aggregation 该设置才可用。</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.nodemanager.remote-app-log-dir-suffix</td>\n<td align=\"left\">logs</td>\n<td align=\"left\">追加到远程日志目录的后缀。日志将被收集到 ${yarn.nodemanager.remote-app-log-dir}/${user}/${thisParam} 下。只有启用 log-aggregation 该设置才可用。</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.nodemanager.aux-services</td>\n<td align=\"left\">mapreduce_shuffle</td>\n<td align=\"left\">Map Reduce 应用程序需要设置的 Shuffle 服务。</td>\n</tr>\n</tbody></table>\n<ul>\n<li>设置 History Server（如果需要移动到其他地方）：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th align=\"left\">参数</th>\n<th align=\"left\">值</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">yarn.log-aggregation.retain-seconds</td>\n<td align=\"left\">-1</td>\n<td align=\"left\">收集的日志在删除前保留多长时间。-1 禁用。注意，这个参数设置太小会对 NameNode 发送垃圾信息。</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.log-aggregation.retain-check-interval-seconds</td>\n<td align=\"left\">-1</td>\n<td align=\"left\">检查收集日志保留的间隔时间。如果设置为 0 或一个负数，那么值被计算为十分之一的聚合日志保留时间。注意，这个参数设置太小会对 NameNode 发送垃圾信息。</td>\n</tr>\n</tbody></table>\n<h5 id=\"etc-hadoop-mapred-site-xml\"><a href=\"#etc-hadoop-mapred-site-xml\" class=\"headerlink\" title=\"etc/hadoop/mapred-site.xml\"></a>etc/hadoop/mapred-site.xml</h5><ul>\n<li>设置 MapReduce Applications：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th align=\"left\">参数</th>\n<th align=\"left\">值</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">mapreduce.framework.name</td>\n<td align=\"left\">yarn</td>\n<td align=\"left\">执行框架设置为 Hadoop YARN。</td>\n</tr>\n<tr>\n<td align=\"left\">mapreduce.map.memory.mb</td>\n<td align=\"left\">1536</td>\n<td align=\"left\">Map 任务较大的资源限制。</td>\n</tr>\n<tr>\n<td align=\"left\">mapreduce.map.java.opts</td>\n<td align=\"left\">-Xmx1024M</td>\n<td align=\"left\">Map 任务子虚拟机较大的堆大小。</td>\n</tr>\n<tr>\n<td align=\"left\">mapreduce.reduce.memory.mb</td>\n<td align=\"left\">3072</td>\n<td align=\"left\">Reduce 任务较大的资源限制。</td>\n</tr>\n<tr>\n<td align=\"left\">mapreduce.reduce.java.opts</td>\n<td align=\"left\">-Xmx2560M</td>\n<td align=\"left\">Reduce 任务子虚拟机较大的堆大小。</td>\n</tr>\n<tr>\n<td align=\"left\">mapreduce.task.io.sort.mb</td>\n<td align=\"left\">512</td>\n<td align=\"left\">当为了提高数据排序性能设置的较高内存限制。</td>\n</tr>\n<tr>\n<td align=\"left\">mapreduce.task.io.sort.factor</td>\n<td align=\"left\">100</td>\n<td align=\"left\">在排序文件时，多个流合并一次。</td>\n</tr>\n<tr>\n<td align=\"left\">mapreduce.reduce.shuffle.parallelcopies</td>\n<td align=\"left\">50</td>\n<td align=\"left\">当 Reduce 从大量 Map 任务获取输出时更高数量的并行拷贝。</td>\n</tr>\n</tbody></table>\n<ul>\n<li>设置 MapReduce JobHistory Server：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th align=\"left\">参数</th>\n<th align=\"left\">值</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">mapreduce.jobhistory.address</td>\n<td align=\"left\">MapReduce JobHistory Server 的 host:port</td>\n<td align=\"left\">默认端口是 10020</td>\n</tr>\n<tr>\n<td align=\"left\">mapreduce.jobhistory.webapp.address</td>\n<td align=\"left\">MapReduce JobHistory Server Web UI 的 host:port</td>\n<td align=\"left\">默认端口是 19888</td>\n</tr>\n<tr>\n<td align=\"left\">mapreduce.jobhistory.intermediate-done-dir</td>\n<td align=\"left\">/mr-history/tmp</td>\n<td align=\"left\">MapReduce 作业写入历史文件的目录</td>\n</tr>\n<tr>\n<td align=\"left\">mapreduce.jobhistory.done-dir</td>\n<td align=\"left\">/mr-history/done</td>\n<td align=\"left\">MR JobHistory Server 管理历史文件的目录</td>\n</tr>\n</tbody></table>\n<h3 id=\"监测-NodeManager-的健康状态：\"><a href=\"#监测-NodeManager-的健康状态：\" class=\"headerlink\" title=\"监测 NodeManager 的健康状态：\"></a>监测 NodeManager 的健康状态：</h3><p>Hadoop 提供了一种机制，管理员可以配置 NodeManager 周期性运行管理脚本来检测一个节点是否健康。</p>\n<p>管理员通过执行任何他们选择的脚本中的检查来监测节点是否在健康状态。如果脚本监测到节点处于异常状态，它必须向标准输出打印一行以 ERROR 开头的字符串。NodeManager 周期性调用脚本并检查它的输出。如果脚本的输出包含字符串 ERROR，像上面描述的，节点的状态报告为 <em>不健康</em> 并且这个节点会被 ResourceManager 列入黑名单。不会再有任务指派到这个节点。然而，NodeManager 会继续执行脚本，因此如果节点再次变为健康的，它会自动被 ResourceManager 从黑名单中移除。节点的健康状态随着脚本的输出，如果节点不健康，管理员可以在 ResourceManager Web 界面获得。当节点变为健康的时候也可以在 Web 界面看到。</p>\n<p>下面在 etc/hadoop/yarn-site.xml 中的参数可以用来控制节点健康监测脚本：</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">参数</th>\n<th align=\"left\">值</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">yarn.nodemanager.health-checker.script.path</td>\n<td align=\"left\">节点健康监测脚本</td>\n<td align=\"left\">节点健康状态检查脚本</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.nodemanager.health-checker.script.opts</td>\n<td align=\"left\">节点健康监测脚本选项</td>\n<td align=\"left\">节点健康状态检查脚本选项</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.nodemanager.health-checker.script.interval-ms</td>\n<td align=\"left\">节点健康监测脚本执行间隔</td>\n<td align=\"left\">执行健康监测脚本的间隔</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.nodemanager.health-checker.script.timeout-ms</td>\n<td align=\"left\">节点健康监测脚本超时时长</td>\n<td align=\"left\">节点健康监测脚本执行超时时长</td>\n</tr>\n</tbody></table>\n<p>健康检查脚本不应该给错误，如果只有一些本地磁盘坏了。NodeManager 有周期性检查本地磁盘健康的能力（特别是检查 nodemanager-local-dirs 和 nodemanager-log-dirs）并且坏掉的目录数量达到 yarn.nodemanager.disk-health-checker.min-healthy-disks 属性设置的值，整个节点被标示为不健康并且这个信息被发送给资源管理器。引导磁盘要么做磁盘阵列，要么通过健康检查脚本识别引导磁盘失败。</p>\n<h3 id=\"Slaves-文件\"><a href=\"#Slaves-文件\" class=\"headerlink\" title=\"Slaves 文件\"></a>Slaves 文件</h3><p>在 etc/hadoop/slaves 文件中列出所有从节点的主机名或者 IP，在一行中。帮助脚本（下面描述的）将使用 etc/hadoop/slaves 一次性在很多主机上执行命令。它不会用在任何基于 Java 的 Hadoop 配置。为了用这个功能，必须为运行 Hadoop 的账号建立 ssh 信任（通过免密码 ssh 或者其他方式，如 Kerberos）。</p>\n<h3 id=\"Hadoop-机架意识\"><a href=\"#Hadoop-机架意识\" class=\"headerlink\" title=\"Hadoop 机架意识\"></a>Hadoop 机架意识</h3><p>很多 Hadoop 组件是机架感知的并且利用网络拓扑提高性能和安全。Hadoop 守护进程通过调用一个管理员配置模块获取集群中从节点的机架信息。查看<a href=\"http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/RackAwareness.html\">机架意识</a>获取更多详细信息。</p>\n<p>强烈推荐配置机架意识优先来启动 HDFS。</p>\n<h3 id=\"日志\"><a href=\"#日志\" class=\"headerlink\" title=\"日志\"></a>日志</h3><p>Hadoop 用 <a href=\"http://logging.apache.org/log4j/2.x/\">Apache log4j</a> 通过 Apache Commons Logging 框架来记录日志。编辑 etc/hadoop/log4j.properties 定制 Hadoop 守护进程的日志配置（日志格式等）。</p>\n<h3 id=\"操作-Hadoop-集群\"><a href=\"#操作-Hadoop-集群\" class=\"headerlink\" title=\"操作 Hadoop 集群\"></a>操作 Hadoop 集群</h3><p>当所有必须的配置完成后，分发所有配置文件到所有机器上的 HADOOP_CONF_DIR 目录。这应该是在所有机器上相同的目录。</p>\n<p>通常，推荐 HDFS 和 YARN 作为分别独立的用户运行。在大多数安装中，HDFS 进程执行为 hdfs。YARN 通常使用 yarn 帐号。</p>\n<h4 id=\"启动-Hadoop\"><a href=\"#启动-Hadoop\" class=\"headerlink\" title=\"启动 Hadoop\"></a>启动 Hadoop</h4><p>启动 Hadoop 集群需要启动 HDFS 和 YARN 集群。</p>\n<p>第一次启动 HDFS，必须先格式化。用 hdfs 帐号格式化一个新的分布式文件系统：</p>\n<pre><code>[hdfs]$ $HADOOP_PREFIX/bin/hdfs namenode -format &lt;cluster_name&gt;\n</code></pre>\n<p>用 hdfs 帐号用下面的命令在指定的节点上启动 HDFS NameNode：</p>\n<pre><code>[hdfs]$ $HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs start namenode\n</code></pre>\n<p>用 hdfs 帐号用下面的命令在每个指定的节点上启动 HDFS DataNode：</p>\n<pre><code>[hdfs]$ $HADOOP_PREFIX/sbin/hadoop-daemons.sh --config $HADOOP_CONF_DIR --script hdfs start datanode\n</code></pre>\n<p>如果 etc/hadoop/slaves 和 ssh 信任登录被配置（查看 <a href=\"http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html\">Single Node Setup</a>)，所有的 HDFS 进程可以用一个工具脚本启动。使用 hdfs 帐号：</p>\n<pre><code>[hdfs]$ $HADOOP_PREFIX/sbin/start-dfs.sh\n</code></pre>\n<p>用下面的命令启动 YARN，用 yarn 帐号在指定的 ResourceManager 上运行：</p>\n<pre><code>[yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR start resourcemanager\n</code></pre>\n<p>用 yarn 帐号在每个指定的节点上运行一个脚本启动 NodeManager：</p>\n<pre><code>[yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemons.sh --config $HADOOP_CONF_DIR start nodemanager\n</code></pre>\n<p>启动一个单独的 WebAppProxy 服务器。在 WebAppProxy 服务器上用 yarn 帐号运行。如果用多台服务器来做负载均衡，那么应该在它们每个上面运行：</p>\n<pre><code>[yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR start proxyserver\n</code></pre>\n<p>如果 etc/hadoop/slaves 和 ssh 信任登录被配置（查看 <a href=\"http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html\">Single Node Setup</a>)，所有的 YARN 进程可以用一个工具脚本启动。使用 yarn 帐号：</p>\n<pre><code>[yarn]$ $HADOOP_PREFIX/sbin/start-yarn.sh\n</code></pre>\n<p>用下面的命令启动 MapReduce 作业历史服务器，在指定的服务器上用 mapred 帐号：</p>\n<pre><code>[mapred]$ $HADOOP_PREFIX/sbin/mr-jobhistory-daemon.sh --config $HADOOP_CONF_DIR start historyserver\n</code></pre>\n<h4 id=\"关闭-Hadoop\"><a href=\"#关闭-Hadoop\" class=\"headerlink\" title=\"关闭 Hadoop\"></a>关闭 Hadoop</h4><p>用下面的命令停止 NameNode，在指定的 NameNode 上用帐号 hdfs 执行：</p>\n<pre><code>[hdfs]$ $HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs stop namenode\n</code></pre>\n<p>用 hdfs 帐号执行一个脚本停止 DataNode：</p>\n<pre><code>[hdfs]$ $HADOOP_PREFIX/sbin/hadoop-daemons.sh --config $HADOOP_CONF_DIR --script hdfs stop datanode\n</code></pre>\n<p>如果 etc/hadoop/slaves 和 ssh 信任登录被配置（查看 <a href=\"http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html\">Single Node Setup</a>)，所有的 HDFS 进程可以用一个工具脚本停止。使用 hdfs 帐号：</p>\n<pre><code>[hdfs]$ $HADOOP_PREFIX/sbin/stop-dfs.sh\n</code></pre>\n<p>用下面的命令停止 ResourceManager，用 yarn 帐号在指定的 ResourceManager 上运行：</p>\n<pre><code>[yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR stop resourcemanager\n</code></pre>\n<p>在从节点上用 yarn 帐号执行一个脚本来停止 NodeManager：</p>\n<pre><code>[yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemons.sh --config $HADOOP_CONF_DIR stop nodemanager\n</code></pre>\n<p>如果 etc/hadoop/slaves 和 ssh 信任登录被配置（查看 <a href=\"http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html\">Single Node Setup</a>)，所有的 YARN 进程可以用一个工具脚本停止。使用 yarn 帐号：</p>\n<pre><code>[yarn]$ $HADOOP_PREFIX/sbin/stop-yarn.sh\n</code></pre>\n<p>停止 WebAppProxy 服务器。用 yarn 帐号在 WebAppProxy 服务器上执行。如果用多台服务器来做负载均衡，那么应该在它们每个上面运行：</p>\n<pre><code>[yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR stop proxyserver\n</code></pre>\n<p>用下面的命令停止 MapReduce 作业历史服务器，在指定的服务器上用 mapred 帐号：</p>\n<pre><code>[mapred]$ $HADOOP_PREFIX/sbin/mr-jobhistory-daemon.sh --config $HADOOP_CONF_DIR stop historyserver\n</code></pre>\n<h3 id=\"Web-用户界面\"><a href=\"#Web-用户界面\" class=\"headerlink\" title=\"Web 用户界面\"></a>Web 用户界面</h3><p>当 Hadoop 集群启动后，像下面描述的运行检查组件的 web-ui：</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">守护进程</th>\n<th align=\"left\">Web 用户界面</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">NameNode</td>\n<td align=\"left\"><a href=\"http://nn_host:port/\">http://nn_host:port/</a></td>\n<td align=\"left\">默认的 HTTP 端口是 50070</td>\n</tr>\n<tr>\n<td align=\"left\">ResourceManager</td>\n<td align=\"left\"><a href=\"http://rm_host:port/\">http://rm_host:port/</a></td>\n<td align=\"left\">默认的 HTTP 端口是 8088</td>\n</tr>\n<tr>\n<td align=\"left\">MapReduce JobHistory Server</td>\n<td align=\"left\"><a href=\"http://jhs_host:port/\">http://jhs_host:port/</a></td>\n<td align=\"left\">默认的 HTTP 端口是 19888</td>\n</tr>\n</tbody></table>\n","site":{"data":{}},"excerpt":"<h3 id=\"目旳\"><a href=\"#目旳\" class=\"headerlink\" title=\"目旳\"></a>目旳</h3><p>这篇文档描述如何安装并配置规模从几台节点到上千台节点的集群。为了练习 Hadoop，可以先在单台机器上安装（参见<a href=\"http://zhang-jc.github.io/2016/06/24/Hadoop%EF%BC%9A%E5%AE%89%E8%A3%85%E5%8D%95%E4%B8%AA%E8%8A%82%E7%82%B9%E7%9A%84%E9%9B%86%E7%BE%A4/\">Hadoop：安装单个节点的集群</a>)。</p>\n<p>这篇文档不含盖像<a href=\"http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SecureMode.html\">安全</a>或高可用等高级主题。</p>","more":"<h3 id=\"前提\"><a href=\"#前提\" class=\"headerlink\" title=\"前提\"></a>前提</h3><ul>\n<li>安装 Java。查阅 <a href=\"http://wiki.apache.org/hadoop/HadoopJavaVersions\">Hadoop Wiki</a> 获取已知的适合版本。</li>\n<li>从 Apache 镜像站点下载 Hadoop 稳定版。</li>\n</ul>\n<h3 id=\"安装\"><a href=\"#安装\" class=\"headerlink\" title=\"安装\"></a>安装</h3><p>安装 Hadoop 集群通常在集群所有机器上解包软件或者通过对应你的操作系统的包管理系统安装。重要的是分割硬件给不同的功能模块。</p>\n<p>通常集群中的一台机器专门设计为 NameNode，另外一台机器专门作为 ResourceManager。这些是主节点。其他服务（例如 Web 应用代理服务器和 MapReduce 任务历史服务器）通常既可以运行在专门的硬件上也可以运行在共享的基础设施上，视负载而定。</p>\n<p>集群中其他机器同时作为 DataNode 和 NodeManager。这些是从节点。</p>\n<h3 id=\"配置-Hadoop-非安全模式\"><a href=\"#配置-Hadoop-非安全模式\" class=\"headerlink\" title=\"配置 Hadoop 非安全模式\"></a>配置 Hadoop 非安全模式</h3><p>Hadoop 的 Java 配置设计为两种重要的配置文件：</p>\n<ul>\n<li>只读的默认配置文件：core-default.xml、hdfs-default.xml、yarn-default.xml 和 mapred-default.xml。</li>\n<li>站点特定配置文件：etc/hadoop/core-site.xml、etc/hadoop/hdfs-site.xml、etc/hadoop/yarn-site.xml 和 etc/hadoop/mapred-site.xml。</li>\n</ul>\n<p>另外，通过 etc/hadoop/hadoop-env.sh 和 etc/hadoop/yarn-env.sh 设置站点特定值可以控制 bin 目录下的 Hadoop 脚本。</p>\n<p>为了配置 Hadoop 集群，需要配置 Hadoop 守护进程执行的环境以及参数。</p>\n<p>HDFS 守护进程是 NameNode、SecondaryNameNode 和 DataNode。YARN 守护进程是 ResourceManager、NodeManager 和 WebAppProxy。如果要使用 MapReduce，则 MapReduce 作业历史记录服务器也将运行。对于大型集群安装，这些通常都运行在独立主机上。</p>\n<h4 id=\"配置-Hadoop-守护进程的环境\"><a href=\"#配置-Hadoop-守护进程的环境\" class=\"headerlink\" title=\"配置 Hadoop 守护进程的环境\"></a>配置 Hadoop 守护进程的环境</h4><p>管理员需要使用 etc/hadoop/hadoop-env.sh 以及可选的 etc/hadoop/mapred-env.sh 和 etc/hadoop/yarn-env.sh 脚本来设置 Hadoop 守护进程环境的个性化站点特定参数。</p>\n<p>至少必需指定 JAVA_HOME，这样才能在每个远程节点上正确的定义。</p>\n<p>管理员可以使用下面表格中展示的配置项配置每个独立的守护进程：</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">守护进程</th>\n<th align=\"left\">环境变量</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">NameNode</td>\n<td align=\"left\">HADOOP_NAMENODE_OPTS</td>\n</tr>\n<tr>\n<td align=\"left\">DataNode</td>\n<td align=\"left\">HADOOP_DATANODE_OPTS</td>\n</tr>\n<tr>\n<td align=\"left\">Secondary NameNode</td>\n<td align=\"left\">HADOOP_SECONDARYNAMENODE_OPTS</td>\n</tr>\n<tr>\n<td align=\"left\">ResourceManager</td>\n<td align=\"left\">YARN_RESOURCEMANAGER_OPTS</td>\n</tr>\n<tr>\n<td align=\"left\">NodeManager</td>\n<td align=\"left\">YARN_NODEMANAGER_OPTS</td>\n</tr>\n<tr>\n<td align=\"left\">WebAppProxy</td>\n<td align=\"left\">YARN_PROXYSERVER_OPTS</td>\n</tr>\n<tr>\n<td align=\"left\">Map Reduce Job History Server</td>\n<td align=\"left\">HADOOP_JOB_HISTORYSERVER_OPTS</td>\n</tr>\n</tbody></table>\n<p>例如，配置 Namenode 使用并行垃圾回收，需要在 hadoop-env.sh 中添加下面的语句：</p>\n<pre><code>export HADOOP_NAMENODE_OPTS=&quot;-XX:+UseParallelGC&quot;\n</code></pre>\n<p>查看 etc/hadoop/hadoop-env.sh 获取另外一个示例。</p>\n<p>其他可以定制的有用配置参数包括：</p>\n<ul>\n<li>HADOOP_PID_DIR － 守护进程 ID 文件存放的目录。</li>\n<li>HADOOP_LOG_DIR － 守护进程日志文件存放的目录。如果日志文件不存在会被自动创建。</li>\n<li>HADOOP_HEAPSIZE / YARN_HEAPSIZE － 可使用的最大堆内存大小（单位：MB），例如参数设置为 1000 则堆被设置为 1000MB。这个是用来设置守护进程堆大小的。默认值为 1000。可以用来为每个守护进程单独设置。</li>\n</ul>\n<p>在大多数情况下，需要指定运行 Hadoop 守护进程的用户可以写入的 HADOOP_PID_DIR 和 HADOOP_LOG_DIR 目录。否则存在符号链接攻击的可能性。</p>\n<p>在全系统 Shell 环境配置中通常也配置 HADOOP_PREFIX。例如，在 /etc/profile.d 内一个简单的脚本：</p>\n<pre><code>HADOOP_PREFIX=/path/to/hadoop\nexport HADOOP_PREFIX\n</code></pre>\n<table>\n<thead>\n<tr>\n<th align=\"left\">守护进程</th>\n<th align=\"left\">环境变量</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">ResourceManager</td>\n<td align=\"left\">YARN_RESOURCEMANAGER_HEAPSIZE</td>\n</tr>\n<tr>\n<td align=\"left\">NodeManager</td>\n<td align=\"left\">YARN_NODEMANAGER_HEAPSIZE</td>\n</tr>\n<tr>\n<td align=\"left\">WebAppProxy</td>\n<td align=\"left\">YARN_PROXYSERVER_HEAPSIZE</td>\n</tr>\n<tr>\n<td align=\"left\">Map Reduce Job History Server</td>\n<td align=\"left\">HADOOP_JOB_HISTORYSERVER_HEAPSIZE</td>\n</tr>\n</tbody></table>\n<h4 id=\"配置-Hadoop-守护进程\"><a href=\"#配置-Hadoop-守护进程\" class=\"headerlink\" title=\"配置 Hadoop 守护进程\"></a>配置 Hadoop 守护进程</h4><p>这一节处理指定配置文件中的重要参数：</p>\n<h5 id=\"etc-hadoop-core-site-xml\"><a href=\"#etc-hadoop-core-site-xml\" class=\"headerlink\" title=\"etc/hadoop/core-site.xml\"></a>etc/hadoop/core-site.xml</h5><table>\n<thead>\n<tr>\n<th align=\"left\">参数</th>\n<th align=\"left\">值</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">fs.defaultFS</td>\n<td align=\"left\">NameNode URI</td>\n<td align=\"left\">hdfs://host:port/</td>\n</tr>\n<tr>\n<td align=\"left\">io.file.buffer.size</td>\n<td align=\"left\">131072</td>\n<td align=\"left\">读/写 SequenceFile 使用的缓冲大小。</td>\n</tr>\n</tbody></table>\n<h5 id=\"etc-hadoop-hdfs-site-xml\"><a href=\"#etc-hadoop-hdfs-site-xml\" class=\"headerlink\" title=\"etc/hadoop/hdfs-site.xml\"></a>etc/hadoop/hdfs-site.xml</h5><ul>\n<li>配置 NameNode：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th align=\"left\">参数</th>\n<th align=\"left\">值</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">dfs.namenode.name.dir</td>\n<td align=\"left\">永久保存 NameNode 命名空间和事务日志的本地文件系统路径。</td>\n<td align=\"left\">如果是一个逗号分隔的目录列表，那么名称表格会冗余的在所有目录中复制一份。</td>\n</tr>\n<tr>\n<td align=\"left\">dfs.hosts / dfs.hosts.exclude</td>\n<td align=\"left\">被允许的 / 排除的 DataNode 列表</td>\n<td align=\"left\">如果必要，使用这些文件控制允许的 DataNode 列表。</td>\n</tr>\n<tr>\n<td align=\"left\">dfs.blocksize</td>\n<td align=\"left\">268435456</td>\n<td align=\"left\">大文件系统 HDFS 块大小为 256MB。</td>\n</tr>\n<tr>\n<td align=\"left\">dfs.namenode.handler.count</td>\n<td align=\"left\">100</td>\n<td align=\"left\">更多 NameNode 服务器线程来处理从大量 DataNode 来的 RPC 请求。</td>\n</tr>\n</tbody></table>\n<ul>\n<li>配置 DataNode：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th align=\"left\">参数</th>\n<th align=\"left\">值</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">dfs.datanode.data.dir</td>\n<td align=\"left\">DataNode 存储数据块的逗号分隔的本地文件系统的路径列表。</td>\n<td align=\"left\">如果是一个逗号分隔的目录列表，数据会存储在所有命名的目录下，通常在不同的设备上。</td>\n</tr>\n</tbody></table>\n<h5 id=\"etc-hadoop-yarn-site-xml\"><a href=\"#etc-hadoop-yarn-site-xml\" class=\"headerlink\" title=\"etc/hadoop/yarn-site.xml\"></a>etc/hadoop/yarn-site.xml</h5><ul>\n<li>设置 ResourceManager 和 NodeManager：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th align=\"left\">参数</th>\n<th align=\"left\">值</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">yarn.acl.enable</td>\n<td align=\"left\">true / false</td>\n<td align=\"left\">启用 ACL？默认为 false。</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.admin.acl</td>\n<td align=\"left\">Admin ACL</td>\n<td align=\"left\">设置集群管理的 ACL。ACL 格式是：user1,user2 group1,group2。默认为特殊字符 *，意思是任何人。只有一个空格的特殊值意味着没有人可以访问。</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.log-aggregation-enable</td>\n<td align=\"left\">false</td>\n<td align=\"left\">配置启用或禁用日志聚合</td>\n</tr>\n</tbody></table>\n<ul>\n<li>设置 ResourceManager：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th align=\"left\">参数</th>\n<th align=\"left\">值</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">yarn.resourcemanager.address</td>\n<td align=\"left\">客户端提交作业的 ResourceManager 的 host:port</td>\n<td align=\"left\">如果设置了 host:port，会重写 yarn.resourcemanager.hostname 设置的主机名</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.resourcemanager.scheduler.address</td>\n<td align=\"left\">ApplicationMasters 告诉 Scheduler 获取资源的 ResourceManager 的 host:port</td>\n<td align=\"left\">如果设置了 host:port，会重写 yarn.resourcemanager.hostname 设置的主机名</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.resourcemanager.resource-tracker.address</td>\n<td align=\"left\">NodeManager 的 ResourceManager 的 host:port</td>\n<td align=\"left\">如果设置了 host:port，会重写 yarn.resourcemanager.hostname 设置的主机名</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.resourcemanager.admin.address</td>\n<td align=\"left\">管理命令使用的 ResourceManager 的 host:port</td>\n<td align=\"left\">如果设置了 host:port，会重写 yarn.resourcemanager.hostname 设置的主机名</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.resourcemanager.webapp.address</td>\n<td align=\"left\">ResourceManager Web 用户界面的 host:port</td>\n<td align=\"left\">如果设置了 host:port，会重写 yarn.resourcemanager.hostname 设置的主机名</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.resourcemanager.hostname</td>\n<td align=\"left\">ResourceManager 主机</td>\n<td align=\"left\">用来替换所有资源设置项 yarn.resourcemanager*address 的单独主机名。启用这个设置 ResourceManager 组件使用默认端口。</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.resourcemanager.scheduler.class</td>\n<td align=\"left\">ResourceManager Scheduler 类</td>\n<td align=\"left\">CapacityScheduler（推荐），FairScheduler（也推荐）或者 FifoScheduler</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.scheduler.minimum-allocation-mb</td>\n<td align=\"left\">在 ResourceManager 中分配给每个容器请求的内存的最小限制。</td>\n<td align=\"left\">单位：MB</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.scheduler.maximum-allocation-mb</td>\n<td align=\"left\">在 ResourceManager 中分配给每个容器请求的内存的最大限制。</td>\n<td align=\"left\">单位：MB</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.resourcemanager.nodes.include-path / yarn.resourcemanager.nodes.exclude-path</td>\n<td align=\"left\">允许的 / 排除的 NodeManager 列表</td>\n<td align=\"left\">如果需要，使用这些文件控制允许的 NodeManager 列表。</td>\n</tr>\n</tbody></table>\n<ul>\n<li>设置 NodeManager：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th align=\"left\">参数</th>\n<th align=\"left\">值</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">yarn.nodemanager.resource.memory-mb</td>\n<td align=\"left\">给 NodeManager 的资源，如可获取的物理内存，单位 MB。</td>\n<td align=\"left\">定义了 NodeManager 上可以获取的全部资源来运行容器。</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.nodemanager.vmem-pmem-ratio</td>\n<td align=\"left\">任务的虚拟内存使用可能超过物理内存的最大比例。</td>\n<td align=\"left\">通过配置该参数每个任务使用的虚拟内存可能超过它可以使用的物理内存限制。通过这个参数 NodeManager 上的任务使用的虚拟内存总量可能超过它的物理内存。</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.nodemanager.local-dirs</td>\n<td align=\"left\">逗号分隔的本地文件系统的路径列表，用来写入中间结果。</td>\n<td align=\"left\">多个路径帮助提高磁盘 I／O。</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.nodemanager.log-dirs</td>\n<td align=\"left\">逗号分隔的本地文件系统的路径列表，用来写入日志。</td>\n<td align=\"left\">多个路径帮助提高磁盘 I／O。</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.nodemanager.log.retain-seconds</td>\n<td align=\"left\">10800</td>\n<td align=\"left\">如果禁用日志收集，在 NodeManager 上适当的保存日志文件的默认时间（单位：秒）。</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.nodemanager.remote-app-log-dir</td>\n<td align=\"left\">/logs</td>\n<td align=\"left\">HDFS 的目录，应用程序完成时会将日志移动到这个目录。需要设置合适的权限。只有启用 log-aggregation 该设置才可用。</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.nodemanager.remote-app-log-dir-suffix</td>\n<td align=\"left\">logs</td>\n<td align=\"left\">追加到远程日志目录的后缀。日志将被收集到 ${yarn.nodemanager.remote-app-log-dir}/${user}/${thisParam} 下。只有启用 log-aggregation 该设置才可用。</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.nodemanager.aux-services</td>\n<td align=\"left\">mapreduce_shuffle</td>\n<td align=\"left\">Map Reduce 应用程序需要设置的 Shuffle 服务。</td>\n</tr>\n</tbody></table>\n<ul>\n<li>设置 History Server（如果需要移动到其他地方）：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th align=\"left\">参数</th>\n<th align=\"left\">值</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">yarn.log-aggregation.retain-seconds</td>\n<td align=\"left\">-1</td>\n<td align=\"left\">收集的日志在删除前保留多长时间。-1 禁用。注意，这个参数设置太小会对 NameNode 发送垃圾信息。</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.log-aggregation.retain-check-interval-seconds</td>\n<td align=\"left\">-1</td>\n<td align=\"left\">检查收集日志保留的间隔时间。如果设置为 0 或一个负数，那么值被计算为十分之一的聚合日志保留时间。注意，这个参数设置太小会对 NameNode 发送垃圾信息。</td>\n</tr>\n</tbody></table>\n<h5 id=\"etc-hadoop-mapred-site-xml\"><a href=\"#etc-hadoop-mapred-site-xml\" class=\"headerlink\" title=\"etc/hadoop/mapred-site.xml\"></a>etc/hadoop/mapred-site.xml</h5><ul>\n<li>设置 MapReduce Applications：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th align=\"left\">参数</th>\n<th align=\"left\">值</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">mapreduce.framework.name</td>\n<td align=\"left\">yarn</td>\n<td align=\"left\">执行框架设置为 Hadoop YARN。</td>\n</tr>\n<tr>\n<td align=\"left\">mapreduce.map.memory.mb</td>\n<td align=\"left\">1536</td>\n<td align=\"left\">Map 任务较大的资源限制。</td>\n</tr>\n<tr>\n<td align=\"left\">mapreduce.map.java.opts</td>\n<td align=\"left\">-Xmx1024M</td>\n<td align=\"left\">Map 任务子虚拟机较大的堆大小。</td>\n</tr>\n<tr>\n<td align=\"left\">mapreduce.reduce.memory.mb</td>\n<td align=\"left\">3072</td>\n<td align=\"left\">Reduce 任务较大的资源限制。</td>\n</tr>\n<tr>\n<td align=\"left\">mapreduce.reduce.java.opts</td>\n<td align=\"left\">-Xmx2560M</td>\n<td align=\"left\">Reduce 任务子虚拟机较大的堆大小。</td>\n</tr>\n<tr>\n<td align=\"left\">mapreduce.task.io.sort.mb</td>\n<td align=\"left\">512</td>\n<td align=\"left\">当为了提高数据排序性能设置的较高内存限制。</td>\n</tr>\n<tr>\n<td align=\"left\">mapreduce.task.io.sort.factor</td>\n<td align=\"left\">100</td>\n<td align=\"left\">在排序文件时，多个流合并一次。</td>\n</tr>\n<tr>\n<td align=\"left\">mapreduce.reduce.shuffle.parallelcopies</td>\n<td align=\"left\">50</td>\n<td align=\"left\">当 Reduce 从大量 Map 任务获取输出时更高数量的并行拷贝。</td>\n</tr>\n</tbody></table>\n<ul>\n<li>设置 MapReduce JobHistory Server：</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th align=\"left\">参数</th>\n<th align=\"left\">值</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">mapreduce.jobhistory.address</td>\n<td align=\"left\">MapReduce JobHistory Server 的 host:port</td>\n<td align=\"left\">默认端口是 10020</td>\n</tr>\n<tr>\n<td align=\"left\">mapreduce.jobhistory.webapp.address</td>\n<td align=\"left\">MapReduce JobHistory Server Web UI 的 host:port</td>\n<td align=\"left\">默认端口是 19888</td>\n</tr>\n<tr>\n<td align=\"left\">mapreduce.jobhistory.intermediate-done-dir</td>\n<td align=\"left\">/mr-history/tmp</td>\n<td align=\"left\">MapReduce 作业写入历史文件的目录</td>\n</tr>\n<tr>\n<td align=\"left\">mapreduce.jobhistory.done-dir</td>\n<td align=\"left\">/mr-history/done</td>\n<td align=\"left\">MR JobHistory Server 管理历史文件的目录</td>\n</tr>\n</tbody></table>\n<h3 id=\"监测-NodeManager-的健康状态：\"><a href=\"#监测-NodeManager-的健康状态：\" class=\"headerlink\" title=\"监测 NodeManager 的健康状态：\"></a>监测 NodeManager 的健康状态：</h3><p>Hadoop 提供了一种机制，管理员可以配置 NodeManager 周期性运行管理脚本来检测一个节点是否健康。</p>\n<p>管理员通过执行任何他们选择的脚本中的检查来监测节点是否在健康状态。如果脚本监测到节点处于异常状态，它必须向标准输出打印一行以 ERROR 开头的字符串。NodeManager 周期性调用脚本并检查它的输出。如果脚本的输出包含字符串 ERROR，像上面描述的，节点的状态报告为 <em>不健康</em> 并且这个节点会被 ResourceManager 列入黑名单。不会再有任务指派到这个节点。然而，NodeManager 会继续执行脚本，因此如果节点再次变为健康的，它会自动被 ResourceManager 从黑名单中移除。节点的健康状态随着脚本的输出，如果节点不健康，管理员可以在 ResourceManager Web 界面获得。当节点变为健康的时候也可以在 Web 界面看到。</p>\n<p>下面在 etc/hadoop/yarn-site.xml 中的参数可以用来控制节点健康监测脚本：</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">参数</th>\n<th align=\"left\">值</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">yarn.nodemanager.health-checker.script.path</td>\n<td align=\"left\">节点健康监测脚本</td>\n<td align=\"left\">节点健康状态检查脚本</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.nodemanager.health-checker.script.opts</td>\n<td align=\"left\">节点健康监测脚本选项</td>\n<td align=\"left\">节点健康状态检查脚本选项</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.nodemanager.health-checker.script.interval-ms</td>\n<td align=\"left\">节点健康监测脚本执行间隔</td>\n<td align=\"left\">执行健康监测脚本的间隔</td>\n</tr>\n<tr>\n<td align=\"left\">yarn.nodemanager.health-checker.script.timeout-ms</td>\n<td align=\"left\">节点健康监测脚本超时时长</td>\n<td align=\"left\">节点健康监测脚本执行超时时长</td>\n</tr>\n</tbody></table>\n<p>健康检查脚本不应该给错误，如果只有一些本地磁盘坏了。NodeManager 有周期性检查本地磁盘健康的能力（特别是检查 nodemanager-local-dirs 和 nodemanager-log-dirs）并且坏掉的目录数量达到 yarn.nodemanager.disk-health-checker.min-healthy-disks 属性设置的值，整个节点被标示为不健康并且这个信息被发送给资源管理器。引导磁盘要么做磁盘阵列，要么通过健康检查脚本识别引导磁盘失败。</p>\n<h3 id=\"Slaves-文件\"><a href=\"#Slaves-文件\" class=\"headerlink\" title=\"Slaves 文件\"></a>Slaves 文件</h3><p>在 etc/hadoop/slaves 文件中列出所有从节点的主机名或者 IP，在一行中。帮助脚本（下面描述的）将使用 etc/hadoop/slaves 一次性在很多主机上执行命令。它不会用在任何基于 Java 的 Hadoop 配置。为了用这个功能，必须为运行 Hadoop 的账号建立 ssh 信任（通过免密码 ssh 或者其他方式，如 Kerberos）。</p>\n<h3 id=\"Hadoop-机架意识\"><a href=\"#Hadoop-机架意识\" class=\"headerlink\" title=\"Hadoop 机架意识\"></a>Hadoop 机架意识</h3><p>很多 Hadoop 组件是机架感知的并且利用网络拓扑提高性能和安全。Hadoop 守护进程通过调用一个管理员配置模块获取集群中从节点的机架信息。查看<a href=\"http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/RackAwareness.html\">机架意识</a>获取更多详细信息。</p>\n<p>强烈推荐配置机架意识优先来启动 HDFS。</p>\n<h3 id=\"日志\"><a href=\"#日志\" class=\"headerlink\" title=\"日志\"></a>日志</h3><p>Hadoop 用 <a href=\"http://logging.apache.org/log4j/2.x/\">Apache log4j</a> 通过 Apache Commons Logging 框架来记录日志。编辑 etc/hadoop/log4j.properties 定制 Hadoop 守护进程的日志配置（日志格式等）。</p>\n<h3 id=\"操作-Hadoop-集群\"><a href=\"#操作-Hadoop-集群\" class=\"headerlink\" title=\"操作 Hadoop 集群\"></a>操作 Hadoop 集群</h3><p>当所有必须的配置完成后，分发所有配置文件到所有机器上的 HADOOP_CONF_DIR 目录。这应该是在所有机器上相同的目录。</p>\n<p>通常，推荐 HDFS 和 YARN 作为分别独立的用户运行。在大多数安装中，HDFS 进程执行为 hdfs。YARN 通常使用 yarn 帐号。</p>\n<h4 id=\"启动-Hadoop\"><a href=\"#启动-Hadoop\" class=\"headerlink\" title=\"启动 Hadoop\"></a>启动 Hadoop</h4><p>启动 Hadoop 集群需要启动 HDFS 和 YARN 集群。</p>\n<p>第一次启动 HDFS，必须先格式化。用 hdfs 帐号格式化一个新的分布式文件系统：</p>\n<pre><code>[hdfs]$ $HADOOP_PREFIX/bin/hdfs namenode -format &lt;cluster_name&gt;\n</code></pre>\n<p>用 hdfs 帐号用下面的命令在指定的节点上启动 HDFS NameNode：</p>\n<pre><code>[hdfs]$ $HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs start namenode\n</code></pre>\n<p>用 hdfs 帐号用下面的命令在每个指定的节点上启动 HDFS DataNode：</p>\n<pre><code>[hdfs]$ $HADOOP_PREFIX/sbin/hadoop-daemons.sh --config $HADOOP_CONF_DIR --script hdfs start datanode\n</code></pre>\n<p>如果 etc/hadoop/slaves 和 ssh 信任登录被配置（查看 <a href=\"http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html\">Single Node Setup</a>)，所有的 HDFS 进程可以用一个工具脚本启动。使用 hdfs 帐号：</p>\n<pre><code>[hdfs]$ $HADOOP_PREFIX/sbin/start-dfs.sh\n</code></pre>\n<p>用下面的命令启动 YARN，用 yarn 帐号在指定的 ResourceManager 上运行：</p>\n<pre><code>[yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR start resourcemanager\n</code></pre>\n<p>用 yarn 帐号在每个指定的节点上运行一个脚本启动 NodeManager：</p>\n<pre><code>[yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemons.sh --config $HADOOP_CONF_DIR start nodemanager\n</code></pre>\n<p>启动一个单独的 WebAppProxy 服务器。在 WebAppProxy 服务器上用 yarn 帐号运行。如果用多台服务器来做负载均衡，那么应该在它们每个上面运行：</p>\n<pre><code>[yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR start proxyserver\n</code></pre>\n<p>如果 etc/hadoop/slaves 和 ssh 信任登录被配置（查看 <a href=\"http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html\">Single Node Setup</a>)，所有的 YARN 进程可以用一个工具脚本启动。使用 yarn 帐号：</p>\n<pre><code>[yarn]$ $HADOOP_PREFIX/sbin/start-yarn.sh\n</code></pre>\n<p>用下面的命令启动 MapReduce 作业历史服务器，在指定的服务器上用 mapred 帐号：</p>\n<pre><code>[mapred]$ $HADOOP_PREFIX/sbin/mr-jobhistory-daemon.sh --config $HADOOP_CONF_DIR start historyserver\n</code></pre>\n<h4 id=\"关闭-Hadoop\"><a href=\"#关闭-Hadoop\" class=\"headerlink\" title=\"关闭 Hadoop\"></a>关闭 Hadoop</h4><p>用下面的命令停止 NameNode，在指定的 NameNode 上用帐号 hdfs 执行：</p>\n<pre><code>[hdfs]$ $HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs stop namenode\n</code></pre>\n<p>用 hdfs 帐号执行一个脚本停止 DataNode：</p>\n<pre><code>[hdfs]$ $HADOOP_PREFIX/sbin/hadoop-daemons.sh --config $HADOOP_CONF_DIR --script hdfs stop datanode\n</code></pre>\n<p>如果 etc/hadoop/slaves 和 ssh 信任登录被配置（查看 <a href=\"http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html\">Single Node Setup</a>)，所有的 HDFS 进程可以用一个工具脚本停止。使用 hdfs 帐号：</p>\n<pre><code>[hdfs]$ $HADOOP_PREFIX/sbin/stop-dfs.sh\n</code></pre>\n<p>用下面的命令停止 ResourceManager，用 yarn 帐号在指定的 ResourceManager 上运行：</p>\n<pre><code>[yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR stop resourcemanager\n</code></pre>\n<p>在从节点上用 yarn 帐号执行一个脚本来停止 NodeManager：</p>\n<pre><code>[yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemons.sh --config $HADOOP_CONF_DIR stop nodemanager\n</code></pre>\n<p>如果 etc/hadoop/slaves 和 ssh 信任登录被配置（查看 <a href=\"http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html\">Single Node Setup</a>)，所有的 YARN 进程可以用一个工具脚本停止。使用 yarn 帐号：</p>\n<pre><code>[yarn]$ $HADOOP_PREFIX/sbin/stop-yarn.sh\n</code></pre>\n<p>停止 WebAppProxy 服务器。用 yarn 帐号在 WebAppProxy 服务器上执行。如果用多台服务器来做负载均衡，那么应该在它们每个上面运行：</p>\n<pre><code>[yarn]$ $HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR stop proxyserver\n</code></pre>\n<p>用下面的命令停止 MapReduce 作业历史服务器，在指定的服务器上用 mapred 帐号：</p>\n<pre><code>[mapred]$ $HADOOP_PREFIX/sbin/mr-jobhistory-daemon.sh --config $HADOOP_CONF_DIR stop historyserver\n</code></pre>\n<h3 id=\"Web-用户界面\"><a href=\"#Web-用户界面\" class=\"headerlink\" title=\"Web 用户界面\"></a>Web 用户界面</h3><p>当 Hadoop 集群启动后，像下面描述的运行检查组件的 web-ui：</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">守护进程</th>\n<th align=\"left\">Web 用户界面</th>\n<th align=\"left\">备注</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">NameNode</td>\n<td align=\"left\"><a href=\"http://nn_host:port/\">http://nn_host:port/</a></td>\n<td align=\"left\">默认的 HTTP 端口是 50070</td>\n</tr>\n<tr>\n<td align=\"left\">ResourceManager</td>\n<td align=\"left\"><a href=\"http://rm_host:port/\">http://rm_host:port/</a></td>\n<td align=\"left\">默认的 HTTP 端口是 8088</td>\n</tr>\n<tr>\n<td align=\"left\">MapReduce JobHistory Server</td>\n<td align=\"left\"><a href=\"http://jhs_host:port/\">http://jhs_host:port/</a></td>\n<td align=\"left\">默认的 HTTP 端口是 19888</td>\n</tr>\n</tbody></table>"},{"title":"嵌入式 Jetty","date":"2016-04-15T07:38:23.000Z","_content":"\n\nJetty 有一个口号，“不要在 Jetty 中部署你的应用，部署 Jetty 在你的应用中！”这个口号意味着构建应用为为标准的 WAR 并且部署到 Jetty 的另外一个选择是，Jetty 被设计为一个软件组件，可以在 Java 程序中像其他 POJO 一样被实例化和使用。换一种方式，以嵌入式模式运行 Jetty 意味着放置一个 HTTP 模块到你的应用中，而不是放置你的应用到 HTTP 服务器。\n\n<!-- more -->\n\n本教程将带领你一步步从最简单的 Jetty 服务器实例化到用标准化部署描述器运行多 Web 应用程序。大部分这些实例的代码是标准 Jetty 项目的一部分。\n\n### 概述\n\n嵌入 Jetty 服务器，下面是通常的步骤，也是本教程实例中展示步骤：\n\n1. 创建一个 [Server](http://download.eclipse.org/jetty/stable-9/apidocs/org/eclipse/jetty/server/Server.html) 实例。  \n2. 添加/配置 [Connectors](http://download.eclipse.org/jetty/stable-9/apidocs/org/eclipse/jetty/server/Connector.html)。  \n3. 添加/配置 [Handlers](http://download.eclipse.org/jetty/stable-9/apidocs/org/eclipse/jetty/server/Handler.html)、[Contexts](http://download.eclipse.org/jetty/stable-9/apidocs/org/eclipse/jetty/server/handler/ContextHandler.html)、[Servlets](http://docs.oracle.com/javaee/6/api/javax/servlet/Servlet.html)。  \n4. 启动 Server。  \n5. 服务器等待或者用自己的线程做一些其他的事情。\n\n### 创建 Server\n\n下面是 SimplestServer.java 文件的代码，实例化并且运行一个可能最简单的 Jetty server。\n\n\t//\n\t//  ========================================================================\n\t//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n\t//  ------------------------------------------------------------------------\n\t//  All rights reserved. This program and the accompanying materials\n\t//  are made available under the terms of the Eclipse Public License v1.0\n\t//  and Apache License v2.0 which accompanies this distribution.\n\t//\n\t//      The Eclipse Public License is available at\n\t//      http://www.eclipse.org/legal/epl-v10.html\n\t//\n\t//      The Apache License v2.0 is available at\n\t//      http://www.opensource.org/licenses/apache2.0.php\n\t//\n\t//  You may elect to redistribute this code under either of these licenses.\n\t//  ========================================================================\n\t//\n\n\tpackage org.eclipse.jetty.embedded;\n\n\timport org.eclipse.jetty.server.Server;\n\n\t/**\n\t * The simplest possible Jetty server.\n\t */\n\tpublic class SimplestServer\n\t{\n\t    public static void main( String[] args ) throws Exception\n\t    {\n\t        Server server = new Server(8080);\n\t        server.start();\n\t        server.dumpStdErr();\n\t        server.join();\n\t    }\n\t}\n\n这将运行一个监听 8080 端口的 HTTP Server。这不是一个有用的 server，因为它没有处理器，因此对所有的请求都将返回 404。\n\n### 使用处理器\n\n为了针对请求生成响应，Jetty 要求在服务器上设置处理器。一个处理器可能：\n\n- 检查/修改 HTTP 请求。\n- 生成完整的 HTTP 响应。\n- 调用其他处理器（见 HandlerWrapper）。\n- 选择一个或者多个处理器调用（见 HandlerCollection）。\n\n#### HelloWorld Handler\n\n下面 HelloHandler.java 的代码展示了一个简单的 hello world 处理器：\n\n\t//\n\t//  ========================================================================\n\t//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n\t//  ------------------------------------------------------------------------\n\t//  All rights reserved. This program and the accompanying materials\n\t//  are made available under the terms of the Eclipse Public License v1.0\n\t//  and Apache License v2.0 which accompanies this distribution.\n\t//\n\t//      The Eclipse Public License is available at\n\t//      http://www.eclipse.org/legal/epl-v10.html\n\t//\n\t//      The Apache License v2.0 is available at\n\t//      http://www.opensource.org/licenses/apache2.0.php\n\t//\n\t//  You may elect to redistribute this code under either of these licenses.\n\t//  ========================================================================\n\t//\n\n\tpackage org.eclipse.jetty.embedded;\n\n\timport java.io.IOException;\n\timport java.io.PrintWriter;\n\n\timport javax.servlet.ServletException;\n\timport javax.servlet.http.HttpServletRequest;\n\timport javax.servlet.http.HttpServletResponse;\n\n\timport org.eclipse.jetty.server.Request;\n\timport org.eclipse.jetty.server.handler.AbstractHandler;\n\n\tpublic class HelloHandler extends AbstractHandler\n\t{\n\t    final String greeting;\n\t    final String body;\n\n\t    public HelloHandler()\n\t    {\n\t        this(\"Hello World\");\n\t    }\n\n\t    public HelloHandler( String greeting )\n\t    {\n\t        this(greeting, null);\n\t    }\n\n\t    public HelloHandler( String greeting, String body )\n\t    {\n\t        this.greeting = greeting;\n\t        this.body = body;\n\t    }\n\n\t    public void handle( String target,\n\t                        Request baseRequest,\n\t                        HttpServletRequest request,\n\t                        HttpServletResponse response ) throws IOException,\n\t                                                      ServletException\n\t    {\n\t        response.setContentType(\"text/html; charset=utf-8\");\n\t        response.setStatus(HttpServletResponse.SC_OK);\n\n\t        PrintWriter out = response.getWriter();\n\n\t        out.println(\"<h1>\" + greeting + \"</h1>\");\n\t        if (body != null)\n\t        {\n\t            out.println(body);\n\t        }\n\n\t        baseRequest.setHandled(true);\n\t    }\n\t}\n\n传给处理方法的参数是：\n\n- target - 请求的目标，可能是一个 URI 或者被命名调度器的名称。\n- baseRequest - 可变的 Jetty 请求对象，这个对象总是未包装的。\n- request - 不变的请求对象，可能被一个过滤器或 servlet 包装过。\n- response - 响应，可能被过滤器或 servlet 包装过。\n\n处理器设置响应的状态、内容类型，并且在使用写入器生成响应体之前标记请求已经被处理。\n\n#### 运行 HelloWorldHandler\n\n为了允许一个处理器处理 HTTP 请求，必须添加处理器到一个服务器实例中。下面 OneHandler.java 代码展示了 Jetty 服务器如何使用 HelloWrold 处理器：\n\n\t//\n\t//  ========================================================================\n\t//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n\t//  ------------------------------------------------------------------------\n\t//  All rights reserved. This program and the accompanying materials\n\t//  are made available under the terms of the Eclipse Public License v1.0\n\t//  and Apache License v2.0 which accompanies this distribution.\n\t//\n\t//      The Eclipse Public License is available at\n\t//      http://www.eclipse.org/legal/epl-v10.html\n\t//\n\t//      The Apache License v2.0 is available at\n\t//      http://www.opensource.org/licenses/apache2.0.php\n\t//\n\t//  You may elect to redistribute this code under either of these licenses.\n\t//  ========================================================================\n\t//\n\n\tpackage org.eclipse.jetty.embedded;\n\n\timport org.eclipse.jetty.server.Server;\n\n\tpublic class OneHandler\n\t{\n\t    public static void main( String[] args ) throws Exception\n\t    {\n\t        Server server = new Server(8080);\n\t        server.setHandler(new HelloHandler());\n\n\t        server.start();\n\t        server.join();\n\t    }\n\t}\n\n在 Jetty 中一个或者更多处理器处理所有请求。一些处理器选择其他特定的处理器（例如，ContextHandlerCollection 使用上下文路径选择 ContextHandler）；其他处理器使用应用逻辑生成响应（例如，ServletHandler 传送请求给应用 Servlet），同时其他处理器做跟生成响应无关的任务（例如，RequestLogHandler 或者 StatisticsHandler）。\n\n后面的章节描述如何结合处理器方面的问题。你可以看到在 Jetty 的 [org.eclipse.jetty.server.handler](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/server/handler/package-summary.html) 包中有一些处理器。\n\n#### Handler Collections and Wrappers\n\n复杂的请求可以通过多种方式组合多个处理器进行处理。Jetty 有多个 [HandlerContainer](http://download.eclipse.org/jetty/stable-9/apidocs/org/eclipse/jetty/server/HandlerContainer.html) 接口的实现：\n\n**[HandlerCollection](http://download.eclipse.org/jetty/stable-9/apidocs/org/eclipse/jetty/server/handler/HandlerCollection.html)**\n\n  管理其他处理器的一个集合，并且按照顺序调用每个处理器。这对组合那些生成响应的处理器和统计数据、记录日志的处理器是有用的。\n\n**[HandlerList](http://download.eclipse.org/jetty/stable-9/apidocs/org/eclipse/jetty/server/handler/HandlerList.html)**\n\n  轮流调用每个处理器的处理器集合，直到抛出一个异常、提交了响应或者 request.isHandled() 返回 true。你可以用它组合根据条件处理一个请求的处理器，例如调用多个上下文直到匹配一个虚拟主机。\n\n**[HandlerWrapper](http://download.eclipse.org/jetty/stable-9/apidocs/org/eclipse/jetty/server/handler/HandlerWrapper.html)**\n\n  处理器的基础类，你可以在面向方面编程风格中一起使用菊花链处理器。例如，一个标准的 web 应用是通过一个 context、session、security和servlet处理器链实现的。\n\n**[ContextHandlerCollection](http://download.eclipse.org/jetty/stable-9/apidocs/org/eclipse/jetty/server/handler/ContextHandlerCollection.html)**\n\n  一个特定的 HandlerCollection，使用请求 URI 最长的前缀（contextPath）选择包含的 ContextHandler 来处理请求。\n\n#### 处理器的作用域\n\n很多 Jetty 中的标准 Servlet 容器是用 HandlerWrappers 实现的，是一个菊花式处理器链：ContextHandler 到 SessionHandler 到 SecurityHandler 到 ServletHandler。然而，因为 servlet 规范的特性，这个链不是一个处理器的纯粹嵌套，因为外部处理器有时需要内部处理器的过程信息。例如，当 ContextHandler 调用应用监听器来通知它们一个请求进入了上下文，它必须已经知道 ServletHandler 分发请求给哪个 servlet，因此 servletPath 方法返回正确的值。\n\nHandlerWrapper 被定制为 [ScopedHandler](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/server/handler/ScopedHandler.html) 抽象类，它支持作用域的菊花链。例如，如果一个 ServletHandler 嵌套在一个 ContextHandler 中，方法的执行顺序和嵌套是：\n\nServer.handle(...)\n  ContextHandler.doScope(...)\n    ServletHandler.doScope(...)\n      ContextHandler.doHandle(...)\n        ServletHandler.doHandle(...)\n          SomeServlet.service(...)\n\n因此，当 ContextHandler 处理请求的时候，它将在 ServletHandler 建立的作用域内进行。\n\n#### Resource Handler\n\n[FileServer](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/FileServer.html) 示例展示了如何使用 ResourceHandler 在当前的工作目录提供静态内容服务。\n\n\t//\n\t//  ========================================================================\n\t//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n\t//  ------------------------------------------------------------------------\n\t//  All rights reserved. This program and the accompanying materials\n\t//  are made available under the terms of the Eclipse Public License v1.0\n\t//  and Apache License v2.0 which accompanies this distribution.\n\t//\n\t//      The Eclipse Public License is available at\n\t//      http://www.eclipse.org/legal/epl-v10.html\n\t//\n\t//      The Apache License v2.0 is available at\n\t//      http://www.opensource.org/licenses/apache2.0.php\n\t//\n\t//  You may elect to redistribute this code under either of these licenses.\n\t//  ========================================================================\n\t//\n\n\tpackage org.eclipse.jetty.embedded;\n\n\timport org.eclipse.jetty.server.Handler;\n\timport org.eclipse.jetty.server.Server;\n\timport org.eclipse.jetty.server.handler.DefaultHandler;\n\timport org.eclipse.jetty.server.handler.HandlerList;\n\timport org.eclipse.jetty.server.handler.ResourceHandler;\n\timport org.eclipse.jetty.server.handler.gzip.GzipHandler;\n\n\t/**\n\t * Simple Jetty FileServer.\n\t * This is a simple example of Jetty configured as a FileServer.\n\t */\n\tpublic class FileServer\n\t{\n\t    public static void main(String[] args) throws Exception\n\t    {\n\t        // Create a basic Jetty server object that will listen on port 8080.  Note that if you set this to port 0\n\t        // then a randomly available port will be assigned that you can either look in the logs for the port,\n\t        // or programmatically obtain it for use in test cases.\n\t        Server server = new Server(8080);\n\n\t        // Create the ResourceHandler. It is the object that will actually handle the request for a given file. It is\n\t        // a Jetty Handler object so it is suitable for chaining with other handlers as you will see in other examples.\n\t        ResourceHandler resource_handler = new ResourceHandler();\n\t        // Configure the ResourceHandler. Setting the resource base indicates where the files should be served out of.\n\t        // In this example it is the current directory but it can be configured to anything that the jvm has access to.\n\t        resource_handler.setDirectoriesListed(true);\n\t        resource_handler.setWelcomeFiles(new String[]{ \"index.html\" });\n\t        resource_handler.setResourceBase(\".\");\n\n\t        // Add the ResourceHandler to the server.\n\t        GzipHandler gzip = new GzipHandler();\n\t        server.setHandler(gzip);\n\t        HandlerList handlers = new HandlerList();\n\t        handlers.setHandlers(new Handler[] { resource_handler, new DefaultHandler() });\n\t        gzip.setHandler(handlers);\n\n\t        // Start things up! By using the server.join() the server thread will join with the current thread.\n\t        // See \"http://docs.oracle.com/javase/1.5.0/docs/api/java/lang/Thread.html#join()\" for more details.\n\t        server.start();\n\t        server.join();\n\t    }\n\t}\n\n注意，ResourceHandler 和 DefaultHandler 使用了一个 HandlerList，因此 DefaultHandler 为所有找不到静态资源的请求生成良好的 404 响应。\n\n### 嵌入式 Connectors\n\n在前面的示例中，Server 实例传入了一个端口号，并且在内部创建一个在这个端口监听请求的 Connector 默认实例。然而，通常当嵌入 Jetty 的时候，需要明确实例化，并且给 Server 实例配置一个或多个 Connectors。\n\n#### 一个 Connector\n\n下面的示例，[OneConnector.java](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/OneConnector.html)，实例化、配置并添加一个 HTTP 连接器实例到服务器：\n\n\t//\n\t//  ========================================================================\n\t//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n\t//  ------------------------------------------------------------------------\n\t//  All rights reserved. This program and the accompanying materials\n\t//  are made available under the terms of the Eclipse Public License v1.0\n\t//  and Apache License v2.0 which accompanies this distribution.\n\t//\n\t//      The Eclipse Public License is available at\n\t//      http://www.eclipse.org/legal/epl-v10.html\n\t//\n\t//      The Apache License v2.0 is available at\n\t//      http://www.opensource.org/licenses/apache2.0.php\n\t//\n\t//  You may elect to redistribute this code under either of these licenses.\n\t//  ========================================================================\n\t//\n\n\tpackage org.eclipse.jetty.embedded;\n\n\timport org.eclipse.jetty.server.Server;\n\timport org.eclipse.jetty.server.ServerConnector;\n\n\t/**\n\t * A Jetty server with one connectors.\n\t */\n\tpublic class OneConnector\n\t{\n\t    public static void main( String[] args ) throws Exception\n\t    {\n\t        // The Server\n\t        Server server = new Server();\n\n\t        // HTTP connector\n\t        ServerConnector http = new ServerConnector(server);\n\t        http.setHost(\"localhost\");\n\t        http.setPort(8080);\n\t        http.setIdleTimeout(30000);\n\n\t        // Set the connector\n\t        server.addConnector(http);\n\n\t        // Set a handler\n\t        server.setHandler(new HelloHandler());\n\n\t        // Start the server\n\t        server.start();\n\t        server.join();\n\t    }\n\t}\n\n在这个示例中，连接器处理 HTTP 协议，这是 [ServerConnector](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/server/ServerConnector.html) 类默认的。\n\n#### 多个 Connectors\n\n当配置多个连接器（例如，HTTP 和 HTTPS）时，共享 HTTP 通用参数的配置是可取的。为了达到这个目的，你需要明确配置 ConnectionFactory 实例和 ServerConnector 类，并且跟 HTTP 通用配置一起提供。\n\n[ManyConnectors 示例](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/ManyConnectors.html) 配置了有两个 ServerConnector 实例的服务器：HTTP 连接器有一个 [HTTPConnectionFactory](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/server/HttpConnectionFactory.html) 实例；HTTPS 连接器有一个链接到 HttpConnectionFactory 的 SslConnectionFactory。两个 HttpConnectionFactories 在同一个 [HttpConfiguration](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/server/HttpConfiguration.html) 上配置，然而 HTTPS 工厂用一个包装的配置，所以可以添加 [SecureRequestCustomizer](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/server/SecureRequestCustomizer.html)。\n\n### 嵌入 Servlets\n\n[Servlets](https://en.wikipedia.org/wiki/Java_servlet) 是提供处理 HTTP 请求应用逻辑的一般方式。Servlets 跟 Jetty 的处理器是相同的，除非请求对象是不可变的，因此不能被修改。在 Jetty 中 Servlets 通过一个 [ServletHandler](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/MinimalServlets.html) 来处理。它使用标准的路径映射让 Servlet 和请求匹配；设置请求的 servletPath 和路径信息；传送请求到 servlet，可能通过 Filters 生成响应。\n\n[MinimalServlets 示例](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/MinimalServlets.html) 创建了一个 ServletHandler 实例并且配置了一个 HelloServlet:\n\n\t//\n\t//  ========================================================================\n\t//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n\t//  ------------------------------------------------------------------------\n\t//  All rights reserved. This program and the accompanying materials\n\t//  are made available under the terms of the Eclipse Public License v1.0\n\t//  and Apache License v2.0 which accompanies this distribution.\n\t//\n\t//      The Eclipse Public License is available at\n\t//      http://www.eclipse.org/legal/epl-v10.html\n\t//\n\t//      The Apache License v2.0 is available at\n\t//      http://www.opensource.org/licenses/apache2.0.php\n\t//\n\t//  You may elect to redistribute this code under either of these licenses.\n\t//  ========================================================================\n\t//\n\n\tpackage org.eclipse.jetty.embedded;\n\n\timport java.io.IOException;\n\n\timport javax.servlet.ServletException;\n\timport javax.servlet.http.HttpServlet;\n\timport javax.servlet.http.HttpServletRequest;\n\timport javax.servlet.http.HttpServletResponse;\n\n\timport org.eclipse.jetty.server.Server;\n\timport org.eclipse.jetty.servlet.ServletHandler;\n\n\tpublic class MinimalServlets\n\t{\n\t    public static void main( String[] args ) throws Exception\n\t    {\n\t        // Create a basic jetty server object that will listen on port 8080.\n\t        // Note that if you set this to port 0 then a randomly available port\n\t        // will be assigned that you can either look in the logs for the port,\n\t        // or programmatically obtain it for use in test cases.\n\t        Server server = new Server(8080);\n\n\t        // The ServletHandler is a dead simple way to create a context handler\n\t        // that is backed by an instance of a Servlet.\n\t        // This handler then needs to be registered with the Server object.\n\t        ServletHandler handler = new ServletHandler();\n\t        server.setHandler(handler);\n\n\t        // Passing in the class for the Servlet allows jetty to instantiate an\n\t        // instance of that Servlet and mount it on a given context path.\n\n\t        // IMPORTANT:\n\t        // This is a raw Servlet, not a Servlet that has been configured\n\t        // through a web.xml @WebServlet annotation, or anything similar.\n\t        handler.addServletWithMapping(HelloServlet.class, \"/*\");\n\n\t        // Start things up!\n\t        server.start();\n\n\t        // The use of server.join() the will make the current thread join and\n\t        // wait until the server is done executing.\n\t        // See\n\t        // http://docs.oracle.com/javase/7/docs/api/java/lang/Thread.html#join()\n\t        server.join();\n\t    }\n\n\t    @SuppressWarnings(\"serial\")\n\t    public static class HelloServlet extends HttpServlet\n\t    {\n\t        @Override\n\t        protected void doGet( HttpServletRequest request,\n\t                              HttpServletResponse response ) throws ServletException,\n\t                                                            IOException\n\t        {\n\t            response.setContentType(\"text/html\");\n\t            response.setStatus(HttpServletResponse.SC_OK);\n\t            response.getWriter().println(\"<h1>Hello from HelloServlet</h1>\");\n\t        }\n\t    }\n\t}\n\n### 嵌入 Contexts\n\n[ContextHandler](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/OneContext.html) 是一个 ScopedHandler 只来响应那些 URI 前缀匹配已配置的上下文路径的请求。匹配上下文路径的请求会响应的调整它们的路径方法，并且上下文生命周期内是可获取的，可能包含：\n\n- 当请求在作用域内处理时，Classloader 设置为线程上下文的类加载器。\n- 通过过 [ServletContext](http://docs.oracle.com/javaee/6/api/javax/servlet/ServletContext.html) API 可以获取属性的集合。\n- 通过过 [ServletContext](http://docs.oracle.com/javaee/6/api/javax/servlet/ServletContext.html) API 可以获取参数的集合。\n- 通过过 [ServletContext](http://docs.oracle.com/javaee/6/api/javax/servlet/ServletContext.html) API 为静态资源的请求提供一个基本的 Resource 作为文档根。\n- 虚拟主机名称集合。\n\n下面的 [OneContext 示例](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/OneContext.html) 建立了一个包装了 [HelloHandler](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/HelloHandler.html) 的上下文：\n\n\t//\n\t//  ========================================================================\n\t//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n\t//  ------------------------------------------------------------------------\n\t//  All rights reserved. This program and the accompanying materials\n\t//  are made available under the terms of the Eclipse Public License v1.0\n\t//  and Apache License v2.0 which accompanies this distribution.\n\t//\n\t//      The Eclipse Public License is available at\n\t//      http://www.eclipse.org/legal/epl-v10.html\n\t//\n\t//      The Apache License v2.0 is available at\n\t//      http://www.opensource.org/licenses/apache2.0.php\n\t//\n\t//  You may elect to redistribute this code under either of these licenses.\n\t//  ========================================================================\n\t//\n\n\tpackage org.eclipse.jetty.embedded;\n\n\timport org.eclipse.jetty.server.Server;\n\timport org.eclipse.jetty.server.handler.ContextHandler;\n\n\tpublic class OneContext\n\t{\n\t    public static void main( String[] args ) throws Exception\n\t    {\n\t        Server server = new Server( 8080 );\n\n\t        // Add a single handler on context \"/hello\"\n\t        ContextHandler context = new ContextHandler();\n\t        context.setContextPath( \"/hello\" );\n\t        context.setHandler( new HelloHandler() );\n\n\t        // Can be accessed using http://localhost:8080/hello\n\n\t        server.setHandler( context );\n\n\t        // Start the server\n\t        server.start();\n\t        server.join();\n\t    }\n\t}\n\n当有很多上下文出现时，你可以嵌入一个 ContextHandlerCollection 来有效的测试一个请求的 URI，然后选择匹配到的 ContextHandler。[ManyContexts 示例](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/ManyContexts.html) 展示了可以配置多个上下文：\n\n\t//\n\t//  ========================================================================\n\t//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n\t//  ------------------------------------------------------------------------\n\t//  All rights reserved. This program and the accompanying materials\n\t//  are made available under the terms of the Eclipse Public License v1.0\n\t//  and Apache License v2.0 which accompanies this distribution.\n\t//\n\t//      The Eclipse Public License is available at\n\t//      http://www.eclipse.org/legal/epl-v10.html\n\t//\n\t//      The Apache License v2.0 is available at\n\t//      http://www.opensource.org/licenses/apache2.0.php\n\t//\n\t//  You may elect to redistribute this code under either of these licenses.\n\t//  ========================================================================\n\t//\n\n\tpackage org.eclipse.jetty.embedded;\n\n\timport org.eclipse.jetty.server.Handler;\n\timport org.eclipse.jetty.server.Server;\n\timport org.eclipse.jetty.server.handler.ContextHandler;\n\timport org.eclipse.jetty.server.handler.ContextHandlerCollection;\n\n\tpublic class ManyContexts\n\t{\n\t    public static void main( String[] args ) throws Exception\n\t    {\n\t        Server server = new Server(8080);\n\n\t        ContextHandler context = new ContextHandler(\"/\");\n\t        context.setContextPath(\"/\");\n\t        context.setHandler(new HelloHandler(\"Root Hello\"));\n\n\t        ContextHandler contextFR = new ContextHandler(\"/fr\");\n\t        contextFR.setHandler(new HelloHandler(\"Bonjoir\"));\n\n\t        ContextHandler contextIT = new ContextHandler(\"/it\");\n\t        contextIT.setHandler(new HelloHandler(\"Bongiorno\"));\n\n\t        ContextHandler contextV = new ContextHandler(\"/\");\n\t        contextV.setVirtualHosts(new String[] { \"127.0.0.2\" });\n\t        contextV.setHandler(new HelloHandler(\"Virtual Hello\"));\n\n\t        ContextHandlerCollection contexts = new ContextHandlerCollection();\n\t        contexts.setHandlers(new Handler[] { context, contextFR, contextIT,\n\t                contextV });\n\n\t        server.setHandler(contexts);\n\n\t        server.start();\n\t        server.join();\n\t    }\n\t}\n\n### 嵌入 ServletContexts\n\n[ServletContextHandler](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/servlet/ServletContextHandler.html) 是一个支持普通 session 和 Servlet 的特定的 ContextHandler。下面的 [OneServletContext 示例](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/OneServletContext.html) 实例化了一个 [DefaultServlet](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/servlet/DefaultServlet.html) 从 /tmp/ 提供静态内容，并且实例化一个 DumpServlet 创建 session 并存储请求的基本详细信息：\n\n\t//\n\t//  ========================================================================\n\t//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n\t//  ------------------------------------------------------------------------\n\t//  All rights reserved. This program and the accompanying materials\n\t//  are made available under the terms of the Eclipse Public License v1.0\n\t//  and Apache License v2.0 which accompanies this distribution.\n\t//\n\t//      The Eclipse Public License is available at\n\t//      http://www.eclipse.org/legal/epl-v10.html\n\t//\n\t//      The Apache License v2.0 is available at\n\t//      http://www.opensource.org/licenses/apache2.0.php\n\t//\n\t//  You may elect to redistribute this code under either of these licenses.\n\t//  ========================================================================\n\t//\n\n\tpackage org.eclipse.jetty.embedded;\n\n\timport org.eclipse.jetty.server.Server;\n\timport org.eclipse.jetty.servlet.DefaultServlet;\n\timport org.eclipse.jetty.servlet.ServletContextHandler;\n\n\tpublic class OneServletContext\n\t{\n\t    public static void main( String[] args ) throws Exception\n\t    {\n\t        Server server = new Server(8080);\n\n\t        ServletContextHandler context = new ServletContextHandler(\n\t                ServletContextHandler.SESSIONS);\n\t        context.setContextPath(\"/\");\n\t        context.setResourceBase(System.getProperty(\"java.io.tmpdir\"));\n\t        server.setHandler(context);\n\n\t        // Add dump servlet\n\t        context.addServlet(DumpServlet.class, \"/dump/*\");\n\t        // Add default servlet\n\t        context.addServlet(DefaultServlet.class, \"/\");\n\n\t        server.start();\n\t        server.join();\n\t    }\n\t}\n\n### 嵌入 Web Applications\n\n[WebAppContext](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/webapp/WebAppContext.html) 是一个 ServletContextHandler 的扩展，它使用[标准设计](https://en.wikipedia.org/wiki/WAR_(file_format))和 web.xml 配置 servlet、filter 和 web.xml 或注解的其他特性。下面的 [OneWebApp 示例](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/OneWebApp.html) 配置了 Jetty 的测试 web 应用。Web 应用程序可以使用容器提供的的资源，在这个案例中需要并配置一个 LoginService：\n\n\t//\n\t//  ========================================================================\n\t//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n\t//  ------------------------------------------------------------------------\n\t//  All rights reserved. This program and the accompanying materials\n\t//  are made available under the terms of the Eclipse Public License v1.0\n\t//  and Apache License v2.0 which accompanies this distribution.\n\t//\n\t//      The Eclipse Public License is available at\n\t//      http://www.eclipse.org/legal/epl-v10.html\n\t//\n\t//      The Apache License v2.0 is available at\n\t//      http://www.opensource.org/licenses/apache2.0.php\n\t//\n\t//  You may elect to redistribute this code under either of these licenses.\n\t//  ========================================================================\n\t//\n\n\tpackage org.eclipse.jetty.embedded;\n\n\timport java.io.File;\n\timport java.lang.management.ManagementFactory;\n\n\timport org.eclipse.jetty.jmx.MBeanContainer;\n\timport org.eclipse.jetty.security.HashLoginService;\n\timport org.eclipse.jetty.server.Server;\n\timport org.eclipse.jetty.server.handler.AllowSymLinkAliasChecker;\n\timport org.eclipse.jetty.webapp.WebAppContext;\n\n\tpublic class OneWebApp\n\t{\n\t    public static void main( String[] args ) throws Exception\n\t    {\n\t        // Create a basic jetty server object that will listen on port 8080.\n\t        // Note that if you set this to port 0 then a randomly available port\n\t        // will be assigned that you can either look in the logs for the port,\n\t        // or programmatically obtain it for use in test cases.\n\t        Server server = new Server(8080);\n\n\t        // Setup JMX\n\t        MBeanContainer mbContainer = new MBeanContainer(\n\t                ManagementFactory.getPlatformMBeanServer());\n\t        server.addBean(mbContainer);\n\n\t        // The WebAppContext is the entity that controls the environment in\n\t        // which a web application lives and breathes. In this example the\n\t        // context path is being set to \"/\" so it is suitable for serving root\n\t        // context requests and then we see it setting the location of the war.\n\t        // A whole host of other configurations are available, ranging from\n\t        // configuring to support annotation scanning in the webapp (through\n\t        // PlusConfiguration) to choosing where the webapp will unpack itself.\n\t        WebAppContext webapp = new WebAppContext();\n\t        webapp.setContextPath(\"/\");\n\t        File warFile = new File(\n\t                \"../../tests/test-jmx/jmx-webapp/target/jmx-webapp\");\n\t        webapp.setWar(warFile.getAbsolutePath());\n\n\t        // A WebAppContext is a ContextHandler as well so it needs to be set to\n\t        // the server so it is aware of where to send the appropriate requests.\n\t        server.setHandler(webapp);\n\n\t        // Start things up!\n\t        server.start();\n\n\t        // The use of server.join() the will make the current thread join and\n\t        // wait until the server is done executing.\n\t        // See http://docs.oracle.com/javase/7/docs/api/java/lang/Thread.html#join()\n\t        server.join();\n\t    }\n\t}\n\n### 喜欢 Jetty XML\n\n配置 Jetty 服务器实例的典型方式是通过 jetty.xml 关联配置文件。然而 Jetty XML 配置格式只是在代码中可以做的简单的表达；也可以很简单的写嵌入代码准确实现 jetty.xml 配置。下面的 [LikeJettyXml 示例](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/LikeJettyXml.html) 在代码中呈现了从配置文件获取的行为：\n\n- jetty.xml\n- jetty-jmx.xml\n- jetty-http.xml\n- jetty-https.xml\n- jetty-deploy.xml\n- jetty-stats.xml\n- jetty-requestlog.xml\n- jetty-lowresources.xml\n- test-realm.xml\n\n\n\t//\n\t//  ========================================================================\n\t//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n\t//  ------------------------------------------------------------------------\n\t//  All rights reserved. This program and the accompanying materials\n\t//  are made available under the terms of the Eclipse Public License v1.0\n\t//  and Apache License v2.0 which accompanies this distribution.\n\t//\n\t//      The Eclipse Public License is available at\n\t//      http://www.eclipse.org/legal/epl-v10.html\n\t//\n\t//      The Apache License v2.0 is available at\n\t//      http://www.opensource.org/licenses/apache2.0.php\n\t//\n\t//  You may elect to redistribute this code under either of these licenses.\n\t//  ========================================================================\n\t//\n\n\tpackage org.eclipse.jetty.embedded;\n\n\timport java.io.File;\n\timport java.io.FileNotFoundException;\n\timport java.lang.management.ManagementFactory;\n\n\timport org.eclipse.jetty.deploy.DeploymentManager;\n\timport org.eclipse.jetty.deploy.PropertiesConfigurationManager;\n\timport org.eclipse.jetty.deploy.bindings.DebugListenerBinding;\n\timport org.eclipse.jetty.deploy.providers.WebAppProvider;\n\timport org.eclipse.jetty.http.HttpVersion;\n\timport org.eclipse.jetty.jmx.MBeanContainer;\n\timport org.eclipse.jetty.security.HashLoginService;\n\timport org.eclipse.jetty.server.DebugListener;\n\timport org.eclipse.jetty.server.Handler;\n\timport org.eclipse.jetty.server.HttpConfiguration;\n\timport org.eclipse.jetty.server.HttpConnectionFactory;\n\timport org.eclipse.jetty.server.LowResourceMonitor;\n\timport org.eclipse.jetty.server.NCSARequestLog;\n\timport org.eclipse.jetty.server.SecureRequestCustomizer;\n\timport org.eclipse.jetty.server.Server;\n\timport org.eclipse.jetty.server.ServerConnector;\n\timport org.eclipse.jetty.server.SslConnectionFactory;\n\timport org.eclipse.jetty.server.handler.ContextHandlerCollection;\n\timport org.eclipse.jetty.server.handler.DefaultHandler;\n\timport org.eclipse.jetty.server.handler.HandlerCollection;\n\timport org.eclipse.jetty.server.handler.RequestLogHandler;\n\timport org.eclipse.jetty.server.handler.StatisticsHandler;\n\timport org.eclipse.jetty.util.ssl.SslContextFactory;\n\timport org.eclipse.jetty.util.thread.QueuedThreadPool;\n\timport org.eclipse.jetty.util.thread.ScheduledExecutorScheduler;\n\timport org.eclipse.jetty.webapp.Configuration;\n\n\t/**\n\t * Starts the Jetty Distribution's demo-base directory using entirely\n\t * embedded jetty techniques.\n\t */\n\tpublic class LikeJettyXml\n\t{\n\t    public static void main( String[] args ) throws Exception\n\t    {\n\t        // Path to as-built jetty-distribution directory\n\t        String jettyHomeBuild = \"../../jetty-distribution/target/distribution\";\n\n\t        // Find jetty home and base directories\n\t        String homePath = System.getProperty(\"jetty.home\", jettyHomeBuild);\n\t        File homeDir = new File(homePath);\n\t        if (!homeDir.exists())\n\t        {\n\t            throw new FileNotFoundException(homeDir.getAbsolutePath());\n\t        }\n\t        String basePath = System.getProperty(\"jetty.base\", homeDir + \"/demo-base\");\n\t        File baseDir = new File(basePath);\n\t        if(!baseDir.exists())\n\t        {\n\t            throw new FileNotFoundException(baseDir.getAbsolutePath());\n\t        }\n\n\t        // Configure jetty.home and jetty.base system properties\n\t        String jetty_home = homeDir.getAbsolutePath();\n\t        String jetty_base = baseDir.getAbsolutePath();\n\t        System.setProperty(\"jetty.home\", jetty_home);\n\t        System.setProperty(\"jetty.base\", jetty_base);\n\n\n\t        // === jetty.xml ===\n\t        // Setup Threadpool\n\t        QueuedThreadPool threadPool = new QueuedThreadPool();\n\t        threadPool.setMaxThreads(500);\n\n\t        // Server\n\t        Server server = new Server(threadPool);\n\n\t        // Scheduler\n\t        server.addBean(new ScheduledExecutorScheduler());\n\n\t        // HTTP Configuration\n\t        HttpConfiguration http_config = new HttpConfiguration();\n\t        http_config.setSecureScheme(\"https\");\n\t        http_config.setSecurePort(8443);\n\t        http_config.setOutputBufferSize(32768);\n\t        http_config.setRequestHeaderSize(8192);\n\t        http_config.setResponseHeaderSize(8192);\n\t        http_config.setSendServerVersion(true);\n\t        http_config.setSendDateHeader(false);\n\t        // httpConfig.addCustomizer(new ForwardedRequestCustomizer());\n\n\t        // Handler Structure\n\t        HandlerCollection handlers = new HandlerCollection();\n\t        ContextHandlerCollection contexts = new ContextHandlerCollection();\n\t        handlers.setHandlers(new Handler[] { contexts, new DefaultHandler() });\n\t        server.setHandler(handlers);\n\n\t        // Extra options\n\t        server.setDumpAfterStart(false);\n\t        server.setDumpBeforeStop(false);\n\t        server.setStopAtShutdown(true);\n\n\t        // === jetty-jmx.xml ===\n\t        MBeanContainer mbContainer = new MBeanContainer(\n\t                ManagementFactory.getPlatformMBeanServer());\n\t        server.addBean(mbContainer);\n\n\n\t        // === jetty-http.xml ===\n\t        ServerConnector http = new ServerConnector(server,\n\t                new HttpConnectionFactory(http_config));\n\t        http.setPort(8080);\n\t        http.setIdleTimeout(30000);\n\t        server.addConnector(http);\n\n\n\t        // === jetty-https.xml ===\n\t        // SSL Context Factory\n\t        SslContextFactory sslContextFactory = new SslContextFactory();\n\t        sslContextFactory.setKeyStorePath(jetty_home + \"/../../../jetty-server/src/test/config/etc/keystore\");\n\t        sslContextFactory.setKeyStorePassword(\"OBF:1vny1zlo1x8e1vnw1vn61x8g1zlu1vn4\");\n\t        sslContextFactory.setKeyManagerPassword(\"OBF:1u2u1wml1z7s1z7a1wnl1u2g\");\n\t        sslContextFactory.setTrustStorePath(jetty_home + \"/../../../jetty-server/src/test/config/etc/keystore\");\n\t        sslContextFactory.setTrustStorePassword(\"OBF:1vny1zlo1x8e1vnw1vn61x8g1zlu1vn4\");\n\t        sslContextFactory.setExcludeCipherSuites(\"SSL_RSA_WITH_DES_CBC_SHA\",\n\t                \"SSL_DHE_RSA_WITH_DES_CBC_SHA\", \"SSL_DHE_DSS_WITH_DES_CBC_SHA\",\n\t                \"SSL_RSA_EXPORT_WITH_RC4_40_MD5\",\n\t                \"SSL_RSA_EXPORT_WITH_DES40_CBC_SHA\",\n\t                \"SSL_DHE_RSA_EXPORT_WITH_DES40_CBC_SHA\",\n\t                \"SSL_DHE_DSS_EXPORT_WITH_DES40_CBC_SHA\");\n\n\t        // SSL HTTP Configuration\n\t        HttpConfiguration https_config = new HttpConfiguration(http_config);\n\t        https_config.addCustomizer(new SecureRequestCustomizer());\n\n\t        // SSL Connector\n\t        ServerConnector sslConnector = new ServerConnector(server,\n\t            new SslConnectionFactory(sslContextFactory,HttpVersion.HTTP_1_1.asString()),\n\t            new HttpConnectionFactory(https_config));\n\t        sslConnector.setPort(8443);\n\t        server.addConnector(sslConnector);\n\n\n\t        // === jetty-deploy.xml ===\n\t        DeploymentManager deployer = new DeploymentManager();\n\t        DebugListener debug = new DebugListener(System.out,true,true,true);\n\t        server.addBean(debug);        \n\t        deployer.addLifeCycleBinding(new DebugListenerBinding(debug));\n\t        deployer.setContexts(contexts);\n\t        deployer.setContextAttribute(\n\t                \"org.eclipse.jetty.server.webapp.ContainerIncludeJarPattern\",\n\t                \".*/servlet-api-[^/]*\\\\.jar$\");\n\n\t        WebAppProvider webapp_provider = new WebAppProvider();\n\t        webapp_provider.setMonitoredDirName(jetty_base + \"/webapps\");\n\t        webapp_provider.setDefaultsDescriptor(jetty_home + \"/etc/webdefault.xml\");\n\t        webapp_provider.setScanInterval(1);\n\t        webapp_provider.setExtractWars(true);\n\t        webapp_provider.setConfigurationManager(new PropertiesConfigurationManager());\n\n\t        deployer.addAppProvider(webapp_provider);\n\t        server.addBean(deployer);\n\n\t        // === setup jetty plus ==\n\t        Configuration.ClassList.setServerDefault(server).addAfter(\n\t                \"org.eclipse.jetty.webapp.FragmentConfiguration\",\n\t                \"org.eclipse.jetty.plus.webapp.EnvConfiguration\",\n\t                \"org.eclipse.jetty.plus.webapp.PlusConfiguration\");\n\n\t        // === jetty-stats.xml ===\n\t        StatisticsHandler stats = new StatisticsHandler();\n\t        stats.setHandler(server.getHandler());\n\t        server.setHandler(stats);\n\n\n\t        // === jetty-requestlog.xml ===\n\t        NCSARequestLog requestLog = new NCSARequestLog();\n\t        requestLog.setFilename(jetty_home + \"/logs/yyyy_mm_dd.request.log\");\n\t        requestLog.setFilenameDateFormat(\"yyyy_MM_dd\");\n\t        requestLog.setRetainDays(90);\n\t        requestLog.setAppend(true);\n\t        requestLog.setExtended(true);\n\t        requestLog.setLogCookies(false);\n\t        requestLog.setLogTimeZone(\"GMT\");\n\t        RequestLogHandler requestLogHandler = new RequestLogHandler();\n\t        requestLogHandler.setRequestLog(requestLog);\n\t        handlers.addHandler(requestLogHandler);\n\n\n\t        // === jetty-lowresources.xml ===\n\t        LowResourceMonitor lowResourcesMonitor=new LowResourceMonitor(server);\n\t        lowResourcesMonitor.setPeriod(1000);\n\t        lowResourcesMonitor.setLowResourcesIdleTimeout(200);\n\t        lowResourcesMonitor.setMonitorThreads(true);\n\t        lowResourcesMonitor.setMaxConnections(0);\n\t        lowResourcesMonitor.setMaxMemory(0);\n\t        lowResourcesMonitor.setMaxLowResourcesTime(5000);\n\t        server.addBean(lowResourcesMonitor);\n\n\n\t        // === test-realm.xml ===\n\t        HashLoginService login = new HashLoginService();\n\t        login.setName(\"Test Realm\");\n\t        login.setConfig(jetty_base + \"/etc/realm.properties\");\n\t        login.setHotReload(false);\n\t        server.addBean(login);\n\n\t        // Start the server\n\t        server.start();\n\t        server.join();\n\t    }\n\t}\n","source":"_posts/嵌入式-Jetty.md","raw":"title: 嵌入式 Jetty\ntags:\n  - Jetty\ncategories:\n  - 开发工具\n  - Jetty\ndate: 2016-04-15 15:38:23\n---\n\n\nJetty 有一个口号，“不要在 Jetty 中部署你的应用，部署 Jetty 在你的应用中！”这个口号意味着构建应用为为标准的 WAR 并且部署到 Jetty 的另外一个选择是，Jetty 被设计为一个软件组件，可以在 Java 程序中像其他 POJO 一样被实例化和使用。换一种方式，以嵌入式模式运行 Jetty 意味着放置一个 HTTP 模块到你的应用中，而不是放置你的应用到 HTTP 服务器。\n\n<!-- more -->\n\n本教程将带领你一步步从最简单的 Jetty 服务器实例化到用标准化部署描述器运行多 Web 应用程序。大部分这些实例的代码是标准 Jetty 项目的一部分。\n\n### 概述\n\n嵌入 Jetty 服务器，下面是通常的步骤，也是本教程实例中展示步骤：\n\n1. 创建一个 [Server](http://download.eclipse.org/jetty/stable-9/apidocs/org/eclipse/jetty/server/Server.html) 实例。  \n2. 添加/配置 [Connectors](http://download.eclipse.org/jetty/stable-9/apidocs/org/eclipse/jetty/server/Connector.html)。  \n3. 添加/配置 [Handlers](http://download.eclipse.org/jetty/stable-9/apidocs/org/eclipse/jetty/server/Handler.html)、[Contexts](http://download.eclipse.org/jetty/stable-9/apidocs/org/eclipse/jetty/server/handler/ContextHandler.html)、[Servlets](http://docs.oracle.com/javaee/6/api/javax/servlet/Servlet.html)。  \n4. 启动 Server。  \n5. 服务器等待或者用自己的线程做一些其他的事情。\n\n### 创建 Server\n\n下面是 SimplestServer.java 文件的代码，实例化并且运行一个可能最简单的 Jetty server。\n\n\t//\n\t//  ========================================================================\n\t//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n\t//  ------------------------------------------------------------------------\n\t//  All rights reserved. This program and the accompanying materials\n\t//  are made available under the terms of the Eclipse Public License v1.0\n\t//  and Apache License v2.0 which accompanies this distribution.\n\t//\n\t//      The Eclipse Public License is available at\n\t//      http://www.eclipse.org/legal/epl-v10.html\n\t//\n\t//      The Apache License v2.0 is available at\n\t//      http://www.opensource.org/licenses/apache2.0.php\n\t//\n\t//  You may elect to redistribute this code under either of these licenses.\n\t//  ========================================================================\n\t//\n\n\tpackage org.eclipse.jetty.embedded;\n\n\timport org.eclipse.jetty.server.Server;\n\n\t/**\n\t * The simplest possible Jetty server.\n\t */\n\tpublic class SimplestServer\n\t{\n\t    public static void main( String[] args ) throws Exception\n\t    {\n\t        Server server = new Server(8080);\n\t        server.start();\n\t        server.dumpStdErr();\n\t        server.join();\n\t    }\n\t}\n\n这将运行一个监听 8080 端口的 HTTP Server。这不是一个有用的 server，因为它没有处理器，因此对所有的请求都将返回 404。\n\n### 使用处理器\n\n为了针对请求生成响应，Jetty 要求在服务器上设置处理器。一个处理器可能：\n\n- 检查/修改 HTTP 请求。\n- 生成完整的 HTTP 响应。\n- 调用其他处理器（见 HandlerWrapper）。\n- 选择一个或者多个处理器调用（见 HandlerCollection）。\n\n#### HelloWorld Handler\n\n下面 HelloHandler.java 的代码展示了一个简单的 hello world 处理器：\n\n\t//\n\t//  ========================================================================\n\t//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n\t//  ------------------------------------------------------------------------\n\t//  All rights reserved. This program and the accompanying materials\n\t//  are made available under the terms of the Eclipse Public License v1.0\n\t//  and Apache License v2.0 which accompanies this distribution.\n\t//\n\t//      The Eclipse Public License is available at\n\t//      http://www.eclipse.org/legal/epl-v10.html\n\t//\n\t//      The Apache License v2.0 is available at\n\t//      http://www.opensource.org/licenses/apache2.0.php\n\t//\n\t//  You may elect to redistribute this code under either of these licenses.\n\t//  ========================================================================\n\t//\n\n\tpackage org.eclipse.jetty.embedded;\n\n\timport java.io.IOException;\n\timport java.io.PrintWriter;\n\n\timport javax.servlet.ServletException;\n\timport javax.servlet.http.HttpServletRequest;\n\timport javax.servlet.http.HttpServletResponse;\n\n\timport org.eclipse.jetty.server.Request;\n\timport org.eclipse.jetty.server.handler.AbstractHandler;\n\n\tpublic class HelloHandler extends AbstractHandler\n\t{\n\t    final String greeting;\n\t    final String body;\n\n\t    public HelloHandler()\n\t    {\n\t        this(\"Hello World\");\n\t    }\n\n\t    public HelloHandler( String greeting )\n\t    {\n\t        this(greeting, null);\n\t    }\n\n\t    public HelloHandler( String greeting, String body )\n\t    {\n\t        this.greeting = greeting;\n\t        this.body = body;\n\t    }\n\n\t    public void handle( String target,\n\t                        Request baseRequest,\n\t                        HttpServletRequest request,\n\t                        HttpServletResponse response ) throws IOException,\n\t                                                      ServletException\n\t    {\n\t        response.setContentType(\"text/html; charset=utf-8\");\n\t        response.setStatus(HttpServletResponse.SC_OK);\n\n\t        PrintWriter out = response.getWriter();\n\n\t        out.println(\"<h1>\" + greeting + \"</h1>\");\n\t        if (body != null)\n\t        {\n\t            out.println(body);\n\t        }\n\n\t        baseRequest.setHandled(true);\n\t    }\n\t}\n\n传给处理方法的参数是：\n\n- target - 请求的目标，可能是一个 URI 或者被命名调度器的名称。\n- baseRequest - 可变的 Jetty 请求对象，这个对象总是未包装的。\n- request - 不变的请求对象，可能被一个过滤器或 servlet 包装过。\n- response - 响应，可能被过滤器或 servlet 包装过。\n\n处理器设置响应的状态、内容类型，并且在使用写入器生成响应体之前标记请求已经被处理。\n\n#### 运行 HelloWorldHandler\n\n为了允许一个处理器处理 HTTP 请求，必须添加处理器到一个服务器实例中。下面 OneHandler.java 代码展示了 Jetty 服务器如何使用 HelloWrold 处理器：\n\n\t//\n\t//  ========================================================================\n\t//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n\t//  ------------------------------------------------------------------------\n\t//  All rights reserved. This program and the accompanying materials\n\t//  are made available under the terms of the Eclipse Public License v1.0\n\t//  and Apache License v2.0 which accompanies this distribution.\n\t//\n\t//      The Eclipse Public License is available at\n\t//      http://www.eclipse.org/legal/epl-v10.html\n\t//\n\t//      The Apache License v2.0 is available at\n\t//      http://www.opensource.org/licenses/apache2.0.php\n\t//\n\t//  You may elect to redistribute this code under either of these licenses.\n\t//  ========================================================================\n\t//\n\n\tpackage org.eclipse.jetty.embedded;\n\n\timport org.eclipse.jetty.server.Server;\n\n\tpublic class OneHandler\n\t{\n\t    public static void main( String[] args ) throws Exception\n\t    {\n\t        Server server = new Server(8080);\n\t        server.setHandler(new HelloHandler());\n\n\t        server.start();\n\t        server.join();\n\t    }\n\t}\n\n在 Jetty 中一个或者更多处理器处理所有请求。一些处理器选择其他特定的处理器（例如，ContextHandlerCollection 使用上下文路径选择 ContextHandler）；其他处理器使用应用逻辑生成响应（例如，ServletHandler 传送请求给应用 Servlet），同时其他处理器做跟生成响应无关的任务（例如，RequestLogHandler 或者 StatisticsHandler）。\n\n后面的章节描述如何结合处理器方面的问题。你可以看到在 Jetty 的 [org.eclipse.jetty.server.handler](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/server/handler/package-summary.html) 包中有一些处理器。\n\n#### Handler Collections and Wrappers\n\n复杂的请求可以通过多种方式组合多个处理器进行处理。Jetty 有多个 [HandlerContainer](http://download.eclipse.org/jetty/stable-9/apidocs/org/eclipse/jetty/server/HandlerContainer.html) 接口的实现：\n\n**[HandlerCollection](http://download.eclipse.org/jetty/stable-9/apidocs/org/eclipse/jetty/server/handler/HandlerCollection.html)**\n\n  管理其他处理器的一个集合，并且按照顺序调用每个处理器。这对组合那些生成响应的处理器和统计数据、记录日志的处理器是有用的。\n\n**[HandlerList](http://download.eclipse.org/jetty/stable-9/apidocs/org/eclipse/jetty/server/handler/HandlerList.html)**\n\n  轮流调用每个处理器的处理器集合，直到抛出一个异常、提交了响应或者 request.isHandled() 返回 true。你可以用它组合根据条件处理一个请求的处理器，例如调用多个上下文直到匹配一个虚拟主机。\n\n**[HandlerWrapper](http://download.eclipse.org/jetty/stable-9/apidocs/org/eclipse/jetty/server/handler/HandlerWrapper.html)**\n\n  处理器的基础类，你可以在面向方面编程风格中一起使用菊花链处理器。例如，一个标准的 web 应用是通过一个 context、session、security和servlet处理器链实现的。\n\n**[ContextHandlerCollection](http://download.eclipse.org/jetty/stable-9/apidocs/org/eclipse/jetty/server/handler/ContextHandlerCollection.html)**\n\n  一个特定的 HandlerCollection，使用请求 URI 最长的前缀（contextPath）选择包含的 ContextHandler 来处理请求。\n\n#### 处理器的作用域\n\n很多 Jetty 中的标准 Servlet 容器是用 HandlerWrappers 实现的，是一个菊花式处理器链：ContextHandler 到 SessionHandler 到 SecurityHandler 到 ServletHandler。然而，因为 servlet 规范的特性，这个链不是一个处理器的纯粹嵌套，因为外部处理器有时需要内部处理器的过程信息。例如，当 ContextHandler 调用应用监听器来通知它们一个请求进入了上下文，它必须已经知道 ServletHandler 分发请求给哪个 servlet，因此 servletPath 方法返回正确的值。\n\nHandlerWrapper 被定制为 [ScopedHandler](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/server/handler/ScopedHandler.html) 抽象类，它支持作用域的菊花链。例如，如果一个 ServletHandler 嵌套在一个 ContextHandler 中，方法的执行顺序和嵌套是：\n\nServer.handle(...)\n  ContextHandler.doScope(...)\n    ServletHandler.doScope(...)\n      ContextHandler.doHandle(...)\n        ServletHandler.doHandle(...)\n          SomeServlet.service(...)\n\n因此，当 ContextHandler 处理请求的时候，它将在 ServletHandler 建立的作用域内进行。\n\n#### Resource Handler\n\n[FileServer](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/FileServer.html) 示例展示了如何使用 ResourceHandler 在当前的工作目录提供静态内容服务。\n\n\t//\n\t//  ========================================================================\n\t//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n\t//  ------------------------------------------------------------------------\n\t//  All rights reserved. This program and the accompanying materials\n\t//  are made available under the terms of the Eclipse Public License v1.0\n\t//  and Apache License v2.0 which accompanies this distribution.\n\t//\n\t//      The Eclipse Public License is available at\n\t//      http://www.eclipse.org/legal/epl-v10.html\n\t//\n\t//      The Apache License v2.0 is available at\n\t//      http://www.opensource.org/licenses/apache2.0.php\n\t//\n\t//  You may elect to redistribute this code under either of these licenses.\n\t//  ========================================================================\n\t//\n\n\tpackage org.eclipse.jetty.embedded;\n\n\timport org.eclipse.jetty.server.Handler;\n\timport org.eclipse.jetty.server.Server;\n\timport org.eclipse.jetty.server.handler.DefaultHandler;\n\timport org.eclipse.jetty.server.handler.HandlerList;\n\timport org.eclipse.jetty.server.handler.ResourceHandler;\n\timport org.eclipse.jetty.server.handler.gzip.GzipHandler;\n\n\t/**\n\t * Simple Jetty FileServer.\n\t * This is a simple example of Jetty configured as a FileServer.\n\t */\n\tpublic class FileServer\n\t{\n\t    public static void main(String[] args) throws Exception\n\t    {\n\t        // Create a basic Jetty server object that will listen on port 8080.  Note that if you set this to port 0\n\t        // then a randomly available port will be assigned that you can either look in the logs for the port,\n\t        // or programmatically obtain it for use in test cases.\n\t        Server server = new Server(8080);\n\n\t        // Create the ResourceHandler. It is the object that will actually handle the request for a given file. It is\n\t        // a Jetty Handler object so it is suitable for chaining with other handlers as you will see in other examples.\n\t        ResourceHandler resource_handler = new ResourceHandler();\n\t        // Configure the ResourceHandler. Setting the resource base indicates where the files should be served out of.\n\t        // In this example it is the current directory but it can be configured to anything that the jvm has access to.\n\t        resource_handler.setDirectoriesListed(true);\n\t        resource_handler.setWelcomeFiles(new String[]{ \"index.html\" });\n\t        resource_handler.setResourceBase(\".\");\n\n\t        // Add the ResourceHandler to the server.\n\t        GzipHandler gzip = new GzipHandler();\n\t        server.setHandler(gzip);\n\t        HandlerList handlers = new HandlerList();\n\t        handlers.setHandlers(new Handler[] { resource_handler, new DefaultHandler() });\n\t        gzip.setHandler(handlers);\n\n\t        // Start things up! By using the server.join() the server thread will join with the current thread.\n\t        // See \"http://docs.oracle.com/javase/1.5.0/docs/api/java/lang/Thread.html#join()\" for more details.\n\t        server.start();\n\t        server.join();\n\t    }\n\t}\n\n注意，ResourceHandler 和 DefaultHandler 使用了一个 HandlerList，因此 DefaultHandler 为所有找不到静态资源的请求生成良好的 404 响应。\n\n### 嵌入式 Connectors\n\n在前面的示例中，Server 实例传入了一个端口号，并且在内部创建一个在这个端口监听请求的 Connector 默认实例。然而，通常当嵌入 Jetty 的时候，需要明确实例化，并且给 Server 实例配置一个或多个 Connectors。\n\n#### 一个 Connector\n\n下面的示例，[OneConnector.java](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/OneConnector.html)，实例化、配置并添加一个 HTTP 连接器实例到服务器：\n\n\t//\n\t//  ========================================================================\n\t//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n\t//  ------------------------------------------------------------------------\n\t//  All rights reserved. This program and the accompanying materials\n\t//  are made available under the terms of the Eclipse Public License v1.0\n\t//  and Apache License v2.0 which accompanies this distribution.\n\t//\n\t//      The Eclipse Public License is available at\n\t//      http://www.eclipse.org/legal/epl-v10.html\n\t//\n\t//      The Apache License v2.0 is available at\n\t//      http://www.opensource.org/licenses/apache2.0.php\n\t//\n\t//  You may elect to redistribute this code under either of these licenses.\n\t//  ========================================================================\n\t//\n\n\tpackage org.eclipse.jetty.embedded;\n\n\timport org.eclipse.jetty.server.Server;\n\timport org.eclipse.jetty.server.ServerConnector;\n\n\t/**\n\t * A Jetty server with one connectors.\n\t */\n\tpublic class OneConnector\n\t{\n\t    public static void main( String[] args ) throws Exception\n\t    {\n\t        // The Server\n\t        Server server = new Server();\n\n\t        // HTTP connector\n\t        ServerConnector http = new ServerConnector(server);\n\t        http.setHost(\"localhost\");\n\t        http.setPort(8080);\n\t        http.setIdleTimeout(30000);\n\n\t        // Set the connector\n\t        server.addConnector(http);\n\n\t        // Set a handler\n\t        server.setHandler(new HelloHandler());\n\n\t        // Start the server\n\t        server.start();\n\t        server.join();\n\t    }\n\t}\n\n在这个示例中，连接器处理 HTTP 协议，这是 [ServerConnector](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/server/ServerConnector.html) 类默认的。\n\n#### 多个 Connectors\n\n当配置多个连接器（例如，HTTP 和 HTTPS）时，共享 HTTP 通用参数的配置是可取的。为了达到这个目的，你需要明确配置 ConnectionFactory 实例和 ServerConnector 类，并且跟 HTTP 通用配置一起提供。\n\n[ManyConnectors 示例](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/ManyConnectors.html) 配置了有两个 ServerConnector 实例的服务器：HTTP 连接器有一个 [HTTPConnectionFactory](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/server/HttpConnectionFactory.html) 实例；HTTPS 连接器有一个链接到 HttpConnectionFactory 的 SslConnectionFactory。两个 HttpConnectionFactories 在同一个 [HttpConfiguration](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/server/HttpConfiguration.html) 上配置，然而 HTTPS 工厂用一个包装的配置，所以可以添加 [SecureRequestCustomizer](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/server/SecureRequestCustomizer.html)。\n\n### 嵌入 Servlets\n\n[Servlets](https://en.wikipedia.org/wiki/Java_servlet) 是提供处理 HTTP 请求应用逻辑的一般方式。Servlets 跟 Jetty 的处理器是相同的，除非请求对象是不可变的，因此不能被修改。在 Jetty 中 Servlets 通过一个 [ServletHandler](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/MinimalServlets.html) 来处理。它使用标准的路径映射让 Servlet 和请求匹配；设置请求的 servletPath 和路径信息；传送请求到 servlet，可能通过 Filters 生成响应。\n\n[MinimalServlets 示例](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/MinimalServlets.html) 创建了一个 ServletHandler 实例并且配置了一个 HelloServlet:\n\n\t//\n\t//  ========================================================================\n\t//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n\t//  ------------------------------------------------------------------------\n\t//  All rights reserved. This program and the accompanying materials\n\t//  are made available under the terms of the Eclipse Public License v1.0\n\t//  and Apache License v2.0 which accompanies this distribution.\n\t//\n\t//      The Eclipse Public License is available at\n\t//      http://www.eclipse.org/legal/epl-v10.html\n\t//\n\t//      The Apache License v2.0 is available at\n\t//      http://www.opensource.org/licenses/apache2.0.php\n\t//\n\t//  You may elect to redistribute this code under either of these licenses.\n\t//  ========================================================================\n\t//\n\n\tpackage org.eclipse.jetty.embedded;\n\n\timport java.io.IOException;\n\n\timport javax.servlet.ServletException;\n\timport javax.servlet.http.HttpServlet;\n\timport javax.servlet.http.HttpServletRequest;\n\timport javax.servlet.http.HttpServletResponse;\n\n\timport org.eclipse.jetty.server.Server;\n\timport org.eclipse.jetty.servlet.ServletHandler;\n\n\tpublic class MinimalServlets\n\t{\n\t    public static void main( String[] args ) throws Exception\n\t    {\n\t        // Create a basic jetty server object that will listen on port 8080.\n\t        // Note that if you set this to port 0 then a randomly available port\n\t        // will be assigned that you can either look in the logs for the port,\n\t        // or programmatically obtain it for use in test cases.\n\t        Server server = new Server(8080);\n\n\t        // The ServletHandler is a dead simple way to create a context handler\n\t        // that is backed by an instance of a Servlet.\n\t        // This handler then needs to be registered with the Server object.\n\t        ServletHandler handler = new ServletHandler();\n\t        server.setHandler(handler);\n\n\t        // Passing in the class for the Servlet allows jetty to instantiate an\n\t        // instance of that Servlet and mount it on a given context path.\n\n\t        // IMPORTANT:\n\t        // This is a raw Servlet, not a Servlet that has been configured\n\t        // through a web.xml @WebServlet annotation, or anything similar.\n\t        handler.addServletWithMapping(HelloServlet.class, \"/*\");\n\n\t        // Start things up!\n\t        server.start();\n\n\t        // The use of server.join() the will make the current thread join and\n\t        // wait until the server is done executing.\n\t        // See\n\t        // http://docs.oracle.com/javase/7/docs/api/java/lang/Thread.html#join()\n\t        server.join();\n\t    }\n\n\t    @SuppressWarnings(\"serial\")\n\t    public static class HelloServlet extends HttpServlet\n\t    {\n\t        @Override\n\t        protected void doGet( HttpServletRequest request,\n\t                              HttpServletResponse response ) throws ServletException,\n\t                                                            IOException\n\t        {\n\t            response.setContentType(\"text/html\");\n\t            response.setStatus(HttpServletResponse.SC_OK);\n\t            response.getWriter().println(\"<h1>Hello from HelloServlet</h1>\");\n\t        }\n\t    }\n\t}\n\n### 嵌入 Contexts\n\n[ContextHandler](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/OneContext.html) 是一个 ScopedHandler 只来响应那些 URI 前缀匹配已配置的上下文路径的请求。匹配上下文路径的请求会响应的调整它们的路径方法，并且上下文生命周期内是可获取的，可能包含：\n\n- 当请求在作用域内处理时，Classloader 设置为线程上下文的类加载器。\n- 通过过 [ServletContext](http://docs.oracle.com/javaee/6/api/javax/servlet/ServletContext.html) API 可以获取属性的集合。\n- 通过过 [ServletContext](http://docs.oracle.com/javaee/6/api/javax/servlet/ServletContext.html) API 可以获取参数的集合。\n- 通过过 [ServletContext](http://docs.oracle.com/javaee/6/api/javax/servlet/ServletContext.html) API 为静态资源的请求提供一个基本的 Resource 作为文档根。\n- 虚拟主机名称集合。\n\n下面的 [OneContext 示例](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/OneContext.html) 建立了一个包装了 [HelloHandler](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/HelloHandler.html) 的上下文：\n\n\t//\n\t//  ========================================================================\n\t//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n\t//  ------------------------------------------------------------------------\n\t//  All rights reserved. This program and the accompanying materials\n\t//  are made available under the terms of the Eclipse Public License v1.0\n\t//  and Apache License v2.0 which accompanies this distribution.\n\t//\n\t//      The Eclipse Public License is available at\n\t//      http://www.eclipse.org/legal/epl-v10.html\n\t//\n\t//      The Apache License v2.0 is available at\n\t//      http://www.opensource.org/licenses/apache2.0.php\n\t//\n\t//  You may elect to redistribute this code under either of these licenses.\n\t//  ========================================================================\n\t//\n\n\tpackage org.eclipse.jetty.embedded;\n\n\timport org.eclipse.jetty.server.Server;\n\timport org.eclipse.jetty.server.handler.ContextHandler;\n\n\tpublic class OneContext\n\t{\n\t    public static void main( String[] args ) throws Exception\n\t    {\n\t        Server server = new Server( 8080 );\n\n\t        // Add a single handler on context \"/hello\"\n\t        ContextHandler context = new ContextHandler();\n\t        context.setContextPath( \"/hello\" );\n\t        context.setHandler( new HelloHandler() );\n\n\t        // Can be accessed using http://localhost:8080/hello\n\n\t        server.setHandler( context );\n\n\t        // Start the server\n\t        server.start();\n\t        server.join();\n\t    }\n\t}\n\n当有很多上下文出现时，你可以嵌入一个 ContextHandlerCollection 来有效的测试一个请求的 URI，然后选择匹配到的 ContextHandler。[ManyContexts 示例](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/ManyContexts.html) 展示了可以配置多个上下文：\n\n\t//\n\t//  ========================================================================\n\t//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n\t//  ------------------------------------------------------------------------\n\t//  All rights reserved. This program and the accompanying materials\n\t//  are made available under the terms of the Eclipse Public License v1.0\n\t//  and Apache License v2.0 which accompanies this distribution.\n\t//\n\t//      The Eclipse Public License is available at\n\t//      http://www.eclipse.org/legal/epl-v10.html\n\t//\n\t//      The Apache License v2.0 is available at\n\t//      http://www.opensource.org/licenses/apache2.0.php\n\t//\n\t//  You may elect to redistribute this code under either of these licenses.\n\t//  ========================================================================\n\t//\n\n\tpackage org.eclipse.jetty.embedded;\n\n\timport org.eclipse.jetty.server.Handler;\n\timport org.eclipse.jetty.server.Server;\n\timport org.eclipse.jetty.server.handler.ContextHandler;\n\timport org.eclipse.jetty.server.handler.ContextHandlerCollection;\n\n\tpublic class ManyContexts\n\t{\n\t    public static void main( String[] args ) throws Exception\n\t    {\n\t        Server server = new Server(8080);\n\n\t        ContextHandler context = new ContextHandler(\"/\");\n\t        context.setContextPath(\"/\");\n\t        context.setHandler(new HelloHandler(\"Root Hello\"));\n\n\t        ContextHandler contextFR = new ContextHandler(\"/fr\");\n\t        contextFR.setHandler(new HelloHandler(\"Bonjoir\"));\n\n\t        ContextHandler contextIT = new ContextHandler(\"/it\");\n\t        contextIT.setHandler(new HelloHandler(\"Bongiorno\"));\n\n\t        ContextHandler contextV = new ContextHandler(\"/\");\n\t        contextV.setVirtualHosts(new String[] { \"127.0.0.2\" });\n\t        contextV.setHandler(new HelloHandler(\"Virtual Hello\"));\n\n\t        ContextHandlerCollection contexts = new ContextHandlerCollection();\n\t        contexts.setHandlers(new Handler[] { context, contextFR, contextIT,\n\t                contextV });\n\n\t        server.setHandler(contexts);\n\n\t        server.start();\n\t        server.join();\n\t    }\n\t}\n\n### 嵌入 ServletContexts\n\n[ServletContextHandler](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/servlet/ServletContextHandler.html) 是一个支持普通 session 和 Servlet 的特定的 ContextHandler。下面的 [OneServletContext 示例](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/OneServletContext.html) 实例化了一个 [DefaultServlet](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/servlet/DefaultServlet.html) 从 /tmp/ 提供静态内容，并且实例化一个 DumpServlet 创建 session 并存储请求的基本详细信息：\n\n\t//\n\t//  ========================================================================\n\t//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n\t//  ------------------------------------------------------------------------\n\t//  All rights reserved. This program and the accompanying materials\n\t//  are made available under the terms of the Eclipse Public License v1.0\n\t//  and Apache License v2.0 which accompanies this distribution.\n\t//\n\t//      The Eclipse Public License is available at\n\t//      http://www.eclipse.org/legal/epl-v10.html\n\t//\n\t//      The Apache License v2.0 is available at\n\t//      http://www.opensource.org/licenses/apache2.0.php\n\t//\n\t//  You may elect to redistribute this code under either of these licenses.\n\t//  ========================================================================\n\t//\n\n\tpackage org.eclipse.jetty.embedded;\n\n\timport org.eclipse.jetty.server.Server;\n\timport org.eclipse.jetty.servlet.DefaultServlet;\n\timport org.eclipse.jetty.servlet.ServletContextHandler;\n\n\tpublic class OneServletContext\n\t{\n\t    public static void main( String[] args ) throws Exception\n\t    {\n\t        Server server = new Server(8080);\n\n\t        ServletContextHandler context = new ServletContextHandler(\n\t                ServletContextHandler.SESSIONS);\n\t        context.setContextPath(\"/\");\n\t        context.setResourceBase(System.getProperty(\"java.io.tmpdir\"));\n\t        server.setHandler(context);\n\n\t        // Add dump servlet\n\t        context.addServlet(DumpServlet.class, \"/dump/*\");\n\t        // Add default servlet\n\t        context.addServlet(DefaultServlet.class, \"/\");\n\n\t        server.start();\n\t        server.join();\n\t    }\n\t}\n\n### 嵌入 Web Applications\n\n[WebAppContext](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/webapp/WebAppContext.html) 是一个 ServletContextHandler 的扩展，它使用[标准设计](https://en.wikipedia.org/wiki/WAR_(file_format))和 web.xml 配置 servlet、filter 和 web.xml 或注解的其他特性。下面的 [OneWebApp 示例](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/OneWebApp.html) 配置了 Jetty 的测试 web 应用。Web 应用程序可以使用容器提供的的资源，在这个案例中需要并配置一个 LoginService：\n\n\t//\n\t//  ========================================================================\n\t//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n\t//  ------------------------------------------------------------------------\n\t//  All rights reserved. This program and the accompanying materials\n\t//  are made available under the terms of the Eclipse Public License v1.0\n\t//  and Apache License v2.0 which accompanies this distribution.\n\t//\n\t//      The Eclipse Public License is available at\n\t//      http://www.eclipse.org/legal/epl-v10.html\n\t//\n\t//      The Apache License v2.0 is available at\n\t//      http://www.opensource.org/licenses/apache2.0.php\n\t//\n\t//  You may elect to redistribute this code under either of these licenses.\n\t//  ========================================================================\n\t//\n\n\tpackage org.eclipse.jetty.embedded;\n\n\timport java.io.File;\n\timport java.lang.management.ManagementFactory;\n\n\timport org.eclipse.jetty.jmx.MBeanContainer;\n\timport org.eclipse.jetty.security.HashLoginService;\n\timport org.eclipse.jetty.server.Server;\n\timport org.eclipse.jetty.server.handler.AllowSymLinkAliasChecker;\n\timport org.eclipse.jetty.webapp.WebAppContext;\n\n\tpublic class OneWebApp\n\t{\n\t    public static void main( String[] args ) throws Exception\n\t    {\n\t        // Create a basic jetty server object that will listen on port 8080.\n\t        // Note that if you set this to port 0 then a randomly available port\n\t        // will be assigned that you can either look in the logs for the port,\n\t        // or programmatically obtain it for use in test cases.\n\t        Server server = new Server(8080);\n\n\t        // Setup JMX\n\t        MBeanContainer mbContainer = new MBeanContainer(\n\t                ManagementFactory.getPlatformMBeanServer());\n\t        server.addBean(mbContainer);\n\n\t        // The WebAppContext is the entity that controls the environment in\n\t        // which a web application lives and breathes. In this example the\n\t        // context path is being set to \"/\" so it is suitable for serving root\n\t        // context requests and then we see it setting the location of the war.\n\t        // A whole host of other configurations are available, ranging from\n\t        // configuring to support annotation scanning in the webapp (through\n\t        // PlusConfiguration) to choosing where the webapp will unpack itself.\n\t        WebAppContext webapp = new WebAppContext();\n\t        webapp.setContextPath(\"/\");\n\t        File warFile = new File(\n\t                \"../../tests/test-jmx/jmx-webapp/target/jmx-webapp\");\n\t        webapp.setWar(warFile.getAbsolutePath());\n\n\t        // A WebAppContext is a ContextHandler as well so it needs to be set to\n\t        // the server so it is aware of where to send the appropriate requests.\n\t        server.setHandler(webapp);\n\n\t        // Start things up!\n\t        server.start();\n\n\t        // The use of server.join() the will make the current thread join and\n\t        // wait until the server is done executing.\n\t        // See http://docs.oracle.com/javase/7/docs/api/java/lang/Thread.html#join()\n\t        server.join();\n\t    }\n\t}\n\n### 喜欢 Jetty XML\n\n配置 Jetty 服务器实例的典型方式是通过 jetty.xml 关联配置文件。然而 Jetty XML 配置格式只是在代码中可以做的简单的表达；也可以很简单的写嵌入代码准确实现 jetty.xml 配置。下面的 [LikeJettyXml 示例](http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/LikeJettyXml.html) 在代码中呈现了从配置文件获取的行为：\n\n- jetty.xml\n- jetty-jmx.xml\n- jetty-http.xml\n- jetty-https.xml\n- jetty-deploy.xml\n- jetty-stats.xml\n- jetty-requestlog.xml\n- jetty-lowresources.xml\n- test-realm.xml\n\n\n\t//\n\t//  ========================================================================\n\t//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n\t//  ------------------------------------------------------------------------\n\t//  All rights reserved. This program and the accompanying materials\n\t//  are made available under the terms of the Eclipse Public License v1.0\n\t//  and Apache License v2.0 which accompanies this distribution.\n\t//\n\t//      The Eclipse Public License is available at\n\t//      http://www.eclipse.org/legal/epl-v10.html\n\t//\n\t//      The Apache License v2.0 is available at\n\t//      http://www.opensource.org/licenses/apache2.0.php\n\t//\n\t//  You may elect to redistribute this code under either of these licenses.\n\t//  ========================================================================\n\t//\n\n\tpackage org.eclipse.jetty.embedded;\n\n\timport java.io.File;\n\timport java.io.FileNotFoundException;\n\timport java.lang.management.ManagementFactory;\n\n\timport org.eclipse.jetty.deploy.DeploymentManager;\n\timport org.eclipse.jetty.deploy.PropertiesConfigurationManager;\n\timport org.eclipse.jetty.deploy.bindings.DebugListenerBinding;\n\timport org.eclipse.jetty.deploy.providers.WebAppProvider;\n\timport org.eclipse.jetty.http.HttpVersion;\n\timport org.eclipse.jetty.jmx.MBeanContainer;\n\timport org.eclipse.jetty.security.HashLoginService;\n\timport org.eclipse.jetty.server.DebugListener;\n\timport org.eclipse.jetty.server.Handler;\n\timport org.eclipse.jetty.server.HttpConfiguration;\n\timport org.eclipse.jetty.server.HttpConnectionFactory;\n\timport org.eclipse.jetty.server.LowResourceMonitor;\n\timport org.eclipse.jetty.server.NCSARequestLog;\n\timport org.eclipse.jetty.server.SecureRequestCustomizer;\n\timport org.eclipse.jetty.server.Server;\n\timport org.eclipse.jetty.server.ServerConnector;\n\timport org.eclipse.jetty.server.SslConnectionFactory;\n\timport org.eclipse.jetty.server.handler.ContextHandlerCollection;\n\timport org.eclipse.jetty.server.handler.DefaultHandler;\n\timport org.eclipse.jetty.server.handler.HandlerCollection;\n\timport org.eclipse.jetty.server.handler.RequestLogHandler;\n\timport org.eclipse.jetty.server.handler.StatisticsHandler;\n\timport org.eclipse.jetty.util.ssl.SslContextFactory;\n\timport org.eclipse.jetty.util.thread.QueuedThreadPool;\n\timport org.eclipse.jetty.util.thread.ScheduledExecutorScheduler;\n\timport org.eclipse.jetty.webapp.Configuration;\n\n\t/**\n\t * Starts the Jetty Distribution's demo-base directory using entirely\n\t * embedded jetty techniques.\n\t */\n\tpublic class LikeJettyXml\n\t{\n\t    public static void main( String[] args ) throws Exception\n\t    {\n\t        // Path to as-built jetty-distribution directory\n\t        String jettyHomeBuild = \"../../jetty-distribution/target/distribution\";\n\n\t        // Find jetty home and base directories\n\t        String homePath = System.getProperty(\"jetty.home\", jettyHomeBuild);\n\t        File homeDir = new File(homePath);\n\t        if (!homeDir.exists())\n\t        {\n\t            throw new FileNotFoundException(homeDir.getAbsolutePath());\n\t        }\n\t        String basePath = System.getProperty(\"jetty.base\", homeDir + \"/demo-base\");\n\t        File baseDir = new File(basePath);\n\t        if(!baseDir.exists())\n\t        {\n\t            throw new FileNotFoundException(baseDir.getAbsolutePath());\n\t        }\n\n\t        // Configure jetty.home and jetty.base system properties\n\t        String jetty_home = homeDir.getAbsolutePath();\n\t        String jetty_base = baseDir.getAbsolutePath();\n\t        System.setProperty(\"jetty.home\", jetty_home);\n\t        System.setProperty(\"jetty.base\", jetty_base);\n\n\n\t        // === jetty.xml ===\n\t        // Setup Threadpool\n\t        QueuedThreadPool threadPool = new QueuedThreadPool();\n\t        threadPool.setMaxThreads(500);\n\n\t        // Server\n\t        Server server = new Server(threadPool);\n\n\t        // Scheduler\n\t        server.addBean(new ScheduledExecutorScheduler());\n\n\t        // HTTP Configuration\n\t        HttpConfiguration http_config = new HttpConfiguration();\n\t        http_config.setSecureScheme(\"https\");\n\t        http_config.setSecurePort(8443);\n\t        http_config.setOutputBufferSize(32768);\n\t        http_config.setRequestHeaderSize(8192);\n\t        http_config.setResponseHeaderSize(8192);\n\t        http_config.setSendServerVersion(true);\n\t        http_config.setSendDateHeader(false);\n\t        // httpConfig.addCustomizer(new ForwardedRequestCustomizer());\n\n\t        // Handler Structure\n\t        HandlerCollection handlers = new HandlerCollection();\n\t        ContextHandlerCollection contexts = new ContextHandlerCollection();\n\t        handlers.setHandlers(new Handler[] { contexts, new DefaultHandler() });\n\t        server.setHandler(handlers);\n\n\t        // Extra options\n\t        server.setDumpAfterStart(false);\n\t        server.setDumpBeforeStop(false);\n\t        server.setStopAtShutdown(true);\n\n\t        // === jetty-jmx.xml ===\n\t        MBeanContainer mbContainer = new MBeanContainer(\n\t                ManagementFactory.getPlatformMBeanServer());\n\t        server.addBean(mbContainer);\n\n\n\t        // === jetty-http.xml ===\n\t        ServerConnector http = new ServerConnector(server,\n\t                new HttpConnectionFactory(http_config));\n\t        http.setPort(8080);\n\t        http.setIdleTimeout(30000);\n\t        server.addConnector(http);\n\n\n\t        // === jetty-https.xml ===\n\t        // SSL Context Factory\n\t        SslContextFactory sslContextFactory = new SslContextFactory();\n\t        sslContextFactory.setKeyStorePath(jetty_home + \"/../../../jetty-server/src/test/config/etc/keystore\");\n\t        sslContextFactory.setKeyStorePassword(\"OBF:1vny1zlo1x8e1vnw1vn61x8g1zlu1vn4\");\n\t        sslContextFactory.setKeyManagerPassword(\"OBF:1u2u1wml1z7s1z7a1wnl1u2g\");\n\t        sslContextFactory.setTrustStorePath(jetty_home + \"/../../../jetty-server/src/test/config/etc/keystore\");\n\t        sslContextFactory.setTrustStorePassword(\"OBF:1vny1zlo1x8e1vnw1vn61x8g1zlu1vn4\");\n\t        sslContextFactory.setExcludeCipherSuites(\"SSL_RSA_WITH_DES_CBC_SHA\",\n\t                \"SSL_DHE_RSA_WITH_DES_CBC_SHA\", \"SSL_DHE_DSS_WITH_DES_CBC_SHA\",\n\t                \"SSL_RSA_EXPORT_WITH_RC4_40_MD5\",\n\t                \"SSL_RSA_EXPORT_WITH_DES40_CBC_SHA\",\n\t                \"SSL_DHE_RSA_EXPORT_WITH_DES40_CBC_SHA\",\n\t                \"SSL_DHE_DSS_EXPORT_WITH_DES40_CBC_SHA\");\n\n\t        // SSL HTTP Configuration\n\t        HttpConfiguration https_config = new HttpConfiguration(http_config);\n\t        https_config.addCustomizer(new SecureRequestCustomizer());\n\n\t        // SSL Connector\n\t        ServerConnector sslConnector = new ServerConnector(server,\n\t            new SslConnectionFactory(sslContextFactory,HttpVersion.HTTP_1_1.asString()),\n\t            new HttpConnectionFactory(https_config));\n\t        sslConnector.setPort(8443);\n\t        server.addConnector(sslConnector);\n\n\n\t        // === jetty-deploy.xml ===\n\t        DeploymentManager deployer = new DeploymentManager();\n\t        DebugListener debug = new DebugListener(System.out,true,true,true);\n\t        server.addBean(debug);        \n\t        deployer.addLifeCycleBinding(new DebugListenerBinding(debug));\n\t        deployer.setContexts(contexts);\n\t        deployer.setContextAttribute(\n\t                \"org.eclipse.jetty.server.webapp.ContainerIncludeJarPattern\",\n\t                \".*/servlet-api-[^/]*\\\\.jar$\");\n\n\t        WebAppProvider webapp_provider = new WebAppProvider();\n\t        webapp_provider.setMonitoredDirName(jetty_base + \"/webapps\");\n\t        webapp_provider.setDefaultsDescriptor(jetty_home + \"/etc/webdefault.xml\");\n\t        webapp_provider.setScanInterval(1);\n\t        webapp_provider.setExtractWars(true);\n\t        webapp_provider.setConfigurationManager(new PropertiesConfigurationManager());\n\n\t        deployer.addAppProvider(webapp_provider);\n\t        server.addBean(deployer);\n\n\t        // === setup jetty plus ==\n\t        Configuration.ClassList.setServerDefault(server).addAfter(\n\t                \"org.eclipse.jetty.webapp.FragmentConfiguration\",\n\t                \"org.eclipse.jetty.plus.webapp.EnvConfiguration\",\n\t                \"org.eclipse.jetty.plus.webapp.PlusConfiguration\");\n\n\t        // === jetty-stats.xml ===\n\t        StatisticsHandler stats = new StatisticsHandler();\n\t        stats.setHandler(server.getHandler());\n\t        server.setHandler(stats);\n\n\n\t        // === jetty-requestlog.xml ===\n\t        NCSARequestLog requestLog = new NCSARequestLog();\n\t        requestLog.setFilename(jetty_home + \"/logs/yyyy_mm_dd.request.log\");\n\t        requestLog.setFilenameDateFormat(\"yyyy_MM_dd\");\n\t        requestLog.setRetainDays(90);\n\t        requestLog.setAppend(true);\n\t        requestLog.setExtended(true);\n\t        requestLog.setLogCookies(false);\n\t        requestLog.setLogTimeZone(\"GMT\");\n\t        RequestLogHandler requestLogHandler = new RequestLogHandler();\n\t        requestLogHandler.setRequestLog(requestLog);\n\t        handlers.addHandler(requestLogHandler);\n\n\n\t        // === jetty-lowresources.xml ===\n\t        LowResourceMonitor lowResourcesMonitor=new LowResourceMonitor(server);\n\t        lowResourcesMonitor.setPeriod(1000);\n\t        lowResourcesMonitor.setLowResourcesIdleTimeout(200);\n\t        lowResourcesMonitor.setMonitorThreads(true);\n\t        lowResourcesMonitor.setMaxConnections(0);\n\t        lowResourcesMonitor.setMaxMemory(0);\n\t        lowResourcesMonitor.setMaxLowResourcesTime(5000);\n\t        server.addBean(lowResourcesMonitor);\n\n\n\t        // === test-realm.xml ===\n\t        HashLoginService login = new HashLoginService();\n\t        login.setName(\"Test Realm\");\n\t        login.setConfig(jetty_base + \"/etc/realm.properties\");\n\t        login.setHotReload(false);\n\t        server.addBean(login);\n\n\t        // Start the server\n\t        server.start();\n\t        server.join();\n\t    }\n\t}\n","slug":"嵌入式-Jetty","published":1,"updated":"2021-07-19T16:28:00.308Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckrdmphxi00ysitd352ys3clj","content":"<p>Jetty 有一个口号，“不要在 Jetty 中部署你的应用，部署 Jetty 在你的应用中！”这个口号意味着构建应用为为标准的 WAR 并且部署到 Jetty 的另外一个选择是，Jetty 被设计为一个软件组件，可以在 Java 程序中像其他 POJO 一样被实例化和使用。换一种方式，以嵌入式模式运行 Jetty 意味着放置一个 HTTP 模块到你的应用中，而不是放置你的应用到 HTTP 服务器。</p>\n<span id=\"more\"></span>\n\n<p>本教程将带领你一步步从最简单的 Jetty 服务器实例化到用标准化部署描述器运行多 Web 应用程序。大部分这些实例的代码是标准 Jetty 项目的一部分。</p>\n<h3 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h3><p>嵌入 Jetty 服务器，下面是通常的步骤，也是本教程实例中展示步骤：</p>\n<ol>\n<li>创建一个 <a href=\"http://download.eclipse.org/jetty/stable-9/apidocs/org/eclipse/jetty/server/Server.html\">Server</a> 实例。  </li>\n<li>添加/配置 <a href=\"http://download.eclipse.org/jetty/stable-9/apidocs/org/eclipse/jetty/server/Connector.html\">Connectors</a>。  </li>\n<li>添加/配置 <a href=\"http://download.eclipse.org/jetty/stable-9/apidocs/org/eclipse/jetty/server/Handler.html\">Handlers</a>、<a href=\"http://download.eclipse.org/jetty/stable-9/apidocs/org/eclipse/jetty/server/handler/ContextHandler.html\">Contexts</a>、<a href=\"http://docs.oracle.com/javaee/6/api/javax/servlet/Servlet.html\">Servlets</a>。  </li>\n<li>启动 Server。  </li>\n<li>服务器等待或者用自己的线程做一些其他的事情。</li>\n</ol>\n<h3 id=\"创建-Server\"><a href=\"#创建-Server\" class=\"headerlink\" title=\"创建 Server\"></a>创建 Server</h3><p>下面是 SimplestServer.java 文件的代码，实例化并且运行一个可能最简单的 Jetty server。</p>\n<pre><code>//\n//  ========================================================================\n//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n//  ------------------------------------------------------------------------\n//  All rights reserved. This program and the accompanying materials\n//  are made available under the terms of the Eclipse Public License v1.0\n//  and Apache License v2.0 which accompanies this distribution.\n//\n//      The Eclipse Public License is available at\n//      http://www.eclipse.org/legal/epl-v10.html\n//\n//      The Apache License v2.0 is available at\n//      http://www.opensource.org/licenses/apache2.0.php\n//\n//  You may elect to redistribute this code under either of these licenses.\n//  ========================================================================\n//\n\npackage org.eclipse.jetty.embedded;\n\nimport org.eclipse.jetty.server.Server;\n\n/**\n * The simplest possible Jetty server.\n */\npublic class SimplestServer\n&#123;\n    public static void main( String[] args ) throws Exception\n    &#123;\n        Server server = new Server(8080);\n        server.start();\n        server.dumpStdErr();\n        server.join();\n    &#125;\n&#125;\n</code></pre>\n<p>这将运行一个监听 8080 端口的 HTTP Server。这不是一个有用的 server，因为它没有处理器，因此对所有的请求都将返回 404。</p>\n<h3 id=\"使用处理器\"><a href=\"#使用处理器\" class=\"headerlink\" title=\"使用处理器\"></a>使用处理器</h3><p>为了针对请求生成响应，Jetty 要求在服务器上设置处理器。一个处理器可能：</p>\n<ul>\n<li>检查/修改 HTTP 请求。</li>\n<li>生成完整的 HTTP 响应。</li>\n<li>调用其他处理器（见 HandlerWrapper）。</li>\n<li>选择一个或者多个处理器调用（见 HandlerCollection）。</li>\n</ul>\n<h4 id=\"HelloWorld-Handler\"><a href=\"#HelloWorld-Handler\" class=\"headerlink\" title=\"HelloWorld Handler\"></a>HelloWorld Handler</h4><p>下面 HelloHandler.java 的代码展示了一个简单的 hello world 处理器：</p>\n<pre><code>//\n//  ========================================================================\n//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n//  ------------------------------------------------------------------------\n//  All rights reserved. This program and the accompanying materials\n//  are made available under the terms of the Eclipse Public License v1.0\n//  and Apache License v2.0 which accompanies this distribution.\n//\n//      The Eclipse Public License is available at\n//      http://www.eclipse.org/legal/epl-v10.html\n//\n//      The Apache License v2.0 is available at\n//      http://www.opensource.org/licenses/apache2.0.php\n//\n//  You may elect to redistribute this code under either of these licenses.\n//  ========================================================================\n//\n\npackage org.eclipse.jetty.embedded;\n\nimport java.io.IOException;\nimport java.io.PrintWriter;\n\nimport javax.servlet.ServletException;\nimport javax.servlet.http.HttpServletRequest;\nimport javax.servlet.http.HttpServletResponse;\n\nimport org.eclipse.jetty.server.Request;\nimport org.eclipse.jetty.server.handler.AbstractHandler;\n\npublic class HelloHandler extends AbstractHandler\n&#123;\n    final String greeting;\n    final String body;\n\n    public HelloHandler()\n    &#123;\n        this(&quot;Hello World&quot;);\n    &#125;\n\n    public HelloHandler( String greeting )\n    &#123;\n        this(greeting, null);\n    &#125;\n\n    public HelloHandler( String greeting, String body )\n    &#123;\n        this.greeting = greeting;\n        this.body = body;\n    &#125;\n\n    public void handle( String target,\n                        Request baseRequest,\n                        HttpServletRequest request,\n                        HttpServletResponse response ) throws IOException,\n                                                      ServletException\n    &#123;\n        response.setContentType(&quot;text/html; charset=utf-8&quot;);\n        response.setStatus(HttpServletResponse.SC_OK);\n\n        PrintWriter out = response.getWriter();\n\n        out.println(&quot;&lt;h1&gt;&quot; + greeting + &quot;&lt;/h1&gt;&quot;);\n        if (body != null)\n        &#123;\n            out.println(body);\n        &#125;\n\n        baseRequest.setHandled(true);\n    &#125;\n&#125;\n</code></pre>\n<p>传给处理方法的参数是：</p>\n<ul>\n<li>target - 请求的目标，可能是一个 URI 或者被命名调度器的名称。</li>\n<li>baseRequest - 可变的 Jetty 请求对象，这个对象总是未包装的。</li>\n<li>request - 不变的请求对象，可能被一个过滤器或 servlet 包装过。</li>\n<li>response - 响应，可能被过滤器或 servlet 包装过。</li>\n</ul>\n<p>处理器设置响应的状态、内容类型，并且在使用写入器生成响应体之前标记请求已经被处理。</p>\n<h4 id=\"运行-HelloWorldHandler\"><a href=\"#运行-HelloWorldHandler\" class=\"headerlink\" title=\"运行 HelloWorldHandler\"></a>运行 HelloWorldHandler</h4><p>为了允许一个处理器处理 HTTP 请求，必须添加处理器到一个服务器实例中。下面 OneHandler.java 代码展示了 Jetty 服务器如何使用 HelloWrold 处理器：</p>\n<pre><code>//\n//  ========================================================================\n//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n//  ------------------------------------------------------------------------\n//  All rights reserved. This program and the accompanying materials\n//  are made available under the terms of the Eclipse Public License v1.0\n//  and Apache License v2.0 which accompanies this distribution.\n//\n//      The Eclipse Public License is available at\n//      http://www.eclipse.org/legal/epl-v10.html\n//\n//      The Apache License v2.0 is available at\n//      http://www.opensource.org/licenses/apache2.0.php\n//\n//  You may elect to redistribute this code under either of these licenses.\n//  ========================================================================\n//\n\npackage org.eclipse.jetty.embedded;\n\nimport org.eclipse.jetty.server.Server;\n\npublic class OneHandler\n&#123;\n    public static void main( String[] args ) throws Exception\n    &#123;\n        Server server = new Server(8080);\n        server.setHandler(new HelloHandler());\n\n        server.start();\n        server.join();\n    &#125;\n&#125;\n</code></pre>\n<p>在 Jetty 中一个或者更多处理器处理所有请求。一些处理器选择其他特定的处理器（例如，ContextHandlerCollection 使用上下文路径选择 ContextHandler）；其他处理器使用应用逻辑生成响应（例如，ServletHandler 传送请求给应用 Servlet），同时其他处理器做跟生成响应无关的任务（例如，RequestLogHandler 或者 StatisticsHandler）。</p>\n<p>后面的章节描述如何结合处理器方面的问题。你可以看到在 Jetty 的 <a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/server/handler/package-summary.html\">org.eclipse.jetty.server.handler</a> 包中有一些处理器。</p>\n<h4 id=\"Handler-Collections-and-Wrappers\"><a href=\"#Handler-Collections-and-Wrappers\" class=\"headerlink\" title=\"Handler Collections and Wrappers\"></a>Handler Collections and Wrappers</h4><p>复杂的请求可以通过多种方式组合多个处理器进行处理。Jetty 有多个 <a href=\"http://download.eclipse.org/jetty/stable-9/apidocs/org/eclipse/jetty/server/HandlerContainer.html\">HandlerContainer</a> 接口的实现：</p>\n<p><strong><a href=\"http://download.eclipse.org/jetty/stable-9/apidocs/org/eclipse/jetty/server/handler/HandlerCollection.html\">HandlerCollection</a></strong></p>\n<p>  管理其他处理器的一个集合，并且按照顺序调用每个处理器。这对组合那些生成响应的处理器和统计数据、记录日志的处理器是有用的。</p>\n<p><strong><a href=\"http://download.eclipse.org/jetty/stable-9/apidocs/org/eclipse/jetty/server/handler/HandlerList.html\">HandlerList</a></strong></p>\n<p>  轮流调用每个处理器的处理器集合，直到抛出一个异常、提交了响应或者 request.isHandled() 返回 true。你可以用它组合根据条件处理一个请求的处理器，例如调用多个上下文直到匹配一个虚拟主机。</p>\n<p><strong><a href=\"http://download.eclipse.org/jetty/stable-9/apidocs/org/eclipse/jetty/server/handler/HandlerWrapper.html\">HandlerWrapper</a></strong></p>\n<p>  处理器的基础类，你可以在面向方面编程风格中一起使用菊花链处理器。例如，一个标准的 web 应用是通过一个 context、session、security和servlet处理器链实现的。</p>\n<p><strong><a href=\"http://download.eclipse.org/jetty/stable-9/apidocs/org/eclipse/jetty/server/handler/ContextHandlerCollection.html\">ContextHandlerCollection</a></strong></p>\n<p>  一个特定的 HandlerCollection，使用请求 URI 最长的前缀（contextPath）选择包含的 ContextHandler 来处理请求。</p>\n<h4 id=\"处理器的作用域\"><a href=\"#处理器的作用域\" class=\"headerlink\" title=\"处理器的作用域\"></a>处理器的作用域</h4><p>很多 Jetty 中的标准 Servlet 容器是用 HandlerWrappers 实现的，是一个菊花式处理器链：ContextHandler 到 SessionHandler 到 SecurityHandler 到 ServletHandler。然而，因为 servlet 规范的特性，这个链不是一个处理器的纯粹嵌套，因为外部处理器有时需要内部处理器的过程信息。例如，当 ContextHandler 调用应用监听器来通知它们一个请求进入了上下文，它必须已经知道 ServletHandler 分发请求给哪个 servlet，因此 servletPath 方法返回正确的值。</p>\n<p>HandlerWrapper 被定制为 <a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/server/handler/ScopedHandler.html\">ScopedHandler</a> 抽象类，它支持作用域的菊花链。例如，如果一个 ServletHandler 嵌套在一个 ContextHandler 中，方法的执行顺序和嵌套是：</p>\n<p>Server.handle(…)<br>  ContextHandler.doScope(…)<br>    ServletHandler.doScope(…)<br>      ContextHandler.doHandle(…)<br>        ServletHandler.doHandle(…)<br>          SomeServlet.service(…)</p>\n<p>因此，当 ContextHandler 处理请求的时候，它将在 ServletHandler 建立的作用域内进行。</p>\n<h4 id=\"Resource-Handler\"><a href=\"#Resource-Handler\" class=\"headerlink\" title=\"Resource Handler\"></a>Resource Handler</h4><p><a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/FileServer.html\">FileServer</a> 示例展示了如何使用 ResourceHandler 在当前的工作目录提供静态内容服务。</p>\n<pre><code>//\n//  ========================================================================\n//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n//  ------------------------------------------------------------------------\n//  All rights reserved. This program and the accompanying materials\n//  are made available under the terms of the Eclipse Public License v1.0\n//  and Apache License v2.0 which accompanies this distribution.\n//\n//      The Eclipse Public License is available at\n//      http://www.eclipse.org/legal/epl-v10.html\n//\n//      The Apache License v2.0 is available at\n//      http://www.opensource.org/licenses/apache2.0.php\n//\n//  You may elect to redistribute this code under either of these licenses.\n//  ========================================================================\n//\n\npackage org.eclipse.jetty.embedded;\n\nimport org.eclipse.jetty.server.Handler;\nimport org.eclipse.jetty.server.Server;\nimport org.eclipse.jetty.server.handler.DefaultHandler;\nimport org.eclipse.jetty.server.handler.HandlerList;\nimport org.eclipse.jetty.server.handler.ResourceHandler;\nimport org.eclipse.jetty.server.handler.gzip.GzipHandler;\n\n/**\n * Simple Jetty FileServer.\n * This is a simple example of Jetty configured as a FileServer.\n */\npublic class FileServer\n&#123;\n    public static void main(String[] args) throws Exception\n    &#123;\n        // Create a basic Jetty server object that will listen on port 8080.  Note that if you set this to port 0\n        // then a randomly available port will be assigned that you can either look in the logs for the port,\n        // or programmatically obtain it for use in test cases.\n        Server server = new Server(8080);\n\n        // Create the ResourceHandler. It is the object that will actually handle the request for a given file. It is\n        // a Jetty Handler object so it is suitable for chaining with other handlers as you will see in other examples.\n        ResourceHandler resource_handler = new ResourceHandler();\n        // Configure the ResourceHandler. Setting the resource base indicates where the files should be served out of.\n        // In this example it is the current directory but it can be configured to anything that the jvm has access to.\n        resource_handler.setDirectoriesListed(true);\n        resource_handler.setWelcomeFiles(new String[]&#123; &quot;index.html&quot; &#125;);\n        resource_handler.setResourceBase(&quot;.&quot;);\n\n        // Add the ResourceHandler to the server.\n        GzipHandler gzip = new GzipHandler();\n        server.setHandler(gzip);\n        HandlerList handlers = new HandlerList();\n        handlers.setHandlers(new Handler[] &#123; resource_handler, new DefaultHandler() &#125;);\n        gzip.setHandler(handlers);\n\n        // Start things up! By using the server.join() the server thread will join with the current thread.\n        // See &quot;http://docs.oracle.com/javase/1.5.0/docs/api/java/lang/Thread.html#join()&quot; for more details.\n        server.start();\n        server.join();\n    &#125;\n&#125;\n</code></pre>\n<p>注意，ResourceHandler 和 DefaultHandler 使用了一个 HandlerList，因此 DefaultHandler 为所有找不到静态资源的请求生成良好的 404 响应。</p>\n<h3 id=\"嵌入式-Connectors\"><a href=\"#嵌入式-Connectors\" class=\"headerlink\" title=\"嵌入式 Connectors\"></a>嵌入式 Connectors</h3><p>在前面的示例中，Server 实例传入了一个端口号，并且在内部创建一个在这个端口监听请求的 Connector 默认实例。然而，通常当嵌入 Jetty 的时候，需要明确实例化，并且给 Server 实例配置一个或多个 Connectors。</p>\n<h4 id=\"一个-Connector\"><a href=\"#一个-Connector\" class=\"headerlink\" title=\"一个 Connector\"></a>一个 Connector</h4><p>下面的示例，<a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/OneConnector.html\">OneConnector.java</a>，实例化、配置并添加一个 HTTP 连接器实例到服务器：</p>\n<pre><code>//\n//  ========================================================================\n//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n//  ------------------------------------------------------------------------\n//  All rights reserved. This program and the accompanying materials\n//  are made available under the terms of the Eclipse Public License v1.0\n//  and Apache License v2.0 which accompanies this distribution.\n//\n//      The Eclipse Public License is available at\n//      http://www.eclipse.org/legal/epl-v10.html\n//\n//      The Apache License v2.0 is available at\n//      http://www.opensource.org/licenses/apache2.0.php\n//\n//  You may elect to redistribute this code under either of these licenses.\n//  ========================================================================\n//\n\npackage org.eclipse.jetty.embedded;\n\nimport org.eclipse.jetty.server.Server;\nimport org.eclipse.jetty.server.ServerConnector;\n\n/**\n * A Jetty server with one connectors.\n */\npublic class OneConnector\n&#123;\n    public static void main( String[] args ) throws Exception\n    &#123;\n        // The Server\n        Server server = new Server();\n\n        // HTTP connector\n        ServerConnector http = new ServerConnector(server);\n        http.setHost(&quot;localhost&quot;);\n        http.setPort(8080);\n        http.setIdleTimeout(30000);\n\n        // Set the connector\n        server.addConnector(http);\n\n        // Set a handler\n        server.setHandler(new HelloHandler());\n\n        // Start the server\n        server.start();\n        server.join();\n    &#125;\n&#125;\n</code></pre>\n<p>在这个示例中，连接器处理 HTTP 协议，这是 <a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/server/ServerConnector.html\">ServerConnector</a> 类默认的。</p>\n<h4 id=\"多个-Connectors\"><a href=\"#多个-Connectors\" class=\"headerlink\" title=\"多个 Connectors\"></a>多个 Connectors</h4><p>当配置多个连接器（例如，HTTP 和 HTTPS）时，共享 HTTP 通用参数的配置是可取的。为了达到这个目的，你需要明确配置 ConnectionFactory 实例和 ServerConnector 类，并且跟 HTTP 通用配置一起提供。</p>\n<p><a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/ManyConnectors.html\">ManyConnectors 示例</a> 配置了有两个 ServerConnector 实例的服务器：HTTP 连接器有一个 <a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/server/HttpConnectionFactory.html\">HTTPConnectionFactory</a> 实例；HTTPS 连接器有一个链接到 HttpConnectionFactory 的 SslConnectionFactory。两个 HttpConnectionFactories 在同一个 <a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/server/HttpConfiguration.html\">HttpConfiguration</a> 上配置，然而 HTTPS 工厂用一个包装的配置，所以可以添加 <a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/server/SecureRequestCustomizer.html\">SecureRequestCustomizer</a>。</p>\n<h3 id=\"嵌入-Servlets\"><a href=\"#嵌入-Servlets\" class=\"headerlink\" title=\"嵌入 Servlets\"></a>嵌入 Servlets</h3><p><a href=\"https://en.wikipedia.org/wiki/Java_servlet\">Servlets</a> 是提供处理 HTTP 请求应用逻辑的一般方式。Servlets 跟 Jetty 的处理器是相同的，除非请求对象是不可变的，因此不能被修改。在 Jetty 中 Servlets 通过一个 <a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/MinimalServlets.html\">ServletHandler</a> 来处理。它使用标准的路径映射让 Servlet 和请求匹配；设置请求的 servletPath 和路径信息；传送请求到 servlet，可能通过 Filters 生成响应。</p>\n<p><a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/MinimalServlets.html\">MinimalServlets 示例</a> 创建了一个 ServletHandler 实例并且配置了一个 HelloServlet:</p>\n<pre><code>//\n//  ========================================================================\n//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n//  ------------------------------------------------------------------------\n//  All rights reserved. This program and the accompanying materials\n//  are made available under the terms of the Eclipse Public License v1.0\n//  and Apache License v2.0 which accompanies this distribution.\n//\n//      The Eclipse Public License is available at\n//      http://www.eclipse.org/legal/epl-v10.html\n//\n//      The Apache License v2.0 is available at\n//      http://www.opensource.org/licenses/apache2.0.php\n//\n//  You may elect to redistribute this code under either of these licenses.\n//  ========================================================================\n//\n\npackage org.eclipse.jetty.embedded;\n\nimport java.io.IOException;\n\nimport javax.servlet.ServletException;\nimport javax.servlet.http.HttpServlet;\nimport javax.servlet.http.HttpServletRequest;\nimport javax.servlet.http.HttpServletResponse;\n\nimport org.eclipse.jetty.server.Server;\nimport org.eclipse.jetty.servlet.ServletHandler;\n\npublic class MinimalServlets\n&#123;\n    public static void main( String[] args ) throws Exception\n    &#123;\n        // Create a basic jetty server object that will listen on port 8080.\n        // Note that if you set this to port 0 then a randomly available port\n        // will be assigned that you can either look in the logs for the port,\n        // or programmatically obtain it for use in test cases.\n        Server server = new Server(8080);\n\n        // The ServletHandler is a dead simple way to create a context handler\n        // that is backed by an instance of a Servlet.\n        // This handler then needs to be registered with the Server object.\n        ServletHandler handler = new ServletHandler();\n        server.setHandler(handler);\n\n        // Passing in the class for the Servlet allows jetty to instantiate an\n        // instance of that Servlet and mount it on a given context path.\n\n        // IMPORTANT:\n        // This is a raw Servlet, not a Servlet that has been configured\n        // through a web.xml @WebServlet annotation, or anything similar.\n        handler.addServletWithMapping(HelloServlet.class, &quot;/*&quot;);\n\n        // Start things up!\n        server.start();\n\n        // The use of server.join() the will make the current thread join and\n        // wait until the server is done executing.\n        // See\n        // http://docs.oracle.com/javase/7/docs/api/java/lang/Thread.html#join()\n        server.join();\n    &#125;\n\n    @SuppressWarnings(&quot;serial&quot;)\n    public static class HelloServlet extends HttpServlet\n    &#123;\n        @Override\n        protected void doGet( HttpServletRequest request,\n                              HttpServletResponse response ) throws ServletException,\n                                                            IOException\n        &#123;\n            response.setContentType(&quot;text/html&quot;);\n            response.setStatus(HttpServletResponse.SC_OK);\n            response.getWriter().println(&quot;&lt;h1&gt;Hello from HelloServlet&lt;/h1&gt;&quot;);\n        &#125;\n    &#125;\n&#125;\n</code></pre>\n<h3 id=\"嵌入-Contexts\"><a href=\"#嵌入-Contexts\" class=\"headerlink\" title=\"嵌入 Contexts\"></a>嵌入 Contexts</h3><p><a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/OneContext.html\">ContextHandler</a> 是一个 ScopedHandler 只来响应那些 URI 前缀匹配已配置的上下文路径的请求。匹配上下文路径的请求会响应的调整它们的路径方法，并且上下文生命周期内是可获取的，可能包含：</p>\n<ul>\n<li>当请求在作用域内处理时，Classloader 设置为线程上下文的类加载器。</li>\n<li>通过过 <a href=\"http://docs.oracle.com/javaee/6/api/javax/servlet/ServletContext.html\">ServletContext</a> API 可以获取属性的集合。</li>\n<li>通过过 <a href=\"http://docs.oracle.com/javaee/6/api/javax/servlet/ServletContext.html\">ServletContext</a> API 可以获取参数的集合。</li>\n<li>通过过 <a href=\"http://docs.oracle.com/javaee/6/api/javax/servlet/ServletContext.html\">ServletContext</a> API 为静态资源的请求提供一个基本的 Resource 作为文档根。</li>\n<li>虚拟主机名称集合。</li>\n</ul>\n<p>下面的 <a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/OneContext.html\">OneContext 示例</a> 建立了一个包装了 <a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/HelloHandler.html\">HelloHandler</a> 的上下文：</p>\n<pre><code>//\n//  ========================================================================\n//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n//  ------------------------------------------------------------------------\n//  All rights reserved. This program and the accompanying materials\n//  are made available under the terms of the Eclipse Public License v1.0\n//  and Apache License v2.0 which accompanies this distribution.\n//\n//      The Eclipse Public License is available at\n//      http://www.eclipse.org/legal/epl-v10.html\n//\n//      The Apache License v2.0 is available at\n//      http://www.opensource.org/licenses/apache2.0.php\n//\n//  You may elect to redistribute this code under either of these licenses.\n//  ========================================================================\n//\n\npackage org.eclipse.jetty.embedded;\n\nimport org.eclipse.jetty.server.Server;\nimport org.eclipse.jetty.server.handler.ContextHandler;\n\npublic class OneContext\n&#123;\n    public static void main( String[] args ) throws Exception\n    &#123;\n        Server server = new Server( 8080 );\n\n        // Add a single handler on context &quot;/hello&quot;\n        ContextHandler context = new ContextHandler();\n        context.setContextPath( &quot;/hello&quot; );\n        context.setHandler( new HelloHandler() );\n\n        // Can be accessed using http://localhost:8080/hello\n\n        server.setHandler( context );\n\n        // Start the server\n        server.start();\n        server.join();\n    &#125;\n&#125;\n</code></pre>\n<p>当有很多上下文出现时，你可以嵌入一个 ContextHandlerCollection 来有效的测试一个请求的 URI，然后选择匹配到的 ContextHandler。<a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/ManyContexts.html\">ManyContexts 示例</a> 展示了可以配置多个上下文：</p>\n<pre><code>//\n//  ========================================================================\n//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n//  ------------------------------------------------------------------------\n//  All rights reserved. This program and the accompanying materials\n//  are made available under the terms of the Eclipse Public License v1.0\n//  and Apache License v2.0 which accompanies this distribution.\n//\n//      The Eclipse Public License is available at\n//      http://www.eclipse.org/legal/epl-v10.html\n//\n//      The Apache License v2.0 is available at\n//      http://www.opensource.org/licenses/apache2.0.php\n//\n//  You may elect to redistribute this code under either of these licenses.\n//  ========================================================================\n//\n\npackage org.eclipse.jetty.embedded;\n\nimport org.eclipse.jetty.server.Handler;\nimport org.eclipse.jetty.server.Server;\nimport org.eclipse.jetty.server.handler.ContextHandler;\nimport org.eclipse.jetty.server.handler.ContextHandlerCollection;\n\npublic class ManyContexts\n&#123;\n    public static void main( String[] args ) throws Exception\n    &#123;\n        Server server = new Server(8080);\n\n        ContextHandler context = new ContextHandler(&quot;/&quot;);\n        context.setContextPath(&quot;/&quot;);\n        context.setHandler(new HelloHandler(&quot;Root Hello&quot;));\n\n        ContextHandler contextFR = new ContextHandler(&quot;/fr&quot;);\n        contextFR.setHandler(new HelloHandler(&quot;Bonjoir&quot;));\n\n        ContextHandler contextIT = new ContextHandler(&quot;/it&quot;);\n        contextIT.setHandler(new HelloHandler(&quot;Bongiorno&quot;));\n\n        ContextHandler contextV = new ContextHandler(&quot;/&quot;);\n        contextV.setVirtualHosts(new String[] &#123; &quot;127.0.0.2&quot; &#125;);\n        contextV.setHandler(new HelloHandler(&quot;Virtual Hello&quot;));\n\n        ContextHandlerCollection contexts = new ContextHandlerCollection();\n        contexts.setHandlers(new Handler[] &#123; context, contextFR, contextIT,\n                contextV &#125;);\n\n        server.setHandler(contexts);\n\n        server.start();\n        server.join();\n    &#125;\n&#125;\n</code></pre>\n<h3 id=\"嵌入-ServletContexts\"><a href=\"#嵌入-ServletContexts\" class=\"headerlink\" title=\"嵌入 ServletContexts\"></a>嵌入 ServletContexts</h3><p><a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/servlet/ServletContextHandler.html\">ServletContextHandler</a> 是一个支持普通 session 和 Servlet 的特定的 ContextHandler。下面的 <a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/OneServletContext.html\">OneServletContext 示例</a> 实例化了一个 <a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/servlet/DefaultServlet.html\">DefaultServlet</a> 从 /tmp/ 提供静态内容，并且实例化一个 DumpServlet 创建 session 并存储请求的基本详细信息：</p>\n<pre><code>//\n//  ========================================================================\n//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n//  ------------------------------------------------------------------------\n//  All rights reserved. This program and the accompanying materials\n//  are made available under the terms of the Eclipse Public License v1.0\n//  and Apache License v2.0 which accompanies this distribution.\n//\n//      The Eclipse Public License is available at\n//      http://www.eclipse.org/legal/epl-v10.html\n//\n//      The Apache License v2.0 is available at\n//      http://www.opensource.org/licenses/apache2.0.php\n//\n//  You may elect to redistribute this code under either of these licenses.\n//  ========================================================================\n//\n\npackage org.eclipse.jetty.embedded;\n\nimport org.eclipse.jetty.server.Server;\nimport org.eclipse.jetty.servlet.DefaultServlet;\nimport org.eclipse.jetty.servlet.ServletContextHandler;\n\npublic class OneServletContext\n&#123;\n    public static void main( String[] args ) throws Exception\n    &#123;\n        Server server = new Server(8080);\n\n        ServletContextHandler context = new ServletContextHandler(\n                ServletContextHandler.SESSIONS);\n        context.setContextPath(&quot;/&quot;);\n        context.setResourceBase(System.getProperty(&quot;java.io.tmpdir&quot;));\n        server.setHandler(context);\n\n        // Add dump servlet\n        context.addServlet(DumpServlet.class, &quot;/dump/*&quot;);\n        // Add default servlet\n        context.addServlet(DefaultServlet.class, &quot;/&quot;);\n\n        server.start();\n        server.join();\n    &#125;\n&#125;\n</code></pre>\n<h3 id=\"嵌入-Web-Applications\"><a href=\"#嵌入-Web-Applications\" class=\"headerlink\" title=\"嵌入 Web Applications\"></a>嵌入 Web Applications</h3><p><a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/webapp/WebAppContext.html\">WebAppContext</a> 是一个 ServletContextHandler 的扩展，它使用<a href=\"https://en.wikipedia.org/wiki/WAR_(file_format)\">标准设计</a>和 web.xml 配置 servlet、filter 和 web.xml 或注解的其他特性。下面的 <a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/OneWebApp.html\">OneWebApp 示例</a> 配置了 Jetty 的测试 web 应用。Web 应用程序可以使用容器提供的的资源，在这个案例中需要并配置一个 LoginService：</p>\n<pre><code>//\n//  ========================================================================\n//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n//  ------------------------------------------------------------------------\n//  All rights reserved. This program and the accompanying materials\n//  are made available under the terms of the Eclipse Public License v1.0\n//  and Apache License v2.0 which accompanies this distribution.\n//\n//      The Eclipse Public License is available at\n//      http://www.eclipse.org/legal/epl-v10.html\n//\n//      The Apache License v2.0 is available at\n//      http://www.opensource.org/licenses/apache2.0.php\n//\n//  You may elect to redistribute this code under either of these licenses.\n//  ========================================================================\n//\n\npackage org.eclipse.jetty.embedded;\n\nimport java.io.File;\nimport java.lang.management.ManagementFactory;\n\nimport org.eclipse.jetty.jmx.MBeanContainer;\nimport org.eclipse.jetty.security.HashLoginService;\nimport org.eclipse.jetty.server.Server;\nimport org.eclipse.jetty.server.handler.AllowSymLinkAliasChecker;\nimport org.eclipse.jetty.webapp.WebAppContext;\n\npublic class OneWebApp\n&#123;\n    public static void main( String[] args ) throws Exception\n    &#123;\n        // Create a basic jetty server object that will listen on port 8080.\n        // Note that if you set this to port 0 then a randomly available port\n        // will be assigned that you can either look in the logs for the port,\n        // or programmatically obtain it for use in test cases.\n        Server server = new Server(8080);\n\n        // Setup JMX\n        MBeanContainer mbContainer = new MBeanContainer(\n                ManagementFactory.getPlatformMBeanServer());\n        server.addBean(mbContainer);\n\n        // The WebAppContext is the entity that controls the environment in\n        // which a web application lives and breathes. In this example the\n        // context path is being set to &quot;/&quot; so it is suitable for serving root\n        // context requests and then we see it setting the location of the war.\n        // A whole host of other configurations are available, ranging from\n        // configuring to support annotation scanning in the webapp (through\n        // PlusConfiguration) to choosing where the webapp will unpack itself.\n        WebAppContext webapp = new WebAppContext();\n        webapp.setContextPath(&quot;/&quot;);\n        File warFile = new File(\n                &quot;../../tests/test-jmx/jmx-webapp/target/jmx-webapp&quot;);\n        webapp.setWar(warFile.getAbsolutePath());\n\n        // A WebAppContext is a ContextHandler as well so it needs to be set to\n        // the server so it is aware of where to send the appropriate requests.\n        server.setHandler(webapp);\n\n        // Start things up!\n        server.start();\n\n        // The use of server.join() the will make the current thread join and\n        // wait until the server is done executing.\n        // See http://docs.oracle.com/javase/7/docs/api/java/lang/Thread.html#join()\n        server.join();\n    &#125;\n&#125;\n</code></pre>\n<h3 id=\"喜欢-Jetty-XML\"><a href=\"#喜欢-Jetty-XML\" class=\"headerlink\" title=\"喜欢 Jetty XML\"></a>喜欢 Jetty XML</h3><p>配置 Jetty 服务器实例的典型方式是通过 jetty.xml 关联配置文件。然而 Jetty XML 配置格式只是在代码中可以做的简单的表达；也可以很简单的写嵌入代码准确实现 jetty.xml 配置。下面的 <a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/LikeJettyXml.html\">LikeJettyXml 示例</a> 在代码中呈现了从配置文件获取的行为：</p>\n<ul>\n<li>jetty.xml</li>\n<li>jetty-jmx.xml</li>\n<li>jetty-http.xml</li>\n<li>jetty-https.xml</li>\n<li>jetty-deploy.xml</li>\n<li>jetty-stats.xml</li>\n<li>jetty-requestlog.xml</li>\n<li>jetty-lowresources.xml</li>\n<li>test-realm.xml</li>\n</ul>\n<pre><code>//\n//  ========================================================================\n//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n//  ------------------------------------------------------------------------\n//  All rights reserved. This program and the accompanying materials\n//  are made available under the terms of the Eclipse Public License v1.0\n//  and Apache License v2.0 which accompanies this distribution.\n//\n//      The Eclipse Public License is available at\n//      http://www.eclipse.org/legal/epl-v10.html\n//\n//      The Apache License v2.0 is available at\n//      http://www.opensource.org/licenses/apache2.0.php\n//\n//  You may elect to redistribute this code under either of these licenses.\n//  ========================================================================\n//\n\npackage org.eclipse.jetty.embedded;\n\nimport java.io.File;\nimport java.io.FileNotFoundException;\nimport java.lang.management.ManagementFactory;\n\nimport org.eclipse.jetty.deploy.DeploymentManager;\nimport org.eclipse.jetty.deploy.PropertiesConfigurationManager;\nimport org.eclipse.jetty.deploy.bindings.DebugListenerBinding;\nimport org.eclipse.jetty.deploy.providers.WebAppProvider;\nimport org.eclipse.jetty.http.HttpVersion;\nimport org.eclipse.jetty.jmx.MBeanContainer;\nimport org.eclipse.jetty.security.HashLoginService;\nimport org.eclipse.jetty.server.DebugListener;\nimport org.eclipse.jetty.server.Handler;\nimport org.eclipse.jetty.server.HttpConfiguration;\nimport org.eclipse.jetty.server.HttpConnectionFactory;\nimport org.eclipse.jetty.server.LowResourceMonitor;\nimport org.eclipse.jetty.server.NCSARequestLog;\nimport org.eclipse.jetty.server.SecureRequestCustomizer;\nimport org.eclipse.jetty.server.Server;\nimport org.eclipse.jetty.server.ServerConnector;\nimport org.eclipse.jetty.server.SslConnectionFactory;\nimport org.eclipse.jetty.server.handler.ContextHandlerCollection;\nimport org.eclipse.jetty.server.handler.DefaultHandler;\nimport org.eclipse.jetty.server.handler.HandlerCollection;\nimport org.eclipse.jetty.server.handler.RequestLogHandler;\nimport org.eclipse.jetty.server.handler.StatisticsHandler;\nimport org.eclipse.jetty.util.ssl.SslContextFactory;\nimport org.eclipse.jetty.util.thread.QueuedThreadPool;\nimport org.eclipse.jetty.util.thread.ScheduledExecutorScheduler;\nimport org.eclipse.jetty.webapp.Configuration;\n\n/**\n * Starts the Jetty Distribution&#39;s demo-base directory using entirely\n * embedded jetty techniques.\n */\npublic class LikeJettyXml\n&#123;\n    public static void main( String[] args ) throws Exception\n    &#123;\n        // Path to as-built jetty-distribution directory\n        String jettyHomeBuild = &quot;../../jetty-distribution/target/distribution&quot;;\n\n        // Find jetty home and base directories\n        String homePath = System.getProperty(&quot;jetty.home&quot;, jettyHomeBuild);\n        File homeDir = new File(homePath);\n        if (!homeDir.exists())\n        &#123;\n            throw new FileNotFoundException(homeDir.getAbsolutePath());\n        &#125;\n        String basePath = System.getProperty(&quot;jetty.base&quot;, homeDir + &quot;/demo-base&quot;);\n        File baseDir = new File(basePath);\n        if(!baseDir.exists())\n        &#123;\n            throw new FileNotFoundException(baseDir.getAbsolutePath());\n        &#125;\n\n        // Configure jetty.home and jetty.base system properties\n        String jetty_home = homeDir.getAbsolutePath();\n        String jetty_base = baseDir.getAbsolutePath();\n        System.setProperty(&quot;jetty.home&quot;, jetty_home);\n        System.setProperty(&quot;jetty.base&quot;, jetty_base);\n\n\n        // === jetty.xml ===\n        // Setup Threadpool\n        QueuedThreadPool threadPool = new QueuedThreadPool();\n        threadPool.setMaxThreads(500);\n\n        // Server\n        Server server = new Server(threadPool);\n\n        // Scheduler\n        server.addBean(new ScheduledExecutorScheduler());\n\n        // HTTP Configuration\n        HttpConfiguration http_config = new HttpConfiguration();\n        http_config.setSecureScheme(&quot;https&quot;);\n        http_config.setSecurePort(8443);\n        http_config.setOutputBufferSize(32768);\n        http_config.setRequestHeaderSize(8192);\n        http_config.setResponseHeaderSize(8192);\n        http_config.setSendServerVersion(true);\n        http_config.setSendDateHeader(false);\n        // httpConfig.addCustomizer(new ForwardedRequestCustomizer());\n\n        // Handler Structure\n        HandlerCollection handlers = new HandlerCollection();\n        ContextHandlerCollection contexts = new ContextHandlerCollection();\n        handlers.setHandlers(new Handler[] &#123; contexts, new DefaultHandler() &#125;);\n        server.setHandler(handlers);\n\n        // Extra options\n        server.setDumpAfterStart(false);\n        server.setDumpBeforeStop(false);\n        server.setStopAtShutdown(true);\n\n        // === jetty-jmx.xml ===\n        MBeanContainer mbContainer = new MBeanContainer(\n                ManagementFactory.getPlatformMBeanServer());\n        server.addBean(mbContainer);\n\n\n        // === jetty-http.xml ===\n        ServerConnector http = new ServerConnector(server,\n                new HttpConnectionFactory(http_config));\n        http.setPort(8080);\n        http.setIdleTimeout(30000);\n        server.addConnector(http);\n\n\n        // === jetty-https.xml ===\n        // SSL Context Factory\n        SslContextFactory sslContextFactory = new SslContextFactory();\n        sslContextFactory.setKeyStorePath(jetty_home + &quot;/../../../jetty-server/src/test/config/etc/keystore&quot;);\n        sslContextFactory.setKeyStorePassword(&quot;OBF:1vny1zlo1x8e1vnw1vn61x8g1zlu1vn4&quot;);\n        sslContextFactory.setKeyManagerPassword(&quot;OBF:1u2u1wml1z7s1z7a1wnl1u2g&quot;);\n        sslContextFactory.setTrustStorePath(jetty_home + &quot;/../../../jetty-server/src/test/config/etc/keystore&quot;);\n        sslContextFactory.setTrustStorePassword(&quot;OBF:1vny1zlo1x8e1vnw1vn61x8g1zlu1vn4&quot;);\n        sslContextFactory.setExcludeCipherSuites(&quot;SSL_RSA_WITH_DES_CBC_SHA&quot;,\n                &quot;SSL_DHE_RSA_WITH_DES_CBC_SHA&quot;, &quot;SSL_DHE_DSS_WITH_DES_CBC_SHA&quot;,\n                &quot;SSL_RSA_EXPORT_WITH_RC4_40_MD5&quot;,\n                &quot;SSL_RSA_EXPORT_WITH_DES40_CBC_SHA&quot;,\n                &quot;SSL_DHE_RSA_EXPORT_WITH_DES40_CBC_SHA&quot;,\n                &quot;SSL_DHE_DSS_EXPORT_WITH_DES40_CBC_SHA&quot;);\n\n        // SSL HTTP Configuration\n        HttpConfiguration https_config = new HttpConfiguration(http_config);\n        https_config.addCustomizer(new SecureRequestCustomizer());\n\n        // SSL Connector\n        ServerConnector sslConnector = new ServerConnector(server,\n            new SslConnectionFactory(sslContextFactory,HttpVersion.HTTP_1_1.asString()),\n            new HttpConnectionFactory(https_config));\n        sslConnector.setPort(8443);\n        server.addConnector(sslConnector);\n\n\n        // === jetty-deploy.xml ===\n        DeploymentManager deployer = new DeploymentManager();\n        DebugListener debug = new DebugListener(System.out,true,true,true);\n        server.addBean(debug);        \n        deployer.addLifeCycleBinding(new DebugListenerBinding(debug));\n        deployer.setContexts(contexts);\n        deployer.setContextAttribute(\n                &quot;org.eclipse.jetty.server.webapp.ContainerIncludeJarPattern&quot;,\n                &quot;.*/servlet-api-[^/]*\\\\.jar$&quot;);\n\n        WebAppProvider webapp_provider = new WebAppProvider();\n        webapp_provider.setMonitoredDirName(jetty_base + &quot;/webapps&quot;);\n        webapp_provider.setDefaultsDescriptor(jetty_home + &quot;/etc/webdefault.xml&quot;);\n        webapp_provider.setScanInterval(1);\n        webapp_provider.setExtractWars(true);\n        webapp_provider.setConfigurationManager(new PropertiesConfigurationManager());\n\n        deployer.addAppProvider(webapp_provider);\n        server.addBean(deployer);\n\n        // === setup jetty plus ==\n        Configuration.ClassList.setServerDefault(server).addAfter(\n                &quot;org.eclipse.jetty.webapp.FragmentConfiguration&quot;,\n                &quot;org.eclipse.jetty.plus.webapp.EnvConfiguration&quot;,\n                &quot;org.eclipse.jetty.plus.webapp.PlusConfiguration&quot;);\n\n        // === jetty-stats.xml ===\n        StatisticsHandler stats = new StatisticsHandler();\n        stats.setHandler(server.getHandler());\n        server.setHandler(stats);\n\n\n        // === jetty-requestlog.xml ===\n        NCSARequestLog requestLog = new NCSARequestLog();\n        requestLog.setFilename(jetty_home + &quot;/logs/yyyy_mm_dd.request.log&quot;);\n        requestLog.setFilenameDateFormat(&quot;yyyy_MM_dd&quot;);\n        requestLog.setRetainDays(90);\n        requestLog.setAppend(true);\n        requestLog.setExtended(true);\n        requestLog.setLogCookies(false);\n        requestLog.setLogTimeZone(&quot;GMT&quot;);\n        RequestLogHandler requestLogHandler = new RequestLogHandler();\n        requestLogHandler.setRequestLog(requestLog);\n        handlers.addHandler(requestLogHandler);\n\n\n        // === jetty-lowresources.xml ===\n        LowResourceMonitor lowResourcesMonitor=new LowResourceMonitor(server);\n        lowResourcesMonitor.setPeriod(1000);\n        lowResourcesMonitor.setLowResourcesIdleTimeout(200);\n        lowResourcesMonitor.setMonitorThreads(true);\n        lowResourcesMonitor.setMaxConnections(0);\n        lowResourcesMonitor.setMaxMemory(0);\n        lowResourcesMonitor.setMaxLowResourcesTime(5000);\n        server.addBean(lowResourcesMonitor);\n\n\n        // === test-realm.xml ===\n        HashLoginService login = new HashLoginService();\n        login.setName(&quot;Test Realm&quot;);\n        login.setConfig(jetty_base + &quot;/etc/realm.properties&quot;);\n        login.setHotReload(false);\n        server.addBean(login);\n\n        // Start the server\n        server.start();\n        server.join();\n    &#125;\n&#125;\n</code></pre>\n","site":{"data":{}},"excerpt":"<p>Jetty 有一个口号，“不要在 Jetty 中部署你的应用，部署 Jetty 在你的应用中！”这个口号意味着构建应用为为标准的 WAR 并且部署到 Jetty 的另外一个选择是，Jetty 被设计为一个软件组件，可以在 Java 程序中像其他 POJO 一样被实例化和使用。换一种方式，以嵌入式模式运行 Jetty 意味着放置一个 HTTP 模块到你的应用中，而不是放置你的应用到 HTTP 服务器。</p>","more":"<p>本教程将带领你一步步从最简单的 Jetty 服务器实例化到用标准化部署描述器运行多 Web 应用程序。大部分这些实例的代码是标准 Jetty 项目的一部分。</p>\n<h3 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h3><p>嵌入 Jetty 服务器，下面是通常的步骤，也是本教程实例中展示步骤：</p>\n<ol>\n<li>创建一个 <a href=\"http://download.eclipse.org/jetty/stable-9/apidocs/org/eclipse/jetty/server/Server.html\">Server</a> 实例。  </li>\n<li>添加/配置 <a href=\"http://download.eclipse.org/jetty/stable-9/apidocs/org/eclipse/jetty/server/Connector.html\">Connectors</a>。  </li>\n<li>添加/配置 <a href=\"http://download.eclipse.org/jetty/stable-9/apidocs/org/eclipse/jetty/server/Handler.html\">Handlers</a>、<a href=\"http://download.eclipse.org/jetty/stable-9/apidocs/org/eclipse/jetty/server/handler/ContextHandler.html\">Contexts</a>、<a href=\"http://docs.oracle.com/javaee/6/api/javax/servlet/Servlet.html\">Servlets</a>。  </li>\n<li>启动 Server。  </li>\n<li>服务器等待或者用自己的线程做一些其他的事情。</li>\n</ol>\n<h3 id=\"创建-Server\"><a href=\"#创建-Server\" class=\"headerlink\" title=\"创建 Server\"></a>创建 Server</h3><p>下面是 SimplestServer.java 文件的代码，实例化并且运行一个可能最简单的 Jetty server。</p>\n<pre><code>//\n//  ========================================================================\n//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n//  ------------------------------------------------------------------------\n//  All rights reserved. This program and the accompanying materials\n//  are made available under the terms of the Eclipse Public License v1.0\n//  and Apache License v2.0 which accompanies this distribution.\n//\n//      The Eclipse Public License is available at\n//      http://www.eclipse.org/legal/epl-v10.html\n//\n//      The Apache License v2.0 is available at\n//      http://www.opensource.org/licenses/apache2.0.php\n//\n//  You may elect to redistribute this code under either of these licenses.\n//  ========================================================================\n//\n\npackage org.eclipse.jetty.embedded;\n\nimport org.eclipse.jetty.server.Server;\n\n/**\n * The simplest possible Jetty server.\n */\npublic class SimplestServer\n&#123;\n    public static void main( String[] args ) throws Exception\n    &#123;\n        Server server = new Server(8080);\n        server.start();\n        server.dumpStdErr();\n        server.join();\n    &#125;\n&#125;\n</code></pre>\n<p>这将运行一个监听 8080 端口的 HTTP Server。这不是一个有用的 server，因为它没有处理器，因此对所有的请求都将返回 404。</p>\n<h3 id=\"使用处理器\"><a href=\"#使用处理器\" class=\"headerlink\" title=\"使用处理器\"></a>使用处理器</h3><p>为了针对请求生成响应，Jetty 要求在服务器上设置处理器。一个处理器可能：</p>\n<ul>\n<li>检查/修改 HTTP 请求。</li>\n<li>生成完整的 HTTP 响应。</li>\n<li>调用其他处理器（见 HandlerWrapper）。</li>\n<li>选择一个或者多个处理器调用（见 HandlerCollection）。</li>\n</ul>\n<h4 id=\"HelloWorld-Handler\"><a href=\"#HelloWorld-Handler\" class=\"headerlink\" title=\"HelloWorld Handler\"></a>HelloWorld Handler</h4><p>下面 HelloHandler.java 的代码展示了一个简单的 hello world 处理器：</p>\n<pre><code>//\n//  ========================================================================\n//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n//  ------------------------------------------------------------------------\n//  All rights reserved. This program and the accompanying materials\n//  are made available under the terms of the Eclipse Public License v1.0\n//  and Apache License v2.0 which accompanies this distribution.\n//\n//      The Eclipse Public License is available at\n//      http://www.eclipse.org/legal/epl-v10.html\n//\n//      The Apache License v2.0 is available at\n//      http://www.opensource.org/licenses/apache2.0.php\n//\n//  You may elect to redistribute this code under either of these licenses.\n//  ========================================================================\n//\n\npackage org.eclipse.jetty.embedded;\n\nimport java.io.IOException;\nimport java.io.PrintWriter;\n\nimport javax.servlet.ServletException;\nimport javax.servlet.http.HttpServletRequest;\nimport javax.servlet.http.HttpServletResponse;\n\nimport org.eclipse.jetty.server.Request;\nimport org.eclipse.jetty.server.handler.AbstractHandler;\n\npublic class HelloHandler extends AbstractHandler\n&#123;\n    final String greeting;\n    final String body;\n\n    public HelloHandler()\n    &#123;\n        this(&quot;Hello World&quot;);\n    &#125;\n\n    public HelloHandler( String greeting )\n    &#123;\n        this(greeting, null);\n    &#125;\n\n    public HelloHandler( String greeting, String body )\n    &#123;\n        this.greeting = greeting;\n        this.body = body;\n    &#125;\n\n    public void handle( String target,\n                        Request baseRequest,\n                        HttpServletRequest request,\n                        HttpServletResponse response ) throws IOException,\n                                                      ServletException\n    &#123;\n        response.setContentType(&quot;text/html; charset=utf-8&quot;);\n        response.setStatus(HttpServletResponse.SC_OK);\n\n        PrintWriter out = response.getWriter();\n\n        out.println(&quot;&lt;h1&gt;&quot; + greeting + &quot;&lt;/h1&gt;&quot;);\n        if (body != null)\n        &#123;\n            out.println(body);\n        &#125;\n\n        baseRequest.setHandled(true);\n    &#125;\n&#125;\n</code></pre>\n<p>传给处理方法的参数是：</p>\n<ul>\n<li>target - 请求的目标，可能是一个 URI 或者被命名调度器的名称。</li>\n<li>baseRequest - 可变的 Jetty 请求对象，这个对象总是未包装的。</li>\n<li>request - 不变的请求对象，可能被一个过滤器或 servlet 包装过。</li>\n<li>response - 响应，可能被过滤器或 servlet 包装过。</li>\n</ul>\n<p>处理器设置响应的状态、内容类型，并且在使用写入器生成响应体之前标记请求已经被处理。</p>\n<h4 id=\"运行-HelloWorldHandler\"><a href=\"#运行-HelloWorldHandler\" class=\"headerlink\" title=\"运行 HelloWorldHandler\"></a>运行 HelloWorldHandler</h4><p>为了允许一个处理器处理 HTTP 请求，必须添加处理器到一个服务器实例中。下面 OneHandler.java 代码展示了 Jetty 服务器如何使用 HelloWrold 处理器：</p>\n<pre><code>//\n//  ========================================================================\n//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n//  ------------------------------------------------------------------------\n//  All rights reserved. This program and the accompanying materials\n//  are made available under the terms of the Eclipse Public License v1.0\n//  and Apache License v2.0 which accompanies this distribution.\n//\n//      The Eclipse Public License is available at\n//      http://www.eclipse.org/legal/epl-v10.html\n//\n//      The Apache License v2.0 is available at\n//      http://www.opensource.org/licenses/apache2.0.php\n//\n//  You may elect to redistribute this code under either of these licenses.\n//  ========================================================================\n//\n\npackage org.eclipse.jetty.embedded;\n\nimport org.eclipse.jetty.server.Server;\n\npublic class OneHandler\n&#123;\n    public static void main( String[] args ) throws Exception\n    &#123;\n        Server server = new Server(8080);\n        server.setHandler(new HelloHandler());\n\n        server.start();\n        server.join();\n    &#125;\n&#125;\n</code></pre>\n<p>在 Jetty 中一个或者更多处理器处理所有请求。一些处理器选择其他特定的处理器（例如，ContextHandlerCollection 使用上下文路径选择 ContextHandler）；其他处理器使用应用逻辑生成响应（例如，ServletHandler 传送请求给应用 Servlet），同时其他处理器做跟生成响应无关的任务（例如，RequestLogHandler 或者 StatisticsHandler）。</p>\n<p>后面的章节描述如何结合处理器方面的问题。你可以看到在 Jetty 的 <a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/server/handler/package-summary.html\">org.eclipse.jetty.server.handler</a> 包中有一些处理器。</p>\n<h4 id=\"Handler-Collections-and-Wrappers\"><a href=\"#Handler-Collections-and-Wrappers\" class=\"headerlink\" title=\"Handler Collections and Wrappers\"></a>Handler Collections and Wrappers</h4><p>复杂的请求可以通过多种方式组合多个处理器进行处理。Jetty 有多个 <a href=\"http://download.eclipse.org/jetty/stable-9/apidocs/org/eclipse/jetty/server/HandlerContainer.html\">HandlerContainer</a> 接口的实现：</p>\n<p><strong><a href=\"http://download.eclipse.org/jetty/stable-9/apidocs/org/eclipse/jetty/server/handler/HandlerCollection.html\">HandlerCollection</a></strong></p>\n<p>  管理其他处理器的一个集合，并且按照顺序调用每个处理器。这对组合那些生成响应的处理器和统计数据、记录日志的处理器是有用的。</p>\n<p><strong><a href=\"http://download.eclipse.org/jetty/stable-9/apidocs/org/eclipse/jetty/server/handler/HandlerList.html\">HandlerList</a></strong></p>\n<p>  轮流调用每个处理器的处理器集合，直到抛出一个异常、提交了响应或者 request.isHandled() 返回 true。你可以用它组合根据条件处理一个请求的处理器，例如调用多个上下文直到匹配一个虚拟主机。</p>\n<p><strong><a href=\"http://download.eclipse.org/jetty/stable-9/apidocs/org/eclipse/jetty/server/handler/HandlerWrapper.html\">HandlerWrapper</a></strong></p>\n<p>  处理器的基础类，你可以在面向方面编程风格中一起使用菊花链处理器。例如，一个标准的 web 应用是通过一个 context、session、security和servlet处理器链实现的。</p>\n<p><strong><a href=\"http://download.eclipse.org/jetty/stable-9/apidocs/org/eclipse/jetty/server/handler/ContextHandlerCollection.html\">ContextHandlerCollection</a></strong></p>\n<p>  一个特定的 HandlerCollection，使用请求 URI 最长的前缀（contextPath）选择包含的 ContextHandler 来处理请求。</p>\n<h4 id=\"处理器的作用域\"><a href=\"#处理器的作用域\" class=\"headerlink\" title=\"处理器的作用域\"></a>处理器的作用域</h4><p>很多 Jetty 中的标准 Servlet 容器是用 HandlerWrappers 实现的，是一个菊花式处理器链：ContextHandler 到 SessionHandler 到 SecurityHandler 到 ServletHandler。然而，因为 servlet 规范的特性，这个链不是一个处理器的纯粹嵌套，因为外部处理器有时需要内部处理器的过程信息。例如，当 ContextHandler 调用应用监听器来通知它们一个请求进入了上下文，它必须已经知道 ServletHandler 分发请求给哪个 servlet，因此 servletPath 方法返回正确的值。</p>\n<p>HandlerWrapper 被定制为 <a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/server/handler/ScopedHandler.html\">ScopedHandler</a> 抽象类，它支持作用域的菊花链。例如，如果一个 ServletHandler 嵌套在一个 ContextHandler 中，方法的执行顺序和嵌套是：</p>\n<p>Server.handle(…)<br>  ContextHandler.doScope(…)<br>    ServletHandler.doScope(…)<br>      ContextHandler.doHandle(…)<br>        ServletHandler.doHandle(…)<br>          SomeServlet.service(…)</p>\n<p>因此，当 ContextHandler 处理请求的时候，它将在 ServletHandler 建立的作用域内进行。</p>\n<h4 id=\"Resource-Handler\"><a href=\"#Resource-Handler\" class=\"headerlink\" title=\"Resource Handler\"></a>Resource Handler</h4><p><a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/FileServer.html\">FileServer</a> 示例展示了如何使用 ResourceHandler 在当前的工作目录提供静态内容服务。</p>\n<pre><code>//\n//  ========================================================================\n//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n//  ------------------------------------------------------------------------\n//  All rights reserved. This program and the accompanying materials\n//  are made available under the terms of the Eclipse Public License v1.0\n//  and Apache License v2.0 which accompanies this distribution.\n//\n//      The Eclipse Public License is available at\n//      http://www.eclipse.org/legal/epl-v10.html\n//\n//      The Apache License v2.0 is available at\n//      http://www.opensource.org/licenses/apache2.0.php\n//\n//  You may elect to redistribute this code under either of these licenses.\n//  ========================================================================\n//\n\npackage org.eclipse.jetty.embedded;\n\nimport org.eclipse.jetty.server.Handler;\nimport org.eclipse.jetty.server.Server;\nimport org.eclipse.jetty.server.handler.DefaultHandler;\nimport org.eclipse.jetty.server.handler.HandlerList;\nimport org.eclipse.jetty.server.handler.ResourceHandler;\nimport org.eclipse.jetty.server.handler.gzip.GzipHandler;\n\n/**\n * Simple Jetty FileServer.\n * This is a simple example of Jetty configured as a FileServer.\n */\npublic class FileServer\n&#123;\n    public static void main(String[] args) throws Exception\n    &#123;\n        // Create a basic Jetty server object that will listen on port 8080.  Note that if you set this to port 0\n        // then a randomly available port will be assigned that you can either look in the logs for the port,\n        // or programmatically obtain it for use in test cases.\n        Server server = new Server(8080);\n\n        // Create the ResourceHandler. It is the object that will actually handle the request for a given file. It is\n        // a Jetty Handler object so it is suitable for chaining with other handlers as you will see in other examples.\n        ResourceHandler resource_handler = new ResourceHandler();\n        // Configure the ResourceHandler. Setting the resource base indicates where the files should be served out of.\n        // In this example it is the current directory but it can be configured to anything that the jvm has access to.\n        resource_handler.setDirectoriesListed(true);\n        resource_handler.setWelcomeFiles(new String[]&#123; &quot;index.html&quot; &#125;);\n        resource_handler.setResourceBase(&quot;.&quot;);\n\n        // Add the ResourceHandler to the server.\n        GzipHandler gzip = new GzipHandler();\n        server.setHandler(gzip);\n        HandlerList handlers = new HandlerList();\n        handlers.setHandlers(new Handler[] &#123; resource_handler, new DefaultHandler() &#125;);\n        gzip.setHandler(handlers);\n\n        // Start things up! By using the server.join() the server thread will join with the current thread.\n        // See &quot;http://docs.oracle.com/javase/1.5.0/docs/api/java/lang/Thread.html#join()&quot; for more details.\n        server.start();\n        server.join();\n    &#125;\n&#125;\n</code></pre>\n<p>注意，ResourceHandler 和 DefaultHandler 使用了一个 HandlerList，因此 DefaultHandler 为所有找不到静态资源的请求生成良好的 404 响应。</p>\n<h3 id=\"嵌入式-Connectors\"><a href=\"#嵌入式-Connectors\" class=\"headerlink\" title=\"嵌入式 Connectors\"></a>嵌入式 Connectors</h3><p>在前面的示例中，Server 实例传入了一个端口号，并且在内部创建一个在这个端口监听请求的 Connector 默认实例。然而，通常当嵌入 Jetty 的时候，需要明确实例化，并且给 Server 实例配置一个或多个 Connectors。</p>\n<h4 id=\"一个-Connector\"><a href=\"#一个-Connector\" class=\"headerlink\" title=\"一个 Connector\"></a>一个 Connector</h4><p>下面的示例，<a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/OneConnector.html\">OneConnector.java</a>，实例化、配置并添加一个 HTTP 连接器实例到服务器：</p>\n<pre><code>//\n//  ========================================================================\n//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n//  ------------------------------------------------------------------------\n//  All rights reserved. This program and the accompanying materials\n//  are made available under the terms of the Eclipse Public License v1.0\n//  and Apache License v2.0 which accompanies this distribution.\n//\n//      The Eclipse Public License is available at\n//      http://www.eclipse.org/legal/epl-v10.html\n//\n//      The Apache License v2.0 is available at\n//      http://www.opensource.org/licenses/apache2.0.php\n//\n//  You may elect to redistribute this code under either of these licenses.\n//  ========================================================================\n//\n\npackage org.eclipse.jetty.embedded;\n\nimport org.eclipse.jetty.server.Server;\nimport org.eclipse.jetty.server.ServerConnector;\n\n/**\n * A Jetty server with one connectors.\n */\npublic class OneConnector\n&#123;\n    public static void main( String[] args ) throws Exception\n    &#123;\n        // The Server\n        Server server = new Server();\n\n        // HTTP connector\n        ServerConnector http = new ServerConnector(server);\n        http.setHost(&quot;localhost&quot;);\n        http.setPort(8080);\n        http.setIdleTimeout(30000);\n\n        // Set the connector\n        server.addConnector(http);\n\n        // Set a handler\n        server.setHandler(new HelloHandler());\n\n        // Start the server\n        server.start();\n        server.join();\n    &#125;\n&#125;\n</code></pre>\n<p>在这个示例中，连接器处理 HTTP 协议，这是 <a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/server/ServerConnector.html\">ServerConnector</a> 类默认的。</p>\n<h4 id=\"多个-Connectors\"><a href=\"#多个-Connectors\" class=\"headerlink\" title=\"多个 Connectors\"></a>多个 Connectors</h4><p>当配置多个连接器（例如，HTTP 和 HTTPS）时，共享 HTTP 通用参数的配置是可取的。为了达到这个目的，你需要明确配置 ConnectionFactory 实例和 ServerConnector 类，并且跟 HTTP 通用配置一起提供。</p>\n<p><a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/ManyConnectors.html\">ManyConnectors 示例</a> 配置了有两个 ServerConnector 实例的服务器：HTTP 连接器有一个 <a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/server/HttpConnectionFactory.html\">HTTPConnectionFactory</a> 实例；HTTPS 连接器有一个链接到 HttpConnectionFactory 的 SslConnectionFactory。两个 HttpConnectionFactories 在同一个 <a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/server/HttpConfiguration.html\">HttpConfiguration</a> 上配置，然而 HTTPS 工厂用一个包装的配置，所以可以添加 <a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/server/SecureRequestCustomizer.html\">SecureRequestCustomizer</a>。</p>\n<h3 id=\"嵌入-Servlets\"><a href=\"#嵌入-Servlets\" class=\"headerlink\" title=\"嵌入 Servlets\"></a>嵌入 Servlets</h3><p><a href=\"https://en.wikipedia.org/wiki/Java_servlet\">Servlets</a> 是提供处理 HTTP 请求应用逻辑的一般方式。Servlets 跟 Jetty 的处理器是相同的，除非请求对象是不可变的，因此不能被修改。在 Jetty 中 Servlets 通过一个 <a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/MinimalServlets.html\">ServletHandler</a> 来处理。它使用标准的路径映射让 Servlet 和请求匹配；设置请求的 servletPath 和路径信息；传送请求到 servlet，可能通过 Filters 生成响应。</p>\n<p><a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/MinimalServlets.html\">MinimalServlets 示例</a> 创建了一个 ServletHandler 实例并且配置了一个 HelloServlet:</p>\n<pre><code>//\n//  ========================================================================\n//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n//  ------------------------------------------------------------------------\n//  All rights reserved. This program and the accompanying materials\n//  are made available under the terms of the Eclipse Public License v1.0\n//  and Apache License v2.0 which accompanies this distribution.\n//\n//      The Eclipse Public License is available at\n//      http://www.eclipse.org/legal/epl-v10.html\n//\n//      The Apache License v2.0 is available at\n//      http://www.opensource.org/licenses/apache2.0.php\n//\n//  You may elect to redistribute this code under either of these licenses.\n//  ========================================================================\n//\n\npackage org.eclipse.jetty.embedded;\n\nimport java.io.IOException;\n\nimport javax.servlet.ServletException;\nimport javax.servlet.http.HttpServlet;\nimport javax.servlet.http.HttpServletRequest;\nimport javax.servlet.http.HttpServletResponse;\n\nimport org.eclipse.jetty.server.Server;\nimport org.eclipse.jetty.servlet.ServletHandler;\n\npublic class MinimalServlets\n&#123;\n    public static void main( String[] args ) throws Exception\n    &#123;\n        // Create a basic jetty server object that will listen on port 8080.\n        // Note that if you set this to port 0 then a randomly available port\n        // will be assigned that you can either look in the logs for the port,\n        // or programmatically obtain it for use in test cases.\n        Server server = new Server(8080);\n\n        // The ServletHandler is a dead simple way to create a context handler\n        // that is backed by an instance of a Servlet.\n        // This handler then needs to be registered with the Server object.\n        ServletHandler handler = new ServletHandler();\n        server.setHandler(handler);\n\n        // Passing in the class for the Servlet allows jetty to instantiate an\n        // instance of that Servlet and mount it on a given context path.\n\n        // IMPORTANT:\n        // This is a raw Servlet, not a Servlet that has been configured\n        // through a web.xml @WebServlet annotation, or anything similar.\n        handler.addServletWithMapping(HelloServlet.class, &quot;/*&quot;);\n\n        // Start things up!\n        server.start();\n\n        // The use of server.join() the will make the current thread join and\n        // wait until the server is done executing.\n        // See\n        // http://docs.oracle.com/javase/7/docs/api/java/lang/Thread.html#join()\n        server.join();\n    &#125;\n\n    @SuppressWarnings(&quot;serial&quot;)\n    public static class HelloServlet extends HttpServlet\n    &#123;\n        @Override\n        protected void doGet( HttpServletRequest request,\n                              HttpServletResponse response ) throws ServletException,\n                                                            IOException\n        &#123;\n            response.setContentType(&quot;text/html&quot;);\n            response.setStatus(HttpServletResponse.SC_OK);\n            response.getWriter().println(&quot;&lt;h1&gt;Hello from HelloServlet&lt;/h1&gt;&quot;);\n        &#125;\n    &#125;\n&#125;\n</code></pre>\n<h3 id=\"嵌入-Contexts\"><a href=\"#嵌入-Contexts\" class=\"headerlink\" title=\"嵌入 Contexts\"></a>嵌入 Contexts</h3><p><a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/OneContext.html\">ContextHandler</a> 是一个 ScopedHandler 只来响应那些 URI 前缀匹配已配置的上下文路径的请求。匹配上下文路径的请求会响应的调整它们的路径方法，并且上下文生命周期内是可获取的，可能包含：</p>\n<ul>\n<li>当请求在作用域内处理时，Classloader 设置为线程上下文的类加载器。</li>\n<li>通过过 <a href=\"http://docs.oracle.com/javaee/6/api/javax/servlet/ServletContext.html\">ServletContext</a> API 可以获取属性的集合。</li>\n<li>通过过 <a href=\"http://docs.oracle.com/javaee/6/api/javax/servlet/ServletContext.html\">ServletContext</a> API 可以获取参数的集合。</li>\n<li>通过过 <a href=\"http://docs.oracle.com/javaee/6/api/javax/servlet/ServletContext.html\">ServletContext</a> API 为静态资源的请求提供一个基本的 Resource 作为文档根。</li>\n<li>虚拟主机名称集合。</li>\n</ul>\n<p>下面的 <a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/OneContext.html\">OneContext 示例</a> 建立了一个包装了 <a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/HelloHandler.html\">HelloHandler</a> 的上下文：</p>\n<pre><code>//\n//  ========================================================================\n//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n//  ------------------------------------------------------------------------\n//  All rights reserved. This program and the accompanying materials\n//  are made available under the terms of the Eclipse Public License v1.0\n//  and Apache License v2.0 which accompanies this distribution.\n//\n//      The Eclipse Public License is available at\n//      http://www.eclipse.org/legal/epl-v10.html\n//\n//      The Apache License v2.0 is available at\n//      http://www.opensource.org/licenses/apache2.0.php\n//\n//  You may elect to redistribute this code under either of these licenses.\n//  ========================================================================\n//\n\npackage org.eclipse.jetty.embedded;\n\nimport org.eclipse.jetty.server.Server;\nimport org.eclipse.jetty.server.handler.ContextHandler;\n\npublic class OneContext\n&#123;\n    public static void main( String[] args ) throws Exception\n    &#123;\n        Server server = new Server( 8080 );\n\n        // Add a single handler on context &quot;/hello&quot;\n        ContextHandler context = new ContextHandler();\n        context.setContextPath( &quot;/hello&quot; );\n        context.setHandler( new HelloHandler() );\n\n        // Can be accessed using http://localhost:8080/hello\n\n        server.setHandler( context );\n\n        // Start the server\n        server.start();\n        server.join();\n    &#125;\n&#125;\n</code></pre>\n<p>当有很多上下文出现时，你可以嵌入一个 ContextHandlerCollection 来有效的测试一个请求的 URI，然后选择匹配到的 ContextHandler。<a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/ManyContexts.html\">ManyContexts 示例</a> 展示了可以配置多个上下文：</p>\n<pre><code>//\n//  ========================================================================\n//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n//  ------------------------------------------------------------------------\n//  All rights reserved. This program and the accompanying materials\n//  are made available under the terms of the Eclipse Public License v1.0\n//  and Apache License v2.0 which accompanies this distribution.\n//\n//      The Eclipse Public License is available at\n//      http://www.eclipse.org/legal/epl-v10.html\n//\n//      The Apache License v2.0 is available at\n//      http://www.opensource.org/licenses/apache2.0.php\n//\n//  You may elect to redistribute this code under either of these licenses.\n//  ========================================================================\n//\n\npackage org.eclipse.jetty.embedded;\n\nimport org.eclipse.jetty.server.Handler;\nimport org.eclipse.jetty.server.Server;\nimport org.eclipse.jetty.server.handler.ContextHandler;\nimport org.eclipse.jetty.server.handler.ContextHandlerCollection;\n\npublic class ManyContexts\n&#123;\n    public static void main( String[] args ) throws Exception\n    &#123;\n        Server server = new Server(8080);\n\n        ContextHandler context = new ContextHandler(&quot;/&quot;);\n        context.setContextPath(&quot;/&quot;);\n        context.setHandler(new HelloHandler(&quot;Root Hello&quot;));\n\n        ContextHandler contextFR = new ContextHandler(&quot;/fr&quot;);\n        contextFR.setHandler(new HelloHandler(&quot;Bonjoir&quot;));\n\n        ContextHandler contextIT = new ContextHandler(&quot;/it&quot;);\n        contextIT.setHandler(new HelloHandler(&quot;Bongiorno&quot;));\n\n        ContextHandler contextV = new ContextHandler(&quot;/&quot;);\n        contextV.setVirtualHosts(new String[] &#123; &quot;127.0.0.2&quot; &#125;);\n        contextV.setHandler(new HelloHandler(&quot;Virtual Hello&quot;));\n\n        ContextHandlerCollection contexts = new ContextHandlerCollection();\n        contexts.setHandlers(new Handler[] &#123; context, contextFR, contextIT,\n                contextV &#125;);\n\n        server.setHandler(contexts);\n\n        server.start();\n        server.join();\n    &#125;\n&#125;\n</code></pre>\n<h3 id=\"嵌入-ServletContexts\"><a href=\"#嵌入-ServletContexts\" class=\"headerlink\" title=\"嵌入 ServletContexts\"></a>嵌入 ServletContexts</h3><p><a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/servlet/ServletContextHandler.html\">ServletContextHandler</a> 是一个支持普通 session 和 Servlet 的特定的 ContextHandler。下面的 <a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/OneServletContext.html\">OneServletContext 示例</a> 实例化了一个 <a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/servlet/DefaultServlet.html\">DefaultServlet</a> 从 /tmp/ 提供静态内容，并且实例化一个 DumpServlet 创建 session 并存储请求的基本详细信息：</p>\n<pre><code>//\n//  ========================================================================\n//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n//  ------------------------------------------------------------------------\n//  All rights reserved. This program and the accompanying materials\n//  are made available under the terms of the Eclipse Public License v1.0\n//  and Apache License v2.0 which accompanies this distribution.\n//\n//      The Eclipse Public License is available at\n//      http://www.eclipse.org/legal/epl-v10.html\n//\n//      The Apache License v2.0 is available at\n//      http://www.opensource.org/licenses/apache2.0.php\n//\n//  You may elect to redistribute this code under either of these licenses.\n//  ========================================================================\n//\n\npackage org.eclipse.jetty.embedded;\n\nimport org.eclipse.jetty.server.Server;\nimport org.eclipse.jetty.servlet.DefaultServlet;\nimport org.eclipse.jetty.servlet.ServletContextHandler;\n\npublic class OneServletContext\n&#123;\n    public static void main( String[] args ) throws Exception\n    &#123;\n        Server server = new Server(8080);\n\n        ServletContextHandler context = new ServletContextHandler(\n                ServletContextHandler.SESSIONS);\n        context.setContextPath(&quot;/&quot;);\n        context.setResourceBase(System.getProperty(&quot;java.io.tmpdir&quot;));\n        server.setHandler(context);\n\n        // Add dump servlet\n        context.addServlet(DumpServlet.class, &quot;/dump/*&quot;);\n        // Add default servlet\n        context.addServlet(DefaultServlet.class, &quot;/&quot;);\n\n        server.start();\n        server.join();\n    &#125;\n&#125;\n</code></pre>\n<h3 id=\"嵌入-Web-Applications\"><a href=\"#嵌入-Web-Applications\" class=\"headerlink\" title=\"嵌入 Web Applications\"></a>嵌入 Web Applications</h3><p><a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/webapp/WebAppContext.html\">WebAppContext</a> 是一个 ServletContextHandler 的扩展，它使用<a href=\"https://en.wikipedia.org/wiki/WAR_(file_format)\">标准设计</a>和 web.xml 配置 servlet、filter 和 web.xml 或注解的其他特性。下面的 <a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/OneWebApp.html\">OneWebApp 示例</a> 配置了 Jetty 的测试 web 应用。Web 应用程序可以使用容器提供的的资源，在这个案例中需要并配置一个 LoginService：</p>\n<pre><code>//\n//  ========================================================================\n//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n//  ------------------------------------------------------------------------\n//  All rights reserved. This program and the accompanying materials\n//  are made available under the terms of the Eclipse Public License v1.0\n//  and Apache License v2.0 which accompanies this distribution.\n//\n//      The Eclipse Public License is available at\n//      http://www.eclipse.org/legal/epl-v10.html\n//\n//      The Apache License v2.0 is available at\n//      http://www.opensource.org/licenses/apache2.0.php\n//\n//  You may elect to redistribute this code under either of these licenses.\n//  ========================================================================\n//\n\npackage org.eclipse.jetty.embedded;\n\nimport java.io.File;\nimport java.lang.management.ManagementFactory;\n\nimport org.eclipse.jetty.jmx.MBeanContainer;\nimport org.eclipse.jetty.security.HashLoginService;\nimport org.eclipse.jetty.server.Server;\nimport org.eclipse.jetty.server.handler.AllowSymLinkAliasChecker;\nimport org.eclipse.jetty.webapp.WebAppContext;\n\npublic class OneWebApp\n&#123;\n    public static void main( String[] args ) throws Exception\n    &#123;\n        // Create a basic jetty server object that will listen on port 8080.\n        // Note that if you set this to port 0 then a randomly available port\n        // will be assigned that you can either look in the logs for the port,\n        // or programmatically obtain it for use in test cases.\n        Server server = new Server(8080);\n\n        // Setup JMX\n        MBeanContainer mbContainer = new MBeanContainer(\n                ManagementFactory.getPlatformMBeanServer());\n        server.addBean(mbContainer);\n\n        // The WebAppContext is the entity that controls the environment in\n        // which a web application lives and breathes. In this example the\n        // context path is being set to &quot;/&quot; so it is suitable for serving root\n        // context requests and then we see it setting the location of the war.\n        // A whole host of other configurations are available, ranging from\n        // configuring to support annotation scanning in the webapp (through\n        // PlusConfiguration) to choosing where the webapp will unpack itself.\n        WebAppContext webapp = new WebAppContext();\n        webapp.setContextPath(&quot;/&quot;);\n        File warFile = new File(\n                &quot;../../tests/test-jmx/jmx-webapp/target/jmx-webapp&quot;);\n        webapp.setWar(warFile.getAbsolutePath());\n\n        // A WebAppContext is a ContextHandler as well so it needs to be set to\n        // the server so it is aware of where to send the appropriate requests.\n        server.setHandler(webapp);\n\n        // Start things up!\n        server.start();\n\n        // The use of server.join() the will make the current thread join and\n        // wait until the server is done executing.\n        // See http://docs.oracle.com/javase/7/docs/api/java/lang/Thread.html#join()\n        server.join();\n    &#125;\n&#125;\n</code></pre>\n<h3 id=\"喜欢-Jetty-XML\"><a href=\"#喜欢-Jetty-XML\" class=\"headerlink\" title=\"喜欢 Jetty XML\"></a>喜欢 Jetty XML</h3><p>配置 Jetty 服务器实例的典型方式是通过 jetty.xml 关联配置文件。然而 Jetty XML 配置格式只是在代码中可以做的简单的表达；也可以很简单的写嵌入代码准确实现 jetty.xml 配置。下面的 <a href=\"http://download.eclipse.org/jetty/stable-9/xref/org/eclipse/jetty/embedded/LikeJettyXml.html\">LikeJettyXml 示例</a> 在代码中呈现了从配置文件获取的行为：</p>\n<ul>\n<li>jetty.xml</li>\n<li>jetty-jmx.xml</li>\n<li>jetty-http.xml</li>\n<li>jetty-https.xml</li>\n<li>jetty-deploy.xml</li>\n<li>jetty-stats.xml</li>\n<li>jetty-requestlog.xml</li>\n<li>jetty-lowresources.xml</li>\n<li>test-realm.xml</li>\n</ul>\n<pre><code>//\n//  ========================================================================\n//  Copyright (c) 1995-2016 Mort Bay Consulting Pty. Ltd.\n//  ------------------------------------------------------------------------\n//  All rights reserved. This program and the accompanying materials\n//  are made available under the terms of the Eclipse Public License v1.0\n//  and Apache License v2.0 which accompanies this distribution.\n//\n//      The Eclipse Public License is available at\n//      http://www.eclipse.org/legal/epl-v10.html\n//\n//      The Apache License v2.0 is available at\n//      http://www.opensource.org/licenses/apache2.0.php\n//\n//  You may elect to redistribute this code under either of these licenses.\n//  ========================================================================\n//\n\npackage org.eclipse.jetty.embedded;\n\nimport java.io.File;\nimport java.io.FileNotFoundException;\nimport java.lang.management.ManagementFactory;\n\nimport org.eclipse.jetty.deploy.DeploymentManager;\nimport org.eclipse.jetty.deploy.PropertiesConfigurationManager;\nimport org.eclipse.jetty.deploy.bindings.DebugListenerBinding;\nimport org.eclipse.jetty.deploy.providers.WebAppProvider;\nimport org.eclipse.jetty.http.HttpVersion;\nimport org.eclipse.jetty.jmx.MBeanContainer;\nimport org.eclipse.jetty.security.HashLoginService;\nimport org.eclipse.jetty.server.DebugListener;\nimport org.eclipse.jetty.server.Handler;\nimport org.eclipse.jetty.server.HttpConfiguration;\nimport org.eclipse.jetty.server.HttpConnectionFactory;\nimport org.eclipse.jetty.server.LowResourceMonitor;\nimport org.eclipse.jetty.server.NCSARequestLog;\nimport org.eclipse.jetty.server.SecureRequestCustomizer;\nimport org.eclipse.jetty.server.Server;\nimport org.eclipse.jetty.server.ServerConnector;\nimport org.eclipse.jetty.server.SslConnectionFactory;\nimport org.eclipse.jetty.server.handler.ContextHandlerCollection;\nimport org.eclipse.jetty.server.handler.DefaultHandler;\nimport org.eclipse.jetty.server.handler.HandlerCollection;\nimport org.eclipse.jetty.server.handler.RequestLogHandler;\nimport org.eclipse.jetty.server.handler.StatisticsHandler;\nimport org.eclipse.jetty.util.ssl.SslContextFactory;\nimport org.eclipse.jetty.util.thread.QueuedThreadPool;\nimport org.eclipse.jetty.util.thread.ScheduledExecutorScheduler;\nimport org.eclipse.jetty.webapp.Configuration;\n\n/**\n * Starts the Jetty Distribution&#39;s demo-base directory using entirely\n * embedded jetty techniques.\n */\npublic class LikeJettyXml\n&#123;\n    public static void main( String[] args ) throws Exception\n    &#123;\n        // Path to as-built jetty-distribution directory\n        String jettyHomeBuild = &quot;../../jetty-distribution/target/distribution&quot;;\n\n        // Find jetty home and base directories\n        String homePath = System.getProperty(&quot;jetty.home&quot;, jettyHomeBuild);\n        File homeDir = new File(homePath);\n        if (!homeDir.exists())\n        &#123;\n            throw new FileNotFoundException(homeDir.getAbsolutePath());\n        &#125;\n        String basePath = System.getProperty(&quot;jetty.base&quot;, homeDir + &quot;/demo-base&quot;);\n        File baseDir = new File(basePath);\n        if(!baseDir.exists())\n        &#123;\n            throw new FileNotFoundException(baseDir.getAbsolutePath());\n        &#125;\n\n        // Configure jetty.home and jetty.base system properties\n        String jetty_home = homeDir.getAbsolutePath();\n        String jetty_base = baseDir.getAbsolutePath();\n        System.setProperty(&quot;jetty.home&quot;, jetty_home);\n        System.setProperty(&quot;jetty.base&quot;, jetty_base);\n\n\n        // === jetty.xml ===\n        // Setup Threadpool\n        QueuedThreadPool threadPool = new QueuedThreadPool();\n        threadPool.setMaxThreads(500);\n\n        // Server\n        Server server = new Server(threadPool);\n\n        // Scheduler\n        server.addBean(new ScheduledExecutorScheduler());\n\n        // HTTP Configuration\n        HttpConfiguration http_config = new HttpConfiguration();\n        http_config.setSecureScheme(&quot;https&quot;);\n        http_config.setSecurePort(8443);\n        http_config.setOutputBufferSize(32768);\n        http_config.setRequestHeaderSize(8192);\n        http_config.setResponseHeaderSize(8192);\n        http_config.setSendServerVersion(true);\n        http_config.setSendDateHeader(false);\n        // httpConfig.addCustomizer(new ForwardedRequestCustomizer());\n\n        // Handler Structure\n        HandlerCollection handlers = new HandlerCollection();\n        ContextHandlerCollection contexts = new ContextHandlerCollection();\n        handlers.setHandlers(new Handler[] &#123; contexts, new DefaultHandler() &#125;);\n        server.setHandler(handlers);\n\n        // Extra options\n        server.setDumpAfterStart(false);\n        server.setDumpBeforeStop(false);\n        server.setStopAtShutdown(true);\n\n        // === jetty-jmx.xml ===\n        MBeanContainer mbContainer = new MBeanContainer(\n                ManagementFactory.getPlatformMBeanServer());\n        server.addBean(mbContainer);\n\n\n        // === jetty-http.xml ===\n        ServerConnector http = new ServerConnector(server,\n                new HttpConnectionFactory(http_config));\n        http.setPort(8080);\n        http.setIdleTimeout(30000);\n        server.addConnector(http);\n\n\n        // === jetty-https.xml ===\n        // SSL Context Factory\n        SslContextFactory sslContextFactory = new SslContextFactory();\n        sslContextFactory.setKeyStorePath(jetty_home + &quot;/../../../jetty-server/src/test/config/etc/keystore&quot;);\n        sslContextFactory.setKeyStorePassword(&quot;OBF:1vny1zlo1x8e1vnw1vn61x8g1zlu1vn4&quot;);\n        sslContextFactory.setKeyManagerPassword(&quot;OBF:1u2u1wml1z7s1z7a1wnl1u2g&quot;);\n        sslContextFactory.setTrustStorePath(jetty_home + &quot;/../../../jetty-server/src/test/config/etc/keystore&quot;);\n        sslContextFactory.setTrustStorePassword(&quot;OBF:1vny1zlo1x8e1vnw1vn61x8g1zlu1vn4&quot;);\n        sslContextFactory.setExcludeCipherSuites(&quot;SSL_RSA_WITH_DES_CBC_SHA&quot;,\n                &quot;SSL_DHE_RSA_WITH_DES_CBC_SHA&quot;, &quot;SSL_DHE_DSS_WITH_DES_CBC_SHA&quot;,\n                &quot;SSL_RSA_EXPORT_WITH_RC4_40_MD5&quot;,\n                &quot;SSL_RSA_EXPORT_WITH_DES40_CBC_SHA&quot;,\n                &quot;SSL_DHE_RSA_EXPORT_WITH_DES40_CBC_SHA&quot;,\n                &quot;SSL_DHE_DSS_EXPORT_WITH_DES40_CBC_SHA&quot;);\n\n        // SSL HTTP Configuration\n        HttpConfiguration https_config = new HttpConfiguration(http_config);\n        https_config.addCustomizer(new SecureRequestCustomizer());\n\n        // SSL Connector\n        ServerConnector sslConnector = new ServerConnector(server,\n            new SslConnectionFactory(sslContextFactory,HttpVersion.HTTP_1_1.asString()),\n            new HttpConnectionFactory(https_config));\n        sslConnector.setPort(8443);\n        server.addConnector(sslConnector);\n\n\n        // === jetty-deploy.xml ===\n        DeploymentManager deployer = new DeploymentManager();\n        DebugListener debug = new DebugListener(System.out,true,true,true);\n        server.addBean(debug);        \n        deployer.addLifeCycleBinding(new DebugListenerBinding(debug));\n        deployer.setContexts(contexts);\n        deployer.setContextAttribute(\n                &quot;org.eclipse.jetty.server.webapp.ContainerIncludeJarPattern&quot;,\n                &quot;.*/servlet-api-[^/]*\\\\.jar$&quot;);\n\n        WebAppProvider webapp_provider = new WebAppProvider();\n        webapp_provider.setMonitoredDirName(jetty_base + &quot;/webapps&quot;);\n        webapp_provider.setDefaultsDescriptor(jetty_home + &quot;/etc/webdefault.xml&quot;);\n        webapp_provider.setScanInterval(1);\n        webapp_provider.setExtractWars(true);\n        webapp_provider.setConfigurationManager(new PropertiesConfigurationManager());\n\n        deployer.addAppProvider(webapp_provider);\n        server.addBean(deployer);\n\n        // === setup jetty plus ==\n        Configuration.ClassList.setServerDefault(server).addAfter(\n                &quot;org.eclipse.jetty.webapp.FragmentConfiguration&quot;,\n                &quot;org.eclipse.jetty.plus.webapp.EnvConfiguration&quot;,\n                &quot;org.eclipse.jetty.plus.webapp.PlusConfiguration&quot;);\n\n        // === jetty-stats.xml ===\n        StatisticsHandler stats = new StatisticsHandler();\n        stats.setHandler(server.getHandler());\n        server.setHandler(stats);\n\n\n        // === jetty-requestlog.xml ===\n        NCSARequestLog requestLog = new NCSARequestLog();\n        requestLog.setFilename(jetty_home + &quot;/logs/yyyy_mm_dd.request.log&quot;);\n        requestLog.setFilenameDateFormat(&quot;yyyy_MM_dd&quot;);\n        requestLog.setRetainDays(90);\n        requestLog.setAppend(true);\n        requestLog.setExtended(true);\n        requestLog.setLogCookies(false);\n        requestLog.setLogTimeZone(&quot;GMT&quot;);\n        RequestLogHandler requestLogHandler = new RequestLogHandler();\n        requestLogHandler.setRequestLog(requestLog);\n        handlers.addHandler(requestLogHandler);\n\n\n        // === jetty-lowresources.xml ===\n        LowResourceMonitor lowResourcesMonitor=new LowResourceMonitor(server);\n        lowResourcesMonitor.setPeriod(1000);\n        lowResourcesMonitor.setLowResourcesIdleTimeout(200);\n        lowResourcesMonitor.setMonitorThreads(true);\n        lowResourcesMonitor.setMaxConnections(0);\n        lowResourcesMonitor.setMaxMemory(0);\n        lowResourcesMonitor.setMaxLowResourcesTime(5000);\n        server.addBean(lowResourcesMonitor);\n\n\n        // === test-realm.xml ===\n        HashLoginService login = new HashLoginService();\n        login.setName(&quot;Test Realm&quot;);\n        login.setConfig(jetty_base + &quot;/etc/realm.properties&quot;);\n        login.setHotReload(false);\n        server.addBean(login);\n\n        // Start the server\n        server.start();\n        server.join();\n    &#125;\n&#125;\n</code></pre>"}],"PostAsset":[],"PostCategory":[{"post_id":"ckrdmphku0000itd3cgmh1jp7","category_id":"ckrdmphl10002itd3g51r6c2s","_id":"ckrdmphlk000qitd3frh33hhr"},{"post_id":"ckrdmphku0000itd3cgmh1jp7","category_id":"ckrdmphld000hitd38twhb5sw","_id":"ckrdmphlm000vitd3661r3ws1"},{"post_id":"ckrdmphkz0001itd37n6yah8m","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphlq0014itd34t7ibkgl"},{"post_id":"ckrdmphkz0001itd37n6yah8m","category_id":"ckrdmphlk000sitd38ca2bjm8","_id":"ckrdmphls0019itd3eryn9q4m"},{"post_id":"ckrdmphl40005itd3cqqwe1j7","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphlx001fitd31bzm6td2"},{"post_id":"ckrdmphl40005itd3cqqwe1j7","category_id":"ckrdmphlq0016itd392hc3jb3","_id":"ckrdmphlz001jitd3dxpngvya"},{"post_id":"ckrdmphle000jitd3e70o6b63","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphm4001sitd34fwd0850"},{"post_id":"ckrdmphle000jitd3e70o6b63","category_id":"ckrdmphlq0016itd392hc3jb3","_id":"ckrdmphm5001witd35qq4ecl3"},{"post_id":"ckrdmphl60009itd34zezgytj","category_id":"ckrdmphlh000litd30oyhfkeb","_id":"ckrdmphmf002hitd34lp39rpo"},{"post_id":"ckrdmphl60009itd34zezgytj","category_id":"ckrdmphm90024itd38jug783d","_id":"ckrdmphmg002litd39bx5207k"},{"post_id":"ckrdmphl7000aitd30qxeg0e7","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphms003aitd36a25hp37"},{"post_id":"ckrdmphl7000aitd30qxeg0e7","category_id":"ckrdmphmn002zitd3dmqvawqg","_id":"ckrdmphmt003citd38fxch18i"},{"post_id":"ckrdmphls001aitd3gisfgpkx","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphn2003sitd3a2nwh88q"},{"post_id":"ckrdmphls001aitd3gisfgpkx","category_id":"ckrdmphmv003fitd37eq59bod","_id":"ckrdmphn3003witd37qfwagmb"},{"post_id":"ckrdmphlv001ditd37gji5252","category_id":"ckrdmphlh000litd30oyhfkeb","_id":"ckrdmphn40040itd3c2ax7ehz"},{"post_id":"ckrdmphlv001ditd37gji5252","category_id":"ckrdmphmz003nitd382nfb1c0","_id":"ckrdmphn60044itd337nnga3s"},{"post_id":"ckrdmphlc000fitd3dfus9zq1","category_id":"ckrdmphlv001citd32wx0c5wu","_id":"ckrdmphn70049itd35bm9227o"},{"post_id":"ckrdmphlc000fitd3dfus9zq1","category_id":"ckrdmphn2003uitd30zcn5mx2","_id":"ckrdmphn8004citd3dvkod3t2"},{"post_id":"ckrdmphlw001eitd3d7flb8as","category_id":"ckrdmphlh000litd30oyhfkeb","_id":"ckrdmphna004gitd3akjt9lar"},{"post_id":"ckrdmphlw001eitd3d7flb8as","category_id":"ckrdmphmz003nitd382nfb1c0","_id":"ckrdmphnb004kitd347pa6eqq"},{"post_id":"ckrdmphly001iitd312n48w93","category_id":"ckrdmphlh000litd30oyhfkeb","_id":"ckrdmphnd004oitd3b9bhbyws"},{"post_id":"ckrdmphly001iitd312n48w93","category_id":"ckrdmphmz003nitd382nfb1c0","_id":"ckrdmphnf004sitd3axzofluq"},{"post_id":"ckrdmphlz001kitd38c4re4d8","category_id":"ckrdmphlh000litd30oyhfkeb","_id":"ckrdmphng004witd3fg1h1ifh"},{"post_id":"ckrdmphlz001kitd38c4re4d8","category_id":"ckrdmphmz003nitd382nfb1c0","_id":"ckrdmphni004zitd3e4p63i45"},{"post_id":"ckrdmphm1001nitd3hd8qfvzd","category_id":"ckrdmphlh000litd30oyhfkeb","_id":"ckrdmphnk0054itd3apzv5mkf"},{"post_id":"ckrdmphm1001nitd3hd8qfvzd","category_id":"ckrdmphmz003nitd382nfb1c0","_id":"ckrdmphnl0057itd3d9i87xsf"},{"post_id":"ckrdmphnf004titd32cqj89lz","category_id":"ckrdmphl10002itd3g51r6c2s","_id":"ckrdmphnm005bitd3fkex286q"},{"post_id":"ckrdmphnf004titd32cqj89lz","category_id":"ckrdmphld000hitd38twhb5sw","_id":"ckrdmphno005fitd3a3kv217c"},{"post_id":"ckrdmphlf000kitd35pld4lta","category_id":"ckrdmphlv001citd32wx0c5wu","_id":"ckrdmphnq005jitd33e28fusn"},{"post_id":"ckrdmphlf000kitd35pld4lta","category_id":"ckrdmphn2003uitd30zcn5mx2","_id":"ckrdmphns005nitd376bl4pm1"},{"post_id":"ckrdmphm2001pitd361sch2at","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphnt005ritd39hzsd385"},{"post_id":"ckrdmphm2001pitd361sch2at","category_id":"ckrdmphnj0053itd327habqx0","_id":"ckrdmphnv005vitd3fpa8dhhn"},{"post_id":"ckrdmphlh000nitd3a7eof7cj","category_id":"ckrdmphlv001citd32wx0c5wu","_id":"ckrdmphnx005zitd3hscnahef"},{"post_id":"ckrdmphlh000nitd3a7eof7cj","category_id":"ckrdmphn2003uitd30zcn5mx2","_id":"ckrdmphnz0063itd31r0z9eqp"},{"post_id":"ckrdmphm5001xitd3ex3c3irx","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmpho10069itd30be6ds1i"},{"post_id":"ckrdmphm5001xitd3ex3c3irx","category_id":"ckrdmphnt005sitd3f93f81tu","_id":"ckrdmpho2006bitd3gikm8d97"},{"post_id":"ckrdmphm70020itd365zy6lvt","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmpho4006gitd39zeg45c1"},{"post_id":"ckrdmphm70020itd365zy6lvt","category_id":"ckrdmphnt005sitd3f93f81tu","_id":"ckrdmpho5006jitd32jxjd67w"},{"post_id":"ckrdmphnz0064itd3cbcl80j3","category_id":"ckrdmphlv001citd32wx0c5wu","_id":"ckrdmpho7006oitd39n8n6rf5"},{"post_id":"ckrdmphnz0064itd3cbcl80j3","category_id":"ckrdmphn2003uitd30zcn5mx2","_id":"ckrdmpho8006ritd335bpdgox"},{"post_id":"ckrdmphlj000pitd3d9jy00lz","category_id":"ckrdmphlv001citd32wx0c5wu","_id":"ckrdmphoa006witd36may20p5"},{"post_id":"ckrdmphlj000pitd3d9jy00lz","category_id":"ckrdmphn2003uitd30zcn5mx2","_id":"ckrdmphob006zitd3ayfjemq6"},{"post_id":"ckrdmpho2006aitd3b3uzgwwi","category_id":"ckrdmphlh000litd30oyhfkeb","_id":"ckrdmphoc0074itd38np3e9if"},{"post_id":"ckrdmpho2006aitd3b3uzgwwi","category_id":"ckrdmphmz003nitd382nfb1c0","_id":"ckrdmphoe0076itd36x2mdaka"},{"post_id":"ckrdmpho3006eitd36u51b1gi","category_id":"ckrdmphlv001citd32wx0c5wu","_id":"ckrdmphof0079itd3be28aht3"},{"post_id":"ckrdmpho3006eitd36u51b1gi","category_id":"ckrdmphn2003uitd30zcn5mx2","_id":"ckrdmphog007citd3dilmg9z6"},{"post_id":"ckrdmphm80023itd3471t8tsz","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphoh007fitd38x4wayd3"},{"post_id":"ckrdmphm80023itd3471t8tsz","category_id":"ckrdmphnt005sitd3f93f81tu","_id":"ckrdmphoj007iitd3efjc0ris"},{"post_id":"ckrdmphm90026itd3b8xa6ypq","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphok007litd3ghk23chc"},{"post_id":"ckrdmphm90026itd3b8xa6ypq","category_id":"ckrdmphnt005sitd3f93f81tu","_id":"ckrdmphom007oitd37b2m1vg7"},{"post_id":"ckrdmpho9006uitd3h9rw247k","category_id":"ckrdmphlh000litd30oyhfkeb","_id":"ckrdmphoq007sitd3dkmw2qyw"},{"post_id":"ckrdmpho9006uitd3h9rw247k","category_id":"ckrdmphmz003nitd382nfb1c0","_id":"ckrdmphor007witd393xgfwi6"},{"post_id":"ckrdmphma0028itd3ek65drxp","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphos0080itd30rqb63r3"},{"post_id":"ckrdmphma0028itd3ek65drxp","category_id":"ckrdmphnt005sitd3f93f81tu","_id":"ckrdmphot0083itd3evflf24x"},{"post_id":"ckrdmphmb0029itd3d8ib3rmp","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphov0087itd3hjkcdanp"},{"post_id":"ckrdmphmb0029itd3d8ib3rmp","category_id":"ckrdmphnt005sitd3f93f81tu","_id":"ckrdmphow008bitd3asib9l7n"},{"post_id":"ckrdmphmd002ditd39qf095sg","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphox008fitd352zy4jer"},{"post_id":"ckrdmphmd002ditd39qf095sg","category_id":"ckrdmphnt005sitd3f93f81tu","_id":"ckrdmphoy008jitd34whfdh23"},{"post_id":"ckrdmphll000titd38wzg2ind","category_id":"ckrdmphlv001citd32wx0c5wu","_id":"ckrdmphp0008nitd39wx7570m"},{"post_id":"ckrdmphll000titd38wzg2ind","category_id":"ckrdmphn2003uitd30zcn5mx2","_id":"ckrdmphp1008ritd30joi8xwt"},{"post_id":"ckrdmphme002fitd30pfzb3m1","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphp2008vitd32ejq686w"},{"post_id":"ckrdmphme002fitd30pfzb3m1","category_id":"ckrdmphnt005sitd3f93f81tu","_id":"ckrdmphp4008zitd3az3na90z"},{"post_id":"ckrdmphmf002kitd32jhrg8hd","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphp50093itd39egb6stj"},{"post_id":"ckrdmphmf002kitd32jhrg8hd","category_id":"ckrdmphnt005sitd3f93f81tu","_id":"ckrdmphp70097itd39ebr0x7t"},{"post_id":"ckrdmphot0082itd39xw9ddg3","category_id":"ckrdmphme002gitd3c07whctp","_id":"ckrdmphp8009bitd3b4rfdyqo"},{"post_id":"ckrdmphot0082itd39xw9ddg3","category_id":"ckrdmphos007zitd3av7c4e8d","_id":"ckrdmphp9009fitd3bdu4beeo"},{"post_id":"ckrdmphlm000witd33buvddqm","category_id":"ckrdmphme002gitd3c07whctp","_id":"ckrdmphpb009jitd35jzt46xa"},{"post_id":"ckrdmphlm000witd33buvddqm","category_id":"ckrdmphos007zitd3av7c4e8d","_id":"ckrdmphpc009nitd33xwq461x"},{"post_id":"ckrdmphmg002nitd30f3a2hbd","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphpe009ritd36f580pwo"},{"post_id":"ckrdmphmg002nitd30f3a2hbd","category_id":"ckrdmphnt005sitd3f93f81tu","_id":"ckrdmphpg009vitd31ztgh9qf"},{"post_id":"ckrdmphoy008iitd35d2fbgj9","category_id":"ckrdmphlh000litd30oyhfkeb","_id":"ckrdmphpk009zitd3b5m03tno"},{"post_id":"ckrdmphoy008iitd35d2fbgj9","category_id":"ckrdmphm90024itd38jug783d","_id":"ckrdmphpm00a3itd34wqt2aoz"},{"post_id":"ckrdmphmi002qitd34obx8llc","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphpn00a7itd3aksla0qm"},{"post_id":"ckrdmphmi002qitd34obx8llc","category_id":"ckrdmphnt005sitd3f93f81tu","_id":"ckrdmphpp00abitd3bawg24n5"},{"post_id":"ckrdmphp0008qitd3455i3p9h","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphpt00afitd3ag2y3vi8"},{"post_id":"ckrdmphp0008qitd3455i3p9h","category_id":"ckrdmphmn002zitd3dmqvawqg","_id":"ckrdmphpu00ajitd3eq0z34fv"},{"post_id":"ckrdmphln000yitd3gi9ydez7","category_id":"ckrdmphmh002pitd32f3dbd90","_id":"ckrdmphpw00anitd346s8h0jd"},{"post_id":"ckrdmphln000yitd3gi9ydez7","category_id":"ckrdmphp0008oitd30ofe3icl","_id":"ckrdmphpx00aritd3d67z9yfb"},{"post_id":"ckrdmphp2008uitd3hxcd7o64","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphpz00avitd38kf9e2h2"},{"post_id":"ckrdmphp2008uitd3hxcd7o64","category_id":"ckrdmphmn002zitd3dmqvawqg","_id":"ckrdmphq000azitd3hvz0fiof"},{"post_id":"ckrdmphmj002ritd3crvp62qa","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphq200b3itd367cgfoke"},{"post_id":"ckrdmphmj002ritd3crvp62qa","category_id":"ckrdmphnt005sitd3f93f81tu","_id":"ckrdmphq400b6itd3eqw2a5j1"},{"post_id":"ckrdmphp50092itd3gxuahxu5","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphq500bbitd3hl662c95"},{"post_id":"ckrdmphp50092itd3gxuahxu5","category_id":"ckrdmphmn002zitd3dmqvawqg","_id":"ckrdmphq800beitd31z5h5qna"},{"post_id":"ckrdmphp60096itd3flyu178b","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphqa00bjitd3e0cj97ac"},{"post_id":"ckrdmphp60096itd3flyu178b","category_id":"ckrdmphmn002zitd3dmqvawqg","_id":"ckrdmphqb00bmitd332fa4vf4"},{"post_id":"ckrdmphml002vitd3e6nc5615","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphqd00britd30jp37sjf"},{"post_id":"ckrdmphml002vitd3e6nc5615","category_id":"ckrdmphnt005sitd3f93f81tu","_id":"ckrdmphqe00buitd30h8x13q5"},{"post_id":"ckrdmphp7009aitd30zi77fga","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphqg00bzitd347jr3srp"},{"post_id":"ckrdmphp7009aitd30zi77fga","category_id":"ckrdmphmn002zitd3dmqvawqg","_id":"ckrdmphqh00c2itd3coy41tkz"},{"post_id":"ckrdmphp9009eitd33wle7qim","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphqj00c7itd30m34e0g3"},{"post_id":"ckrdmphp9009eitd33wle7qim","category_id":"ckrdmphnt005sitd3f93f81tu","_id":"ckrdmphqk00caitd36xhe9nh4"},{"post_id":"ckrdmphlo0011itd3ead94i46","category_id":"ckrdmphme002gitd3c07whctp","_id":"ckrdmphql00cfitd300nf7i9u"},{"post_id":"ckrdmphlo0011itd3ead94i46","category_id":"ckrdmphp8009ditd34kim7a8b","_id":"ckrdmphqn00ciitd3gblmcl78"},{"post_id":"ckrdmphmm002xitd34dt51kmg","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphqo00cnitd3d7kbbl81"},{"post_id":"ckrdmphmm002xitd34dt51kmg","category_id":"ckrdmphnt005sitd3f93f81tu","_id":"ckrdmphqp00cqitd35ysu1deg"},{"post_id":"ckrdmphmn0030itd3h1hb350l","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphqr00cvitd3f95b1c8r"},{"post_id":"ckrdmphmn0030itd3h1hb350l","category_id":"ckrdmphnt005sitd3f93f81tu","_id":"ckrdmphqs00cyitd321w2ed0o"},{"post_id":"ckrdmphpj009yitd32yfpg5kv","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphqu00d3itd3gtvm7qky"},{"post_id":"ckrdmphpj009yitd32yfpg5kv","category_id":"ckrdmphlq0016itd392hc3jb3","_id":"ckrdmphqv00d6itd36ku8eqop"},{"post_id":"ckrdmphmo0031itd3auueea6j","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphqw00dbitd3gx76bz68"},{"post_id":"ckrdmphmo0031itd3auueea6j","category_id":"ckrdmphnt005sitd3f93f81tu","_id":"ckrdmphqx00deitd30wnodt3c"},{"post_id":"ckrdmphmp0035itd3ahdk2dmf","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphqz00djitd35rina136"},{"post_id":"ckrdmphmp0035itd3ahdk2dmf","category_id":"ckrdmphnt005sitd3f93f81tu","_id":"ckrdmphr000dmitd35e0n9ogb"},{"post_id":"ckrdmphlp0013itd3cg1u86ky","category_id":"ckrdmphlv001citd32wx0c5wu","_id":"ckrdmphr200dritd3ddxoelln"},{"post_id":"ckrdmphlp0013itd3cg1u86ky","category_id":"ckrdmphn2003uitd30zcn5mx2","_id":"ckrdmphr300duitd3264kgeg0"},{"post_id":"ckrdmphmr0037itd30sol64z3","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphr400dzitd38j3n1bpv"},{"post_id":"ckrdmphmr0037itd30sol64z3","category_id":"ckrdmphpw00apitd3abusfzfz","_id":"ckrdmphr500e2itd3dohs91vf"},{"post_id":"ckrdmphpy00auitd31m93df49","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphr700e7itd33u5n1y1w"},{"post_id":"ckrdmphpy00auitd31m93df49","category_id":"ckrdmphmv003fitd37eq59bod","_id":"ckrdmphr800eaitd38uizf8nw"},{"post_id":"ckrdmphq200b2itd32go5eqzy","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphrb00efitd3ctcu7hf5"},{"post_id":"ckrdmphq200b2itd32go5eqzy","category_id":"ckrdmphmv003fitd37eq59bod","_id":"ckrdmphrc00eiitd3c56tejaq"},{"post_id":"ckrdmphms003bitd3fdqv4ufv","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphre00enitd3hjbuesgm"},{"post_id":"ckrdmphms003bitd3fdqv4ufv","category_id":"ckrdmphq000ayitd3fj842bd3","_id":"ckrdmphrf00eqitd38ful9kp2"},{"post_id":"ckrdmphlr0018itd37rp35qhw","category_id":"ckrdmphlv001citd32wx0c5wu","_id":"ckrdmphrg00evitd33of915x7"},{"post_id":"ckrdmphlr0018itd37rp35qhw","category_id":"ckrdmphn2003uitd30zcn5mx2","_id":"ckrdmphrh00eyitd3dwkegnep"},{"post_id":"ckrdmphq600bditd37w9ac3v5","category_id":"ckrdmphme002gitd3c07whctp","_id":"ckrdmphrj00f3itd3g85tfra1"},{"post_id":"ckrdmphq600bditd37w9ac3v5","category_id":"ckrdmphp8009ditd34kim7a8b","_id":"ckrdmphrk00f6itd3d8peea6u"},{"post_id":"ckrdmphq900biitd30zvw26fx","category_id":"ckrdmphlh000litd30oyhfkeb","_id":"ckrdmphrl00faitd3epk42t15"},{"post_id":"ckrdmphq900biitd30zvw26fx","category_id":"ckrdmphmz003nitd382nfb1c0","_id":"ckrdmphrn00feitd3d87e7xv9"},{"post_id":"ckrdmphmu003ditd315tm3mml","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphro00fiitd362byc2li"},{"post_id":"ckrdmphmu003ditd315tm3mml","category_id":"ckrdmphq000ayitd3fj842bd3","_id":"ckrdmphrq00fmitd3gn694e8i"},{"post_id":"ckrdmphqa00blitd3fuvr0e56","category_id":"ckrdmphmh002pitd32f3dbd90","_id":"ckrdmphrr00fqitd3bbln2w21"},{"post_id":"ckrdmphqa00blitd3fuvr0e56","category_id":"ckrdmphp0008oitd30ofe3icl","_id":"ckrdmphrt00fuitd3ef034kda"},{"post_id":"ckrdmphmv003hitd395tecb6h","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphru00fyitd3ekxigmxt"},{"post_id":"ckrdmphmv003hitd395tecb6h","category_id":"ckrdmphq000ayitd3fj842bd3","_id":"ckrdmphrv00g2itd3bpr0de6s"},{"post_id":"ckrdmphqd00btitd3fyb804n6","category_id":"ckrdmphme002gitd3c07whctp","_id":"ckrdmphrx00g6itd395lb3usr"},{"post_id":"ckrdmphqd00btitd3fyb804n6","category_id":"ckrdmphos007zitd3av7c4e8d","_id":"ckrdmphrz00gaitd3g4k3hdfn"},{"post_id":"ckrdmphmw003jitd31vi66god","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphs000geitd31ve0d4tz"},{"post_id":"ckrdmphmw003jitd31vi66god","category_id":"ckrdmphq000ayitd3fj842bd3","_id":"ckrdmphs300giitd33evhgr0y"},{"post_id":"ckrdmphqi00c6itd3dtskb4pb","category_id":"ckrdmphlv001citd32wx0c5wu","_id":"ckrdmphs500gmitd3170z9e3j"},{"post_id":"ckrdmphqi00c6itd3dtskb4pb","category_id":"ckrdmphn2003uitd30zcn5mx2","_id":"ckrdmphsa00gqitd386r23vwt"},{"post_id":"ckrdmphmy003mitd3b1yx4ylr","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphsb00guitd34qs44ikg"},{"post_id":"ckrdmphmy003mitd3b1yx4ylr","category_id":"ckrdmphq000ayitd3fj842bd3","_id":"ckrdmphsg00gyitd36vl8cdk2"},{"post_id":"ckrdmphmz003pitd3gz803y3s","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphsh00h1itd3ajf54c98"},{"post_id":"ckrdmphmz003pitd3gz803y3s","category_id":"ckrdmphq000ayitd3fj842bd3","_id":"ckrdmphsi00h6itd350ab2xew"},{"post_id":"ckrdmphqo00cmitd3bcfxbdjr","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphsj00h9itd325750eig"},{"post_id":"ckrdmphqo00cmitd3bcfxbdjr","category_id":"ckrdmphnt005sitd3f93f81tu","_id":"ckrdmphsl00hditd3hkv416s2"},{"post_id":"ckrdmphn1003ritd3bpc37519","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphsm00hhitd3b17gbmyf"},{"post_id":"ckrdmphn1003ritd3bpc37519","category_id":"ckrdmphq000ayitd3fj842bd3","_id":"ckrdmphsn00hlitd3c31xealz"},{"post_id":"ckrdmphqp00cpitd349po09w5","category_id":"ckrdmphlh000litd30oyhfkeb","_id":"ckrdmphso00hpitd34r9fa6zi"},{"post_id":"ckrdmphqp00cpitd349po09w5","category_id":"ckrdmphmz003nitd382nfb1c0","_id":"ckrdmphsq00htitd3h83d2uf8"},{"post_id":"ckrdmphqq00cuitd395x22jl5","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphsr00hxitd3f1pvcm7v"},{"post_id":"ckrdmphqq00cuitd395x22jl5","category_id":"ckrdmphnt005sitd3f93f81tu","_id":"ckrdmphst00i1itd3b6g46hl9"},{"post_id":"ckrdmphn2003vitd39gtffy6a","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphsv00i5itd31jnv0mac"},{"post_id":"ckrdmphn2003vitd39gtffy6a","category_id":"ckrdmphq000ayitd3fj842bd3","_id":"ckrdmphsx00i9itd3ap7f8pn9"},{"post_id":"ckrdmphn4003yitd3fttq7xig","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphsy00iditd3b5wwa99y"},{"post_id":"ckrdmphn4003yitd3fttq7xig","category_id":"ckrdmphq000ayitd3fj842bd3","_id":"ckrdmpht000ihitd3emuo5v11"},{"post_id":"ckrdmphn50042itd3h3nuake6","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmpht100ikitd3agzr3rw4"},{"post_id":"ckrdmphn50042itd3h3nuake6","category_id":"ckrdmphqv00d8itd39jpo12sh","_id":"ckrdmpht300ipitd3bqk1h60o"},{"post_id":"ckrdmphqx00dditd33kigfdo1","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmpht400isitd379pmca1i"},{"post_id":"ckrdmphqx00dditd33kigfdo1","category_id":"ckrdmphnt005sitd3f93f81tu","_id":"ckrdmpht600ixitd31iqvhf1x"},{"post_id":"ckrdmphqy00diitd3hbuy374w","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmpht800j0itd39hu4dupl"},{"post_id":"ckrdmphqy00diitd3hbuy374w","category_id":"ckrdmphnt005sitd3f93f81tu","_id":"ckrdmphtb00j5itd3gifsa4to"},{"post_id":"ckrdmphn60046itd3bdke7eu1","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphtc00j8itd39pziai7h"},{"post_id":"ckrdmphn60046itd3bdke7eu1","category_id":"ckrdmphq000ayitd3fj842bd3","_id":"ckrdmphte00jditd3e0huchki"},{"post_id":"ckrdmphr100dqitd36jap55d6","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphtg00jgitd331ik3yi1"},{"post_id":"ckrdmphr100dqitd36jap55d6","category_id":"ckrdmphq000ayitd3fj842bd3","_id":"ckrdmphth00jkitd376dvd8lw"},{"post_id":"ckrdmphn8004aitd3fr7lcyia","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphth00jmitd3au3ubnfx"},{"post_id":"ckrdmphn8004aitd3fr7lcyia","category_id":"ckrdmphq000ayitd3fj842bd3","_id":"ckrdmphti00jpitd31nwq2drk"},{"post_id":"ckrdmphr200dtitd3g4nw5enl","category_id":"ckrdmphlv001citd32wx0c5wu","_id":"ckrdmphti00jsitd3abueh0bc"},{"post_id":"ckrdmphr200dtitd3g4nw5enl","category_id":"ckrdmphn2003uitd30zcn5mx2","_id":"ckrdmphtj00jvitd32u0cbz54"},{"post_id":"ckrdmphn9004ditd3fiar1jtk","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphtk00jyitd3eonqdehr"},{"post_id":"ckrdmphn9004ditd3fiar1jtk","category_id":"ckrdmphr300dxitd3amim1w6t","_id":"ckrdmphtl00k1itd33w5wdedw"},{"post_id":"ckrdmphr400e1itd39rik1ijg","category_id":"ckrdmphmh002pitd32f3dbd90","_id":"ckrdmphtl00k4itd30szc1qt0"},{"post_id":"ckrdmphr400e1itd39rik1ijg","category_id":"ckrdmphp0008oitd30ofe3icl","_id":"ckrdmphtm00k7itd3aqr343no"},{"post_id":"ckrdmphr600e6itd3bxdv3zf2","category_id":"ckrdmphlh000litd30oyhfkeb","_id":"ckrdmphtp00kaitd36bnda0dp"},{"post_id":"ckrdmphr600e6itd3bxdv3zf2","category_id":"ckrdmphm90024itd38jug783d","_id":"ckrdmphtq00kditd3c1clc1tk"},{"post_id":"ckrdmphna004hitd3bedr4mqf","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphtr00kgitd3g5z8hncc"},{"post_id":"ckrdmphna004hitd3bedr4mqf","category_id":"ckrdmphr300dxitd3amim1w6t","_id":"ckrdmphtt00kjitd3ebx4gyv9"},{"post_id":"ckrdmphr800e9itd3exev62k6","category_id":"ckrdmphlh000litd30oyhfkeb","_id":"ckrdmphtt00kmitd39yj2acqv"},{"post_id":"ckrdmphr800e9itd3exev62k6","category_id":"ckrdmphm90024itd38jug783d","_id":"ckrdmphtu00kpitd367d6bzzw"},{"post_id":"ckrdmphr900eeitd33cx3ev68","category_id":"ckrdmphlv001citd32wx0c5wu","_id":"ckrdmphtu00ksitd300c13z7k"},{"post_id":"ckrdmphr900eeitd33cx3ev68","category_id":"ckrdmphn2003uitd30zcn5mx2","_id":"ckrdmphtv00kvitd3892ffzck"},{"post_id":"ckrdmphnc004litd32qq03ujs","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphtw00kyitd3dstda11n"},{"post_id":"ckrdmphnc004litd32qq03ujs","category_id":"ckrdmphr300dxitd3amim1w6t","_id":"ckrdmphtx00l1itd33afo9mft"},{"post_id":"ckrdmphnd004pitd393aq1oyr","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphtx00l4itd35vob9yaa"},{"post_id":"ckrdmphnd004pitd393aq1oyr","category_id":"ckrdmphr300dxitd3amim1w6t","_id":"ckrdmphty00l7itd39iix1547"},{"post_id":"ckrdmphre00epitd3hm155ijx","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphtz00laitd37o049doe"},{"post_id":"ckrdmphre00epitd3hm155ijx","category_id":"ckrdmphmn002zitd3dmqvawqg","_id":"ckrdmphtz00lditd3dfn97xou"},{"post_id":"ckrdmphng004xitd36bg4c948","category_id":"ckrdmphl10002itd3g51r6c2s","_id":"ckrdmphu000lgitd3dvz1be0t"},{"post_id":"ckrdmphng004xitd36bg4c948","category_id":"ckrdmphrf00esitd3fcy056pv","_id":"ckrdmphu100ljitd3bxh3bk5t"},{"post_id":"ckrdmphri00f1itd37eg4bpds","category_id":"ckrdmphlv001citd32wx0c5wu","_id":"ckrdmphu100llitd3amougl4e"},{"post_id":"ckrdmphri00f1itd37eg4bpds","category_id":"ckrdmphn2003uitd30zcn5mx2","_id":"ckrdmphu200lpitd3huw21mnp"},{"post_id":"ckrdmphrk00f5itd37ydw2smx","category_id":"ckrdmphlh000litd30oyhfkeb","_id":"ckrdmphu200lritd38vrw63ru"},{"post_id":"ckrdmphrk00f5itd37ydw2smx","category_id":"ckrdmphmz003nitd382nfb1c0","_id":"ckrdmphu300lvitd3bgzrcuah"},{"post_id":"ckrdmphnj0051itd31d2m4d7p","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphu400lxitd360k98et2"},{"post_id":"ckrdmphnj0051itd31d2m4d7p","category_id":"ckrdmphrj00f2itd3984lahfj","_id":"ckrdmphu400m1itd37p7i7bc2"},{"post_id":"ckrdmphrl00f9itd3a0myhbf8","category_id":"ckrdmphlv001citd32wx0c5wu","_id":"ckrdmphu500m3itd3armjf5ey"},{"post_id":"ckrdmphrl00f9itd3a0myhbf8","category_id":"ckrdmphn2003uitd30zcn5mx2","_id":"ckrdmphu500m7itd30kzr5cbb"},{"post_id":"ckrdmphnk0055itd35hc38xe1","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphu600m9itd38h80dttb"},{"post_id":"ckrdmphnk0055itd35hc38xe1","category_id":"ckrdmphrj00f2itd3984lahfj","_id":"ckrdmphu700mditd3c3hsgd1d"},{"post_id":"ckrdmphro00fhitd3bzz6hnjp","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphu700mfitd329vc1ag3"},{"post_id":"ckrdmphro00fhitd3bzz6hnjp","category_id":"ckrdmphlq0016itd392hc3jb3","_id":"ckrdmphu800mjitd3362g8jva"},{"post_id":"ckrdmphrp00flitd39m96g0ef","category_id":"ckrdmphlv001citd32wx0c5wu","_id":"ckrdmphu800mlitd384n46inx"},{"post_id":"ckrdmphrp00flitd39m96g0ef","category_id":"ckrdmphn2003uitd30zcn5mx2","_id":"ckrdmphu900mpitd36fvn6g7l"},{"post_id":"ckrdmphnl0059itd31vybd5w8","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphua00mritd3auv5elbw"},{"post_id":"ckrdmphnl0059itd31vybd5w8","category_id":"ckrdmphrj00f2itd3984lahfj","_id":"ckrdmphua00muitd3gdcggd1r"},{"post_id":"ckrdmphrr00fpitd31n4j1ily","category_id":"ckrdmphme002gitd3c07whctp","_id":"ckrdmphub00mxitd36x9hcg03"},{"post_id":"ckrdmphrr00fpitd31n4j1ily","category_id":"ckrdmphp8009ditd34kim7a8b","_id":"ckrdmphub00mzitd31gu7c8ah"},{"post_id":"ckrdmphnn005ditd3afsc8vcx","category_id":"ckrdmphlh000litd30oyhfkeb","_id":"ckrdmphuc00n3itd3fx009g22"},{"post_id":"ckrdmphnn005ditd3afsc8vcx","category_id":"ckrdmphrs00fritd39njve9ao","_id":"ckrdmphuc00n5itd38byybh56"},{"post_id":"ckrdmphrt00fxitd3fjrte5n6","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphud00n9itd3e98xe7o1"},{"post_id":"ckrdmphrt00fxitd3fjrte5n6","category_id":"ckrdmphrj00f2itd3984lahfj","_id":"ckrdmphud00nbitd3bfgz57r8"},{"post_id":"ckrdmphrv00g1itd3hemehpk9","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphue00nfitd3aw3b110a"},{"post_id":"ckrdmphrv00g1itd3hemehpk9","category_id":"ckrdmphnt005sitd3f93f81tu","_id":"ckrdmphue00nhitd3evjv14yy"},{"post_id":"ckrdmphm4001titd3hl5i9gk8","category_id":"ckrdmphmh002pitd32f3dbd90","_id":"ckrdmphuf00nlitd3ed29g1kg"},{"post_id":"ckrdmphm4001titd3hl5i9gk8","category_id":"ckrdmphru00g0itd3ao9oaowd","_id":"ckrdmphuf00nnitd3c72u17fv"},{"post_id":"ckrdmphry00g9itd35yfq0na2","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphug00nritd3dxi61c0z"},{"post_id":"ckrdmphry00g9itd35yfq0na2","category_id":"ckrdmphnt005sitd3f93f81tu","_id":"ckrdmphug00ntitd38ye9cmt1"},{"post_id":"ckrdmphnp005gitd3g1qa5ale","category_id":"ckrdmphlh000litd30oyhfkeb","_id":"ckrdmphuh00nxitd39vsraozc"},{"post_id":"ckrdmphnp005gitd3g1qa5ale","category_id":"ckrdmphrs00fritd39njve9ao","_id":"ckrdmphuh00nzitd343q8cfff"},{"post_id":"ckrdmphrz00gditd31mog2g5u","category_id":"ckrdmphlh000litd30oyhfkeb","_id":"ckrdmphui00o3itd37u555hce"},{"post_id":"ckrdmphrz00gditd31mog2g5u","category_id":"ckrdmphm90024itd38jug783d","_id":"ckrdmphui00o5itd342b5gsfj"},{"post_id":"ckrdmphs200ghitd31f0sg29s","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphuj00o9itd34ggmckd3"},{"post_id":"ckrdmphs200ghitd31f0sg29s","category_id":"ckrdmphnt005sitd3f93f81tu","_id":"ckrdmphuk00obitd30bxfgj0d"},{"post_id":"ckrdmphnq005litd3795l8sef","category_id":"ckrdmphlh000litd30oyhfkeb","_id":"ckrdmphul00oeitd382i8gqu4"},{"post_id":"ckrdmphnq005litd3795l8sef","category_id":"ckrdmphrs00fritd39njve9ao","_id":"ckrdmphul00ogitd370kw27ml"},{"post_id":"ckrdmphs400glitd3fhjvh7d2","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphum00okitd3evmcb1br"},{"post_id":"ckrdmphs400glitd3fhjvh7d2","category_id":"ckrdmphnt005sitd3f93f81tu","_id":"ckrdmphun00omitd3b86mdxil"},{"post_id":"ckrdmphns005oitd37byaezoh","category_id":"ckrdmphlh000litd30oyhfkeb","_id":"ckrdmphun00opitd37z0k6d5i"},{"post_id":"ckrdmphns005oitd37byaezoh","category_id":"ckrdmphrs00fritd39njve9ao","_id":"ckrdmphuo00ositd3crxugu25"},{"post_id":"ckrdmphsd00gxitd3fdbk3zp7","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphuo00ouitd389nh0vwu"},{"post_id":"ckrdmphsd00gxitd3fdbk3zp7","category_id":"ckrdmphrj00f2itd3984lahfj","_id":"ckrdmphuq00oxitd3d71ddt66"},{"post_id":"ckrdmphnu005titd33ur96h8x","category_id":"ckrdmphlh000litd30oyhfkeb","_id":"ckrdmphuq00p0itd36z1u1nj1"},{"post_id":"ckrdmphnu005titd33ur96h8x","category_id":"ckrdmphsd00gwitd3dopo5tsf","_id":"ckrdmphur00p3itd36smkfewe"},{"post_id":"ckrdmphsg00h0itd33fiza3b3","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphus00p6itd3d7xsd816"},{"post_id":"ckrdmphsg00h0itd33fiza3b3","category_id":"ckrdmphrj00f2itd3984lahfj","_id":"ckrdmphut00p9itd3hhv19ui7"},{"post_id":"ckrdmphsh00h4itd308ra5l9o","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphut00pcitd3c8mz8dnp"},{"post_id":"ckrdmphsh00h4itd308ra5l9o","category_id":"ckrdmphrj00f2itd3984lahfj","_id":"ckrdmphuu00peitd3gqxx591r"},{"post_id":"ckrdmphsj00h8itd35r71hnak","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphuu00pgitd3976j2d6t"},{"post_id":"ckrdmphsj00h8itd35r71hnak","category_id":"ckrdmphrj00f2itd3984lahfj","_id":"ckrdmphuv00pjitd36cxea29t"},{"post_id":"ckrdmphnv005witd3gw0w4tlt","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphuv00plitd30mmn0zao"},{"post_id":"ckrdmphnv005witd3gw0w4tlt","category_id":"ckrdmphsi00h5itd35d4fas04","_id":"ckrdmphuw00ppitd3cxtgf3wt"},{"post_id":"ckrdmphsk00hcitd3grlx4qol","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphuw00pritd37ga8guba"},{"post_id":"ckrdmphsk00hcitd3grlx4qol","category_id":"ckrdmphrj00f2itd3984lahfj","_id":"ckrdmphuy00pvitd37tk56n77"},{"post_id":"ckrdmphsl00hgitd33d4vbfsx","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphuy00pwitd3fs337pc5"},{"post_id":"ckrdmphsl00hgitd33d4vbfsx","category_id":"ckrdmphrj00f2itd3984lahfj","_id":"ckrdmphuz00q0itd363j57bnr"},{"post_id":"ckrdmphsn00hkitd35bis77se","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphuz00q2itd338394jcp"},{"post_id":"ckrdmphsn00hkitd35bis77se","category_id":"ckrdmphnt005sitd3f93f81tu","_id":"ckrdmphv000q6itd37rm04q6l"},{"post_id":"ckrdmphso00hoitd3gt7d1hvy","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphv000q7itd305g7f4la"},{"post_id":"ckrdmphso00hoitd3gt7d1hvy","category_id":"ckrdmphmv003fitd37eq59bod","_id":"ckrdmphv100qbitd3fyzy68h1"},{"post_id":"ckrdmpho00067itd3ggst3mhl","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphv100qcitd36g3efoql"},{"post_id":"ckrdmpho00067itd3ggst3mhl","category_id":"ckrdmphsn00hnitd30w3x5ko5","_id":"ckrdmphv200qeitd39gvg9thm"},{"post_id":"ckrdmphsp00hsitd3ae9calvd","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphv300qgitd32yncdd69"},{"post_id":"ckrdmphsp00hsitd3ae9calvd","category_id":"ckrdmphrj00f2itd3984lahfj","_id":"ckrdmphv400qiitd3hqfi7tqr"},{"post_id":"ckrdmphsr00hwitd3gw5i5kpl","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphv400qlitd3d6jicbu7"},{"post_id":"ckrdmphsr00hwitd3gw5i5kpl","category_id":"ckrdmphrj00f2itd3984lahfj","_id":"ckrdmphv500qoitd3dv0jf4op"},{"post_id":"ckrdmpho5006iitd380js8v4l","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphv600qqitd31rxhg256"},{"post_id":"ckrdmpho5006iitd380js8v4l","category_id":"ckrdmphsr00hvitd3bhxl5evi","_id":"ckrdmphv600qsitd30qu14lc3"},{"post_id":"ckrdmphss00i0itd35tib9uqe","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphv700quitd30g9p6ez6"},{"post_id":"ckrdmphss00i0itd35tib9uqe","category_id":"ckrdmphrj00f2itd3984lahfj","_id":"ckrdmphv700qwitd3bww9c49y"},{"post_id":"ckrdmphsu00i3itd311pp3sk2","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphv800qzitd3av0tfg3j"},{"post_id":"ckrdmphsu00i3itd311pp3sk2","category_id":"ckrdmphrj00f2itd3984lahfj","_id":"ckrdmphv800r2itd34hk943xy"},{"post_id":"ckrdmpho6006mitd37z9ghqyv","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphv900r5itd3hz0gh5fs"},{"post_id":"ckrdmpho6006mitd37z9ghqyv","category_id":"ckrdmphrj00f2itd3984lahfj","_id":"ckrdmphv900r8itd3fh14c3xy"},{"post_id":"ckrdmphsw00i8itd34gdk87qc","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphva00raitd3cws481ob"},{"post_id":"ckrdmphsw00i8itd34gdk87qc","category_id":"ckrdmphrj00f2itd3984lahfj","_id":"ckrdmphva00rditd37b3saqsg"},{"post_id":"ckrdmphsx00ibitd3h5rk63w7","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphvb00rfitd32hm41cpb"},{"post_id":"ckrdmphsx00ibitd3h5rk63w7","category_id":"ckrdmphrj00f2itd3984lahfj","_id":"ckrdmphvb00rhitd31g0b3d5j"},{"post_id":"ckrdmphsz00igitd3hmgk9f2h","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphvb00rjitd3ezbk5flv"},{"post_id":"ckrdmphsz00igitd3hmgk9f2h","category_id":"ckrdmphrj00f2itd3984lahfj","_id":"ckrdmphvb00rmitd39a8h82hh"},{"post_id":"ckrdmpho8006qitd3dijtahm4","category_id":"ckrdmphlv001citd32wx0c5wu","_id":"ckrdmphvc00roitd3copcek8h"},{"post_id":"ckrdmpho8006qitd3dijtahm4","category_id":"ckrdmphsy00icitd371gmdqc5","_id":"ckrdmphvc00rritd3e4zbdww1"},{"post_id":"ckrdmpht100ijitd32lnygdm6","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphvd00rtitd3b8nz3bd9"},{"post_id":"ckrdmpht100ijitd32lnygdm6","category_id":"ckrdmphrj00f2itd3984lahfj","_id":"ckrdmphvd00rwitd3euqk0iwx"},{"post_id":"ckrdmpht300ioitd3auuh762o","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphve00ryitd3978icrkk"},{"post_id":"ckrdmpht300ioitd3auuh762o","category_id":"ckrdmphrj00f2itd3984lahfj","_id":"ckrdmphve00s1itd3dfgp7bqy"},{"post_id":"ckrdmphoa006yitd38w039cfv","category_id":"ckrdmphlv001citd32wx0c5wu","_id":"ckrdmphvf00s3itd3erl5exn9"},{"post_id":"ckrdmphoa006yitd38w039cfv","category_id":"ckrdmpht200ilitd3h55a0pyn","_id":"ckrdmphvf00s6itd361np7w9t"},{"post_id":"ckrdmpht400iritd3f3gk3iai","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphvf00s7itd33zrxf2it"},{"post_id":"ckrdmpht400iritd3f3gk3iai","category_id":"ckrdmphrj00f2itd3984lahfj","_id":"ckrdmphvg00s8itd354abbrsb"},{"post_id":"ckrdmpht600iwitd3hqdl8pap","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphvg00saitd3dae2bqqs"},{"post_id":"ckrdmpht600iwitd3hqdl8pap","category_id":"ckrdmphrj00f2itd3984lahfj","_id":"ckrdmphvg00sbitd3a2cn8f07"},{"post_id":"ckrdmphoc0072itd328th18wp","category_id":"ckrdmphlv001citd32wx0c5wu","_id":"ckrdmphvg00sditd3a3wnego2"},{"post_id":"ckrdmphoc0072itd328th18wp","category_id":"ckrdmphsy00icitd371gmdqc5","_id":"ckrdmphvh00sfitd34pmb1rk5"},{"post_id":"ckrdmpht700izitd3fkfc3z95","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphvh00siitd3axc71u5c"},{"post_id":"ckrdmpht700izitd3fkfc3z95","category_id":"ckrdmphrj00f2itd3984lahfj","_id":"ckrdmphvh00skitd3cva0eb4v"},{"post_id":"ckrdmphtb00j4itd39zw0drlj","category_id":"ckrdmphlh000litd30oyhfkeb","_id":"ckrdmphvi00snitd3cuspcj8u"},{"post_id":"ckrdmphtb00j4itd39zw0drlj","category_id":"ckrdmphm90024itd38jug783d","_id":"ckrdmphvi00spitd3cvpb7c3c"},{"post_id":"ckrdmphoe0077itd38ck4brmw","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphvj00ssitd3cw734tt3"},{"post_id":"ckrdmphoe0077itd38ck4brmw","category_id":"ckrdmphpw00apitd3abusfzfz","_id":"ckrdmphvj00suitd3cl2v5u8g"},{"post_id":"ckrdmphtc00j7itd3hd50e3o7","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphvk00sxitd3ee4k29vy"},{"post_id":"ckrdmphtc00j7itd3hd50e3o7","category_id":"ckrdmphmv003fitd37eq59bod","_id":"ckrdmphvk00szitd3d2fbeexr"},{"post_id":"ckrdmphte00jcitd31rao8kzf","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphvl00t1itd34gm90cn0"},{"post_id":"ckrdmphte00jcitd31rao8kzf","category_id":"ckrdmphnt005sitd3f93f81tu","_id":"ckrdmphvl00t3itd37kst3lwh"},{"post_id":"ckrdmphog007bitd32wo67mkj","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphvm00t6itd3b3lcc6v7"},{"post_id":"ckrdmphog007bitd32wo67mkj","category_id":"ckrdmphpw00apitd3abusfzfz","_id":"ckrdmphvm00t7itd3f7b08zru"},{"post_id":"ckrdmphtf00jfitd3f32j9xds","category_id":"ckrdmphme002gitd3c07whctp","_id":"ckrdmphvn00t9itd39hgqgpeo"},{"post_id":"ckrdmphtf00jfitd3f32j9xds","category_id":"ckrdmphos007zitd3av7c4e8d","_id":"ckrdmphvn00tbitd31m8hhesy"},{"post_id":"ckrdmphog007ditd3er0lajck","category_id":"ckrdmphlh000litd30oyhfkeb","_id":"ckrdmphvo00teitd33f1kfzye"},{"post_id":"ckrdmphog007ditd3er0lajck","category_id":"ckrdmphtg00jiitd37x246h6z","_id":"ckrdmphvo00tfitd316w4990l"},{"post_id":"ckrdmphoi007hitd37fyy9dpi","category_id":"ckrdmphlh000litd30oyhfkeb","_id":"ckrdmphvp00thitd3fcqr0ypd"},{"post_id":"ckrdmphoi007hitd37fyy9dpi","category_id":"ckrdmphtg00jiitd37x246h6z","_id":"ckrdmphvp00tjitd39t4s05ou"},{"post_id":"ckrdmphoj007jitd3h15vfk2p","category_id":"ckrdmphme002gitd3c07whctp","_id":"ckrdmphvq00tmitd32xmz88uv"},{"post_id":"ckrdmphoj007jitd3h15vfk2p","category_id":"ckrdmphos007zitd3av7c4e8d","_id":"ckrdmphvq00toitd3ea48bdua"},{"post_id":"ckrdmphol007nitd36uvshncz","category_id":"ckrdmphme002gitd3c07whctp","_id":"ckrdmphvq00tqitd3648n48th"},{"post_id":"ckrdmphol007nitd36uvshncz","category_id":"ckrdmphos007zitd3av7c4e8d","_id":"ckrdmphvr00tsitd37fgg9kjo"},{"post_id":"ckrdmphom007qitd39n815o1r","category_id":"ckrdmphme002gitd3c07whctp","_id":"ckrdmphvr00tuitd31793eeqj"},{"post_id":"ckrdmphom007qitd39n815o1r","category_id":"ckrdmphos007zitd3av7c4e8d","_id":"ckrdmphvs00twitd348mz24o6"},{"post_id":"ckrdmphoq007vitd3fcb00t7j","category_id":"ckrdmphme002gitd3c07whctp","_id":"ckrdmphvs00tyitd3hyzy0oyg"},{"post_id":"ckrdmphoq007vitd3fcb00t7j","category_id":"ckrdmphos007zitd3av7c4e8d","_id":"ckrdmphvs00u0itd3fmw738v8"},{"post_id":"ckrdmphos007yitd3fad6g3i0","category_id":"ckrdmphme002gitd3c07whctp","_id":"ckrdmphvs00u1itd36smx0fle"},{"post_id":"ckrdmphos007yitd3fad6g3i0","category_id":"ckrdmphos007zitd3av7c4e8d","_id":"ckrdmphvt00u4itd32d2v5t0t"},{"post_id":"ckrdmphou0086itd3cxqo7vpr","category_id":"ckrdmphme002gitd3c07whctp","_id":"ckrdmphvt00u5itd3be8i6n6q"},{"post_id":"ckrdmphou0086itd3cxqo7vpr","category_id":"ckrdmphp8009ditd34kim7a8b","_id":"ckrdmphvu00u7itd30trbe0c8"},{"post_id":"ckrdmphov008aitd37v3h5i6w","category_id":"ckrdmphme002gitd3c07whctp","_id":"ckrdmphvu00u8itd37f3qd8m5"},{"post_id":"ckrdmphov008aitd37v3h5i6w","category_id":"ckrdmphp8009ditd34kim7a8b","_id":"ckrdmphvu00ubitd3398n58h3"},{"post_id":"ckrdmphox008eitd3fxzv0xhk","category_id":"ckrdmphme002gitd3c07whctp","_id":"ckrdmphvu00uditd3eo6s1nt6"},{"post_id":"ckrdmphox008eitd3fxzv0xhk","category_id":"ckrdmphp8009ditd34kim7a8b","_id":"ckrdmphvv00ugitd3awoeckxf"},{"post_id":"ckrdmphoz008mitd39ld42mgw","category_id":"ckrdmphmh002pitd32f3dbd90","_id":"ckrdmphvv00uhitd39uke6s3n"},{"post_id":"ckrdmphoz008mitd39ld42mgw","category_id":"ckrdmphp0008oitd30ofe3icl","_id":"ckrdmphvw00ukitd3hovv1vdo"},{"post_id":"ckrdmphpa009iitd3awb0fihr","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphvw00ulitd347k2bepm"},{"post_id":"ckrdmphpa009iitd3awb0fihr","category_id":"ckrdmphqv00d8itd39jpo12sh","_id":"ckrdmphvw00unitd3bpqi43ts"},{"post_id":"ckrdmphpb009mitd308t9enft","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphvw00uoitd3anlde9ru"},{"post_id":"ckrdmphpb009mitd308t9enft","category_id":"ckrdmphqv00d8itd39jpo12sh","_id":"ckrdmphvx00uqitd30bhv99ox"},{"post_id":"ckrdmphpd009qitd3f1q74v01","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphvx00usitd31l5c4one"},{"post_id":"ckrdmphpd009qitd3f1q74v01","category_id":"ckrdmphqv00d8itd39jpo12sh","_id":"ckrdmphvy00uvitd3hsb00f21"},{"post_id":"ckrdmphpe009uitd3dadeea4d","category_id":"ckrdmphl10002itd3g51r6c2s","_id":"ckrdmphvy00uxitd3b00rhuml"},{"post_id":"ckrdmphpe009uitd3dadeea4d","category_id":"ckrdmphu300luitd3fom44yt1","_id":"ckrdmphvy00v0itd398di1hl8"},{"post_id":"ckrdmphpl00a2itd3ed0g550n","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphvz00v2itd3equo4ho3"},{"post_id":"ckrdmphpl00a2itd3ed0g550n","category_id":"ckrdmphsn00hnitd30w3x5ko5","_id":"ckrdmphvz00v4itd38r3z8ac2"},{"post_id":"ckrdmphpm00a6itd35xstgfga","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphvz00v6itd3d2eu7s3k"},{"post_id":"ckrdmphpm00a6itd35xstgfga","category_id":"ckrdmphsn00hnitd30w3x5ko5","_id":"ckrdmphw000v9itd3hjsce4u3"},{"post_id":"ckrdmphpo00aaitd3249c5t7p","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphw000vbitd3bdap5xum"},{"post_id":"ckrdmphpo00aaitd3249c5t7p","category_id":"ckrdmphsn00hnitd30w3x5ko5","_id":"ckrdmphw100veitd3bpge0bv9"},{"post_id":"ckrdmphpr00aeitd3gasa1ljm","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphw100vgitd3buhxgljt"},{"post_id":"ckrdmphpr00aeitd3gasa1ljm","category_id":"ckrdmphsn00hnitd30w3x5ko5","_id":"ckrdmphw100vjitd3dbk8fscz"},{"post_id":"ckrdmphpt00aiitd34qsj8mo8","category_id":"ckrdmphl10002itd3g51r6c2s","_id":"ckrdmphw200vlitd39nvhajf8"},{"post_id":"ckrdmphpt00aiitd34qsj8mo8","category_id":"ckrdmphu900moitd3erzqcate","_id":"ckrdmphw200vnitd369oh83vu"},{"post_id":"ckrdmphpv00amitd3hmxh1a9h","category_id":"ckrdmphl10002itd3g51r6c2s","_id":"ckrdmphw200vpitd347u11qfr"},{"post_id":"ckrdmphpv00amitd3hmxh1a9h","category_id":"ckrdmphu900moitd3erzqcate","_id":"ckrdmphw300vritd3azg7ayqg"},{"post_id":"ckrdmphpw00aqitd3f2i83yg8","category_id":"ckrdmphl10002itd3g51r6c2s","_id":"ckrdmphw300vtitd3eoiy76vb"},{"post_id":"ckrdmphpw00aqitd3f2i83yg8","category_id":"ckrdmphu900moitd3erzqcate","_id":"ckrdmphw300vvitd3286r120k"},{"post_id":"ckrdmphpz00axitd39tp849rp","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphw400vxitd3gjn69gv1"},{"post_id":"ckrdmphpz00axitd39tp849rp","category_id":"ckrdmphq000ayitd3fj842bd3","_id":"ckrdmphw500vyitd3gxa3ccvg"},{"post_id":"ckrdmphq300b5itd32mw7evcj","category_id":"ckrdmphlv001citd32wx0c5wu","_id":"ckrdmphw600w1itd330bf9zaa"},{"post_id":"ckrdmphq300b5itd32mw7evcj","category_id":"ckrdmphue00nditd37ljn7n2f","_id":"ckrdmphw600w3itd3hrgian6s"},{"post_id":"ckrdmphq500baitd32pbx58e1","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphw600w6itd3e1zw7zv0"},{"post_id":"ckrdmphq500baitd32pbx58e1","category_id":"ckrdmphuf00nkitd33qds82za","_id":"ckrdmphw700w8itd33fdc4sdx"},{"post_id":"ckrdmphqf00byitd3brmcejh4","category_id":"ckrdmphlv001citd32wx0c5wu","_id":"ckrdmphw700wbitd3cttugu2u"},{"post_id":"ckrdmphqf00byitd3brmcejh4","category_id":"ckrdmphue00nditd37ljn7n2f","_id":"ckrdmphw700wditd3dqsogra0"},{"post_id":"ckrdmphqg00c1itd3fyw2gb25","category_id":"ckrdmphlv001citd32wx0c5wu","_id":"ckrdmphw800wfitd34gj2e2g2"},{"post_id":"ckrdmphqg00c1itd3fyw2gb25","category_id":"ckrdmphue00nditd37ljn7n2f","_id":"ckrdmphw800whitd3fhzw9jt3"},{"post_id":"ckrdmphqj00c9itd34ptl7xgp","category_id":"ckrdmphlv001citd32wx0c5wu","_id":"ckrdmphw900wkitd314jpdybx"},{"post_id":"ckrdmphqj00c9itd34ptl7xgp","category_id":"ckrdmpht200ilitd3h55a0pyn","_id":"ckrdmphw900wlitd3cxdoalqo"},{"post_id":"ckrdmphql00ceitd3gqo7aulb","category_id":"ckrdmphlv001citd32wx0c5wu","_id":"ckrdmphw900wnitd3e0vcdafq"},{"post_id":"ckrdmphql00ceitd3gqo7aulb","category_id":"ckrdmpht200ilitd3h55a0pyn","_id":"ckrdmphwa00wpitd3ax2tbyp5"},{"post_id":"ckrdmphqm00chitd34ckh5jg7","category_id":"ckrdmphlh000litd30oyhfkeb","_id":"ckrdmphwa00wsitd3bx7q4ycf"},{"post_id":"ckrdmphqm00chitd34ckh5jg7","category_id":"ckrdmphuk00oditd3cp4s49a9","_id":"ckrdmphwa00wuitd3fuu0hewe"},{"post_id":"ckrdmphqs00cxitd39uop4cqi","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphwb00wwitd38rsf6rpj"},{"post_id":"ckrdmphqs00cxitd39uop4cqi","category_id":"ckrdmphul00ojitd3as045qeq","_id":"ckrdmphwb00wyitd3hmyvcu1i"},{"post_id":"ckrdmphqt00d2itd31ci8dfpp","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphwc00x0itd3fnnca2dz"},{"post_id":"ckrdmphqt00d2itd31ci8dfpp","category_id":"ckrdmphul00ojitd3as045qeq","_id":"ckrdmphwc00x2itd37zwb4aa4"},{"post_id":"ckrdmphqu00d5itd35bjqdute","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphwc00x4itd3emnh1xap"},{"post_id":"ckrdmphqu00d5itd35bjqdute","category_id":"ckrdmphrj00f2itd3984lahfj","_id":"ckrdmphwd00x6itd38v9f5arr"},{"post_id":"ckrdmphqw00daitd3fngrfz5c","category_id":"ckrdmphlv001citd32wx0c5wu","_id":"ckrdmphwd00x8itd30o6wc19u"},{"post_id":"ckrdmphqw00daitd3fngrfz5c","category_id":"ckrdmphur00p1itd36woaers4","_id":"ckrdmphwd00xaitd39kmne8ih"},{"post_id":"ckrdmphqz00dlitd34uhccx7u","category_id":"ckrdmphmh002pitd32f3dbd90","_id":"ckrdmphwe00xcitd3ad5d76aj"},{"post_id":"ckrdmphqz00dlitd34uhccx7u","category_id":"ckrdmphut00p7itd34c7bas3v","_id":"ckrdmphwe00xeitd32qtx3vf1"},{"post_id":"ckrdmphr300dyitd31ovlfg8d","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphwf00xgitd37wauhcy7"},{"post_id":"ckrdmphr300dyitd31ovlfg8d","category_id":"ckrdmphrj00f2itd3984lahfj","_id":"ckrdmphwf00xiitd32qf56nf8"},{"post_id":"ckrdmphrb00ehitd34lco69wu","category_id":"ckrdmphmh002pitd32f3dbd90","_id":"ckrdmphwg00xkitd3bc8pffny"},{"post_id":"ckrdmphrb00ehitd34lco69wu","category_id":"ckrdmphut00p7itd34c7bas3v","_id":"ckrdmphwg00xmitd3a7fs2mhm"},{"post_id":"ckrdmphrd00emitd3dh89h6a1","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphwh00xoitd36pwb70at"},{"post_id":"ckrdmphrd00emitd3dh89h6a1","category_id":"ckrdmphsi00h5itd35d4fas04","_id":"ckrdmphwh00xqitd3dubfc09x"},{"post_id":"ckrdmphrg00euitd3hpnd06p0","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphwi00xsitd3gh7w66e8"},{"post_id":"ckrdmphrg00euitd3hpnd06p0","category_id":"ckrdmphux00ptitd3g0743koe","_id":"ckrdmphwi00xuitd386ow1f68"},{"post_id":"ckrdmphrh00exitd3grfmd0y8","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphwj00xwitd34v4mcj9j"},{"post_id":"ckrdmphrh00exitd3grfmd0y8","category_id":"ckrdmphsn00hnitd30w3x5ko5","_id":"ckrdmphwj00xyitd3acb02trs"},{"post_id":"ckrdmphrm00fditd33umk7vzj","category_id":"ckrdmphlo000zitd36325ex8g","_id":"ckrdmphwk00y0itd301kc4bw5"},{"post_id":"ckrdmphrm00fditd33umk7vzj","category_id":"ckrdmphsn00hnitd30w3x5ko5","_id":"ckrdmphwk00y2itd3ey43ftg1"},{"post_id":"ckrdmphrs00ftitd3bmqcd5ml","category_id":"ckrdmphv100qaitd344ex2xwx","_id":"ckrdmphwl00y4itd34jc9an1w"},{"post_id":"ckrdmphrw00g5itd34j67e5ie","category_id":"ckrdmphmh002pitd32f3dbd90","_id":"ckrdmphwl00y6itd34hu84toj"},{"post_id":"ckrdmphrw00g5itd34j67e5ie","category_id":"ckrdmphut00p7itd34c7bas3v","_id":"ckrdmphwm00y8itd39cxl0f72"},{"post_id":"ckrdmphnx0060itd30ezr81et","category_id":"ckrdmphsl00heitd3augqh62c","_id":"ckrdmphwm00yaitd32ish97st"},{"post_id":"ckrdmphnx0060itd30ezr81et","category_id":"ckrdmphv600qtitd3bxcc4w0n","_id":"ckrdmphwm00ycitd38uflg0bt"},{"post_id":"ckrdmphs900gpitd30p833eap","category_id":"ckrdmphv100qaitd344ex2xwx","_id":"ckrdmphwn00yeitd3c9ym9yvk"},{"post_id":"ckrdmphs900gpitd30p833eap","category_id":"ckrdmphv700qyitd3gtsg4jd6","_id":"ckrdmphwn00ygitd3e5l40jqd"},{"post_id":"ckrdmphsb00gtitd33j4z7rno","category_id":"ckrdmphv100qaitd344ex2xwx","_id":"ckrdmphwo00yiitd3369k7why"},{"post_id":"ckrdmphsb00gtitd33j4z7rno","category_id":"ckrdmphv700qyitd3gtsg4jd6","_id":"ckrdmphwo00ykitd30h1kf2sl"},{"post_id":"ckrdmphx800ynitd33svn9gea","category_id":"ckrdmphl60007itd3d7k2ekpn","_id":"ckrdmphxa00yqitd3bmivc85u"},{"post_id":"ckrdmphx800ynitd33svn9gea","category_id":"ckrdmphnt005sitd3f93f81tu","_id":"ckrdmphxa00yritd31bfu1fd9"},{"post_id":"ckrdmphxi00ysitd352ys3clj","category_id":"ckrdmphlh000litd30oyhfkeb","_id":"ckrdmphxj00yuitd31bht7tgt"},{"post_id":"ckrdmphxi00ysitd352ys3clj","category_id":"ckrdmphsd00gwitd3dopo5tsf","_id":"ckrdmphxj00yvitd3cgms8j8b"}],"PostTag":[{"post_id":"ckrdmphku0000itd3cgmh1jp7","tag_id":"ckrdmphl20003itd3gr1u7wak","_id":"ckrdmphlb000eitd3e5bw5jju"},{"post_id":"ckrdmphku0000itd3cgmh1jp7","tag_id":"ckrdmphl60008itd38wnfau7m","_id":"ckrdmphld000gitd3g5bnabfi"},{"post_id":"ckrdmphkz0001itd37n6yah8m","tag_id":"ckrdmphl8000citd3gae28pbv","_id":"ckrdmphlj000oitd32bozfbvd"},{"post_id":"ckrdmphkz0001itd37n6yah8m","tag_id":"ckrdmphld000iitd37pxteeoc","_id":"ckrdmphlk000ritd3af160vnp"},{"post_id":"ckrdmphll000titd38wzg2ind","tag_id":"ckrdmphlh000mitd34uoscu8l","_id":"ckrdmphln000xitd3djcthp5r"},{"post_id":"ckrdmphl40005itd3cqqwe1j7","tag_id":"ckrdmphlh000mitd34uoscu8l","_id":"ckrdmphlp0012itd349cmfr7d"},{"post_id":"ckrdmphl40005itd3cqqwe1j7","tag_id":"ckrdmphll000uitd3dacu8e6h","_id":"ckrdmphlq0015itd35cjn5rop"},{"post_id":"ckrdmphl50006itd3f5xke0kx","tag_id":"ckrdmphlh000mitd34uoscu8l","_id":"ckrdmphm2001oitd31xracy82"},{"post_id":"ckrdmphl50006itd3f5xke0kx","tag_id":"ckrdmphl60008itd38wnfau7m","_id":"ckrdmphm3001qitd3h5n722qp"},{"post_id":"ckrdmphl50006itd3f5xke0kx","tag_id":"ckrdmphlv001bitd3dv543wos","_id":"ckrdmphm5001vitd3875o32f0"},{"post_id":"ckrdmphl50006itd3f5xke0kx","tag_id":"ckrdmphlx001gitd3b2d72izy","_id":"ckrdmphm6001yitd3bwly9a1j"},{"post_id":"ckrdmphl60009itd34zezgytj","tag_id":"ckrdmphm0001mitd34afdhypo","_id":"ckrdmphm80022itd3a4x32t69"},{"post_id":"ckrdmphl60009itd34zezgytj","tag_id":"ckrdmphlh000mitd34uoscu8l","_id":"ckrdmphm90025itd30ojgavql"},{"post_id":"ckrdmphl7000aitd30qxeg0e7","tag_id":"ckrdmphlx001gitd3b2d72izy","_id":"ckrdmphmd002citd3dzk87w5l"},{"post_id":"ckrdmphl7000aitd30qxeg0e7","tag_id":"ckrdmphlh000mitd34uoscu8l","_id":"ckrdmphme002eitd3e8j84vqg"},{"post_id":"ckrdmphl7000aitd30qxeg0e7","tag_id":"ckrdmphl60008itd38wnfau7m","_id":"ckrdmphmf002jitd3cce40xef"},{"post_id":"ckrdmphlc000fitd3dfus9zq1","tag_id":"ckrdmphlh000mitd34uoscu8l","_id":"ckrdmphmg002mitd36uzs4f4d"},{"post_id":"ckrdmphle000jitd3e70o6b63","tag_id":"ckrdmphlh000mitd34uoscu8l","_id":"ckrdmphml002uitd304bwhxe0"},{"post_id":"ckrdmphle000jitd3e70o6b63","tag_id":"ckrdmphll000uitd3dacu8e6h","_id":"ckrdmphmm002witd341euefav"},{"post_id":"ckrdmphlf000kitd35pld4lta","tag_id":"ckrdmphlx001gitd3b2d72izy","_id":"ckrdmphmp0034itd3anq39gra"},{"post_id":"ckrdmphlf000kitd35pld4lta","tag_id":"ckrdmphlh000mitd34uoscu8l","_id":"ckrdmphmr0036itd39wtrhwfd"},{"post_id":"ckrdmphlh000nitd3a7eof7cj","tag_id":"ckrdmphlh000mitd34uoscu8l","_id":"ckrdmphmv003gitd38mf8h5zm"},{"post_id":"ckrdmphlh000nitd3a7eof7cj","tag_id":"ckrdmphlx001gitd3b2d72izy","_id":"ckrdmphmw003iitd3g8upg41s"},{"post_id":"ckrdmphlj000pitd3d9jy00lz","tag_id":"ckrdmphlx001gitd3b2d72izy","_id":"ckrdmphmy003litd3fl3j00v8"},{"post_id":"ckrdmphlj000pitd3d9jy00lz","tag_id":"ckrdmphlh000mitd34uoscu8l","_id":"ckrdmphmz003oitd39yn4d2rl"},{"post_id":"ckrdmphlm000witd33buvddqm","tag_id":"ckrdmphmx003kitd30pxm6t33","_id":"ckrdmphn2003titd39a4ob698"},{"post_id":"ckrdmphln000yitd3gi9ydez7","tag_id":"ckrdmphlx001gitd3b2d72izy","_id":"ckrdmphn4003zitd3hwfob99b"},{"post_id":"ckrdmphln000yitd3gi9ydez7","tag_id":"ckrdmphlh000mitd34uoscu8l","_id":"ckrdmphn60043itd3cvopbc68"},{"post_id":"ckrdmphln000yitd3gi9ydez7","tag_id":"ckrdmphl60008itd38wnfau7m","_id":"ckrdmphn70047itd37rs5hafp"},{"post_id":"ckrdmphlo0011itd3ead94i46","tag_id":"ckrdmphlx001gitd3b2d72izy","_id":"ckrdmphna004eitd31o0q3mpp"},{"post_id":"ckrdmphlo0011itd3ead94i46","tag_id":"ckrdmphlh000mitd34uoscu8l","_id":"ckrdmphnb004iitd3azas4j4e"},{"post_id":"ckrdmphlo0011itd3ead94i46","tag_id":"ckrdmphlv001bitd3dv543wos","_id":"ckrdmphnd004mitd34gv21e8l"},{"post_id":"ckrdmphlp0013itd3cg1u86ky","tag_id":"ckrdmphlx001gitd3b2d72izy","_id":"ckrdmphne004qitd3ah5w1ke5"},{"post_id":"ckrdmphlp0013itd3cg1u86ky","tag_id":"ckrdmphlh000mitd34uoscu8l","_id":"ckrdmphnf004uitd3hmmj46qm"},{"post_id":"ckrdmphlr0018itd37rp35qhw","tag_id":"ckrdmphlx001gitd3b2d72izy","_id":"ckrdmphnh004yitd39e6s9wce"},{"post_id":"ckrdmphlr0018itd37rp35qhw","tag_id":"ckrdmphlh000mitd34uoscu8l","_id":"ckrdmphnj0052itd3cag30hy8"},{"post_id":"ckrdmphnf004titd32cqj89lz","tag_id":"ckrdmphl60008itd38wnfau7m","_id":"ckrdmphnl0056itd3aqhi27lf"},{"post_id":"ckrdmphnf004titd32cqj89lz","tag_id":"ckrdmphl20003itd3gr1u7wak","_id":"ckrdmphnm005aitd3fzb4awb5"},{"post_id":"ckrdmphnf004titd32cqj89lz","tag_id":"ckrdmphlh000mitd34uoscu8l","_id":"ckrdmphno005eitd39yz11atq"},{"post_id":"ckrdmphnf004titd32cqj89lz","tag_id":"ckrdmphlx001gitd3b2d72izy","_id":"ckrdmphnp005iitd30se4b63o"},{"post_id":"ckrdmphls001aitd3gisfgpkx","tag_id":"ckrdmphne004ritd3fiml0d0e","_id":"ckrdmphnr005mitd38ngi3iqb"},{"post_id":"ckrdmphls001aitd3gisfgpkx","tag_id":"ckrdmphni0050itd3gwpk8421","_id":"ckrdmphnt005qitd35f3286u0"},{"post_id":"ckrdmphlv001ditd37gji5252","tag_id":"ckrdmphnl0058itd3a4q57aj1","_id":"ckrdmphnv005uitd3ddvb7hvl"},{"post_id":"ckrdmphlw001eitd3d7flb8as","tag_id":"ckrdmphnl0058itd3a4q57aj1","_id":"ckrdmphnw005yitd34qep2nu4"},{"post_id":"ckrdmphly001iitd312n48w93","tag_id":"ckrdmphnl0058itd3a4q57aj1","_id":"ckrdmphny0062itd34ucq8wfh"},{"post_id":"ckrdmphlz001kitd38c4re4d8","tag_id":"ckrdmphnl0058itd3a4q57aj1","_id":"ckrdmpho00066itd3gupw3jo6"},{"post_id":"ckrdmpho00067itd3ggst3mhl","tag_id":"ckrdmphlx001gitd3b2d72izy","_id":"ckrdmpho3006ditd39f2wgtap"},{"post_id":"ckrdmphm1001nitd3hd8qfvzd","tag_id":"ckrdmphnl0058itd3a4q57aj1","_id":"ckrdmpho5006hitd3d4jiglz0"},{"post_id":"ckrdmpho2006aitd3b3uzgwwi","tag_id":"ckrdmphlx001gitd3b2d72izy","_id":"ckrdmpho6006litd3ci4lfcu8"},{"post_id":"ckrdmpho2006aitd3b3uzgwwi","tag_id":"ckrdmphnl0058itd3a4q57aj1","_id":"ckrdmpho8006pitd30zkjgrlv"},{"post_id":"ckrdmpho3006eitd36u51b1gi","tag_id":"ckrdmphlx001gitd3b2d72izy","_id":"ckrdmpho9006titd30fkud8x9"},{"post_id":"ckrdmphm2001pitd361sch2at","tag_id":"ckrdmpho2006citd379lm40z3","_id":"ckrdmphoa006xitd3emy13qfs"},{"post_id":"ckrdmphm2001pitd361sch2at","tag_id":"ckrdmpho6006kitd3aqquf963","_id":"ckrdmphob0070itd34dhtetqq"},{"post_id":"ckrdmphm4001titd3hl5i9gk8","tag_id":"ckrdmpho9006sitd3g3trd7ns","_id":"ckrdmphom007pitd3ch7x18sb"},{"post_id":"ckrdmphm4001titd3hl5i9gk8","tag_id":"ckrdmphob0071itd3d2jfb1tq","_id":"ckrdmphoq007titd3bpap958s"},{"post_id":"ckrdmphm4001titd3hl5i9gk8","tag_id":"ckrdmphm0001mitd34afdhypo","_id":"ckrdmphor007xitd3b2h4cv6u"},{"post_id":"ckrdmphm4001titd3hl5i9gk8","tag_id":"ckrdmphof007aitd34cf2hewg","_id":"ckrdmphot0081itd3etcq8ap8"},{"post_id":"ckrdmphm4001titd3hl5i9gk8","tag_id":"ckrdmphoi007gitd35pz07zhc","_id":"ckrdmphou0085itd3dx5bgk5j"},{"post_id":"ckrdmphoj007jitd3h15vfk2p","tag_id":"ckrdmphmx003kitd30pxm6t33","_id":"ckrdmphov0089itd37cjrdvp2"},{"post_id":"ckrdmphol007nitd36uvshncz","tag_id":"ckrdmphmx003kitd30pxm6t33","_id":"ckrdmphow008ditd3atr94bfv"},{"post_id":"ckrdmphom007qitd39n815o1r","tag_id":"ckrdmphmx003kitd30pxm6t33","_id":"ckrdmphoy008hitd3gfcn9fvm"},{"post_id":"ckrdmphoq007vitd3fcb00t7j","tag_id":"ckrdmphmx003kitd30pxm6t33","_id":"ckrdmphoz008litd34wrb9yt6"},{"post_id":"ckrdmphos007yitd3fad6g3i0","tag_id":"ckrdmphmx003kitd30pxm6t33","_id":"ckrdmphp0008pitd3d33ub2xw"},{"post_id":"ckrdmphm5001xitd3ex3c3irx","tag_id":"ckrdmphol007mitd30zq5fwu3","_id":"ckrdmphp1008titd3bu9tdgdu"},{"post_id":"ckrdmphm5001xitd3ex3c3irx","tag_id":"ckrdmphoq007uitd38zk0adzd","_id":"ckrdmphp3008witd32ue4gym9"},{"post_id":"ckrdmphot0082itd39xw9ddg3","tag_id":"ckrdmphmx003kitd30pxm6t33","_id":"ckrdmphp40091itd3csfi2tw0"},{"post_id":"ckrdmphou0086itd3cxqo7vpr","tag_id":"ckrdmphlv001bitd3dv543wos","_id":"ckrdmphp50094itd37rb20wtt"},{"post_id":"ckrdmphov008aitd37v3h5i6w","tag_id":"ckrdmphlv001bitd3dv543wos","_id":"ckrdmphp70099itd3aqk40vaq"},{"post_id":"ckrdmphox008eitd3fxzv0xhk","tag_id":"ckrdmphlv001bitd3dv543wos","_id":"ckrdmphp8009citd3255r7wv6"},{"post_id":"ckrdmphm70020itd365zy6lvt","tag_id":"ckrdmphol007mitd30zq5fwu3","_id":"ckrdmphpa009hitd33dnu012h"},{"post_id":"ckrdmphm70020itd365zy6lvt","tag_id":"ckrdmphoq007uitd38zk0adzd","_id":"ckrdmphpb009kitd39dpd4sgr"},{"post_id":"ckrdmphoy008iitd35d2fbgj9","tag_id":"ckrdmphm0001mitd34afdhypo","_id":"ckrdmphpd009pitd3hfji9xxo"},{"post_id":"ckrdmphp0008qitd3455i3p9h","tag_id":"ckrdmphl60008itd38wnfau7m","_id":"ckrdmphpe009sitd3845m52hk"},{"post_id":"ckrdmphp2008uitd3hxcd7o64","tag_id":"ckrdmphl60008itd38wnfau7m","_id":"ckrdmphph009xitd3ame8davc"},{"post_id":"ckrdmphm80023itd3471t8tsz","tag_id":"ckrdmphol007mitd30zq5fwu3","_id":"ckrdmphpk00a0itd352wx3lxj"},{"post_id":"ckrdmphm80023itd3471t8tsz","tag_id":"ckrdmphoq007uitd38zk0adzd","_id":"ckrdmphpm00a5itd3eoc3eo9s"},{"post_id":"ckrdmphp3008yitd357dp3lfk","tag_id":"ckrdmphl60008itd38wnfau7m","_id":"ckrdmphpn00a8itd3187y5jgn"},{"post_id":"ckrdmphp50092itd3gxuahxu5","tag_id":"ckrdmphl60008itd38wnfau7m","_id":"ckrdmphpp00aditd38yla94mg"},{"post_id":"ckrdmphp50092itd3gxuahxu5","tag_id":"ckrdmphmx003kitd30pxm6t33","_id":"ckrdmphpt00agitd34w0h2w96"},{"post_id":"ckrdmphp60096itd3flyu178b","tag_id":"ckrdmphl60008itd38wnfau7m","_id":"ckrdmphpv00alitd33sr8empg"},{"post_id":"ckrdmphp60096itd3flyu178b","tag_id":"ckrdmphlv001bitd3dv543wos","_id":"ckrdmphpw00aoitd36tbe8stp"},{"post_id":"ckrdmphp60096itd3flyu178b","tag_id":"ckrdmphld000iitd37pxteeoc","_id":"ckrdmphpy00atitd3bwdgfhn9"},{"post_id":"ckrdmphp7009aitd30zi77fga","tag_id":"ckrdmphl60008itd38wnfau7m","_id":"ckrdmphpz00awitd3g14k4w4x"},{"post_id":"ckrdmphp7009aitd30zi77fga","tag_id":"ckrdmphmx003kitd30pxm6t33","_id":"ckrdmphq100b1itd32mzz349o"},{"post_id":"ckrdmphm90026itd3b8xa6ypq","tag_id":"ckrdmphol007mitd30zq5fwu3","_id":"ckrdmphq300b4itd3aru5bjpa"},{"post_id":"ckrdmphm90026itd3b8xa6ypq","tag_id":"ckrdmphoq007uitd38zk0adzd","_id":"ckrdmphq400b9itd3ca3mecr3"},{"post_id":"ckrdmphma0028itd3ek65drxp","tag_id":"ckrdmphol007mitd30zq5fwu3","_id":"ckrdmphq500bcitd3a4tv4mm6"},{"post_id":"ckrdmphma0028itd3ek65drxp","tag_id":"ckrdmphoq007uitd38zk0adzd","_id":"ckrdmphq900bhitd36h0n4wd2"},{"post_id":"ckrdmphpj009yitd32yfpg5kv","tag_id":"ckrdmphll000uitd3dacu8e6h","_id":"ckrdmphqa00bkitd35fs3gqib"},{"post_id":"ckrdmphmb0029itd3d8ib3rmp","tag_id":"ckrdmphol007mitd30zq5fwu3","_id":"ckrdmphqc00bpitd3f8rfao2z"},{"post_id":"ckrdmphmb0029itd3d8ib3rmp","tag_id":"ckrdmphoq007uitd38zk0adzd","_id":"ckrdmphqd00bsitd3awow2von"},{"post_id":"ckrdmphmd002ditd39qf095sg","tag_id":"ckrdmphol007mitd30zq5fwu3","_id":"ckrdmphqf00bxitd3arjx6ycl"},{"post_id":"ckrdmphmd002ditd39qf095sg","tag_id":"ckrdmphoq007uitd38zk0adzd","_id":"ckrdmphqg00c0itd34zeu5j2u"},{"post_id":"ckrdmphme002fitd30pfzb3m1","tag_id":"ckrdmphol007mitd30zq5fwu3","_id":"ckrdmphqi00c5itd350lk892i"},{"post_id":"ckrdmphq200b2itd32go5eqzy","tag_id":"ckrdmphni0050itd3gwpk8421","_id":"ckrdmphqj00c8itd38otpcikt"},{"post_id":"ckrdmphq200b2itd32go5eqzy","tag_id":"ckrdmphl8000citd3gae28pbv","_id":"ckrdmphqk00cditd35dfk1foy"},{"post_id":"ckrdmphmf002kitd32jhrg8hd","tag_id":"ckrdmphol007mitd30zq5fwu3","_id":"ckrdmphqm00cgitd3eyhlgc3v"},{"post_id":"ckrdmphmf002kitd32jhrg8hd","tag_id":"ckrdmphoq007uitd38zk0adzd","_id":"ckrdmphqn00clitd3gpfm952v"},{"post_id":"ckrdmphmg002nitd30f3a2hbd","tag_id":"ckrdmphol007mitd30zq5fwu3","_id":"ckrdmphqo00coitd3gtstcvw0"},{"post_id":"ckrdmphmg002nitd30f3a2hbd","tag_id":"ckrdmphne004ritd3fiml0d0e","_id":"ckrdmphqq00ctitd32y3600oo"},{"post_id":"ckrdmphmi002qitd34obx8llc","tag_id":"ckrdmphol007mitd30zq5fwu3","_id":"ckrdmphqr00cwitd3469t0s5p"},{"post_id":"ckrdmphmi002qitd34obx8llc","tag_id":"ckrdmphoq007uitd38zk0adzd","_id":"ckrdmphqt00d1itd32fpi3ohe"},{"post_id":"ckrdmphmj002ritd3crvp62qa","tag_id":"ckrdmphol007mitd30zq5fwu3","_id":"ckrdmphqu00d4itd38tqq3mgx"},{"post_id":"ckrdmphqo00cmitd3bcfxbdjr","tag_id":"ckrdmphl8000citd3gae28pbv","_id":"ckrdmphqw00d9itd3cfqpcicx"},{"post_id":"ckrdmphqo00cmitd3bcfxbdjr","tag_id":"ckrdmphol007mitd30zq5fwu3","_id":"ckrdmphqx00dcitd3c0u6d14w"},{"post_id":"ckrdmphml002vitd3e6nc5615","tag_id":"ckrdmphol007mitd30zq5fwu3","_id":"ckrdmphqy00dgitd3aec3duz2"},{"post_id":"ckrdmphqp00cpitd349po09w5","tag_id":"ckrdmphnl0058itd3a4q57aj1","_id":"ckrdmphqz00dkitd3aacqgp83"},{"post_id":"ckrdmphqq00cuitd395x22jl5","tag_id":"ckrdmphl8000citd3gae28pbv","_id":"ckrdmphr000doitd39qlzfcq6"},{"post_id":"ckrdmphqq00cuitd395x22jl5","tag_id":"ckrdmphol007mitd30zq5fwu3","_id":"ckrdmphr200dsitd3gzdd9efk"},{"post_id":"ckrdmphmm002xitd34dt51kmg","tag_id":"ckrdmphol007mitd30zq5fwu3","_id":"ckrdmphr300dwitd3ekjy8gc0"},{"post_id":"ckrdmphmn0030itd3h1hb350l","tag_id":"ckrdmphl8000citd3gae28pbv","_id":"ckrdmphr400e0itd3696v8cba"},{"post_id":"ckrdmphmn0030itd3h1hb350l","tag_id":"ckrdmphol007mitd30zq5fwu3","_id":"ckrdmphr500e3itd38chfcx9t"},{"post_id":"ckrdmphmo0031itd3auueea6j","tag_id":"ckrdmphol007mitd30zq5fwu3","_id":"ckrdmphr700e8itd37mla7etb"},{"post_id":"ckrdmphmo0031itd3auueea6j","tag_id":"ckrdmphl8000citd3gae28pbv","_id":"ckrdmphr800ebitd3bzb08aep"},{"post_id":"ckrdmphqx00dditd33kigfdo1","tag_id":"ckrdmphol007mitd30zq5fwu3","_id":"ckrdmphrb00egitd3ge0cbanf"},{"post_id":"ckrdmphqy00diitd3hbuy374w","tag_id":"ckrdmphol007mitd30zq5fwu3","_id":"ckrdmphrc00ejitd3ay2c06a7"},{"post_id":"ckrdmphqy00diitd3hbuy374w","tag_id":"ckrdmphoq007uitd38zk0adzd","_id":"ckrdmphre00eoitd3ctqeaugr"},{"post_id":"ckrdmphmp0035itd3ahdk2dmf","tag_id":"ckrdmphl8000citd3gae28pbv","_id":"ckrdmphrf00eritd3ahd779dq"},{"post_id":"ckrdmphmp0035itd3ahdk2dmf","tag_id":"ckrdmphol007mitd30zq5fwu3","_id":"ckrdmphrh00ewitd3advk09ej"},{"post_id":"ckrdmphr200dtitd3g4nw5enl","tag_id":"ckrdmphlx001gitd3b2d72izy","_id":"ckrdmphri00ezitd3hyn07r7n"},{"post_id":"ckrdmphmr0037itd30sol64z3","tag_id":"ckrdmphr000dnitd35cuf37rj","_id":"ckrdmphrk00f4itd3fx566z9s"},{"post_id":"ckrdmphmr0037itd30sol64z3","tag_id":"ckrdmphr300dvitd3egjqdtiv","_id":"ckrdmphrk00f7itd3elmc70pa"},{"post_id":"ckrdmphr600e6itd3bxdv3zf2","tag_id":"ckrdmphm0001mitd34afdhypo","_id":"ckrdmphrm00fcitd3bjzv57sp"},{"post_id":"ckrdmphms003bitd3fdqv4ufv","tag_id":"ckrdmphl8000citd3gae28pbv","_id":"ckrdmphrn00ffitd33mubauz2"},{"post_id":"ckrdmphms003bitd3fdqv4ufv","tag_id":"ckrdmphr500e4itd3fm4hbd6j","_id":"ckrdmphrp00fkitd3glnebamn"},{"post_id":"ckrdmphr800e9itd3exev62k6","tag_id":"ckrdmphm0001mitd34afdhypo","_id":"ckrdmphrq00fnitd3ac393s9h"},{"post_id":"ckrdmphr900eeitd33cx3ev68","tag_id":"ckrdmphlh000mitd34uoscu8l","_id":"ckrdmphrs00fsitd3b8hi365b"},{"post_id":"ckrdmphr900eeitd33cx3ev68","tag_id":"ckrdmphlx001gitd3b2d72izy","_id":"ckrdmphrt00fvitd35gb93ck3"},{"post_id":"ckrdmphmu003ditd315tm3mml","tag_id":"ckrdmphl8000citd3gae28pbv","_id":"ckrdmphru00fzitd3a6nh6g0f"},{"post_id":"ckrdmphmu003ditd315tm3mml","tag_id":"ckrdmphr500e4itd3fm4hbd6j","_id":"ckrdmphrv00g3itd3gj3oe676"},{"post_id":"ckrdmphrd00emitd3dh89h6a1","tag_id":"ckrdmphl8000citd3gae28pbv","_id":"ckrdmphrx00g7itd3gux1at7r"},{"post_id":"ckrdmphrd00emitd3dh89h6a1","tag_id":"ckrdmpho6006kitd3aqquf963","_id":"ckrdmphrz00gbitd3ak8sc51t"},{"post_id":"ckrdmphmv003hitd395tecb6h","tag_id":"ckrdmphr500e4itd3fm4hbd6j","_id":"ckrdmphs100gfitd37o8zhd4b"},{"post_id":"ckrdmphre00epitd3hm155ijx","tag_id":"ckrdmphl60008itd38wnfau7m","_id":"ckrdmphs400gjitd3g8xk1pho"},{"post_id":"ckrdmphmw003jitd31vi66god","tag_id":"ckrdmphl8000citd3gae28pbv","_id":"ckrdmphs900gnitd3e1t79lgs"},{"post_id":"ckrdmphmw003jitd31vi66god","tag_id":"ckrdmphr500e4itd3fm4hbd6j","_id":"ckrdmphsa00gritd3evzr7lvh"},{"post_id":"ckrdmphrk00f5itd37ydw2smx","tag_id":"ckrdmphnl0058itd3a4q57aj1","_id":"ckrdmphsc00gvitd3hxdq91tt"},{"post_id":"ckrdmphrl00f9itd3a0myhbf8","tag_id":"ckrdmphlx001gitd3b2d72izy","_id":"ckrdmphsg00gzitd30iv88m8d"},{"post_id":"ckrdmphmy003mitd3b1yx4ylr","tag_id":"ckrdmphl8000citd3gae28pbv","_id":"ckrdmphsh00h3itd34cmp0qk0"},{"post_id":"ckrdmphmy003mitd3b1yx4ylr","tag_id":"ckrdmphr500e4itd3fm4hbd6j","_id":"ckrdmphsi00h7itd3cg053k2i"},{"post_id":"ckrdmphmy003mitd3b1yx4ylr","tag_id":"ckrdmphol007mitd30zq5fwu3","_id":"ckrdmphsk00hbitd38gkh28ky"},{"post_id":"ckrdmphro00fhitd3bzz6hnjp","tag_id":"ckrdmphll000uitd3dacu8e6h","_id":"ckrdmphsl00hfitd386mwenfi"},{"post_id":"ckrdmphro00fhitd3bzz6hnjp","tag_id":"ckrdmphm0001mitd34afdhypo","_id":"ckrdmphsm00hjitd3gm90f0bc"},{"post_id":"ckrdmphro00fhitd3bzz6hnjp","tag_id":"ckrdmpho6006kitd3aqquf963","_id":"ckrdmphsn00hmitd33sd27cn5"},{"post_id":"ckrdmphmz003pitd3gz803y3s","tag_id":"ckrdmphr500e4itd3fm4hbd6j","_id":"ckrdmphsp00hritd37jvyb3k7"},{"post_id":"ckrdmphrp00flitd39m96g0ef","tag_id":"ckrdmphlx001gitd3b2d72izy","_id":"ckrdmphsq00huitd3270cg5zg"},{"post_id":"ckrdmphrr00fpitd31n4j1ily","tag_id":"ckrdmphlv001bitd3dv543wos","_id":"ckrdmphss00hzitd38g9m1jxj"},{"post_id":"ckrdmphn1003ritd3bpc37519","tag_id":"ckrdmphr500e4itd3fm4hbd6j","_id":"ckrdmphst00i2itd3eq9v4t30"},{"post_id":"ckrdmphn2003vitd39gtffy6a","tag_id":"ckrdmphr500e4itd3fm4hbd6j","_id":"ckrdmphsw00i7itd3fyka4qgv"},{"post_id":"ckrdmphry00g9itd35yfq0na2","tag_id":"ckrdmphol007mitd30zq5fwu3","_id":"ckrdmphsx00iaitd37bav3ek2"},{"post_id":"ckrdmphry00g9itd35yfq0na2","tag_id":"ckrdmphoq007uitd38zk0adzd","_id":"ckrdmphsz00ifitd3fslvg7xq"},{"post_id":"ckrdmphrz00gditd31mog2g5u","tag_id":"ckrdmphm0001mitd34afdhypo","_id":"ckrdmpht000iiitd3hkfud9u4"},{"post_id":"ckrdmphn4003yitd3fttq7xig","tag_id":"ckrdmphr500e4itd3fm4hbd6j","_id":"ckrdmpht200initd35byt4i3p"},{"post_id":"ckrdmphn4003yitd3fttq7xig","tag_id":"ckrdmphol007mitd30zq5fwu3","_id":"ckrdmpht300iqitd3bw441e0s"},{"post_id":"ckrdmphn4003yitd3fttq7xig","tag_id":"ckrdmphl8000citd3gae28pbv","_id":"ckrdmpht500iuitd3e9b4efdl"},{"post_id":"ckrdmphs200ghitd31f0sg29s","tag_id":"ckrdmphol007mitd30zq5fwu3","_id":"ckrdmpht600iyitd36qzyabsc"},{"post_id":"ckrdmphs200ghitd31f0sg29s","tag_id":"ckrdmphoq007uitd38zk0adzd","_id":"ckrdmphta00j2itd31cj2cww4"},{"post_id":"ckrdmphn50042itd3h3nuake6","tag_id":"ckrdmphr500e4itd3fm4hbd6j","_id":"ckrdmphtb00j6itd3gx9wei1q"},{"post_id":"ckrdmphn50042itd3h3nuake6","tag_id":"ckrdmphsb00gsitd36dq69pah","_id":"ckrdmphtd00j9itd39vdwceiy"},{"post_id":"ckrdmphn50042itd3h3nuake6","tag_id":"ckrdmphl8000citd3gae28pbv","_id":"ckrdmphtf00jeitd3h6ft0hn8"},{"post_id":"ckrdmphsn00hkitd35bis77se","tag_id":"ckrdmphol007mitd30zq5fwu3","_id":"ckrdmphtg00jhitd3777j9cyg"},{"post_id":"ckrdmphsn00hkitd35bis77se","tag_id":"ckrdmphoq007uitd38zk0adzd","_id":"ckrdmphth00jlitd37cdj9do3"},{"post_id":"ckrdmphn60046itd3bdke7eu1","tag_id":"ckrdmphol007mitd30zq5fwu3","_id":"ckrdmphth00jnitd35bblah2u"},{"post_id":"ckrdmphn60046itd3bdke7eu1","tag_id":"ckrdmphr500e4itd3fm4hbd6j","_id":"ckrdmphti00jritd3ctmf4ef8"},{"post_id":"ckrdmphn60046itd3bdke7eu1","tag_id":"ckrdmphoq007uitd38zk0adzd","_id":"ckrdmphtj00jtitd33384c7n3"},{"post_id":"ckrdmphso00hoitd3gt7d1hvy","tag_id":"ckrdmphni0050itd3gwpk8421","_id":"ckrdmphtk00jxitd35wtcf70g"},{"post_id":"ckrdmphso00hoitd3gt7d1hvy","tag_id":"ckrdmphl8000citd3gae28pbv","_id":"ckrdmphtk00jzitd367pm7vez"},{"post_id":"ckrdmphn8004aitd3fr7lcyia","tag_id":"ckrdmphl8000citd3gae28pbv","_id":"ckrdmphtl00k3itd3fy3f5u85"},{"post_id":"ckrdmphn8004aitd3fr7lcyia","tag_id":"ckrdmphr500e4itd3fm4hbd6j","_id":"ckrdmphtl00k5itd3eicecnmw"},{"post_id":"ckrdmphn8004aitd3fr7lcyia","tag_id":"ckrdmphsb00gsitd36dq69pah","_id":"ckrdmphtp00k9itd375nqf6ae"},{"post_id":"ckrdmphn9004ditd3fiar1jtk","tag_id":"ckrdmphsv00i6itd37g7v3hwg","_id":"ckrdmphtp00kbitd30jimhwg9"},{"post_id":"ckrdmphn9004ditd3fiar1jtk","tag_id":"ckrdmphm0001mitd34afdhypo","_id":"ckrdmphtr00kfitd31kyq8yiv"},{"post_id":"ckrdmphna004hitd3bedr4mqf","tag_id":"ckrdmphsv00i6itd37g7v3hwg","_id":"ckrdmphts00khitd32abn40la"},{"post_id":"ckrdmphna004hitd3bedr4mqf","tag_id":"ckrdmphl8000citd3gae28pbv","_id":"ckrdmphtt00klitd373ai1951"},{"post_id":"ckrdmphnc004litd32qq03ujs","tag_id":"ckrdmphsv00i6itd37g7v3hwg","_id":"ckrdmphtt00knitd355ucca2e"},{"post_id":"ckrdmphnc004litd32qq03ujs","tag_id":"ckrdmphl60008itd38wnfau7m","_id":"ckrdmphtu00kritd38alu5omo"},{"post_id":"ckrdmphnd004pitd393aq1oyr","tag_id":"ckrdmphsv00i6itd37g7v3hwg","_id":"ckrdmphtv00ktitd39txgd4hr"},{"post_id":"ckrdmphnd004pitd393aq1oyr","tag_id":"ckrdmphlx001gitd3b2d72izy","_id":"ckrdmphtw00kxitd3e6tr4742"},{"post_id":"ckrdmphtb00j4itd39zw0drlj","tag_id":"ckrdmphm0001mitd34afdhypo","_id":"ckrdmphtw00kzitd3g0wuhdro"},{"post_id":"ckrdmphng004xitd36bg4c948","tag_id":"ckrdmphta00j1itd31dlscmob","_id":"ckrdmphtx00l3itd39ess8vj2"},{"post_id":"ckrdmphng004xitd36bg4c948","tag_id":"ckrdmphmx003kitd30pxm6t33","_id":"ckrdmphty00l5itd3hm0u7yhv"},{"post_id":"ckrdmphtc00j7itd3hd50e3o7","tag_id":"ckrdmphni0050itd3gwpk8421","_id":"ckrdmphty00l8itd3aab3fp1n"},{"post_id":"ckrdmphtc00j7itd3hd50e3o7","tag_id":"ckrdmphlv001bitd3dv543wos","_id":"ckrdmphtz00lbitd31jr4cuju"},{"post_id":"ckrdmphtc00j7itd3hd50e3o7","tag_id":"ckrdmphr500e4itd3fm4hbd6j","_id":"ckrdmphtz00leitd3bp4qb971"},{"post_id":"ckrdmphtc00j7itd3hd50e3o7","tag_id":"ckrdmphl8000citd3gae28pbv","_id":"ckrdmphu000lhitd397xn2te2"},{"post_id":"ckrdmphte00jcitd31rao8kzf","tag_id":"ckrdmphol007mitd30zq5fwu3","_id":"ckrdmphu100lkitd3682j3ul0"},{"post_id":"ckrdmphte00jcitd31rao8kzf","tag_id":"ckrdmphne004ritd3fiml0d0e","_id":"ckrdmphu200lnitd39eozekc6"},{"post_id":"ckrdmphnj0051itd31d2m4d7p","tag_id":"ckrdmphtd00jbitd34xj9aqjh","_id":"ckrdmphu200lqitd3dzzwcxl3"},{"post_id":"ckrdmphtf00jfitd3f32j9xds","tag_id":"ckrdmphmx003kitd30pxm6t33","_id":"ckrdmphu300ltitd37xsj2lql"},{"post_id":"ckrdmphnk0055itd35hc38xe1","tag_id":"ckrdmphtg00jjitd35s1jackl","_id":"ckrdmphu300lwitd387728tsa"},{"post_id":"ckrdmphnk0055itd35hc38xe1","tag_id":"ckrdmphtd00jbitd34xj9aqjh","_id":"ckrdmphu400lzitd37ub4c39q"},{"post_id":"ckrdmphnl0059itd31vybd5w8","tag_id":"ckrdmphtd00jbitd34xj9aqjh","_id":"ckrdmphu400m2itd39q5thf7b"},{"post_id":"ckrdmphnn005ditd3afsc8vcx","tag_id":"ckrdmphob0071itd3d2jfb1tq","_id":"ckrdmphu500m4itd3h75e2bul"},{"post_id":"ckrdmphnp005gitd3g1qa5ale","tag_id":"ckrdmphob0071itd3d2jfb1tq","_id":"ckrdmphu600m8itd3gj0772fa"},{"post_id":"ckrdmphnq005litd3795l8sef","tag_id":"ckrdmphob0071itd3d2jfb1tq","_id":"ckrdmphu600maitd3dkwthcbx"},{"post_id":"ckrdmphns005oitd37byaezoh","tag_id":"ckrdmphob0071itd3d2jfb1tq","_id":"ckrdmphu700meitd39lvo14sx"},{"post_id":"ckrdmphnu005titd33ur96h8x","tag_id":"ckrdmphtu00kqitd34nrqalbp","_id":"ckrdmphu700mgitd38lb1034o"},{"post_id":"ckrdmphnv005witd3gw0w4tlt","tag_id":"ckrdmphl8000citd3gae28pbv","_id":"ckrdmphu800mkitd39aej0un2"},{"post_id":"ckrdmphnv005witd3gw0w4tlt","tag_id":"ckrdmpho6006kitd3aqquf963","_id":"ckrdmphu900mmitd33gh902gf"},{"post_id":"ckrdmphnx0060itd30ezr81et","tag_id":"ckrdmphtx00l2itd311e425yg","_id":"ckrdmphua00mqitd3022g6k6c"},{"post_id":"ckrdmphnz0064itd3cbcl80j3","tag_id":"ckrdmphlx001gitd3b2d72izy","_id":"ckrdmphua00msitd30cuy6mpn"},{"post_id":"ckrdmphnz0064itd3cbcl80j3","tag_id":"ckrdmphty00l9itd3108p779z","_id":"ckrdmphua00mwitd3ad8ofzv4"},{"post_id":"ckrdmphnz0064itd3cbcl80j3","tag_id":"ckrdmphtz00lfitd35datacg9","_id":"ckrdmphub00myitd32rdah74o"},{"post_id":"ckrdmpho5006iitd380js8v4l","tag_id":"ckrdmphlx001gitd3b2d72izy","_id":"ckrdmphuc00n2itd308vk214m"},{"post_id":"ckrdmpho5006iitd380js8v4l","tag_id":"ckrdmphu100lmitd35ipv89x9","_id":"ckrdmphuc00n4itd32e2w5oy3"},{"post_id":"ckrdmpho6006mitd37z9ghqyv","tag_id":"ckrdmphtd00jbitd34xj9aqjh","_id":"ckrdmphud00n8itd33tcbc1ew"},{"post_id":"ckrdmpho6006mitd37z9ghqyv","tag_id":"ckrdmphu400lyitd3eehc53me","_id":"ckrdmphud00naitd3f4dj2n9j"},{"post_id":"ckrdmpho8006qitd3dijtahm4","tag_id":"ckrdmphu400lyitd3eehc53me","_id":"ckrdmphue00neitd3f2vhaino"},{"post_id":"ckrdmpho8006qitd3dijtahm4","tag_id":"ckrdmphu600mbitd3ai1l9s3t","_id":"ckrdmphue00ngitd37helaq4a"},{"post_id":"ckrdmpho9006uitd3h9rw247k","tag_id":"ckrdmphu400lyitd3eehc53me","_id":"ckrdmphue00njitd36l355044"},{"post_id":"ckrdmpho9006uitd3h9rw247k","tag_id":"ckrdmphnl0058itd3a4q57aj1","_id":"ckrdmphuf00nmitd398074uf1"},{"post_id":"ckrdmphoa006yitd38w039cfv","tag_id":"ckrdmphu400lyitd3eehc53me","_id":"ckrdmphuf00npitd3dvt47wdp"},{"post_id":"ckrdmphoa006yitd38w039cfv","tag_id":"ckrdmphua00mtitd3079tdw0x","_id":"ckrdmphug00nsitd3gdif41mh"},{"post_id":"ckrdmphoc0072itd328th18wp","tag_id":"ckrdmphu400lyitd3eehc53me","_id":"ckrdmphug00nvitd31lt656li"},{"post_id":"ckrdmphod0075itd34igp8gc5","tag_id":"ckrdmphol007mitd30zq5fwu3","_id":"ckrdmphuh00nyitd31m7eh6tt"},{"post_id":"ckrdmphod0075itd34igp8gc5","tag_id":"ckrdmphr500e4itd3fm4hbd6j","_id":"ckrdmphui00o1itd39yrf5nn6"},{"post_id":"ckrdmphod0075itd34igp8gc5","tag_id":"ckrdmphue00niitd3b30n781f","_id":"ckrdmphui00o4itd3en091o0q"},{"post_id":"ckrdmphoe0077itd38ck4brmw","tag_id":"ckrdmphr300dvitd3egjqdtiv","_id":"ckrdmphuj00o6itd3hrp0gvq4"},{"post_id":"ckrdmphog007bitd32wo67mkj","tag_id":"ckrdmphr300dvitd3egjqdtiv","_id":"ckrdmphuk00oaitd3huhhdr5j"},{"post_id":"ckrdmphog007ditd3er0lajck","tag_id":"ckrdmphui00o0itd346o9en2a","_id":"ckrdmphul00ofitd32obxgmja"},{"post_id":"ckrdmphog007ditd3er0lajck","tag_id":"ckrdmphtd00jbitd34xj9aqjh","_id":"ckrdmphul00ohitd3cf2r1f3v"},{"post_id":"ckrdmphoi007hitd37fyy9dpi","tag_id":"ckrdmphui00o0itd346o9en2a","_id":"ckrdmphum00olitd3ennmag2k"},{"post_id":"ckrdmphoi007hitd37fyy9dpi","tag_id":"ckrdmphne004ritd3fiml0d0e","_id":"ckrdmphun00onitd32cg83wjg"},{"post_id":"ckrdmphoz008mitd39ld42mgw","tag_id":"ckrdmphu100lmitd35ipv89x9","_id":"ckrdmphuo00oritd335s0735z"},{"post_id":"ckrdmphp9009eitd33wle7qim","tag_id":"ckrdmphol007mitd30zq5fwu3","_id":"ckrdmphup00owitd33z0k8kbw"},{"post_id":"ckrdmphp9009eitd33wle7qim","tag_id":"ckrdmphun00ooitd397c4e585","_id":"ckrdmphuq00oyitd3cd3gbuy7"},{"post_id":"ckrdmphpa009iitd3awb0fihr","tag_id":"ckrdmphl8000citd3gae28pbv","_id":"ckrdmphur00p2itd30r0i37ie"},{"post_id":"ckrdmphpa009iitd3awb0fihr","tag_id":"ckrdmphsb00gsitd36dq69pah","_id":"ckrdmphus00p4itd38f7jcinz"},{"post_id":"ckrdmphpb009mitd308t9enft","tag_id":"ckrdmphl8000citd3gae28pbv","_id":"ckrdmphut00p8itd3a29sewti"},{"post_id":"ckrdmphpb009mitd308t9enft","tag_id":"ckrdmphsb00gsitd36dq69pah","_id":"ckrdmphut00paitd38ic95911"},{"post_id":"ckrdmphpd009qitd3f1q74v01","tag_id":"ckrdmphsb00gsitd36dq69pah","_id":"ckrdmphuv00phitd38d1xccvw"},{"post_id":"ckrdmphpd009qitd3f1q74v01","tag_id":"ckrdmphr500e4itd3fm4hbd6j","_id":"ckrdmphuv00pkitd39hlo4bba"},{"post_id":"ckrdmphpd009qitd3f1q74v01","tag_id":"ckrdmphl8000citd3gae28pbv","_id":"ckrdmphuw00pnitd3gf6s9dwz"},{"post_id":"ckrdmphpe009uitd3dadeea4d","tag_id":"ckrdmphuu00pfitd3fbao7lyw","_id":"ckrdmphuw00pqitd38nlt5yky"},{"post_id":"ckrdmphpl00a2itd3ed0g550n","tag_id":"ckrdmphu600mbitd3ai1l9s3t","_id":"ckrdmphux00puitd33rtp8wk7"},{"post_id":"ckrdmphpm00a6itd35xstgfga","tag_id":"ckrdmphu600mbitd3ai1l9s3t","_id":"ckrdmphuy00pyitd355yq2mk3"},{"post_id":"ckrdmphpm00a6itd35xstgfga","tag_id":"ckrdmphlx001gitd3b2d72izy","_id":"ckrdmphuz00q1itd3e4na6w0p"},{"post_id":"ckrdmphpo00aaitd3249c5t7p","tag_id":"ckrdmphu600mbitd3ai1l9s3t","_id":"ckrdmphuz00q4itd3fllv6dde"},{"post_id":"ckrdmphpr00aeitd3gasa1ljm","tag_id":"ckrdmphu600mbitd3ai1l9s3t","_id":"ckrdmphv100q9itd3gsjxh2az"},{"post_id":"ckrdmphpt00aiitd34qsj8mo8","tag_id":"ckrdmphv000q8itd3e3b6euin","_id":"ckrdmphv400qjitd30b6q4jl4"},{"post_id":"ckrdmphpt00aiitd34qsj8mo8","tag_id":"ckrdmphtd00jbitd34xj9aqjh","_id":"ckrdmphv400qmitd3dh670fj1"},{"post_id":"ckrdmphpv00amitd3hmxh1a9h","tag_id":"ckrdmphv000q8itd3e3b6euin","_id":"ckrdmphv700qxitd3ew2q5jzg"},{"post_id":"ckrdmphpv00amitd3hmxh1a9h","tag_id":"ckrdmphu100lmitd35ipv89x9","_id":"ckrdmphv800r0itd38zkj957q"},{"post_id":"ckrdmphpv00amitd3hmxh1a9h","tag_id":"ckrdmphv600qritd3dnkphbc1","_id":"ckrdmphv800r3itd3etib2dwf"},{"post_id":"ckrdmphpw00aqitd3f2i83yg8","tag_id":"ckrdmphv000q8itd3e3b6euin","_id":"ckrdmphv900r6itd3gift583y"},{"post_id":"ckrdmphpw00aqitd3f2i83yg8","tag_id":"ckrdmphm0001mitd34afdhypo","_id":"ckrdmphva00r9itd3h41a0uoi"},{"post_id":"ckrdmphpy00auitd31m93df49","tag_id":"ckrdmphni0050itd3gwpk8421","_id":"ckrdmphva00rbitd33t5s8l1x"},{"post_id":"ckrdmphpy00auitd31m93df49","tag_id":"ckrdmphr500e4itd3fm4hbd6j","_id":"ckrdmphvb00reitd3830r4tc9"},{"post_id":"ckrdmphpz00axitd39tp849rp","tag_id":"ckrdmphr500e4itd3fm4hbd6j","_id":"ckrdmphvb00riitd31l782mbt"},{"post_id":"ckrdmphpz00axitd39tp849rp","tag_id":"ckrdmphue00niitd3b30n781f","_id":"ckrdmphvb00rkitd3bcrcfcc8"},{"post_id":"ckrdmphq300b5itd32mw7evcj","tag_id":"ckrdmphvb00rgitd315z26mr8","_id":"ckrdmphvc00rnitd34ulg8yuk"},{"post_id":"ckrdmphq300b5itd32mw7evcj","tag_id":"ckrdmphlx001gitd3b2d72izy","_id":"ckrdmphvc00rpitd3dkvxhmcp"},{"post_id":"ckrdmphq500baitd32pbx58e1","tag_id":"ckrdmphvb00rgitd315z26mr8","_id":"ckrdmphvd00rsitd38lgifm3i"},{"post_id":"ckrdmphq500baitd32pbx58e1","tag_id":"ckrdmphof007aitd34cf2hewg","_id":"ckrdmphvd00ruitd37ppohf67"},{"post_id":"ckrdmphq600bditd37w9ac3v5","tag_id":"ckrdmphvb00rgitd315z26mr8","_id":"ckrdmphve00rxitd39kxthwj8"},{"post_id":"ckrdmphq600bditd37w9ac3v5","tag_id":"ckrdmphlv001bitd3dv543wos","_id":"ckrdmphve00rzitd38wli6kby"},{"post_id":"ckrdmphq900biitd30zvw26fx","tag_id":"ckrdmphvb00rgitd315z26mr8","_id":"ckrdmphvf00s2itd3dc2y74hp"},{"post_id":"ckrdmphq900biitd30zvw26fx","tag_id":"ckrdmphnl0058itd3a4q57aj1","_id":"ckrdmphvf00s4itd3cuy154a0"},{"post_id":"ckrdmphqa00blitd3fuvr0e56","tag_id":"ckrdmphvb00rgitd315z26mr8","_id":"ckrdmphvh00seitd3e0h0guoe"},{"post_id":"ckrdmphqa00blitd3fuvr0e56","tag_id":"ckrdmphlx001gitd3b2d72izy","_id":"ckrdmphvh00sgitd37kx4fyoo"},{"post_id":"ckrdmphqa00blitd3fuvr0e56","tag_id":"ckrdmphvf00s5itd38cxk60wk","_id":"ckrdmphvh00sjitd32mo72oop"},{"post_id":"ckrdmphqa00blitd3fuvr0e56","tag_id":"ckrdmphvg00s9itd3c1ol9uar","_id":"ckrdmphvh00slitd30rdp5er2"},{"post_id":"ckrdmphqc00bqitd3evl67med","tag_id":"ckrdmphvb00rgitd315z26mr8","_id":"ckrdmphvi00soitd3fnu753a2"},{"post_id":"ckrdmphqd00btitd3fyb804n6","tag_id":"ckrdmphmx003kitd30pxm6t33","_id":"ckrdmphvi00sqitd37wru1flg"},{"post_id":"ckrdmphqd00btitd3fyb804n6","tag_id":"ckrdmphvb00rgitd315z26mr8","_id":"ckrdmphvj00stitd31d49ckwj"},{"post_id":"ckrdmphqf00byitd3brmcejh4","tag_id":"ckrdmphvb00rgitd315z26mr8","_id":"ckrdmphvj00svitd3d8ae3ebf"},{"post_id":"ckrdmphqg00c1itd3fyw2gb25","tag_id":"ckrdmphvb00rgitd315z26mr8","_id":"ckrdmphvk00syitd33u70d1op"},{"post_id":"ckrdmphqi00c6itd3dtskb4pb","tag_id":"ckrdmphvb00rgitd315z26mr8","_id":"ckrdmphvl00t2itd3cq4zb9ri"},{"post_id":"ckrdmphqi00c6itd3dtskb4pb","tag_id":"ckrdmphlx001gitd3b2d72izy","_id":"ckrdmphvl00t4itd32b1id3on"},{"post_id":"ckrdmphqj00c9itd34ptl7xgp","tag_id":"ckrdmphua00mtitd3079tdw0x","_id":"ckrdmphvn00taitd370ho4ufy"},{"post_id":"ckrdmphqj00c9itd34ptl7xgp","tag_id":"ckrdmphvb00rgitd315z26mr8","_id":"ckrdmphvn00tcitd3cmudhib8"},{"post_id":"ckrdmphql00ceitd3gqo7aulb","tag_id":"ckrdmphua00mtitd3079tdw0x","_id":"ckrdmphvp00tiitd37r934bkd"},{"post_id":"ckrdmphql00ceitd3gqo7aulb","tag_id":"ckrdmphvb00rgitd315z26mr8","_id":"ckrdmphvp00tkitd398twg9si"},{"post_id":"ckrdmphqm00chitd34ckh5jg7","tag_id":"ckrdmphvo00tgitd3h6fzc1wl","_id":"ckrdmphvq00tnitd30q82fo8l"},{"post_id":"ckrdmphqs00cxitd39uop4cqi","tag_id":"ckrdmphvp00tlitd3a43l5m8m","_id":"ckrdmphvr00tritd3ap7m6x4c"},{"post_id":"ckrdmphqt00d2itd31ci8dfpp","tag_id":"ckrdmphvp00tlitd3a43l5m8m","_id":"ckrdmphvs00tvitd35nqm93bc"},{"post_id":"ckrdmphqu00d5itd35bjqdute","tag_id":"ckrdmphtd00jbitd34xj9aqjh","_id":"ckrdmphvs00tzitd36q8ue6t0"},{"post_id":"ckrdmphqw00daitd3fngrfz5c","tag_id":"ckrdmphvs00txitd3fyat8ry1","_id":"ckrdmphvt00u3itd3dza87ay2"},{"post_id":"ckrdmphqz00dlitd34uhccx7u","tag_id":"ckrdmphr000dnitd35cuf37rj","_id":"ckrdmphvu00uaitd3h3qdajvp"},{"post_id":"ckrdmphqz00dlitd34uhccx7u","tag_id":"ckrdmphr300dvitd3egjqdtiv","_id":"ckrdmphvu00ucitd3dzybbwh0"},{"post_id":"ckrdmphr100dqitd36jap55d6","tag_id":"ckrdmphr500e4itd3fm4hbd6j","_id":"ckrdmphvv00ufitd39i4o96b6"},{"post_id":"ckrdmphr300dyitd31ovlfg8d","tag_id":"ckrdmphtd00jbitd34xj9aqjh","_id":"ckrdmphvw00ujitd32dhhebjt"},{"post_id":"ckrdmphr400e1itd39rik1ijg","tag_id":"ckrdmphu100lmitd35ipv89x9","_id":"ckrdmphvx00uritd36s22df5g"},{"post_id":"ckrdmphr400e1itd39rik1ijg","tag_id":"ckrdmphvw00umitd33wk1hvtt","_id":"ckrdmphvx00utitd34xis7bp8"},{"post_id":"ckrdmphr400e1itd39rik1ijg","tag_id":"ckrdmphmx003kitd30pxm6t33","_id":"ckrdmphvy00uwitd34a194u68"},{"post_id":"ckrdmphrb00ehitd34lco69wu","tag_id":"ckrdmphu100lmitd35ipv89x9","_id":"ckrdmphvy00uyitd3e3kk2l1u"},{"post_id":"ckrdmphrg00euitd3hpnd06p0","tag_id":"ckrdmphvx00uuitd3at9s3rsd","_id":"ckrdmphvz00v1itd3a1qs6tea"},{"post_id":"ckrdmphrh00exitd3grfmd0y8","tag_id":"ckrdmphu600mbitd3ai1l9s3t","_id":"ckrdmphvz00v5itd3efxw3lxz"},{"post_id":"ckrdmphrh00exitd3grfmd0y8","tag_id":"ckrdmphlx001gitd3b2d72izy","_id":"ckrdmphvz00v7itd3dm1p0th1"},{"post_id":"ckrdmphri00f1itd37eg4bpds","tag_id":"ckrdmphlx001gitd3b2d72izy","_id":"ckrdmphw000vaitd3abya50f6"},{"post_id":"ckrdmphri00f1itd37eg4bpds","tag_id":"ckrdmphu600mbitd3ai1l9s3t","_id":"ckrdmphw000vcitd3d4c7878z"},{"post_id":"ckrdmphrm00fditd33umk7vzj","tag_id":"ckrdmphlx001gitd3b2d72izy","_id":"ckrdmphw100vfitd3afnpc8er"},{"post_id":"ckrdmphrm00fditd33umk7vzj","tag_id":"ckrdmphu600mbitd3ai1l9s3t","_id":"ckrdmphw100vhitd38pt1e8z3"},{"post_id":"ckrdmphrs00ftitd3bmqcd5ml","tag_id":"ckrdmphw000vditd3bj796cgn","_id":"ckrdmphw200vkitd3c526gpfy"},{"post_id":"ckrdmphrt00fxitd3fjrte5n6","tag_id":"ckrdmphtd00jbitd34xj9aqjh","_id":"ckrdmphw200voitd32m9t4d33"},{"post_id":"ckrdmphrv00g1itd3hemehpk9","tag_id":"ckrdmphoq007uitd38zk0adzd","_id":"ckrdmphw300vsitd37i8j0x6o"},{"post_id":"ckrdmphrv00g1itd3hemehpk9","tag_id":"ckrdmphvp00tlitd3a43l5m8m","_id":"ckrdmphw300vuitd3d68t0jg5"},{"post_id":"ckrdmphrw00g5itd34j67e5ie","tag_id":"ckrdmphr000dnitd35cuf37rj","_id":"ckrdmphw600w0itd39il2b7w4"},{"post_id":"ckrdmphrw00g5itd34j67e5ie","tag_id":"ckrdmphw300vqitd37fq68qla","_id":"ckrdmphw600w2itd3db2l5lgw"},{"post_id":"ckrdmphrw00g5itd34j67e5ie","tag_id":"ckrdmphu100lmitd35ipv89x9","_id":"ckrdmphw600w5itd3f9h9e1b3"},{"post_id":"ckrdmphs400glitd3fhjvh7d2","tag_id":"ckrdmphol007mitd30zq5fwu3","_id":"ckrdmphw700w7itd3egnxh08i"},{"post_id":"ckrdmphs400glitd3fhjvh7d2","tag_id":"ckrdmphne004ritd3fiml0d0e","_id":"ckrdmphw700waitd355jlbazk"},{"post_id":"ckrdmphs400glitd3fhjvh7d2","tag_id":"ckrdmphui00o0itd346o9en2a","_id":"ckrdmphw700wcitd39m92fl38"},{"post_id":"ckrdmphs900gpitd30p833eap","tag_id":"ckrdmphw600w4itd3a488h6jz","_id":"ckrdmphw800wgitd3dheicbx7"},{"post_id":"ckrdmphs900gpitd30p833eap","tag_id":"ckrdmphw000vditd3bj796cgn","_id":"ckrdmphw800wiitd30zd38aw7"},{"post_id":"ckrdmphsb00gtitd33j4z7rno","tag_id":"ckrdmphw600w4itd3a488h6jz","_id":"ckrdmphw900woitd3gcirfftz"},{"post_id":"ckrdmphsb00gtitd33j4z7rno","tag_id":"ckrdmphw000vditd3bj796cgn","_id":"ckrdmphwa00wqitd36uso3cnn"},{"post_id":"ckrdmphsd00gxitd3fdbk3zp7","tag_id":"ckrdmphtd00jbitd34xj9aqjh","_id":"ckrdmphwa00wtitd30xz90ghm"},{"post_id":"ckrdmphsg00h0itd33fiza3b3","tag_id":"ckrdmphtd00jbitd34xj9aqjh","_id":"ckrdmphwb00wxitd323wc757b"},{"post_id":"ckrdmphsh00h4itd308ra5l9o","tag_id":"ckrdmphtd00jbitd34xj9aqjh","_id":"ckrdmphwc00x1itd35s981qqb"},{"post_id":"ckrdmphsj00h8itd35r71hnak","tag_id":"ckrdmphtd00jbitd34xj9aqjh","_id":"ckrdmphwc00x5itd3bwkddcf9"},{"post_id":"ckrdmphsk00hcitd3grlx4qol","tag_id":"ckrdmphtd00jbitd34xj9aqjh","_id":"ckrdmphwd00x9itd3h6vb3rse"},{"post_id":"ckrdmphsl00hgitd33d4vbfsx","tag_id":"ckrdmphtd00jbitd34xj9aqjh","_id":"ckrdmphwe00xditd37ozfa5op"},{"post_id":"ckrdmphsp00hsitd3ae9calvd","tag_id":"ckrdmphtd00jbitd34xj9aqjh","_id":"ckrdmphwf00xhitd36ykl0xt5"},{"post_id":"ckrdmphsr00hwitd3gw5i5kpl","tag_id":"ckrdmphtd00jbitd34xj9aqjh","_id":"ckrdmphwg00xlitd38rtr5xbs"},{"post_id":"ckrdmphss00i0itd35tib9uqe","tag_id":"ckrdmphtd00jbitd34xj9aqjh","_id":"ckrdmphwh00xpitd3boe84p5b"},{"post_id":"ckrdmphsu00i3itd311pp3sk2","tag_id":"ckrdmphtd00jbitd34xj9aqjh","_id":"ckrdmphwi00xtitd3c0o00ecz"},{"post_id":"ckrdmphsw00i8itd34gdk87qc","tag_id":"ckrdmphtd00jbitd34xj9aqjh","_id":"ckrdmphwj00xxitd3er6z4brv"},{"post_id":"ckrdmphsx00ibitd3h5rk63w7","tag_id":"ckrdmphtd00jbitd34xj9aqjh","_id":"ckrdmphwk00y1itd3gtkca4sh"},{"post_id":"ckrdmphsz00igitd3hmgk9f2h","tag_id":"ckrdmphtd00jbitd34xj9aqjh","_id":"ckrdmphwl00y5itd3cfc80c4j"},{"post_id":"ckrdmpht100ijitd32lnygdm6","tag_id":"ckrdmphtd00jbitd34xj9aqjh","_id":"ckrdmphwm00y9itd3ebia255k"},{"post_id":"ckrdmpht300ioitd3auuh762o","tag_id":"ckrdmphtd00jbitd34xj9aqjh","_id":"ckrdmphwm00yditd372ff8941"},{"post_id":"ckrdmpht400iritd3f3gk3iai","tag_id":"ckrdmphtd00jbitd34xj9aqjh","_id":"ckrdmphwn00yhitd3hujvghch"},{"post_id":"ckrdmpht600iwitd3hqdl8pap","tag_id":"ckrdmphtd00jbitd34xj9aqjh","_id":"ckrdmphwo00ylitd312r87saz"},{"post_id":"ckrdmpht700izitd3fkfc3z95","tag_id":"ckrdmphtd00jbitd34xj9aqjh","_id":"ckrdmphwp00ymitd316rbb7yz"},{"post_id":"ckrdmphx800ynitd33svn9gea","tag_id":"ckrdmphl8000citd3gae28pbv","_id":"ckrdmphx900yoitd32w7yawcg"},{"post_id":"ckrdmphx800ynitd33svn9gea","tag_id":"ckrdmphol007mitd30zq5fwu3","_id":"ckrdmphxa00ypitd3ewkb0ws8"},{"post_id":"ckrdmphxi00ysitd352ys3clj","tag_id":"ckrdmphtu00kqitd34nrqalbp","_id":"ckrdmphxj00ytitd301xs2ks6"}],"Tag":[{"name":"Requests","_id":"ckrdmphl20003itd3gr1u7wak"},{"name":"Python3","_id":"ckrdmphl60008itd38wnfau7m"},{"name":"大数据","_id":"ckrdmphl8000citd3gae28pbv"},{"name":"Azkaban","_id":"ckrdmphld000iitd37pxteeoc"},{"name":"CentOS","_id":"ckrdmphlh000mitd34uoscu8l"},{"name":"Rsyslog","_id":"ckrdmphll000uitd3dacu8e6h"},{"name":"MySQL","_id":"ckrdmphlv001bitd3dv543wos"},{"name":"Linux","_id":"ckrdmphlx001gitd3b2d72izy"},{"name":"Nginx","_id":"ckrdmphm0001mitd34afdhypo"},{"name":"MongoDB","_id":"ckrdmphmx003kitd30pxm6t33"},{"name":"Eclipse","_id":"ckrdmphne004ritd3fiml0d0e"},{"name":"Sqoop","_id":"ckrdmphni0050itd3gwpk8421"},{"name":"Emacs","_id":"ckrdmphnl0058itd3a4q57aj1"},{"name":"Flume","_id":"ckrdmpho2006citd379lm40z3"},{"name":"Kafka","_id":"ckrdmpho6006kitd3aqquf963"},{"name":"GitLab","_id":"ckrdmpho9006sitd3g3trd7ns"},{"name":"Jenkins","_id":"ckrdmphob0071itd3d2jfb1tq"},{"name":"Lua","_id":"ckrdmphof007aitd34cf2hewg"},{"name":"OpenResty","_id":"ckrdmphoi007gitd35pz07zhc"},{"name":"Hadoop","_id":"ckrdmphol007mitd30zq5fwu3"},{"name":"HDFS","_id":"ckrdmphoq007uitd38zk0adzd"},{"name":"Hexo","_id":"ckrdmphr000dnitd35cuf37rj"},{"name":"Markdown","_id":"ckrdmphr300dvitd3egjqdtiv"},{"name":"Hive","_id":"ckrdmphr500e4itd3fm4hbd6j"},{"name":"Ranger","_id":"ckrdmphsb00gsitd36dq69pah"},{"name":"Hue","_id":"ckrdmphsv00i6itd37g7v3hwg"},{"name":"JMeter","_id":"ckrdmphta00j1itd31dlscmob"},{"name":"Java","_id":"ckrdmphtd00jbitd34xj9aqjh"},{"name":"设计模式","_id":"ckrdmphtg00jjitd35s1jackl"},{"name":"Jetty","_id":"ckrdmphtu00kqitd34nrqalbp"},{"name":"算法","_id":"ckrdmphtx00l2itd311e425yg"},{"name":"防火墙","_id":"ckrdmphty00l9itd3108p779z"},{"name":"Iptables","_id":"ckrdmphtz00lfitd35datacg9"},{"name":"Node.js","_id":"ckrdmphu100lmitd35ipv89x9"},{"name":"Mac OS","_id":"ckrdmphu400lyitd3eehc53me"},{"name":"Shell","_id":"ckrdmphu600mbitd3ai1l9s3t"},{"name":"VirtualBox","_id":"ckrdmphua00mtitd3079tdw0x"},{"name":"ORC","_id":"ckrdmphue00niitd3b30n781f"},{"name":"Maven","_id":"ckrdmphui00o0itd346o9en2a"},{"name":"Yarn","_id":"ckrdmphun00ooitd397c4e585"},{"name":"Redis","_id":"ckrdmphuu00pfitd3fbao7lyw"},{"name":"Socket.IO","_id":"ckrdmphv000q8itd3e3b6euin"},{"name":"WebSocket","_id":"ckrdmphv600qritd3dnkphbc1"},{"name":"Ubuntu","_id":"ckrdmphvb00rgitd315z26mr8"},{"name":"Google","_id":"ckrdmphvf00s5itd38cxk60wk"},{"name":"Protocol Buggers","_id":"ckrdmphvg00s9itd3c1ol9uar"},{"name":"Visual Studio Code","_id":"ckrdmphvo00tgitd3h6fzc1wl"},{"name":"Zookeeper","_id":"ckrdmphvp00tlitd3a43l5m8m"},{"name":"DOS","_id":"ckrdmphvs00txitd3fyat8ry1"},{"name":"mongoose","_id":"ckrdmphvw00umitd33wk1hvtt"},{"name":"react native","_id":"ckrdmphvx00uuitd3at9s3rsd"},{"name":"经济学","_id":"ckrdmphw000vditd3bj796cgn"},{"name":"GitHub","_id":"ckrdmphw300vqitd37fq68qla"},{"name":"成本","_id":"ckrdmphw600w4itd3a488h6jz"}]}}